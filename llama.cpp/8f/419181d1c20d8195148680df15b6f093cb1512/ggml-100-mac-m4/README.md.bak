### ctest_debug

Runs ctest in debug mode
- status: 0
```
+ ctest --output-on-failure -L main -E test-opt
Test project /Users/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-tokenizer-0-bert-bge
 1/27 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.24 sec
      Start  2: test-tokenizer-0-command-r
 2/27 Test  #2: test-tokenizer-0-command-r ........   Passed    1.75 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/27 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.22 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/27 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.67 sec
      Start  5: test-tokenizer-0-falcon
 5/27 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.42 sec
      Start  6: test-tokenizer-0-gpt-2
 6/27 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.33 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/27 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    1.42 sec
      Start  8: test-tokenizer-0-llama-spm
 8/27 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.07 sec
      Start  9: test-tokenizer-0-mpt
 9/27 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.32 sec
      Start 10: test-tokenizer-0-phi-3
10/27 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.07 sec
      Start 11: test-tokenizer-0-qwen2
11/27 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.99 sec
      Start 12: test-tokenizer-0-refact
12/27 Test #12: test-tokenizer-0-refact ...........   Passed    0.33 sec
      Start 13: test-tokenizer-0-starcoder
13/27 Test #13: test-tokenizer-0-starcoder ........   Passed    0.32 sec
      Start 14: test-tokenizer-1-llama-spm
14/27 Test #14: test-tokenizer-1-llama-spm ........   Passed    1.02 sec
      Start 15: test-log
15/27 Test #15: test-log ..........................   Passed    0.19 sec
      Start 16: test-arg-parser
16/27 Test #16: test-arg-parser ...................   Passed    0.21 sec
      Start 17: test-quantize-fns
17/27 Test #17: test-quantize-fns .................   Passed   24.77 sec
      Start 18: test-quantize-perf
18/27 Test #18: test-quantize-perf ................   Passed    0.31 sec
      Start 19: test-sampling
19/27 Test #19: test-sampling .....................   Passed    2.13 sec
      Start 20: test-chat-template
20/27 Test #20: test-chat-template ................   Passed    0.16 sec
      Start 21: test-grammar-parser
21/27 Test #21: test-grammar-parser ...............   Passed    0.16 sec
      Start 22: test-grammar-integration
22/27 Test #22: test-grammar-integration ..........   Passed    0.22 sec
      Start 23: test-llama-grammar
23/27 Test #23: test-llama-grammar ................   Passed    0.17 sec
      Start 24: test-barrier
24/27 Test #24: test-barrier ......................   Passed    0.77 sec
      Start 25: test-backend-ops
25/27 Test #25: test-backend-ops ..................   Passed  174.65 sec
      Start 26: test-rope
26/27 Test #26: test-rope .........................   Passed    0.32 sec
      Start 29: test-json-schema-to-grammar
27/27 Test #29: test-json-schema-to-grammar .......   Passed    2.21 sec

100% tests passed, 0 tests failed out of 27

Label Time Summary:
main    = 215.47 sec*proc (27 tests)

Total Test time (real) = 215.48 sec

real	3m35.507s
user	7m29.169s
sys	0m5.512s
```

### ctest_release

Runs ctest in release mode
- status: 0
```
+ ctest --output-on-failure -L main
Test project /Users/ggml/work/llama.cpp/build-ci-release
      Start  1: test-tokenizer-0-bert-bge
 1/27 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.24 sec
      Start  2: test-tokenizer-0-command-r
 2/27 Test  #2: test-tokenizer-0-command-r ........   Passed    0.31 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/27 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.04 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/27 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.11 sec
      Start  5: test-tokenizer-0-falcon
 5/27 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.07 sec
      Start  6: test-tokenizer-0-gpt-2
 6/27 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.06 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/27 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.22 sec
      Start  8: test-tokenizer-0-llama-spm
 8/27 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.03 sec
      Start  9: test-tokenizer-0-mpt
 9/27 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.06 sec
      Start 10: test-tokenizer-0-phi-3
10/27 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.03 sec
      Start 11: test-tokenizer-0-qwen2
11/27 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.15 sec
      Start 12: test-tokenizer-0-refact
12/27 Test #12: test-tokenizer-0-refact ...........   Passed    0.06 sec
      Start 13: test-tokenizer-0-starcoder
13/27 Test #13: test-tokenizer-0-starcoder ........   Passed    0.06 sec
      Start 14: test-tokenizer-1-llama-spm
14/27 Test #14: test-tokenizer-1-llama-spm ........   Passed    0.32 sec
      Start 15: test-log
15/27 Test #15: test-log ..........................   Passed    0.23 sec
      Start 16: test-arg-parser
16/27 Test #16: test-arg-parser ...................   Passed    0.18 sec
      Start 17: test-quantize-fns
17/27 Test #17: test-quantize-fns .................   Passed   14.06 sec
      Start 18: test-quantize-perf
18/27 Test #18: test-quantize-perf ................   Passed    0.21 sec
      Start 19: test-sampling
19/27 Test #19: test-sampling .....................   Passed    0.91 sec
      Start 20: test-chat-template
20/27 Test #20: test-chat-template ................   Passed    0.17 sec
      Start 21: test-grammar-parser
21/27 Test #21: test-grammar-parser ...............   Passed    0.17 sec
      Start 22: test-grammar-integration
22/27 Test #22: test-grammar-integration ..........   Passed    0.20 sec
      Start 23: test-llama-grammar
23/27 Test #23: test-llama-grammar ................   Passed    0.17 sec
      Start 24: test-barrier
24/27 Test #24: test-barrier ......................   Passed    0.32 sec
      Start 25: test-backend-ops
25/27 Test #25: test-backend-ops ..................   Passed   28.12 sec
      Start 26: test-rope
26/27 Test #26: test-rope .........................   Passed    0.19 sec
      Start 29: test-json-schema-to-grammar
27/27 Test #29: test-json-schema-to-grammar .......   Passed    2.17 sec

100% tests passed, 0 tests failed out of 27

Label Time Summary:
main    =  49.84 sec*proc (27 tests)

Total Test time (real) =  49.86 sec

real	0m49.882s
user	1m9.836s
sys	0m4.783s
```
### embd_bge_small

BGE Small (BERT):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.135 I build: 4163 (8f419181) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.021.433 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.026.352 I llama_model_loader: loaded meta data with 25 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.026.358 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.026.361 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.026.362 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.026.363 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.026.363 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.026.364 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.026.366 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.026.366 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.026.367 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.026.367 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.026.368 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.026.371 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.026.372 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.026.372 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.026.373 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.026.374 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.026.374 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.026.375 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.031.342 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.032.559 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.032.561 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.032.562 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.032.562 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.032.563 I llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
0.00.032.563 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
0.00.032.564 I llama_model_loader: - kv  24:               general.quantization_version u32              = 2
0.00.032.564 I llama_model_loader: - type  f32:  124 tensors
0.00.032.565 I llama_model_loader: - type  f16:   73 tensors
0.00.037.010 I llm_load_vocab: special tokens cache size = 5
0.00.039.524 I llm_load_vocab: token to piece cache size = 0.2032 MB
0.00.039.528 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.039.529 I llm_load_print_meta: arch             = bert
0.00.039.529 I llm_load_print_meta: vocab type       = WPM
0.00.039.530 I llm_load_print_meta: n_vocab          = 30522
0.00.039.530 I llm_load_print_meta: n_merges         = 0
0.00.039.530 I llm_load_print_meta: vocab_only       = 0
0.00.039.531 I llm_load_print_meta: n_ctx_train      = 512
0.00.039.531 I llm_load_print_meta: n_embd           = 384
0.00.039.531 I llm_load_print_meta: n_layer          = 12
0.00.039.534 I llm_load_print_meta: n_head           = 12
0.00.039.535 I llm_load_print_meta: n_head_kv        = 12
0.00.039.536 I llm_load_print_meta: n_rot            = 32
0.00.039.536 I llm_load_print_meta: n_swa            = 0
0.00.039.536 I llm_load_print_meta: n_embd_head_k    = 32
0.00.039.536 I llm_load_print_meta: n_embd_head_v    = 32
0.00.039.537 I llm_load_print_meta: n_gqa            = 1
0.00.039.539 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.039.539 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.039.540 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.039.541 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.039.543 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.039.543 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.039.543 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.039.545 I llm_load_print_meta: n_ff             = 1536
0.00.039.545 I llm_load_print_meta: n_expert         = 0
0.00.039.545 I llm_load_print_meta: n_expert_used    = 0
0.00.039.545 I llm_load_print_meta: causal attn      = 0
0.00.039.546 I llm_load_print_meta: pooling type     = 2
0.00.039.546 I llm_load_print_meta: rope type        = 2
0.00.039.546 I llm_load_print_meta: rope scaling     = linear
0.00.039.547 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.039.547 I llm_load_print_meta: freq_scale_train = 1
0.00.039.547 I llm_load_print_meta: n_ctx_orig_yarn  = 512
0.00.039.548 I llm_load_print_meta: rope_finetuned   = unknown
0.00.039.548 I llm_load_print_meta: ssm_d_conv       = 0
0.00.039.548 I llm_load_print_meta: ssm_d_inner      = 0
0.00.039.549 I llm_load_print_meta: ssm_d_state      = 0
0.00.039.549 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.039.549 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.039.563 I llm_load_print_meta: model type       = 33M
0.00.039.564 I llm_load_print_meta: model ftype      = F16
0.00.039.564 I llm_load_print_meta: model params     = 33.21 M
0.00.039.565 I llm_load_print_meta: model size       = 63.84 MiB (16.12 BPW) 
0.00.039.565 I llm_load_print_meta: general.name     = Bge Small
0.00.039.566 I llm_load_print_meta: UNK token        = 100 '[UNK]'
0.00.039.566 I llm_load_print_meta: SEP token        = 102 '[SEP]'
0.00.039.567 I llm_load_print_meta: PAD token        = 0 '[PAD]'
0.00.039.567 I llm_load_print_meta: CLS token        = 101 '[CLS]'
0.00.039.567 I llm_load_print_meta: MASK token       = 103 '[MASK]'
0.00.039.568 I llm_load_print_meta: LF token         = 0 '[PAD]'
0.00.039.568 I llm_load_print_meta: max token length = 21
0.00.041.660 I llm_load_tensors: offloading 12 repeating layers to GPU
0.00.041.661 I llm_load_tensors: offloading output layer to GPU
0.00.041.662 I llm_load_tensors: offloaded 13/13 layers to GPU
0.00.041.687 I llm_load_tensors: Metal_Mapped model buffer size =    40.73 MiB
0.00.041.689 I llm_load_tensors:   CPU_Mapped model buffer size =    23.11 MiB
................................................
0.00.042.282 I llama_new_context_with_model: n_seq_max     = 1
0.00.042.283 I llama_new_context_with_model: n_ctx         = 512
0.00.042.283 I llama_new_context_with_model: n_ctx_per_seq = 512
0.00.042.284 I llama_new_context_with_model: n_batch       = 2048
0.00.042.284 I llama_new_context_with_model: n_ubatch      = 2048
0.00.042.284 I llama_new_context_with_model: flash_attn    = 0
0.00.042.285 I llama_new_context_with_model: freq_base     = 10000.0
0.00.042.285 I llama_new_context_with_model: freq_scale    = 1
0.00.042.286 I ggml_metal_init: allocating
0.00.042.295 I ggml_metal_init: found device: Apple M4
0.00.042.299 I ggml_metal_init: picking default device: Apple M4
0.00.043.121 I ggml_metal_init: using embedded metal library
0.00.046.639 I ggml_metal_init: GPU name:   Apple M4
0.00.046.642 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.046.642 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.046.643 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.046.643 I ggml_metal_init: simdgroup reduction   = true
0.00.046.643 I ggml_metal_init: simdgroup matrix mul. = true
0.00.046.643 I ggml_metal_init: has bfloat            = true
0.00.046.644 I ggml_metal_init: use bfloat            = true
0.00.046.644 I ggml_metal_init: hasUnifiedMemory      = true
0.00.046.645 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.058.113 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.058.115 I llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.058.116 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.059.058 I llama_new_context_with_model:      Metal compute buffer size =    16.00 MiB
0.00.059.060 I llama_new_context_with_model:        CPU compute buffer size =     2.51 MiB
0.00.059.060 I llama_new_context_with_model: graph nodes  = 429
0.00.059.060 I llama_new_context_with_model: graph splits = 2
0.00.059.085 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.066.154 I 
0.00.066.169 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 1 | LLAMAFILE = 1 | 
0.00.066.925 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.043984 -0.019895  0.007670 -0.000838  0.001372 -0.037036  0.109436  0.042589  0.092046 -0.015918  0.006773 -0.035692 -0.017889  0.015040  0.018126  0.015860 -0.011308  0.010431 -0.085223 -0.008454  0.091368 -0.017067 -0.060328 -0.024484  0.027524  0.076059  0.027984 -0.014564  0.017645 -0.033268 -0.037874 -0.019012  0.068670 -0.009845 -0.025030  0.072346 -0.046570  0.011026 -0.050252  0.047693  0.032387 -0.011761  0.022055  0.049645  0.010465  0.005795 -0.028860  0.008933 -0.018507 -0.051466 -0.046036  0.030497 -0.035418  0.054204 -0.069650  0.044238  0.029786  0.046305  0.073410 -0.042580  0.076109  0.038851 -0.181182  0.082503  0.042250 -0.064549 -0.060097 -0.017853  0.006472  0.005873  0.017196 -0.026615  0.064578  0.112612  0.035159 -0.067416  0.027073 -0.067295 -0.033493 -0.033219  0.033252  0.013523 -0.003346 -0.037473 -0.052055  0.055155 -0.001982 -0.038276  0.064447  0.028824 -0.043334 -0.029236 -0.039462  0.036315  0.008379 -0.015481 -0.036573  0.018130  0.028600  0.342831 -0.044484  0.056114  0.017624 -0.020867 -0.066808  0.000146 -0.037885 -0.030063 -0.008541 -0.021595  0.000548 -0.003219  0.004020  0.018908 -0.008548  0.025835  0.049433  0.000091  0.050920 -0.042479 -0.031884  0.023606  0.030691 -0.023148 -0.046271 -0.079278  0.115198  0.046757  0.027829 -0.040710  0.067777 -0.022955  0.010332 -0.032937 -0.018305  0.043844  0.024268  0.052413  0.007469  0.008905  0.011244 -0.074655 -0.065561 -0.026750 -0.041192 -0.023884  0.026710  0.006912  0.027741  0.052873 -0.036673  0.057707 -0.000170  0.031764 -0.019760 -0.022083  0.041053 -0.058903  0.019619  0.043144  0.043582  0.041593 -0.022527  0.027037 -0.021833  0.005430 -0.041317 -0.001239  0.024452  0.002088  0.044333 -0.022745  0.043662  0.064763  0.055430  0.037060 -0.000919  0.046111  0.045790 -0.008487  0.063057 -0.073235 -0.011935  0.032107  0.023957  0.014725 -0.033688  0.001083 -0.015827 -0.019002  0.047872  0.110842  0.028428  0.031366 -0.013288 -0.057499  0.006644  0.005140 -0.012262 -0.051443 -0.000965 -0.017642 -0.019440 -0.040937  0.009209 -0.057957  0.050961  0.052340 -0.009617 -0.040259 -0.014075 -0.024877 -0.017251  0.006302  0.006588 -0.026919  0.015600  0.030760  0.002578  0.023206 -0.022199 -0.098557 -0.051105 -0.278031 -0.014994 -0.061553 -0.027218  0.017666 -0.010939 -0.017077  0.035055  0.046994 -0.015432  0.015232 -0.025477  0.047844 -0.005949 -0.000749 -0.061008 -0.068946 -0.060377 -0.035942  0.043319 -0.055047  0.015075  0.000537 -0.058175 -0.010439  0.012643  0.151495  0.127111 -0.013622  0.041993 -0.025672  0.014026 -0.001037 -0.150469  0.044857  0.005320 -0.036274 -0.029801 -0.020192 -0.034891  0.010235  0.033548 -0.048178 -0.051795 -0.017463 -0.023492  0.047349  0.052076 -0.016783 -0.055452  0.025836 -0.005707  0.010710  0.038705  0.008191 -0.009764 -0.105779 -0.027430 -0.096101  0.025068 -0.011260  0.092355  0.056101  0.003758  0.027791  0.002092 -0.051085 -0.039890 -0.013542 -0.044956 -0.015335  0.002910 -0.043498 -0.077951  0.065217 -0.006840 -0.001630 -0.014642  0.071560  0.023714 -0.037180  0.009175  0.001562 -0.032268  0.015468  0.037880  0.000337 -0.053207  0.021321 -0.039829  0.000031  0.013400  0.019798 -0.057879  0.006473 -0.049531 -0.267858  0.039156 -0.067972  0.038248 -0.012333  0.041485 -0.016117  0.052383 -0.071363  0.011369  0.024719 -0.007232  0.082087  0.028535 -0.021504  0.040494 -0.004570 -0.074594 -0.014749  0.020037  0.002304  0.023148  0.197212 -0.043222 -0.025983 -0.004959 -0.019278  0.074258  0.001716 -0.031979 -0.036599 -0.045082  0.000547 -0.011560  0.018126 -0.029461 -0.008463  0.006419  0.050799 -0.014959  0.006176  0.026096 -0.030809  0.048055  0.114093 -0.040826 -0.011457  0.005419 -0.003599  0.025154 -0.059142  0.013753 -0.010399  0.038709  0.051449  0.035413  0.035031 -0.017027  0.026360 -0.014490 -0.050032  0.003219  0.054121  0.039723 -0.039127 

0.00.071.798 I llama_perf_context_print:        load time =      44.71 ms
0.00.071.799 I llama_perf_context_print: prompt eval time =       4.72 ms /     9 tokens (    0.52 ms per token,  1906.38 tokens per second)
0.00.071.799 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.071.800 I llama_perf_context_print:       total time =       5.64 ms /    10 tokens
0.00.071.934 I ggml_metal_free: deallocating

real	0m0.247s
user	0m0.048s
sys	0m0.031s
```
- q8_0:
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.034 I build: 4163 (8f419181) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.023 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.011.234 I llama_model_loader: loaded meta data with 25 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.011.238 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.011.239 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.011.240 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.011.240 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.011.240 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.011.241 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.011.241 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.011.242 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.011.242 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.011.243 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.011.243 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.011.245 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.011.245 I llama_model_loader: - kv  11:                          general.file_type u32              = 7
0.00.011.245 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.011.246 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.011.246 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.011.246 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.011.247 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.013.836 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.014.576 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.014.577 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.014.578 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.014.578 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.014.578 I llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
0.00.014.579 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
0.00.014.579 I llama_model_loader: - kv  24:               general.quantization_version u32              = 2
0.00.014.579 I llama_model_loader: - type  f32:  124 tensors
0.00.014.579 I llama_model_loader: - type q8_0:   73 tensors
0.00.017.211 I llm_load_vocab: special tokens cache size = 5
0.00.018.625 I llm_load_vocab: token to piece cache size = 0.2032 MB
0.00.018.627 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.018.628 I llm_load_print_meta: arch             = bert
0.00.018.628 I llm_load_print_meta: vocab type       = WPM
0.00.018.628 I llm_load_print_meta: n_vocab          = 30522
0.00.018.628 I llm_load_print_meta: n_merges         = 0
0.00.018.629 I llm_load_print_meta: vocab_only       = 0
0.00.018.629 I llm_load_print_meta: n_ctx_train      = 512
0.00.018.629 I llm_load_print_meta: n_embd           = 384
0.00.018.629 I llm_load_print_meta: n_layer          = 12
0.00.018.631 I llm_load_print_meta: n_head           = 12
0.00.018.631 I llm_load_print_meta: n_head_kv        = 12
0.00.018.631 I llm_load_print_meta: n_rot            = 32
0.00.018.632 I llm_load_print_meta: n_swa            = 0
0.00.018.632 I llm_load_print_meta: n_embd_head_k    = 32
0.00.018.632 I llm_load_print_meta: n_embd_head_v    = 32
0.00.018.632 I llm_load_print_meta: n_gqa            = 1
0.00.018.633 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.018.634 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.018.634 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.018.634 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.018.635 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.018.635 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.018.635 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.018.635 I llm_load_print_meta: n_ff             = 1536
0.00.018.636 I llm_load_print_meta: n_expert         = 0
0.00.018.637 I llm_load_print_meta: n_expert_used    = 0
0.00.018.638 I llm_load_print_meta: causal attn      = 0
0.00.018.638 I llm_load_print_meta: pooling type     = 2
0.00.018.638 I llm_load_print_meta: rope type        = 2
0.00.018.638 I llm_load_print_meta: rope scaling     = linear
0.00.018.638 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.018.639 I llm_load_print_meta: freq_scale_train = 1
0.00.018.639 I llm_load_print_meta: n_ctx_orig_yarn  = 512
0.00.018.639 I llm_load_print_meta: rope_finetuned   = unknown
0.00.018.639 I llm_load_print_meta: ssm_d_conv       = 0
0.00.018.639 I llm_load_print_meta: ssm_d_inner      = 0
0.00.018.639 I llm_load_print_meta: ssm_d_state      = 0
0.00.018.640 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.018.640 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.018.645 I llm_load_print_meta: model type       = 33M
0.00.018.645 I llm_load_print_meta: model ftype      = Q8_0
0.00.018.646 I llm_load_print_meta: model params     = 33.21 M
0.00.018.646 I llm_load_print_meta: model size       = 34.38 MiB (8.68 BPW) 
0.00.018.647 I llm_load_print_meta: general.name     = Bge Small
0.00.018.647 I llm_load_print_meta: UNK token        = 100 '[UNK]'
0.00.018.647 I llm_load_print_meta: SEP token        = 102 '[SEP]'
0.00.018.647 I llm_load_print_meta: PAD token        = 0 '[PAD]'
0.00.018.648 I llm_load_print_meta: CLS token        = 101 '[CLS]'
0.00.018.648 I llm_load_print_meta: MASK token       = 103 '[MASK]'
0.00.018.648 I llm_load_print_meta: LF token         = 0 '[PAD]'
0.00.018.648 I llm_load_print_meta: max token length = 21
0.00.019.808 I llm_load_tensors: offloading 12 repeating layers to GPU
0.00.019.809 I llm_load_tensors: offloading output layer to GPU
0.00.019.809 I llm_load_tensors: offloaded 13/13 layers to GPU
0.00.019.815 I llm_load_tensors: Metal_Mapped model buffer size =    21.75 MiB
0.00.019.816 I llm_load_tensors:   CPU_Mapped model buffer size =    12.63 MiB
................................................
0.00.020.172 I llama_new_context_with_model: n_seq_max     = 1
0.00.020.173 I llama_new_context_with_model: n_ctx         = 512
0.00.020.173 I llama_new_context_with_model: n_ctx_per_seq = 512
0.00.020.173 I llama_new_context_with_model: n_batch       = 2048
0.00.020.173 I llama_new_context_with_model: n_ubatch      = 2048
0.00.020.173 I llama_new_context_with_model: flash_attn    = 0
0.00.020.174 I llama_new_context_with_model: freq_base     = 10000.0
0.00.020.174 I llama_new_context_with_model: freq_scale    = 1
0.00.020.174 I ggml_metal_init: allocating
0.00.020.177 I ggml_metal_init: found device: Apple M4
0.00.020.179 I ggml_metal_init: picking default device: Apple M4
0.00.020.671 I ggml_metal_init: using embedded metal library
0.00.022.777 I ggml_metal_init: GPU name:   Apple M4
0.00.022.779 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.022.779 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.022.779 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.022.780 I ggml_metal_init: simdgroup reduction   = true
0.00.022.780 I ggml_metal_init: simdgroup matrix mul. = true
0.00.022.780 I ggml_metal_init: has bfloat            = true
0.00.022.780 I ggml_metal_init: use bfloat            = true
0.00.022.780 I ggml_metal_init: hasUnifiedMemory      = true
0.00.022.781 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.030.603 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.030.605 I llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.030.606 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.031.175 I llama_new_context_with_model:      Metal compute buffer size =    16.00 MiB
0.00.031.176 I llama_new_context_with_model:        CPU compute buffer size =     2.51 MiB
0.00.031.176 I llama_new_context_with_model: graph nodes  = 429
0.00.031.176 I llama_new_context_with_model: graph splits = 2
0.00.031.188 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.036.384 I 
0.00.036.398 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 1 | LLAMAFILE = 1 | 
0.00.036.949 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.044994 -0.019858  0.007986 -0.001410  0.001531 -0.036276  0.109404  0.043235  0.092836 -0.015201  0.005971 -0.036330 -0.018159  0.015015  0.017667  0.014510 -0.011792  0.011812 -0.084764 -0.008081  0.091650 -0.017638 -0.061084 -0.025066  0.026879  0.076180  0.028256 -0.014273  0.017420 -0.033553 -0.037520 -0.018280  0.068843 -0.010129 -0.024896  0.071968 -0.046406  0.011471 -0.050333  0.048931  0.032317 -0.012107  0.021812  0.049796  0.010366  0.005264 -0.028665  0.008581 -0.018508 -0.052358 -0.046410  0.029426 -0.034989  0.053289 -0.069849  0.043767  0.029291  0.046759  0.073664 -0.043090  0.075213  0.038641 -0.180496  0.081648  0.043389 -0.065615 -0.059878 -0.017387  0.006913  0.005155  0.017165 -0.026679  0.064347  0.112564  0.034871 -0.067783  0.027124 -0.066989 -0.034296 -0.033344  0.033189  0.014325 -0.004253 -0.036945 -0.052008  0.054151 -0.001715 -0.038112  0.063016  0.028668 -0.041665 -0.028879 -0.039263  0.036879  0.007718 -0.015657 -0.036270  0.018699  0.029792  0.344948 -0.043880  0.056487  0.018022 -0.021448 -0.065281  0.000253 -0.037910 -0.029663 -0.008828 -0.020643  0.001162 -0.003510  0.003091  0.018576 -0.009637  0.025316  0.049298 -0.000937  0.051568 -0.042363 -0.031550  0.023315  0.030044 -0.023267 -0.045483 -0.079644  0.114489  0.047503  0.027208 -0.041477  0.067717 -0.022845  0.010232 -0.033486 -0.017199  0.044319  0.022956  0.052194  0.008122  0.008112  0.009951 -0.074643 -0.065035 -0.025938 -0.040806 -0.024533  0.027409  0.006078  0.027693  0.052243 -0.036587  0.058682  0.001467  0.032191 -0.019964 -0.021840  0.041805 -0.058801  0.019348  0.042626  0.043999  0.040911 -0.021909  0.028025 -0.022801  0.005520 -0.041266 -0.000085  0.024325  0.001532  0.044802 -0.022888  0.043056  0.065030  0.056292  0.037666 -0.001004  0.047521  0.045624 -0.008614  0.062270 -0.073313 -0.011718  0.032708  0.023348  0.014650 -0.033924  0.001158 -0.015872 -0.018974  0.047982  0.110740  0.029177  0.030881 -0.012325 -0.057836  0.006939  0.004378 -0.011864 -0.051754 -0.003018 -0.017766 -0.020051 -0.041018  0.009118 -0.058568  0.050632  0.051795 -0.008968 -0.040668 -0.014765 -0.024320 -0.015980  0.005823  0.007147 -0.027335  0.016275  0.030734  0.002342  0.023167 -0.022169 -0.098059 -0.050901 -0.277240 -0.015100 -0.061566 -0.026219  0.017141 -0.010285 -0.017522  0.034725  0.047897 -0.016594  0.015489 -0.024132  0.048613 -0.004993 -0.000001 -0.060860 -0.068346 -0.059622 -0.036157  0.043605 -0.056353  0.014713  0.000292 -0.058627 -0.010995  0.012274  0.151597  0.127025 -0.012582  0.042765 -0.025227  0.013629 -0.000381 -0.150686  0.043974  0.004714 -0.036955 -0.029586 -0.019853 -0.034063  0.009522  0.034004 -0.048909 -0.051765 -0.017243 -0.025080  0.047588  0.050981 -0.017553 -0.056746  0.024054 -0.006285  0.011003  0.038719  0.008083 -0.008595 -0.106361 -0.027365 -0.096966  0.025294 -0.011096  0.091579  0.056180  0.004494  0.028066  0.001727 -0.051388 -0.039497 -0.013817 -0.045907 -0.015238  0.002920 -0.043938 -0.077716  0.066102 -0.006195 -0.000885 -0.013768  0.071562  0.023826 -0.036363  0.007848  0.001487 -0.032916  0.016336  0.037378  0.000625 -0.052389  0.021549 -0.039695 -0.000026  0.013496  0.020436 -0.057513  0.005900 -0.049906 -0.267546  0.038686 -0.067509  0.037450 -0.011471  0.041547 -0.015904  0.051347 -0.072313  0.012571  0.024808 -0.007420  0.082069  0.028175 -0.021956  0.040736 -0.003945 -0.074192 -0.014923  0.020417  0.002644  0.023207  0.197144 -0.043068 -0.025876 -0.004886 -0.018582  0.073936  0.001341 -0.031663 -0.037140 -0.044262  0.000049 -0.010741  0.018225 -0.028045 -0.008828  0.006047  0.050183 -0.015017  0.007037  0.025894 -0.030628  0.048075  0.112795 -0.039982 -0.011695  0.005039 -0.002800  0.025072 -0.059720  0.014182 -0.010019  0.038100  0.051088  0.034726  0.035947 -0.017035  0.027243 -0.015391 -0.051195  0.003744  0.053928  0.040014 -0.039140 

0.00.041.327 I llama_perf_context_print:        load time =      27.36 ms
0.00.041.328 I llama_perf_context_print: prompt eval time =       4.24 ms /     9 tokens (    0.47 ms per token,  2121.64 tokens per second)
0.00.041.329 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.041.329 I llama_perf_context_print:       total time =       4.94 ms /    10 tokens
0.00.041.454 I ggml_metal_free: deallocating

real	0m0.052s
user	0m0.027s
sys	0m0.013s
```
### rerank_tiny

Rerank Tiny (Jina):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/rerank-tiny/ggml-model-f16.gguf -p 'what is panda?</s></s>hi\nwhat is panda?</s></s>it'\''s a bear\nwhat is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.' -ngl 99 -c 0 --pooling rank --embd-normalize -1 --verbose-prompt
0.00.000.210 I build: 4163 (8f419181) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.023.591 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.035.069 I llama_model_loader: loaded meta data with 29 key-value pairs and 70 tensors from ../models-mnt/rerank-tiny/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.035.075 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.035.077 I llama_model_loader: - kv   0:                       general.architecture str              = jina-bert-v2
0.00.035.078 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.035.079 I llama_model_loader: - kv   2:                               general.name str              = Jina Bert Implementation
0.00.035.080 I llama_model_loader: - kv   3:                       general.organization str              = Jinaai
0.00.035.081 I llama_model_loader: - kv   4:                         general.size_label str              = 33M
0.00.035.082 I llama_model_loader: - kv   5:                   jina-bert-v2.block_count u32              = 4
0.00.035.083 I llama_model_loader: - kv   6:                jina-bert-v2.context_length u32              = 8192
0.00.035.083 I llama_model_loader: - kv   7:              jina-bert-v2.embedding_length u32              = 384
0.00.035.084 I llama_model_loader: - kv   8:           jina-bert-v2.feed_forward_length u32              = 1536
0.00.035.085 I llama_model_loader: - kv   9:          jina-bert-v2.attention.head_count u32              = 12
0.00.035.088 I llama_model_loader: - kv  10:  jina-bert-v2.attention.layer_norm_epsilon f32              = 0.000000
0.00.035.089 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.035.090 I llama_model_loader: - kv  12:              jina-bert-v2.attention.causal bool             = false
0.00.035.090 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.035.091 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = jina-v1-en
0.00.043.513 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,61056]   = ["<s>", "<pad>", "</s>", "<unk>", "<m...
0.00.045.858 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,61056]   = [3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, ...
0.00.050.691 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,39382]   = ["t h", "i n", "a n", "e r", "th e", ...
0.00.050.693 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.050.693 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
0.00.050.694 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 3
0.00.050.694 I llama_model_loader: - kv  21:          tokenizer.ggml.seperator_token_id u32              = 2
0.00.050.694 I llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 1
0.00.050.695 I llama_model_loader: - kv  23:                tokenizer.ggml.cls_token_id u32              = 0
0.00.050.695 I llama_model_loader: - kv  24:               tokenizer.ggml.mask_token_id u32              = 4
0.00.050.695 I llama_model_loader: - kv  25:            tokenizer.ggml.token_type_count u32              = 2
0.00.050.696 I llama_model_loader: - kv  26:               tokenizer.ggml.add_bos_token bool             = true
0.00.050.696 I llama_model_loader: - kv  27:               tokenizer.ggml.add_eos_token bool             = true
0.00.050.697 I llama_model_loader: - kv  28:               general.quantization_version u32              = 2
0.00.050.697 I llama_model_loader: - type  f32:   41 tensors
0.00.050.698 I llama_model_loader: - type  f16:   29 tensors
0.00.069.370 W llm_load_vocab: empty token at index 5
0.00.073.960 W llm_load_vocab: model vocab missing newline token, using special_pad_id instead
0.00.075.260 W llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
0.00.075.301 I llm_load_vocab: special tokens cache size = 5
0.00.316.836 I llm_load_vocab: token to piece cache size = 1.5060 MB
0.00.316.844 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.316.844 I llm_load_print_meta: arch             = jina-bert-v2
0.00.316.847 I llm_load_print_meta: vocab type       = BPE
0.00.316.847 I llm_load_print_meta: n_vocab          = 61056
0.00.316.847 I llm_load_print_meta: n_merges         = 39382
0.00.316.847 I llm_load_print_meta: vocab_only       = 0
0.00.316.848 I llm_load_print_meta: n_ctx_train      = 8192
0.00.316.848 I llm_load_print_meta: n_embd           = 384
0.00.316.848 I llm_load_print_meta: n_layer          = 4
0.00.316.852 I llm_load_print_meta: n_head           = 12
0.00.316.853 I llm_load_print_meta: n_head_kv        = 12
0.00.316.853 I llm_load_print_meta: n_rot            = 32
0.00.316.853 I llm_load_print_meta: n_swa            = 0
0.00.316.853 I llm_load_print_meta: n_embd_head_k    = 32
0.00.316.853 I llm_load_print_meta: n_embd_head_v    = 32
0.00.316.854 I llm_load_print_meta: n_gqa            = 1
0.00.316.856 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.316.857 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.316.857 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.316.858 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.316.858 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.316.858 I llm_load_print_meta: f_max_alibi_bias = 8.0e+00
0.00.316.859 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.316.859 I llm_load_print_meta: n_ff             = 1536
0.00.316.859 I llm_load_print_meta: n_expert         = 0
0.00.316.859 I llm_load_print_meta: n_expert_used    = 0
0.00.316.860 I llm_load_print_meta: causal attn      = 0
0.00.316.860 I llm_load_print_meta: pooling type     = -1
0.00.316.860 I llm_load_print_meta: rope type        = -1
0.00.316.860 I llm_load_print_meta: rope scaling     = linear
0.00.316.861 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.316.861 I llm_load_print_meta: freq_scale_train = 1
0.00.316.861 I llm_load_print_meta: n_ctx_orig_yarn  = 8192
0.00.316.865 I llm_load_print_meta: rope_finetuned   = unknown
0.00.316.866 I llm_load_print_meta: ssm_d_conv       = 0
0.00.316.867 I llm_load_print_meta: ssm_d_inner      = 0
0.00.316.867 I llm_load_print_meta: ssm_d_state      = 0
0.00.316.868 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.316.868 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.316.887 I llm_load_print_meta: model type       = 33M
0.00.316.887 I llm_load_print_meta: model ftype      = F16
0.00.316.888 I llm_load_print_meta: model params     = 32.90 M
0.00.316.890 I llm_load_print_meta: model size       = 62.78 MiB (16.01 BPW) 
0.00.316.890 I llm_load_print_meta: general.name     = Jina Bert Implementation
0.00.316.890 I llm_load_print_meta: BOS token        = 0 '<s>'
0.00.316.891 I llm_load_print_meta: EOS token        = 2 '</s>'
0.00.316.891 I llm_load_print_meta: UNK token        = 3 '<unk>'
0.00.316.892 I llm_load_print_meta: SEP token        = 2 '</s>'
0.00.316.893 I llm_load_print_meta: PAD token        = 1 '<pad>'
0.00.316.893 I llm_load_print_meta: CLS token        = 0 '<s>'
0.00.316.893 I llm_load_print_meta: MASK token       = 4 '<mask>'
0.00.316.893 I llm_load_print_meta: EOG token        = 2 '</s>'
0.00.316.893 I llm_load_print_meta: max token length = 45
0.00.317.705 I llm_load_tensors: offloading 4 repeating layers to GPU
0.00.317.706 I llm_load_tensors: offloading output layer to GPU
0.00.317.707 I llm_load_tensors: offloaded 5/5 layers to GPU
0.00.317.721 I llm_load_tensors: Metal_Mapped model buffer size =    18.06 MiB
0.00.317.722 I llm_load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
....................
0.00.318.349 I llama_new_context_with_model: n_seq_max     = 1
0.00.318.349 I llama_new_context_with_model: n_ctx         = 8192
0.00.318.350 I llama_new_context_with_model: n_ctx_per_seq = 8192
0.00.318.350 I llama_new_context_with_model: n_batch       = 2048
0.00.318.350 I llama_new_context_with_model: n_ubatch      = 2048
0.00.318.350 I llama_new_context_with_model: flash_attn    = 0
0.00.318.351 I llama_new_context_with_model: freq_base     = 10000.0
0.00.318.351 I llama_new_context_with_model: freq_scale    = 1
0.00.318.351 I ggml_metal_init: allocating
0.00.318.354 I ggml_metal_init: found device: Apple M4
0.00.318.356 I ggml_metal_init: picking default device: Apple M4
0.00.319.023 I ggml_metal_init: using embedded metal library
0.00.321.254 I ggml_metal_init: GPU name:   Apple M4
0.00.321.256 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.321.256 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.321.256 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.321.257 I ggml_metal_init: simdgroup reduction   = true
0.00.321.257 I ggml_metal_init: simdgroup matrix mul. = true
0.00.321.257 I ggml_metal_init: has bfloat            = true
0.00.321.257 I ggml_metal_init: use bfloat            = true
0.00.321.257 I ggml_metal_init: hasUnifiedMemory      = true
0.00.321.258 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.331.500 I llama_kv_cache_init:      Metal KV buffer size =    48.00 MiB
0.00.331.501 I llama_new_context_with_model: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB
0.00.331.502 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.331.981 I llama_new_context_with_model:      Metal compute buffer size =   220.01 MiB
0.00.331.982 I llama_new_context_with_model:        CPU compute buffer size =    22.02 MiB
0.00.331.982 I llama_new_context_with_model: graph nodes  = 154
0.00.331.982 I llama_new_context_with_model: graph splits = 2
0.00.331.994 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.341.372 I 
0.00.341.392 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 1 | LLAMAFILE = 1 | 
0.00.341.534 I main: prompt 0: 'what is panda?</s></s>hi'
0.00.341.535 I main: number of tokens in prompt = 9
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 23233 -> 'hi'
     2 -> '</s>'


0.00.341.538 I main: prompt 1: 'what is panda?</s></s>it's a bear'
0.00.341.538 I main: number of tokens in prompt = 13
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21153 -> 'it'
    37 -> '''
    87 -> 's'
    69 -> 'a'
 25706 -> 'bear'
     2 -> '</s>'


0.00.341.545 I main: prompt 2: 'what is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'
0.00.341.547 I main: number of tokens in prompt = 40
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21215 -> 'he'
 28390 -> 'giant'
 49990 -> 'panda'
    38 -> '('
 21163 -> 'il'
 26237 -> 'uro'
 21223 -> 'po'
 23179 -> 'da'
 36906 -> 'melan'
 26791 -> 'ole'
    89 -> 'u'
 21402 -> 'ca'
 21686 -> '),'
 23314 -> 'sometimes'
 22517 -> 'called'
    69 -> 'a'
 49990 -> 'panda'
 25706 -> 'bear'
 21142 -> 'or'
 22810 -> 'simply'
 49990 -> 'panda'
    42 -> ','
 21152 -> 'is'
    69 -> 'a'
 25706 -> 'bear'
 25677 -> 'species'
 28930 -> 'ende'
 22024 -> 'mic'
 21148 -> 'to'
    76 -> 'h'
 22344 -> 'ina'
    44 -> '.'
     2 -> '</s>'


0.00.342.038 I batch_decode: n_tokens = 62, n_seq = 3

rerank score 0:    0.023
rerank score 1:    0.024
rerank score 2:    0.199

0.00.345.593 I llama_perf_context_print:        load time =     317.77 ms
0.00.345.594 I llama_perf_context_print: prompt eval time =       3.54 ms /    62 tokens (    0.06 ms per token, 17494.36 tokens per second)
0.00.345.596 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.345.597 I llama_perf_context_print:       total time =       4.22 ms /    63 tokens
0.00.345.810 I ggml_metal_free: deallocating

real	0m1.058s
user	0m0.320s
sys	0m0.038s
  - rerank score 0 @ 0.023 OK
  - rerank score 1 @ 0.024 OK
  - rerank score 2 @ 0.199 OK
```
### pythia_1_4b

Pythia 1.4B:
- status: 0
- perplexity:
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
- imatrix:
```
Final estimate: PPL = 10.1498 +/- 3.22650
```
- f16: 
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.127 I build: 4163 (8f419181) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.271 I main: llama backend init
0.00.000.279 I main: load the model and apply lora adapter, if any
0.00.071.482 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.082.655 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.082.667 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.082.671 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.082.671 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.082.672 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.082.672 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.082.673 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.082.675 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.082.675 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.082.676 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.082.677 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.082.677 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.082.678 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.082.679 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.082.684 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.082.685 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.082.685 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.089.825 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.092.079 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.099.144 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.099.151 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.099.151 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.099.152 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.099.153 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.099.154 I llama_model_loader: - type  f32:  194 tensors
0.00.099.155 I llama_model_loader: - type  f16:   98 tensors
0.00.137.343 I llm_load_vocab: special tokens cache size = 25
0.00.145.093 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.145.096 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.145.097 I llm_load_print_meta: arch             = gptneox
0.00.145.097 I llm_load_print_meta: vocab type       = BPE
0.00.145.097 I llm_load_print_meta: n_vocab          = 50304
0.00.145.098 I llm_load_print_meta: n_merges         = 50009
0.00.145.098 I llm_load_print_meta: vocab_only       = 0
0.00.145.098 I llm_load_print_meta: n_ctx_train      = 2048
0.00.145.098 I llm_load_print_meta: n_embd           = 2048
0.00.145.100 I llm_load_print_meta: n_layer          = 24
0.00.145.104 I llm_load_print_meta: n_head           = 16
0.00.145.104 I llm_load_print_meta: n_head_kv        = 16
0.00.145.105 I llm_load_print_meta: n_rot            = 32
0.00.145.105 I llm_load_print_meta: n_swa            = 0
0.00.145.105 I llm_load_print_meta: n_embd_head_k    = 128
0.00.145.105 I llm_load_print_meta: n_embd_head_v    = 128
0.00.145.106 I llm_load_print_meta: n_gqa            = 1
0.00.145.107 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.145.107 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.145.108 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.145.108 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.145.108 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.145.109 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.145.109 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.145.109 I llm_load_print_meta: n_ff             = 8192
0.00.145.110 I llm_load_print_meta: n_expert         = 0
0.00.145.110 I llm_load_print_meta: n_expert_used    = 0
0.00.145.110 I llm_load_print_meta: causal attn      = 1
0.00.145.112 I llm_load_print_meta: pooling type     = 0
0.00.145.112 I llm_load_print_meta: rope type        = 2
0.00.145.112 I llm_load_print_meta: rope scaling     = linear
0.00.145.113 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.145.113 I llm_load_print_meta: freq_scale_train = 1
0.00.145.113 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.145.113 I llm_load_print_meta: rope_finetuned   = unknown
0.00.145.113 I llm_load_print_meta: ssm_d_conv       = 0
0.00.145.114 I llm_load_print_meta: ssm_d_inner      = 0
0.00.145.114 I llm_load_print_meta: ssm_d_state      = 0
0.00.145.114 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.145.114 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.145.126 I llm_load_print_meta: model type       = 1.4B
0.00.145.126 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.145.127 I llm_load_print_meta: model params     = 1.41 B
0.00.145.127 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.145.127 I llm_load_print_meta: general.name     = 1.4B
0.00.145.128 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.145.128 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.145.128 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.145.128 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.145.128 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.145.129 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.145.129 I llm_load_print_meta: max token length = 1024
0.00.147.850 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.147.850 I llm_load_tensors: offloading output layer to GPU
0.00.147.851 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.147.869 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.147.870 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.148.878 I llama_new_context_with_model: n_seq_max     = 1
0.00.148.878 I llama_new_context_with_model: n_ctx         = 2048
0.00.148.879 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.148.879 I llama_new_context_with_model: n_batch       = 2048
0.00.148.879 I llama_new_context_with_model: n_ubatch      = 512
0.00.148.879 I llama_new_context_with_model: flash_attn    = 0
0.00.148.880 I llama_new_context_with_model: freq_base     = 10000.0
0.00.148.880 I llama_new_context_with_model: freq_scale    = 1
0.00.148.880 I ggml_metal_init: allocating
0.00.148.884 I ggml_metal_init: found device: Apple M4
0.00.148.886 I ggml_metal_init: picking default device: Apple M4
0.00.149.537 I ggml_metal_init: using embedded metal library
0.00.160.742 I ggml_metal_init: GPU name:   Apple M4
0.00.160.744 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.160.745 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.160.745 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.160.745 I ggml_metal_init: simdgroup reduction   = true
0.00.160.745 I ggml_metal_init: simdgroup matrix mul. = true
0.00.160.746 I ggml_metal_init: has bfloat            = true
0.00.160.746 I ggml_metal_init: use bfloat            = true
0.00.160.746 I ggml_metal_init: hasUnifiedMemory      = true
0.00.160.747 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.197.269 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.197.274 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.197.293 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.198.190 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.198.191 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.198.191 I llama_new_context_with_model: graph nodes  = 967
0.00.198.191 I llama_new_context_with_model: graph splits = 2
0.00.198.213 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.340.262 I main: llama threadpool init, n_threads = 4
0.00.340.295 I 
0.00.340.319 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 1 | LLAMAFILE = 1 | 
0.00.340.320 I 
0.00.340.531 I sampler seed: 1234
0.00.340.535 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.340.558 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.340.560 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.340.560 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling depressed, and I've had a hard time getting out of bed. I think the only thing I

0.02.190.786 I llama_perf_sampler_print:    sampling time =       1.26 ms /    71 runs   (    0.02 ms per token, 56393.96 tokens per second)
0.02.190.787 I llama_perf_context_print:        load time =     268.77 ms
0.02.190.787 I llama_perf_context_print: prompt eval time =      38.23 ms /     7 tokens (    5.46 ms per token,   183.12 tokens per second)
0.02.190.788 I llama_perf_context_print:        eval time =    1809.06 ms /    63 runs   (   28.72 ms per token,    34.82 tokens per second)
0.02.190.788 I llama_perf_context_print:       total time =    1850.53 ms /    70 tokens
0.02.190.976 I ggml_metal_free: deallocating

real	0m2.525s
user	0m0.152s
sys	0m0.145s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.874 I build: 4163 (8f419181) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.021.914 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.035.119 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.035.126 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.035.129 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.035.130 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.035.130 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.035.131 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.035.132 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.035.133 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.035.134 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.035.134 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.035.135 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.035.136 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.035.136 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.035.137 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.035.139 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.035.140 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.035.140 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.042.764 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.044.736 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.051.461 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.051.462 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.051.463 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.051.463 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.051.464 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.051.464 I llama_model_loader: - type  f32:  194 tensors
0.00.051.465 I llama_model_loader: - type  f16:   98 tensors
0.00.079.747 I llm_load_vocab: special tokens cache size = 25
0.00.086.047 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.086.050 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.086.050 I llm_load_print_meta: arch             = gptneox
0.00.086.050 I llm_load_print_meta: vocab type       = BPE
0.00.086.050 I llm_load_print_meta: n_vocab          = 50304
0.00.086.051 I llm_load_print_meta: n_merges         = 50009
0.00.086.051 I llm_load_print_meta: vocab_only       = 0
0.00.086.051 I llm_load_print_meta: n_ctx_train      = 2048
0.00.086.051 I llm_load_print_meta: n_embd           = 2048
0.00.086.051 I llm_load_print_meta: n_layer          = 24
0.00.086.054 I llm_load_print_meta: n_head           = 16
0.00.086.057 I llm_load_print_meta: n_head_kv        = 16
0.00.086.057 I llm_load_print_meta: n_rot            = 32
0.00.086.057 I llm_load_print_meta: n_swa            = 0
0.00.086.057 I llm_load_print_meta: n_embd_head_k    = 128
0.00.086.057 I llm_load_print_meta: n_embd_head_v    = 128
0.00.086.058 I llm_load_print_meta: n_gqa            = 1
0.00.086.058 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.086.059 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.086.060 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.086.060 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.086.060 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.086.060 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.086.060 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.086.061 I llm_load_print_meta: n_ff             = 8192
0.00.086.061 I llm_load_print_meta: n_expert         = 0
0.00.086.061 I llm_load_print_meta: n_expert_used    = 0
0.00.086.061 I llm_load_print_meta: causal attn      = 1
0.00.086.063 I llm_load_print_meta: pooling type     = 0
0.00.086.063 I llm_load_print_meta: rope type        = 2
0.00.086.063 I llm_load_print_meta: rope scaling     = linear
0.00.086.063 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.086.064 I llm_load_print_meta: freq_scale_train = 1
0.00.086.064 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.086.064 I llm_load_print_meta: rope_finetuned   = unknown
0.00.086.064 I llm_load_print_meta: ssm_d_conv       = 0
0.00.086.064 I llm_load_print_meta: ssm_d_inner      = 0
0.00.086.064 I llm_load_print_meta: ssm_d_state      = 0
0.00.086.064 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.086.065 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.086.076 I llm_load_print_meta: model type       = 1.4B
0.00.086.077 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.086.077 I llm_load_print_meta: model params     = 1.41 B
0.00.086.078 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.086.078 I llm_load_print_meta: general.name     = 1.4B
0.00.086.078 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.086.078 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.086.078 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.086.078 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.086.079 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.086.079 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.086.079 I llm_load_print_meta: max token length = 1024
0.00.088.829 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.088.829 I llm_load_tensors: offloading output layer to GPU
0.00.088.830 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.088.840 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.088.841 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.089.816 I llama_new_context_with_model: n_seq_max     = 1
0.00.089.817 I llama_new_context_with_model: n_ctx         = 128
0.00.089.817 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.089.818 I llama_new_context_with_model: n_batch       = 128
0.00.089.818 I llama_new_context_with_model: n_ubatch      = 128
0.00.089.818 I llama_new_context_with_model: flash_attn    = 0
0.00.089.818 I llama_new_context_with_model: freq_base     = 10000.0
0.00.089.819 I llama_new_context_with_model: freq_scale    = 1
0.00.089.819 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.089.819 I ggml_metal_init: allocating
0.00.089.825 I ggml_metal_init: found device: Apple M4
0.00.089.827 I ggml_metal_init: picking default device: Apple M4
0.00.090.396 I ggml_metal_init: using embedded metal library
0.00.092.437 I ggml_metal_init: GPU name:   Apple M4
0.00.092.438 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.092.439 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.092.439 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.092.439 I ggml_metal_init: simdgroup reduction   = true
0.00.092.440 I ggml_metal_init: simdgroup matrix mul. = true
0.00.092.440 I ggml_metal_init: has bfloat            = true
0.00.092.440 I ggml_metal_init: use bfloat            = true
0.00.092.440 I ggml_metal_init: hasUnifiedMemory      = true
0.00.092.441 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.101.014 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.101.016 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.101.030 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.101.958 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.101.959 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.101.960 I llama_new_context_with_model: graph nodes  = 967
0.00.101.960 I llama_new_context_with_model: graph splits = 2
0.00.101.972 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.487.294 I 
0.01.487.321 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 1 | LLAMAFILE = 1 | 
0.01.487.355 I perplexity: tokenizing the input ..
0.01.497.423 I perplexity: tokenization took 10.067 ms
0.01.497.431 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.616.851 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.618.333 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.618.349 I llama_perf_context_print:        load time =    1465.37 ms
0.01.618.350 I llama_perf_context_print: prompt eval time =     119.16 ms /   128 tokens (    0.93 ms per token,  1074.17 tokens per second)
0.01.618.351 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.618.351 I llama_perf_context_print:       total time =     131.06 ms /   129 tokens
0.01.618.810 I ggml_metal_free: deallocating

real	0m1.808s
user	0m0.112s
sys	0m0.256s
```
- q8_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.044 I build: 4163 (8f419181) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.076 I main: llama backend init
0.00.000.078 I main: load the model and apply lora adapter, if any
0.00.009.540 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.191 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.016.195 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.202 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.202 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.204 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.204 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.205 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.206 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.206 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.207 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.208 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.209 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.209 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.210 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.211 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.212 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.212 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.996 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.049 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.969 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.970 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.970 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.971 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.971 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.971 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.024.972 I llama_model_loader: - type  f32:  194 tensors
0.00.024.972 I llama_model_loader: - type q8_0:   98 tensors
0.00.046.357 I llm_load_vocab: special tokens cache size = 25
0.00.052.444 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.449 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.450 I llm_load_print_meta: arch             = gptneox
0.00.052.450 I llm_load_print_meta: vocab type       = BPE
0.00.052.452 I llm_load_print_meta: n_vocab          = 50304
0.00.052.452 I llm_load_print_meta: n_merges         = 50009
0.00.052.453 I llm_load_print_meta: vocab_only       = 0
0.00.052.453 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.453 I llm_load_print_meta: n_embd           = 2048
0.00.052.453 I llm_load_print_meta: n_layer          = 24
0.00.052.459 I llm_load_print_meta: n_head           = 16
0.00.052.460 I llm_load_print_meta: n_head_kv        = 16
0.00.052.460 I llm_load_print_meta: n_rot            = 32
0.00.052.460 I llm_load_print_meta: n_swa            = 0
0.00.052.460 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.460 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.461 I llm_load_print_meta: n_gqa            = 1
0.00.052.462 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.463 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.464 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.464 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.464 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.464 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.466 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.467 I llm_load_print_meta: n_ff             = 8192
0.00.052.467 I llm_load_print_meta: n_expert         = 0
0.00.052.468 I llm_load_print_meta: n_expert_used    = 0
0.00.052.468 I llm_load_print_meta: causal attn      = 1
0.00.052.468 I llm_load_print_meta: pooling type     = 0
0.00.052.468 I llm_load_print_meta: rope type        = 2
0.00.052.469 I llm_load_print_meta: rope scaling     = linear
0.00.052.469 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.469 I llm_load_print_meta: freq_scale_train = 1
0.00.052.470 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.470 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.470 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.470 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.470 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.470 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.470 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.483 I llm_load_print_meta: model type       = 1.4B
0.00.052.483 I llm_load_print_meta: model ftype      = Q8_0
0.00.052.483 I llm_load_print_meta: model params     = 1.41 B
0.00.052.484 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.052.484 I llm_load_print_meta: general.name     = 1.4B
0.00.052.484 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.484 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.484 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.485 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.485 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.052.486 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.486 I llm_load_print_meta: max token length = 1024
0.00.054.924 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.925 I llm_load_tensors: offloading output layer to GPU
0.00.054.925 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.935 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.054.936 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.055.962 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.963 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.963 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.963 I llama_new_context_with_model: n_batch       = 2048
0.00.055.964 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.964 I llama_new_context_with_model: flash_attn    = 0
0.00.055.964 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.964 I llama_new_context_with_model: freq_scale    = 1
0.00.055.965 I ggml_metal_init: allocating
0.00.055.969 I ggml_metal_init: found device: Apple M4
0.00.055.971 I ggml_metal_init: picking default device: Apple M4
0.00.056.640 I ggml_metal_init: using embedded metal library
0.00.058.800 I ggml_metal_init: GPU name:   Apple M4
0.00.058.802 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.803 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.803 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.803 I ggml_metal_init: simdgroup reduction   = true
0.00.058.804 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.804 I ggml_metal_init: has bfloat            = true
0.00.058.804 I ggml_metal_init: use bfloat            = true
0.00.058.805 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.805 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.090.350 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.090.357 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.090.382 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.091.536 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.091.538 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.091.538 I llama_new_context_with_model: graph nodes  = 967
0.00.091.538 I llama_new_context_with_model: graph splits = 2
0.00.091.553 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.058.928 I main: llama threadpool init, n_threads = 4
0.01.058.964 I 
0.01.058.985 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 1 | LLAMAFILE = 1 | 
0.01.058.987 I 
0.01.059.215 I sampler seed: 1234
0.01.059.220 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.059.275 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.059.279 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.059.279 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.145.719 I llama_perf_sampler_print:    sampling time =       1.38 ms /    71 runs   (    0.02 ms per token, 51412.02 tokens per second)
0.02.145.720 I llama_perf_context_print:        load time =    1049.38 ms
0.02.145.721 I llama_perf_context_print: prompt eval time =      33.75 ms /     7 tokens (    4.82 ms per token,   207.42 tokens per second)
0.02.145.722 I llama_perf_context_print:        eval time =    1049.78 ms /    63 runs   (   16.66 ms per token,    60.01 tokens per second)
0.02.145.722 I llama_perf_context_print:       total time =    1086.79 ms /    70 tokens
0.02.145.913 I ggml_metal_free: deallocating

real	0m2.164s
user	0m0.110s
sys	0m0.260s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.288 I build: 4163 (8f419181) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.224 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.541 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.017.546 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.547 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.547 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.548 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.548 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.548 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.549 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.550 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.550 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.550 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.551 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.551 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.552 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.553 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.554 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.554 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.451 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.930 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.029.148 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.029.150 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.029.150 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.029.150 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.029.151 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.029.151 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.029.152 I llama_model_loader: - type  f32:  194 tensors
0.00.029.152 I llama_model_loader: - type q8_0:   98 tensors
0.00.053.366 I llm_load_vocab: special tokens cache size = 25
0.00.059.614 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.059.616 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.059.617 I llm_load_print_meta: arch             = gptneox
0.00.059.617 I llm_load_print_meta: vocab type       = BPE
0.00.059.617 I llm_load_print_meta: n_vocab          = 50304
0.00.059.617 I llm_load_print_meta: n_merges         = 50009
0.00.059.617 I llm_load_print_meta: vocab_only       = 0
0.00.059.618 I llm_load_print_meta: n_ctx_train      = 2048
0.00.059.618 I llm_load_print_meta: n_embd           = 2048
0.00.059.618 I llm_load_print_meta: n_layer          = 24
0.00.059.620 I llm_load_print_meta: n_head           = 16
0.00.059.621 I llm_load_print_meta: n_head_kv        = 16
0.00.059.621 I llm_load_print_meta: n_rot            = 32
0.00.059.621 I llm_load_print_meta: n_swa            = 0
0.00.059.621 I llm_load_print_meta: n_embd_head_k    = 128
0.00.059.621 I llm_load_print_meta: n_embd_head_v    = 128
0.00.059.622 I llm_load_print_meta: n_gqa            = 1
0.00.059.622 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.059.624 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.059.624 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.059.625 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.059.625 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.059.625 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.059.625 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.059.626 I llm_load_print_meta: n_ff             = 8192
0.00.059.626 I llm_load_print_meta: n_expert         = 0
0.00.059.626 I llm_load_print_meta: n_expert_used    = 0
0.00.059.628 I llm_load_print_meta: causal attn      = 1
0.00.059.628 I llm_load_print_meta: pooling type     = 0
0.00.059.628 I llm_load_print_meta: rope type        = 2
0.00.059.628 I llm_load_print_meta: rope scaling     = linear
0.00.059.628 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.059.629 I llm_load_print_meta: freq_scale_train = 1
0.00.059.629 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.059.629 I llm_load_print_meta: rope_finetuned   = unknown
0.00.059.629 I llm_load_print_meta: ssm_d_conv       = 0
0.00.059.629 I llm_load_print_meta: ssm_d_inner      = 0
0.00.059.629 I llm_load_print_meta: ssm_d_state      = 0
0.00.059.629 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.059.630 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.059.641 I llm_load_print_meta: model type       = 1.4B
0.00.059.641 I llm_load_print_meta: model ftype      = Q8_0
0.00.059.641 I llm_load_print_meta: model params     = 1.41 B
0.00.059.642 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.059.642 I llm_load_print_meta: general.name     = 1.4B
0.00.059.642 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.059.643 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.059.643 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.059.643 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.059.643 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.059.643 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.059.644 I llm_load_print_meta: max token length = 1024
0.00.061.194 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.061.194 I llm_load_tensors: offloading output layer to GPU
0.00.061.194 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.061.203 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.061.204 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.062.022 I llama_new_context_with_model: n_seq_max     = 1
0.00.062.022 I llama_new_context_with_model: n_ctx         = 128
0.00.062.023 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.062.023 I llama_new_context_with_model: n_batch       = 128
0.00.062.023 I llama_new_context_with_model: n_ubatch      = 128
0.00.062.023 I llama_new_context_with_model: flash_attn    = 0
0.00.062.023 I llama_new_context_with_model: freq_base     = 10000.0
0.00.062.024 I llama_new_context_with_model: freq_scale    = 1
0.00.062.024 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.062.024 I ggml_metal_init: allocating
0.00.062.029 I ggml_metal_init: found device: Apple M4
0.00.062.032 I ggml_metal_init: picking default device: Apple M4
0.00.062.588 I ggml_metal_init: using embedded metal library
0.00.064.610 I ggml_metal_init: GPU name:   Apple M4
0.00.064.611 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.064.612 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.064.612 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.064.612 I ggml_metal_init: simdgroup reduction   = true
0.00.064.612 I ggml_metal_init: simdgroup matrix mul. = true
0.00.064.613 I ggml_metal_init: has bfloat            = true
0.00.064.613 I ggml_metal_init: use bfloat            = true
0.00.064.613 I ggml_metal_init: hasUnifiedMemory      = true
0.00.064.614 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.074.618 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.074.622 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.074.638 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.075.497 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.075.498 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.075.498 I llama_new_context_with_model: graph nodes  = 967
0.00.075.498 I llama_new_context_with_model: graph splits = 2
0.00.075.506 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.876.825 I 
0.00.876.863 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 1 | LLAMAFILE = 1 | 
0.00.876.896 I perplexity: tokenizing the input ..
0.00.884.895 I perplexity: tokenization took 8.001 ms
0.00.884.898 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.007.392 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.008.667 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.008.695 I llama_perf_context_print:        load time =     866.59 ms
0.01.008.696 I llama_perf_context_print: prompt eval time =     122.26 ms /   128 tokens (    0.96 ms per token,  1046.98 tokens per second)
0.01.008.697 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.008.697 I llama_perf_context_print:       total time =     131.87 ms /   129 tokens
0.01.009.208 I ggml_metal_free: deallocating

real	0m1.026s
user	0m0.087s
sys	0m0.163s
```
- q4_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.053 I build: 4163 (8f419181) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.086 I main: llama backend init
0.00.000.088 I main: load the model and apply lora adapter, if any
0.00.010.828 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.381 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.017.385 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.387 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.388 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.388 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.389 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.389 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.390 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.390 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.391 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.391 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.391 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.392 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.392 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.393 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.394 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.395 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.131 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.209 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.037 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.039 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.039 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.039 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.040 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.040 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.026.041 I llama_model_loader: - type  f32:  194 tensors
0.00.026.041 I llama_model_loader: - type q4_0:   97 tensors
0.00.026.041 I llama_model_loader: - type q6_K:    1 tensors
0.00.046.543 I llm_load_vocab: special tokens cache size = 25
0.00.052.577 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.581 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.581 I llm_load_print_meta: arch             = gptneox
0.00.052.582 I llm_load_print_meta: vocab type       = BPE
0.00.052.582 I llm_load_print_meta: n_vocab          = 50304
0.00.052.582 I llm_load_print_meta: n_merges         = 50009
0.00.052.584 I llm_load_print_meta: vocab_only       = 0
0.00.052.585 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.585 I llm_load_print_meta: n_embd           = 2048
0.00.052.585 I llm_load_print_meta: n_layer          = 24
0.00.052.591 I llm_load_print_meta: n_head           = 16
0.00.052.592 I llm_load_print_meta: n_head_kv        = 16
0.00.052.592 I llm_load_print_meta: n_rot            = 32
0.00.052.592 I llm_load_print_meta: n_swa            = 0
0.00.052.592 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.592 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.593 I llm_load_print_meta: n_gqa            = 1
0.00.052.594 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.595 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.596 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.596 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.596 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.596 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.596 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.597 I llm_load_print_meta: n_ff             = 8192
0.00.052.599 I llm_load_print_meta: n_expert         = 0
0.00.052.599 I llm_load_print_meta: n_expert_used    = 0
0.00.052.599 I llm_load_print_meta: causal attn      = 1
0.00.052.600 I llm_load_print_meta: pooling type     = 0
0.00.052.600 I llm_load_print_meta: rope type        = 2
0.00.052.600 I llm_load_print_meta: rope scaling     = linear
0.00.052.600 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.601 I llm_load_print_meta: freq_scale_train = 1
0.00.052.601 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.601 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.601 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.601 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.601 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.602 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.602 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.615 I llm_load_print_meta: model type       = 1.4B
0.00.052.615 I llm_load_print_meta: model ftype      = Q4_0
0.00.052.615 I llm_load_print_meta: model params     = 1.41 B
0.00.052.616 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.052.616 I llm_load_print_meta: general.name     = 1.4B
0.00.052.616 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.616 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.617 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.617 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.617 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.052.618 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.618 I llm_load_print_meta: max token length = 1024
0.00.054.898 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.899 I llm_load_tensors: offloading output layer to GPU
0.00.054.899 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.909 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.054.911 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.056.005 I llama_new_context_with_model: n_seq_max     = 1
0.00.056.006 I llama_new_context_with_model: n_ctx         = 2048
0.00.056.006 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.056.006 I llama_new_context_with_model: n_batch       = 2048
0.00.056.006 I llama_new_context_with_model: n_ubatch      = 512
0.00.056.007 I llama_new_context_with_model: flash_attn    = 0
0.00.056.007 I llama_new_context_with_model: freq_base     = 10000.0
0.00.056.007 I llama_new_context_with_model: freq_scale    = 1
0.00.056.008 I ggml_metal_init: allocating
0.00.056.011 I ggml_metal_init: found device: Apple M4
0.00.056.013 I ggml_metal_init: picking default device: Apple M4
0.00.056.667 I ggml_metal_init: using embedded metal library
0.00.058.759 I ggml_metal_init: GPU name:   Apple M4
0.00.058.761 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.761 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.762 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.762 I ggml_metal_init: simdgroup reduction   = true
0.00.058.762 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.762 I ggml_metal_init: has bfloat            = true
0.00.058.762 I ggml_metal_init: use bfloat            = true
0.00.058.763 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.763 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.090.336 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.090.346 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.090.371 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.091.599 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.091.600 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.091.601 I llama_new_context_with_model: graph nodes  = 967
0.00.091.601 I llama_new_context_with_model: graph splits = 2
0.00.091.616 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.644.800 I main: llama threadpool init, n_threads = 4
0.00.644.830 I 
0.00.644.853 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 1 | LLAMAFILE = 1 | 
0.00.644.853 I 
0.00.645.000 I sampler seed: 1234
0.00.645.005 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.645.014 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.645.014 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.645.015 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.320.165 I llama_perf_sampler_print:    sampling time =       1.25 ms /    71 runs   (    0.02 ms per token, 56936.65 tokens per second)
0.01.320.166 I llama_perf_context_print:        load time =     633.97 ms
0.01.320.166 I llama_perf_context_print: prompt eval time =      32.70 ms /     7 tokens (    4.67 ms per token,   214.04 tokens per second)
0.01.320.167 I llama_perf_context_print:        eval time =     639.31 ms /    63 runs   (   10.15 ms per token,    98.54 tokens per second)
0.01.320.167 I llama_perf_context_print:       total time =     675.37 ms /    70 tokens
0.01.320.341 I ggml_metal_free: deallocating

real	0m1.338s
user	0m0.108s
sys	0m0.181s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.267 I build: 4163 (8f419181) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.596 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.161 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.017.165 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.167 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.167 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.171 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.172 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.172 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.175 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.175 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.175 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.176 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.176 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.176 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.177 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.179 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.179 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.179 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.832 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.838 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.566 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.567 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.567 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.568 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.568 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.568 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.025.569 I llama_model_loader: - type  f32:  194 tensors
0.00.025.569 I llama_model_loader: - type q4_0:   97 tensors
0.00.025.569 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.439 I llm_load_vocab: special tokens cache size = 25
0.00.051.358 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.360 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.361 I llm_load_print_meta: arch             = gptneox
0.00.051.361 I llm_load_print_meta: vocab type       = BPE
0.00.051.361 I llm_load_print_meta: n_vocab          = 50304
0.00.051.361 I llm_load_print_meta: n_merges         = 50009
0.00.051.362 I llm_load_print_meta: vocab_only       = 0
0.00.051.362 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.362 I llm_load_print_meta: n_embd           = 2048
0.00.051.362 I llm_load_print_meta: n_layer          = 24
0.00.051.365 I llm_load_print_meta: n_head           = 16
0.00.051.366 I llm_load_print_meta: n_head_kv        = 16
0.00.051.366 I llm_load_print_meta: n_rot            = 32
0.00.051.367 I llm_load_print_meta: n_swa            = 0
0.00.051.367 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.367 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.368 I llm_load_print_meta: n_gqa            = 1
0.00.051.368 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.369 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.370 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.370 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.370 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.370 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.370 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.371 I llm_load_print_meta: n_ff             = 8192
0.00.051.371 I llm_load_print_meta: n_expert         = 0
0.00.051.371 I llm_load_print_meta: n_expert_used    = 0
0.00.051.371 I llm_load_print_meta: causal attn      = 1
0.00.051.372 I llm_load_print_meta: pooling type     = 0
0.00.051.372 I llm_load_print_meta: rope type        = 2
0.00.051.372 I llm_load_print_meta: rope scaling     = linear
0.00.051.373 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.374 I llm_load_print_meta: freq_scale_train = 1
0.00.051.374 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.374 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.374 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.374 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.374 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.374 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.376 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.387 I llm_load_print_meta: model type       = 1.4B
0.00.051.387 I llm_load_print_meta: model ftype      = Q4_0
0.00.051.387 I llm_load_print_meta: model params     = 1.41 B
0.00.051.388 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.051.388 I llm_load_print_meta: general.name     = 1.4B
0.00.051.388 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.388 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.388 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.389 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.389 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.389 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.389 I llm_load_print_meta: max token length = 1024
0.00.052.921 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.921 I llm_load_tensors: offloading output layer to GPU
0.00.052.921 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.931 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.052.932 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.053.763 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.764 I llama_new_context_with_model: n_ctx         = 128
0.00.053.765 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.765 I llama_new_context_with_model: n_batch       = 128
0.00.053.765 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.765 I llama_new_context_with_model: flash_attn    = 0
0.00.053.765 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.765 I llama_new_context_with_model: freq_scale    = 1
0.00.053.766 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.766 I ggml_metal_init: allocating
0.00.053.771 I ggml_metal_init: found device: Apple M4
0.00.053.773 I ggml_metal_init: picking default device: Apple M4
0.00.054.324 I ggml_metal_init: using embedded metal library
0.00.056.196 I ggml_metal_init: GPU name:   Apple M4
0.00.056.197 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.198 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.198 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.198 I ggml_metal_init: simdgroup reduction   = true
0.00.056.198 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.198 I ggml_metal_init: has bfloat            = true
0.00.056.198 I ggml_metal_init: use bfloat            = true
0.00.056.199 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.200 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.531 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.533 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.546 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.366 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.367 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.367 I llama_new_context_with_model: graph nodes  = 967
0.00.066.367 I llama_new_context_with_model: graph splits = 2
0.00.066.379 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.552.537 I 
0.00.552.554 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 1 | LLAMAFILE = 1 | 
0.00.552.565 I perplexity: tokenizing the input ..
0.00.559.849 I perplexity: tokenization took 7.283 ms
0.00.559.853 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.682.767 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.684.013 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.684.030 I llama_perf_context_print:        load time =     542.94 ms
0.00.684.031 I llama_perf_context_print: prompt eval time =     122.69 ms /   128 tokens (    0.96 ms per token,  1043.25 tokens per second)
0.00.684.032 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.684.033 I llama_perf_context_print:       total time =     131.49 ms /   129 tokens
0.00.684.473 I ggml_metal_free: deallocating

real	0m0.699s
user	0m0.076s
sys	0m0.109s
```
- q4_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.036 I build: 4163 (8f419181) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.065 I main: llama backend init
0.00.000.067 I main: load the model and apply lora adapter, if any
0.00.008.459 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.216 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.015.221 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.226 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.227 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.227 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.228 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.228 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.229 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.229 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.229 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.230 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.232 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.232 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.232 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.234 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.234 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.234 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.018 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.074 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.815 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.816 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.816 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.817 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.817 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.817 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.023.818 I llama_model_loader: - type  f32:  194 tensors
0.00.023.818 I llama_model_loader: - type q4_1:   97 tensors
0.00.023.819 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.485 I llm_load_vocab: special tokens cache size = 25
0.00.050.334 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.337 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.338 I llm_load_print_meta: arch             = gptneox
0.00.050.338 I llm_load_print_meta: vocab type       = BPE
0.00.050.338 I llm_load_print_meta: n_vocab          = 50304
0.00.050.338 I llm_load_print_meta: n_merges         = 50009
0.00.050.339 I llm_load_print_meta: vocab_only       = 0
0.00.050.339 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.339 I llm_load_print_meta: n_embd           = 2048
0.00.050.339 I llm_load_print_meta: n_layer          = 24
0.00.050.342 I llm_load_print_meta: n_head           = 16
0.00.050.343 I llm_load_print_meta: n_head_kv        = 16
0.00.050.343 I llm_load_print_meta: n_rot            = 32
0.00.050.343 I llm_load_print_meta: n_swa            = 0
0.00.050.343 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.343 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.344 I llm_load_print_meta: n_gqa            = 1
0.00.050.345 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.345 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.346 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.346 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.347 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.347 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.347 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.348 I llm_load_print_meta: n_ff             = 8192
0.00.050.348 I llm_load_print_meta: n_expert         = 0
0.00.050.348 I llm_load_print_meta: n_expert_used    = 0
0.00.050.348 I llm_load_print_meta: causal attn      = 1
0.00.050.348 I llm_load_print_meta: pooling type     = 0
0.00.050.348 I llm_load_print_meta: rope type        = 2
0.00.050.349 I llm_load_print_meta: rope scaling     = linear
0.00.050.349 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.349 I llm_load_print_meta: freq_scale_train = 1
0.00.050.350 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.350 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.350 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.350 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.350 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.350 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.351 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.357 I llm_load_print_meta: model type       = 1.4B
0.00.050.357 I llm_load_print_meta: model ftype      = Q4_1
0.00.050.358 I llm_load_print_meta: model params     = 1.41 B
0.00.050.359 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.050.359 I llm_load_print_meta: general.name     = 1.4B
0.00.050.359 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.359 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.360 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.360 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.360 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.362 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.362 I llm_load_print_meta: max token length = 1024
0.00.051.941 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.941 I llm_load_tensors: offloading output layer to GPU
0.00.051.942 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.946 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.051.947 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.052.781 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.782 I llama_new_context_with_model: n_ctx         = 2048
0.00.052.782 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.052.782 I llama_new_context_with_model: n_batch       = 2048
0.00.052.782 I llama_new_context_with_model: n_ubatch      = 512
0.00.052.782 I llama_new_context_with_model: flash_attn    = 0
0.00.052.783 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.783 I llama_new_context_with_model: freq_scale    = 1
0.00.052.784 I ggml_metal_init: allocating
0.00.052.787 I ggml_metal_init: found device: Apple M4
0.00.052.789 I ggml_metal_init: picking default device: Apple M4
0.00.053.373 I ggml_metal_init: using embedded metal library
0.00.055.275 I ggml_metal_init: GPU name:   Apple M4
0.00.055.277 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.278 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.278 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.278 I ggml_metal_init: simdgroup reduction   = true
0.00.055.278 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.278 I ggml_metal_init: has bfloat            = true
0.00.055.279 I ggml_metal_init: use bfloat            = true
0.00.055.279 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.281 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.082.520 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.082.528 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.082.549 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.083.507 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.083.509 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.083.509 I llama_new_context_with_model: graph nodes  = 967
0.00.083.509 I llama_new_context_with_model: graph splits = 2
0.00.083.523 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.777.597 I main: llama threadpool init, n_threads = 4
0.00.777.635 I 
0.00.777.654 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 1 | LLAMAFILE = 1 | 
0.00.777.657 I 
0.00.777.893 I sampler seed: 1234
0.00.777.896 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.777.933 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.777.936 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.777.936 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.502.705 I llama_perf_sampler_print:    sampling time =       1.22 ms /    71 runs   (    0.02 ms per token, 58436.21 tokens per second)
0.01.502.706 I llama_perf_context_print:        load time =     769.13 ms
0.01.502.707 I llama_perf_context_print: prompt eval time =      33.00 ms /     7 tokens (    4.71 ms per token,   212.12 tokens per second)
0.01.502.707 I llama_perf_context_print:        eval time =     688.88 ms /    63 runs   (   10.93 ms per token,    91.45 tokens per second)
0.01.502.708 I llama_perf_context_print:       total time =     725.11 ms /    70 tokens
0.01.502.913 I ggml_metal_free: deallocating

real	0m1.518s
user	0m0.108s
sys	0m0.180s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.090 I build: 4163 (8f419181) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.807 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.741 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.014.745 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.750 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.751 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.751 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.752 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.752 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.754 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.755 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.755 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.755 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.756 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.756 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.759 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.761 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.761 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.761 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.614 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.642 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.469 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.471 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.471 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.471 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.472 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.472 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.023.472 I llama_model_loader: - type  f32:  194 tensors
0.00.023.473 I llama_model_loader: - type q4_1:   97 tensors
0.00.023.473 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.379 I llm_load_vocab: special tokens cache size = 25
0.00.050.338 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.341 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.342 I llm_load_print_meta: arch             = gptneox
0.00.050.342 I llm_load_print_meta: vocab type       = BPE
0.00.050.342 I llm_load_print_meta: n_vocab          = 50304
0.00.050.342 I llm_load_print_meta: n_merges         = 50009
0.00.050.342 I llm_load_print_meta: vocab_only       = 0
0.00.050.343 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.343 I llm_load_print_meta: n_embd           = 2048
0.00.050.343 I llm_load_print_meta: n_layer          = 24
0.00.050.346 I llm_load_print_meta: n_head           = 16
0.00.050.347 I llm_load_print_meta: n_head_kv        = 16
0.00.050.347 I llm_load_print_meta: n_rot            = 32
0.00.050.347 I llm_load_print_meta: n_swa            = 0
0.00.050.347 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.347 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.348 I llm_load_print_meta: n_gqa            = 1
0.00.050.349 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.350 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.350 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.350 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.351 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.351 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.351 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.352 I llm_load_print_meta: n_ff             = 8192
0.00.050.352 I llm_load_print_meta: n_expert         = 0
0.00.050.352 I llm_load_print_meta: n_expert_used    = 0
0.00.050.354 I llm_load_print_meta: causal attn      = 1
0.00.050.354 I llm_load_print_meta: pooling type     = 0
0.00.050.354 I llm_load_print_meta: rope type        = 2
0.00.050.354 I llm_load_print_meta: rope scaling     = linear
0.00.050.354 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.355 I llm_load_print_meta: freq_scale_train = 1
0.00.050.355 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.355 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.355 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.356 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.356 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.356 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.356 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.368 I llm_load_print_meta: model type       = 1.4B
0.00.050.368 I llm_load_print_meta: model ftype      = Q4_1
0.00.050.368 I llm_load_print_meta: model params     = 1.41 B
0.00.050.369 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.050.369 I llm_load_print_meta: general.name     = 1.4B
0.00.050.369 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.370 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.370 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.370 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.370 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.371 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.371 I llm_load_print_meta: max token length = 1024
0.00.052.344 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.345 I llm_load_tensors: offloading output layer to GPU
0.00.052.345 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.355 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.052.356 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.053.267 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.268 I llama_new_context_with_model: n_ctx         = 128
0.00.053.268 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.268 I llama_new_context_with_model: n_batch       = 128
0.00.053.268 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.268 I llama_new_context_with_model: flash_attn    = 0
0.00.053.269 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.269 I llama_new_context_with_model: freq_scale    = 1
0.00.053.269 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.270 I ggml_metal_init: allocating
0.00.053.275 I ggml_metal_init: found device: Apple M4
0.00.053.283 I ggml_metal_init: picking default device: Apple M4
0.00.053.821 I ggml_metal_init: using embedded metal library
0.00.055.762 I ggml_metal_init: GPU name:   Apple M4
0.00.055.763 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.764 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.764 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.764 I ggml_metal_init: simdgroup reduction   = true
0.00.055.764 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.764 I ggml_metal_init: has bfloat            = true
0.00.055.765 I ggml_metal_init: use bfloat            = true
0.00.055.765 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.767 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.936 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.064.938 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.064.953 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.065.862 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.065.863 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.065.864 I llama_new_context_with_model: graph nodes  = 967
0.00.065.864 I llama_new_context_with_model: graph splits = 2
0.00.065.877 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.696.648 I 
0.00.696.679 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 1 | LLAMAFILE = 1 | 
0.00.696.703 I perplexity: tokenizing the input ..
0.00.704.620 I perplexity: tokenization took 7.914 ms
0.00.704.624 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.827.673 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.828.893 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.828.915 I llama_perf_context_print:        load time =     687.83 ms
0.00.828.917 I llama_perf_context_print: prompt eval time =     122.83 ms /   128 tokens (    0.96 ms per token,  1042.13 tokens per second)
0.00.828.918 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.828.919 I llama_perf_context_print:       total time =     132.27 ms /   129 tokens
0.00.829.391 I ggml_metal_free: deallocating

real	0m0.843s
user	0m0.076s
sys	0m0.122s
```
- q5_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.037 I build: 4163 (8f419181) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.068 I main: llama backend init
0.00.000.071 I main: load the model and apply lora adapter, if any
0.00.009.318 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.096 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.016.100 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.108 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.109 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.109 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.109 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.110 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.111 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.111 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.111 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.112 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.112 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.112 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.113 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.114 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.114 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.114 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.933 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.000 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.750 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.751 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.751 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.751 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.752 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.752 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.024.753 I llama_model_loader: - type  f32:  194 tensors
0.00.024.753 I llama_model_loader: - type q5_0:   97 tensors
0.00.024.753 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.872 I llm_load_vocab: special tokens cache size = 25
0.00.050.899 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.902 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.902 I llm_load_print_meta: arch             = gptneox
0.00.050.902 I llm_load_print_meta: vocab type       = BPE
0.00.050.903 I llm_load_print_meta: n_vocab          = 50304
0.00.050.903 I llm_load_print_meta: n_merges         = 50009
0.00.050.903 I llm_load_print_meta: vocab_only       = 0
0.00.050.903 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.903 I llm_load_print_meta: n_embd           = 2048
0.00.050.904 I llm_load_print_meta: n_layer          = 24
0.00.050.906 I llm_load_print_meta: n_head           = 16
0.00.050.907 I llm_load_print_meta: n_head_kv        = 16
0.00.050.907 I llm_load_print_meta: n_rot            = 32
0.00.050.908 I llm_load_print_meta: n_swa            = 0
0.00.050.908 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.908 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.909 I llm_load_print_meta: n_gqa            = 1
0.00.050.909 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.910 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.911 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.911 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.911 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.911 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.911 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.912 I llm_load_print_meta: n_ff             = 8192
0.00.050.912 I llm_load_print_meta: n_expert         = 0
0.00.050.912 I llm_load_print_meta: n_expert_used    = 0
0.00.050.912 I llm_load_print_meta: causal attn      = 1
0.00.050.913 I llm_load_print_meta: pooling type     = 0
0.00.050.913 I llm_load_print_meta: rope type        = 2
0.00.050.913 I llm_load_print_meta: rope scaling     = linear
0.00.050.913 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.914 I llm_load_print_meta: freq_scale_train = 1
0.00.050.914 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.914 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.914 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.914 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.914 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.915 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.915 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.926 I llm_load_print_meta: model type       = 1.4B
0.00.050.928 I llm_load_print_meta: model ftype      = Q5_0
0.00.050.928 I llm_load_print_meta: model params     = 1.41 B
0.00.050.928 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.050.929 I llm_load_print_meta: general.name     = 1.4B
0.00.050.929 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.930 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.930 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.931 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.931 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.931 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.931 I llm_load_print_meta: max token length = 1024
0.00.052.912 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.912 I llm_load_tensors: offloading output layer to GPU
0.00.052.912 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.922 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.052.923 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.053.874 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.875 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.875 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.875 I llama_new_context_with_model: n_batch       = 2048
0.00.053.875 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.875 I llama_new_context_with_model: flash_attn    = 0
0.00.053.876 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.876 I llama_new_context_with_model: freq_scale    = 1
0.00.053.877 I ggml_metal_init: allocating
0.00.053.879 I ggml_metal_init: found device: Apple M4
0.00.053.881 I ggml_metal_init: picking default device: Apple M4
0.00.054.415 I ggml_metal_init: using embedded metal library
0.00.056.313 I ggml_metal_init: GPU name:   Apple M4
0.00.056.314 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.315 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.315 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.315 I ggml_metal_init: simdgroup reduction   = true
0.00.056.315 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.315 I ggml_metal_init: has bfloat            = true
0.00.056.316 I ggml_metal_init: use bfloat            = true
0.00.056.316 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.318 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.082.932 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.082.938 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.082.956 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.083.855 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.083.856 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.083.856 I llama_new_context_with_model: graph nodes  = 967
0.00.083.857 I llama_new_context_with_model: graph splits = 2
0.00.083.869 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.712.671 I main: llama threadpool init, n_threads = 4
0.00.712.700 I 
0.00.712.721 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 1 | LLAMAFILE = 1 | 
0.00.712.722 I 
0.00.712.863 I sampler seed: 1234
0.00.712.869 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.712.878 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.712.879 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.712.879 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.500.854 I llama_perf_sampler_print:    sampling time =       1.17 ms /    71 runs   (    0.02 ms per token, 60631.94 tokens per second)
0.01.500.855 I llama_perf_context_print:        load time =     703.35 ms
0.01.500.855 I llama_perf_context_print: prompt eval time =      36.62 ms /     7 tokens (    5.23 ms per token,   191.17 tokens per second)
0.01.500.856 I llama_perf_context_print:        eval time =     748.39 ms /    63 runs   (   11.88 ms per token,    84.18 tokens per second)
0.01.500.858 I llama_perf_context_print:       total time =     788.19 ms /    70 tokens
0.01.501.037 I ggml_metal_free: deallocating

real	0m1.518s
user	0m0.107s
sys	0m0.182s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.092 I build: 4163 (8f419181) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.251 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.082 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.016.086 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.087 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.090 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.091 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.091 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.092 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.092 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.097 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.100 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.102 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.102 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.103 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.103 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.105 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.105 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.105 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.681 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.664 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.381 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.383 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.383 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.383 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.384 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.384 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.024.385 I llama_model_loader: - type  f32:  194 tensors
0.00.024.385 I llama_model_loader: - type q5_0:   97 tensors
0.00.024.385 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.473 I llm_load_vocab: special tokens cache size = 25
0.00.050.400 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.403 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.404 I llm_load_print_meta: arch             = gptneox
0.00.050.404 I llm_load_print_meta: vocab type       = BPE
0.00.050.404 I llm_load_print_meta: n_vocab          = 50304
0.00.050.404 I llm_load_print_meta: n_merges         = 50009
0.00.050.405 I llm_load_print_meta: vocab_only       = 0
0.00.050.405 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.405 I llm_load_print_meta: n_embd           = 2048
0.00.050.405 I llm_load_print_meta: n_layer          = 24
0.00.050.408 I llm_load_print_meta: n_head           = 16
0.00.050.409 I llm_load_print_meta: n_head_kv        = 16
0.00.050.409 I llm_load_print_meta: n_rot            = 32
0.00.050.409 I llm_load_print_meta: n_swa            = 0
0.00.050.409 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.409 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.410 I llm_load_print_meta: n_gqa            = 1
0.00.050.411 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.411 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.412 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.412 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.412 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.413 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.413 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.415 I llm_load_print_meta: n_ff             = 8192
0.00.050.417 I llm_load_print_meta: n_expert         = 0
0.00.050.417 I llm_load_print_meta: n_expert_used    = 0
0.00.050.417 I llm_load_print_meta: causal attn      = 1
0.00.050.417 I llm_load_print_meta: pooling type     = 0
0.00.050.417 I llm_load_print_meta: rope type        = 2
0.00.050.417 I llm_load_print_meta: rope scaling     = linear
0.00.050.418 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.418 I llm_load_print_meta: freq_scale_train = 1
0.00.050.418 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.419 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.419 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.419 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.419 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.419 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.419 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.431 I llm_load_print_meta: model type       = 1.4B
0.00.050.431 I llm_load_print_meta: model ftype      = Q5_0
0.00.050.431 I llm_load_print_meta: model params     = 1.41 B
0.00.050.434 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.050.435 I llm_load_print_meta: general.name     = 1.4B
0.00.050.435 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.435 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.435 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.436 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.436 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.437 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.437 I llm_load_print_meta: max token length = 1024
0.00.052.438 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.439 I llm_load_tensors: offloading output layer to GPU
0.00.052.439 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.449 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.052.450 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.053.342 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.343 I llama_new_context_with_model: n_ctx         = 128
0.00.053.343 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.343 I llama_new_context_with_model: n_batch       = 128
0.00.053.343 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.344 I llama_new_context_with_model: flash_attn    = 0
0.00.053.344 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.344 I llama_new_context_with_model: freq_scale    = 1
0.00.053.345 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.345 I ggml_metal_init: allocating
0.00.053.348 I ggml_metal_init: found device: Apple M4
0.00.053.350 I ggml_metal_init: picking default device: Apple M4
0.00.053.910 I ggml_metal_init: using embedded metal library
0.00.055.831 I ggml_metal_init: GPU name:   Apple M4
0.00.055.832 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.833 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.833 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.833 I ggml_metal_init: simdgroup reduction   = true
0.00.055.835 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.835 I ggml_metal_init: has bfloat            = true
0.00.055.835 I ggml_metal_init: use bfloat            = true
0.00.055.836 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.836 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.196 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.199 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.214 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.195 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.196 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.197 I llama_new_context_with_model: graph nodes  = 967
0.00.066.197 I llama_new_context_with_model: graph splits = 2
0.00.066.209 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.639.572 I 
0.00.639.597 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 1 | LLAMAFILE = 1 | 
0.00.639.614 I perplexity: tokenizing the input ..
0.00.647.289 I perplexity: tokenization took 7.673 ms
0.00.647.293 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.781.741 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.783.046 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.783.068 I llama_perf_context_print:        load time =     629.32 ms
0.00.783.070 I llama_perf_context_print: prompt eval time =     134.21 ms /   128 tokens (    1.05 ms per token,   953.72 tokens per second)
0.00.783.071 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.783.071 I llama_perf_context_print:       total time =     143.50 ms /   129 tokens
0.00.783.524 I ggml_metal_free: deallocating

real	0m0.797s
user	0m0.075s
sys	0m0.122s
```
- q5_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.037 I build: 4163 (8f419181) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.065 I main: llama backend init
0.00.000.067 I main: load the model and apply lora adapter, if any
0.00.008.679 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.785 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.015.789 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.794 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.795 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.795 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.795 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.796 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.797 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.797 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.797 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.798 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.798 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.798 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.799 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.800 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.800 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.801 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.512 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.527 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.257 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.259 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.259 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.259 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.260 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.260 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.260 I llama_model_loader: - type  f32:  194 tensors
0.00.024.261 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.261 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.412 I llm_load_vocab: special tokens cache size = 25
0.00.050.433 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.436 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.436 I llm_load_print_meta: arch             = gptneox
0.00.050.436 I llm_load_print_meta: vocab type       = BPE
0.00.050.437 I llm_load_print_meta: n_vocab          = 50304
0.00.050.437 I llm_load_print_meta: n_merges         = 50009
0.00.050.437 I llm_load_print_meta: vocab_only       = 0
0.00.050.437 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.437 I llm_load_print_meta: n_embd           = 2048
0.00.050.437 I llm_load_print_meta: n_layer          = 24
0.00.050.441 I llm_load_print_meta: n_head           = 16
0.00.050.441 I llm_load_print_meta: n_head_kv        = 16
0.00.050.442 I llm_load_print_meta: n_rot            = 32
0.00.050.442 I llm_load_print_meta: n_swa            = 0
0.00.050.442 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.442 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.443 I llm_load_print_meta: n_gqa            = 1
0.00.050.444 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.444 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.445 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.445 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.446 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.446 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.446 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.447 I llm_load_print_meta: n_ff             = 8192
0.00.050.447 I llm_load_print_meta: n_expert         = 0
0.00.050.447 I llm_load_print_meta: n_expert_used    = 0
0.00.050.447 I llm_load_print_meta: causal attn      = 1
0.00.050.447 I llm_load_print_meta: pooling type     = 0
0.00.050.447 I llm_load_print_meta: rope type        = 2
0.00.050.448 I llm_load_print_meta: rope scaling     = linear
0.00.050.448 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.448 I llm_load_print_meta: freq_scale_train = 1
0.00.050.448 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.449 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.449 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.449 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.451 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.451 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.451 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.463 I llm_load_print_meta: model type       = 1.4B
0.00.050.464 I llm_load_print_meta: model ftype      = Q5_1
0.00.050.464 I llm_load_print_meta: model params     = 1.41 B
0.00.050.465 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.050.466 I llm_load_print_meta: general.name     = 1.4B
0.00.050.466 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.466 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.466 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.466 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.467 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.467 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.467 I llm_load_print_meta: max token length = 1024
0.00.052.404 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.404 I llm_load_tensors: offloading output layer to GPU
0.00.052.404 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.414 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.052.415 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.053.417 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.418 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.418 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.418 I llama_new_context_with_model: n_batch       = 2048
0.00.053.418 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.418 I llama_new_context_with_model: flash_attn    = 0
0.00.053.419 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.419 I llama_new_context_with_model: freq_scale    = 1
0.00.053.419 I ggml_metal_init: allocating
0.00.053.425 I ggml_metal_init: found device: Apple M4
0.00.053.428 I ggml_metal_init: picking default device: Apple M4
0.00.053.981 I ggml_metal_init: using embedded metal library
0.00.055.926 I ggml_metal_init: GPU name:   Apple M4
0.00.055.928 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.928 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.928 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.929 I ggml_metal_init: simdgroup reduction   = true
0.00.055.929 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.929 I ggml_metal_init: has bfloat            = true
0.00.055.929 I ggml_metal_init: use bfloat            = true
0.00.055.930 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.931 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.083.179 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.083.184 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.083.202 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.084.100 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.084.101 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.084.101 I llama_new_context_with_model: graph nodes  = 967
0.00.084.101 I llama_new_context_with_model: graph splits = 2
0.00.084.114 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.832.918 I main: llama threadpool init, n_threads = 4
0.00.832.962 I 
0.00.832.986 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 1 | LLAMAFILE = 1 | 
0.00.832.986 I 
0.00.833.163 I sampler seed: 1234
0.00.833.167 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.833.177 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.833.179 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.833.179 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, literature and art.

If one's life is not happy, one can have no other way but

0.01.670.808 I llama_perf_sampler_print:    sampling time =       1.36 ms /    71 runs   (    0.02 ms per token, 52244.30 tokens per second)
0.01.670.809 I llama_perf_context_print:        load time =     824.23 ms
0.01.670.810 I llama_perf_context_print: prompt eval time =      36.38 ms /     7 tokens (    5.20 ms per token,   192.39 tokens per second)
0.01.670.811 I llama_perf_context_print:        eval time =     798.56 ms /    63 runs   (   12.68 ms per token,    78.89 tokens per second)
0.01.670.812 I llama_perf_context_print:       total time =     837.90 ms /    70 tokens
0.01.671.015 I ggml_metal_free: deallocating

real	0m1.685s
user	0m0.106s
sys	0m0.200s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.108 I build: 4163 (8f419181) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.947 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.525 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.015.530 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.531 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.532 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.532 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.532 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.533 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.534 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.534 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.535 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.535 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.535 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.536 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.536 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.538 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.538 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.539 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.231 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.274 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.110 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.111 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.112 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.112 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.112 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.112 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.113 I llama_model_loader: - type  f32:  194 tensors
0.00.024.113 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.114 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.858 I llm_load_vocab: special tokens cache size = 25
0.00.052.076 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.082 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.082 I llm_load_print_meta: arch             = gptneox
0.00.052.083 I llm_load_print_meta: vocab type       = BPE
0.00.052.083 I llm_load_print_meta: n_vocab          = 50304
0.00.052.083 I llm_load_print_meta: n_merges         = 50009
0.00.052.083 I llm_load_print_meta: vocab_only       = 0
0.00.052.084 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.084 I llm_load_print_meta: n_embd           = 2048
0.00.052.084 I llm_load_print_meta: n_layer          = 24
0.00.052.088 I llm_load_print_meta: n_head           = 16
0.00.052.089 I llm_load_print_meta: n_head_kv        = 16
0.00.052.089 I llm_load_print_meta: n_rot            = 32
0.00.052.090 I llm_load_print_meta: n_swa            = 0
0.00.052.090 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.090 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.091 I llm_load_print_meta: n_gqa            = 1
0.00.052.091 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.092 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.092 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.092 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.093 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.093 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.093 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.094 I llm_load_print_meta: n_ff             = 8192
0.00.052.094 I llm_load_print_meta: n_expert         = 0
0.00.052.094 I llm_load_print_meta: n_expert_used    = 0
0.00.052.094 I llm_load_print_meta: causal attn      = 1
0.00.052.094 I llm_load_print_meta: pooling type     = 0
0.00.052.094 I llm_load_print_meta: rope type        = 2
0.00.052.094 I llm_load_print_meta: rope scaling     = linear
0.00.052.096 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.098 I llm_load_print_meta: freq_scale_train = 1
0.00.052.098 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.098 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.098 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.098 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.098 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.098 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.099 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.111 I llm_load_print_meta: model type       = 1.4B
0.00.052.111 I llm_load_print_meta: model ftype      = Q5_1
0.00.052.111 I llm_load_print_meta: model params     = 1.41 B
0.00.052.112 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.052.112 I llm_load_print_meta: general.name     = 1.4B
0.00.052.112 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.112 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.113 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.113 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.113 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.052.113 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.113 I llm_load_print_meta: max token length = 1024
0.00.054.080 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.080 I llm_load_tensors: offloading output layer to GPU
0.00.054.080 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.091 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.054.092 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.055.083 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.084 I llama_new_context_with_model: n_ctx         = 128
0.00.055.084 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.055.085 I llama_new_context_with_model: n_batch       = 128
0.00.055.085 I llama_new_context_with_model: n_ubatch      = 128
0.00.055.085 I llama_new_context_with_model: flash_attn    = 0
0.00.055.085 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.086 I llama_new_context_with_model: freq_scale    = 1
0.00.055.086 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.055.087 I ggml_metal_init: allocating
0.00.055.092 I ggml_metal_init: found device: Apple M4
0.00.055.096 I ggml_metal_init: picking default device: Apple M4
0.00.055.695 I ggml_metal_init: using embedded metal library
0.00.057.722 I ggml_metal_init: GPU name:   Apple M4
0.00.057.724 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.724 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.725 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.725 I ggml_metal_init: simdgroup reduction   = true
0.00.057.725 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.725 I ggml_metal_init: has bfloat            = true
0.00.057.725 I ggml_metal_init: use bfloat            = true
0.00.057.727 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.732 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.603 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.605 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.621 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.524 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.525 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.525 I llama_new_context_with_model: graph nodes  = 967
0.00.067.525 I llama_new_context_with_model: graph splits = 2
0.00.067.538 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.757.260 I 
0.00.757.307 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 1 | LLAMAFILE = 1 | 
0.00.757.352 I perplexity: tokenizing the input ..
0.00.765.263 I perplexity: tokenization took 7.909 ms
0.00.765.266 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.900.225 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.901.493 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.901.512 I llama_perf_context_print:        load time =     748.31 ms
0.00.901.513 I llama_perf_context_print: prompt eval time =     134.73 ms /   128 tokens (    1.05 ms per token,   950.03 tokens per second)
0.00.901.513 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.901.514 I llama_perf_context_print:       total time =     144.25 ms /   129 tokens
0.00.901.857 I ggml_metal_free: deallocating

real	0m0.915s
user	0m0.077s
sys	0m0.144s
```
- q2_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.038 I build: 4163 (8f419181) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.068 I main: llama backend init
0.00.000.070 I main: load the model and apply lora adapter, if any
0.00.010.181 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.897 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.902 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.904 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.905 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.905 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.905 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.906 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.907 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.907 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.907 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.907 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.908 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.908 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.908 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.912 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.912 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.912 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.675 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.681 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.394 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.396 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.396 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.396 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.397 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.397 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.397 I llama_model_loader: - type  f32:  194 tensors
0.00.024.398 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.398 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.398 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.434 I llm_load_vocab: special tokens cache size = 25
0.00.051.407 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.410 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.410 I llm_load_print_meta: arch             = gptneox
0.00.051.411 I llm_load_print_meta: vocab type       = BPE
0.00.051.411 I llm_load_print_meta: n_vocab          = 50304
0.00.051.411 I llm_load_print_meta: n_merges         = 50009
0.00.051.411 I llm_load_print_meta: vocab_only       = 0
0.00.051.412 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.412 I llm_load_print_meta: n_embd           = 2048
0.00.051.412 I llm_load_print_meta: n_layer          = 24
0.00.051.415 I llm_load_print_meta: n_head           = 16
0.00.051.416 I llm_load_print_meta: n_head_kv        = 16
0.00.051.416 I llm_load_print_meta: n_rot            = 32
0.00.051.416 I llm_load_print_meta: n_swa            = 0
0.00.051.416 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.416 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.417 I llm_load_print_meta: n_gqa            = 1
0.00.051.418 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.418 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.419 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.419 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.419 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.422 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.422 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.423 I llm_load_print_meta: n_ff             = 8192
0.00.051.423 I llm_load_print_meta: n_expert         = 0
0.00.051.425 I llm_load_print_meta: n_expert_used    = 0
0.00.051.426 I llm_load_print_meta: causal attn      = 1
0.00.051.426 I llm_load_print_meta: pooling type     = 0
0.00.051.426 I llm_load_print_meta: rope type        = 2
0.00.051.427 I llm_load_print_meta: rope scaling     = linear
0.00.051.427 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.428 I llm_load_print_meta: freq_scale_train = 1
0.00.051.428 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.428 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.428 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.428 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.429 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.429 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.429 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.440 I llm_load_print_meta: model type       = 1.4B
0.00.051.441 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.051.441 I llm_load_print_meta: model params     = 1.41 B
0.00.051.441 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.051.443 I llm_load_print_meta: general.name     = 1.4B
0.00.051.443 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.443 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.443 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.443 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.444 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.444 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.444 I llm_load_print_meta: max token length = 1024
0.00.053.368 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.368 I llm_load_tensors: offloading output layer to GPU
0.00.053.368 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.378 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.053.379 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.054.350 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.351 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.351 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.351 I llama_new_context_with_model: n_batch       = 2048
0.00.054.351 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.351 I llama_new_context_with_model: flash_attn    = 0
0.00.054.352 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.352 I llama_new_context_with_model: freq_scale    = 1
0.00.054.353 I ggml_metal_init: allocating
0.00.054.360 I ggml_metal_init: found device: Apple M4
0.00.054.362 I ggml_metal_init: picking default device: Apple M4
0.00.054.951 I ggml_metal_init: using embedded metal library
0.00.056.896 I ggml_metal_init: GPU name:   Apple M4
0.00.056.897 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.898 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.898 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.898 I ggml_metal_init: simdgroup reduction   = true
0.00.056.898 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.900 I ggml_metal_init: has bfloat            = true
0.00.056.900 I ggml_metal_init: use bfloat            = true
0.00.056.900 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.903 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.083.400 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.083.407 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.083.426 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.084.360 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.084.361 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.084.362 I llama_new_context_with_model: graph nodes  = 967
0.00.084.362 I llama_new_context_with_model: graph splits = 2
0.00.084.375 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.506.377 I main: llama threadpool init, n_threads = 4
0.00.506.409 I 
0.00.506.430 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 1 | LLAMAFILE = 1 | 
0.00.506.430 I 
0.00.506.655 I sampler seed: 1234
0.00.506.660 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.506.671 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.506.672 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.506.672 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.189.654 I llama_perf_sampler_print:    sampling time =       1.25 ms /    71 runs   (    0.02 ms per token, 56891.03 tokens per second)
0.01.189.655 I llama_perf_context_print:        load time =     496.19 ms
0.01.189.655 I llama_perf_context_print: prompt eval time =      35.84 ms /     7 tokens (    5.12 ms per token,   195.32 tokens per second)
0.01.189.656 I llama_perf_context_print:        eval time =     644.02 ms /    63 runs   (   10.22 ms per token,    97.82 tokens per second)
0.01.189.656 I llama_perf_context_print:       total time =     683.28 ms /    70 tokens
0.01.189.825 I ggml_metal_free: deallocating

real	0m1.207s
user	0m0.107s
sys	0m0.145s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.093 I build: 4163 (8f419181) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.786 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.238 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.243 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.245 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.245 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.246 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.246 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.246 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.247 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.248 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.248 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.248 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.249 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.249 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.249 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.251 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.251 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.251 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.945 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.961 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.675 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.676 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.676 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.676 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.677 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.677 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.023.677 I llama_model_loader: - type  f32:  194 tensors
0.00.023.678 I llama_model_loader: - type q2_K:   49 tensors
0.00.023.678 I llama_model_loader: - type q3_K:   48 tensors
0.00.023.678 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.745 I llm_load_vocab: special tokens cache size = 25
0.00.049.730 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.733 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.733 I llm_load_print_meta: arch             = gptneox
0.00.049.734 I llm_load_print_meta: vocab type       = BPE
0.00.049.734 I llm_load_print_meta: n_vocab          = 50304
0.00.049.734 I llm_load_print_meta: n_merges         = 50009
0.00.049.734 I llm_load_print_meta: vocab_only       = 0
0.00.049.734 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.734 I llm_load_print_meta: n_embd           = 2048
0.00.049.735 I llm_load_print_meta: n_layer          = 24
0.00.049.737 I llm_load_print_meta: n_head           = 16
0.00.049.738 I llm_load_print_meta: n_head_kv        = 16
0.00.049.738 I llm_load_print_meta: n_rot            = 32
0.00.049.740 I llm_load_print_meta: n_swa            = 0
0.00.049.741 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.741 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.741 I llm_load_print_meta: n_gqa            = 1
0.00.049.742 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.743 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.744 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.744 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.744 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.744 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.744 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.745 I llm_load_print_meta: n_ff             = 8192
0.00.049.745 I llm_load_print_meta: n_expert         = 0
0.00.049.745 I llm_load_print_meta: n_expert_used    = 0
0.00.049.745 I llm_load_print_meta: causal attn      = 1
0.00.049.746 I llm_load_print_meta: pooling type     = 0
0.00.049.746 I llm_load_print_meta: rope type        = 2
0.00.049.746 I llm_load_print_meta: rope scaling     = linear
0.00.049.747 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.751 I llm_load_print_meta: freq_scale_train = 1
0.00.049.751 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.752 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.753 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.753 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.753 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.754 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.754 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.768 I llm_load_print_meta: model type       = 1.4B
0.00.049.768 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.049.769 I llm_load_print_meta: model params     = 1.41 B
0.00.049.769 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.049.769 I llm_load_print_meta: general.name     = 1.4B
0.00.049.769 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.770 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.770 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.770 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.770 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.049.770 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.770 I llm_load_print_meta: max token length = 1024
0.00.051.625 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.625 I llm_load_tensors: offloading output layer to GPU
0.00.051.625 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.635 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.051.636 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.052.540 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.541 I llama_new_context_with_model: n_ctx         = 128
0.00.052.541 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.541 I llama_new_context_with_model: n_batch       = 128
0.00.052.541 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.541 I llama_new_context_with_model: flash_attn    = 0
0.00.052.542 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.542 I llama_new_context_with_model: freq_scale    = 1
0.00.052.542 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.543 I ggml_metal_init: allocating
0.00.052.546 I ggml_metal_init: found device: Apple M4
0.00.052.548 I ggml_metal_init: picking default device: Apple M4
0.00.053.099 I ggml_metal_init: using embedded metal library
0.00.055.059 I ggml_metal_init: GPU name:   Apple M4
0.00.055.060 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.061 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.061 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.061 I ggml_metal_init: simdgroup reduction   = true
0.00.055.062 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.062 I ggml_metal_init: has bfloat            = true
0.00.055.062 I ggml_metal_init: use bfloat            = true
0.00.055.062 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.064 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.240 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.064.244 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.064.260 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.065.218 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.065.219 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.065.220 I llama_new_context_with_model: graph nodes  = 967
0.00.065.220 I llama_new_context_with_model: graph splits = 2
0.00.065.233 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.435.176 I 
0.00.435.211 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 1 | LLAMAFILE = 1 | 
0.00.435.238 I perplexity: tokenizing the input ..
0.00.443.303 I perplexity: tokenization took 8.064 ms
0.00.443.307 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.575.395 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.576.550 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.576.566 I llama_perf_context_print:        load time =     425.39 ms
0.00.576.566 I llama_perf_context_print: prompt eval time =     131.85 ms /   128 tokens (    1.03 ms per token,   970.79 tokens per second)
0.00.576.567 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.576.568 I llama_perf_context_print:       total time =     141.39 ms /   129 tokens
0.00.576.951 I ggml_metal_free: deallocating

real	0m0.590s
user	0m0.075s
sys	0m0.083s
```
- q3_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.038 I build: 4163 (8f419181) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.068 I main: llama backend init
0.00.000.070 I main: load the model and apply lora adapter, if any
0.00.012.264 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.018.494 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.018.498 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.500 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.501 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.501 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.502 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.502 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.503 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.503 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.503 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.504 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.504 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.504 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.505 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.508 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.508 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.508 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.264 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.318 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.093 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.094 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.095 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.095 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.095 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.096 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.027.096 I llama_model_loader: - type  f32:  194 tensors
0.00.027.097 I llama_model_loader: - type q3_K:   25 tensors
0.00.027.097 I llama_model_loader: - type q4_K:   71 tensors
0.00.027.097 I llama_model_loader: - type q5_K:    1 tensors
0.00.027.097 I llama_model_loader: - type q6_K:    1 tensors
0.00.047.220 I llm_load_vocab: special tokens cache size = 25
0.00.053.209 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.053.212 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.053.212 I llm_load_print_meta: arch             = gptneox
0.00.053.212 I llm_load_print_meta: vocab type       = BPE
0.00.053.212 I llm_load_print_meta: n_vocab          = 50304
0.00.053.213 I llm_load_print_meta: n_merges         = 50009
0.00.053.213 I llm_load_print_meta: vocab_only       = 0
0.00.053.213 I llm_load_print_meta: n_ctx_train      = 2048
0.00.053.213 I llm_load_print_meta: n_embd           = 2048
0.00.053.213 I llm_load_print_meta: n_layer          = 24
0.00.053.216 I llm_load_print_meta: n_head           = 16
0.00.053.217 I llm_load_print_meta: n_head_kv        = 16
0.00.053.217 I llm_load_print_meta: n_rot            = 32
0.00.053.218 I llm_load_print_meta: n_swa            = 0
0.00.053.218 I llm_load_print_meta: n_embd_head_k    = 128
0.00.053.218 I llm_load_print_meta: n_embd_head_v    = 128
0.00.053.219 I llm_load_print_meta: n_gqa            = 1
0.00.053.220 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.053.221 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.053.224 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.053.224 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.053.224 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.053.224 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.053.225 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.053.225 I llm_load_print_meta: n_ff             = 8192
0.00.053.225 I llm_load_print_meta: n_expert         = 0
0.00.053.226 I llm_load_print_meta: n_expert_used    = 0
0.00.053.226 I llm_load_print_meta: causal attn      = 1
0.00.053.226 I llm_load_print_meta: pooling type     = 0
0.00.053.226 I llm_load_print_meta: rope type        = 2
0.00.053.226 I llm_load_print_meta: rope scaling     = linear
0.00.053.227 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.053.227 I llm_load_print_meta: freq_scale_train = 1
0.00.053.227 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.053.227 I llm_load_print_meta: rope_finetuned   = unknown
0.00.053.227 I llm_load_print_meta: ssm_d_conv       = 0
0.00.053.227 I llm_load_print_meta: ssm_d_inner      = 0
0.00.053.227 I llm_load_print_meta: ssm_d_state      = 0
0.00.053.228 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.053.228 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.053.239 I llm_load_print_meta: model type       = 1.4B
0.00.053.239 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.053.240 I llm_load_print_meta: model params     = 1.41 B
0.00.053.240 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.053.240 I llm_load_print_meta: general.name     = 1.4B
0.00.053.241 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.053.241 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.053.242 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.053.242 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.053.242 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.053.242 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.053.242 I llm_load_print_meta: max token length = 1024
0.00.054.770 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.770 I llm_load_tensors: offloading output layer to GPU
0.00.054.771 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.780 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.054.781 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.055.617 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.618 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.618 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.618 I llama_new_context_with_model: n_batch       = 2048
0.00.055.618 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.618 I llama_new_context_with_model: flash_attn    = 0
0.00.055.619 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.620 I llama_new_context_with_model: freq_scale    = 1
0.00.055.620 I ggml_metal_init: allocating
0.00.055.625 I ggml_metal_init: found device: Apple M4
0.00.055.627 I ggml_metal_init: picking default device: Apple M4
0.00.056.161 I ggml_metal_init: using embedded metal library
0.00.058.051 I ggml_metal_init: GPU name:   Apple M4
0.00.058.052 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.053 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.053 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.053 I ggml_metal_init: simdgroup reduction   = true
0.00.058.053 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.054 I ggml_metal_init: has bfloat            = true
0.00.058.054 I ggml_metal_init: use bfloat            = true
0.00.058.054 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.055 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.085.459 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.464 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.481 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.346 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.347 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.347 I llama_new_context_with_model: graph nodes  = 967
0.00.086.348 I llama_new_context_with_model: graph splits = 2
0.00.086.361 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.607.502 I main: llama threadpool init, n_threads = 4
0.00.607.539 I 
0.00.607.558 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 1 | LLAMAFILE = 1 | 
0.00.607.558 I 
0.00.607.714 I sampler seed: 1234
0.00.607.719 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.607.749 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.607.752 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.607.753 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do that is most useful to my fellow men?'"

-Albert Einstein

"The way

0.01.350.709 I llama_perf_sampler_print:    sampling time =       1.33 ms /    71 runs   (    0.02 ms per token, 53383.46 tokens per second)
0.01.350.710 I llama_perf_context_print:        load time =     595.23 ms
0.01.350.711 I llama_perf_context_print: prompt eval time =      35.51 ms /     7 tokens (    5.07 ms per token,   197.16 tokens per second)
0.01.350.712 I llama_perf_context_print:        eval time =     704.46 ms /    63 runs   (   11.18 ms per token,    89.43 tokens per second)
0.01.350.713 I llama_perf_context_print:       total time =     743.21 ms /    70 tokens
0.01.350.888 I ggml_metal_free: deallocating

real	0m1.365s
user	0m0.106s
sys	0m0.164s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.084 I build: 4163 (8f419181) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.273 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.096 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.014.101 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.102 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.103 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.103 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.103 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.104 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.105 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.105 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.105 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.106 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.108 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.109 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.109 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.111 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.111 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.111 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.017.745 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.018.788 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.022.438 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.022.439 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.022.439 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.022.439 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.022.440 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.022.440 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.022.441 I llama_model_loader: - type  f32:  194 tensors
0.00.022.441 I llama_model_loader: - type q3_K:   25 tensors
0.00.022.441 I llama_model_loader: - type q4_K:   71 tensors
0.00.022.442 I llama_model_loader: - type q5_K:    1 tensors
0.00.022.442 I llama_model_loader: - type q6_K:    1 tensors
0.00.042.352 I llm_load_vocab: special tokens cache size = 25
0.00.048.141 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.048.144 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.048.144 I llm_load_print_meta: arch             = gptneox
0.00.048.144 I llm_load_print_meta: vocab type       = BPE
0.00.048.145 I llm_load_print_meta: n_vocab          = 50304
0.00.048.145 I llm_load_print_meta: n_merges         = 50009
0.00.048.145 I llm_load_print_meta: vocab_only       = 0
0.00.048.145 I llm_load_print_meta: n_ctx_train      = 2048
0.00.048.145 I llm_load_print_meta: n_embd           = 2048
0.00.048.145 I llm_load_print_meta: n_layer          = 24
0.00.048.148 I llm_load_print_meta: n_head           = 16
0.00.048.148 I llm_load_print_meta: n_head_kv        = 16
0.00.048.148 I llm_load_print_meta: n_rot            = 32
0.00.048.149 I llm_load_print_meta: n_swa            = 0
0.00.048.149 I llm_load_print_meta: n_embd_head_k    = 128
0.00.048.149 I llm_load_print_meta: n_embd_head_v    = 128
0.00.048.150 I llm_load_print_meta: n_gqa            = 1
0.00.048.150 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.048.151 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.048.151 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.048.152 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.048.152 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.048.152 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.048.152 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.048.153 I llm_load_print_meta: n_ff             = 8192
0.00.048.153 I llm_load_print_meta: n_expert         = 0
0.00.048.153 I llm_load_print_meta: n_expert_used    = 0
0.00.048.153 I llm_load_print_meta: causal attn      = 1
0.00.048.154 I llm_load_print_meta: pooling type     = 0
0.00.048.154 I llm_load_print_meta: rope type        = 2
0.00.048.154 I llm_load_print_meta: rope scaling     = linear
0.00.048.154 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.048.155 I llm_load_print_meta: freq_scale_train = 1
0.00.048.155 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.048.155 I llm_load_print_meta: rope_finetuned   = unknown
0.00.048.155 I llm_load_print_meta: ssm_d_conv       = 0
0.00.048.155 I llm_load_print_meta: ssm_d_inner      = 0
0.00.048.155 I llm_load_print_meta: ssm_d_state      = 0
0.00.048.156 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.048.156 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.048.168 I llm_load_print_meta: model type       = 1.4B
0.00.048.170 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.048.170 I llm_load_print_meta: model params     = 1.41 B
0.00.048.171 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.048.171 I llm_load_print_meta: general.name     = 1.4B
0.00.048.172 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.048.172 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.048.172 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.048.172 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.048.172 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.048.173 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.048.173 I llm_load_print_meta: max token length = 1024
0.00.050.117 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.050.117 I llm_load_tensors: offloading output layer to GPU
0.00.050.117 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.050.127 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.050.128 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.051.043 I llama_new_context_with_model: n_seq_max     = 1
0.00.051.044 I llama_new_context_with_model: n_ctx         = 128
0.00.051.044 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.051.044 I llama_new_context_with_model: n_batch       = 128
0.00.051.044 I llama_new_context_with_model: n_ubatch      = 128
0.00.051.045 I llama_new_context_with_model: flash_attn    = 0
0.00.051.045 I llama_new_context_with_model: freq_base     = 10000.0
0.00.051.045 I llama_new_context_with_model: freq_scale    = 1
0.00.051.046 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.046 I ggml_metal_init: allocating
0.00.051.049 I ggml_metal_init: found device: Apple M4
0.00.051.051 I ggml_metal_init: picking default device: Apple M4
0.00.051.576 I ggml_metal_init: using embedded metal library
0.00.053.518 I ggml_metal_init: GPU name:   Apple M4
0.00.053.520 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.053.520 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.053.520 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.053.521 I ggml_metal_init: simdgroup reduction   = true
0.00.053.521 I ggml_metal_init: simdgroup matrix mul. = true
0.00.053.521 I ggml_metal_init: has bfloat            = true
0.00.053.521 I ggml_metal_init: use bfloat            = true
0.00.053.522 I ggml_metal_init: hasUnifiedMemory      = true
0.00.053.522 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.062.547 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.062.551 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.062.564 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.063.492 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.063.493 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.063.493 I llama_new_context_with_model: graph nodes  = 967
0.00.063.494 I llama_new_context_with_model: graph splits = 2
0.00.063.506 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.513.305 I 
0.00.513.353 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 1 | LLAMAFILE = 1 | 
0.00.513.385 I perplexity: tokenizing the input ..
0.00.521.278 I perplexity: tokenization took 7.892 ms
0.00.521.283 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.653.331 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.654.593 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.654.611 I llama_perf_context_print:        load time =     505.03 ms
0.00.654.612 I llama_perf_context_print: prompt eval time =     131.82 ms /   128 tokens (    1.03 ms per token,   971.04 tokens per second)
0.00.654.613 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.654.614 I llama_perf_context_print:       total time =     141.31 ms /   129 tokens
0.00.655.023 I ggml_metal_free: deallocating

real	0m0.667s
user	0m0.074s
sys	0m0.098s
```
- q4_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.038 I build: 4163 (8f419181) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.069 I main: llama backend init
0.00.000.071 I main: load the model and apply lora adapter, if any
0.00.008.924 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.300 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.015.305 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.307 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.309 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.309 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.309 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.310 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.311 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.311 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.311 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.312 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.312 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.312 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.313 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.315 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.316 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.316 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.199 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.279 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.130 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.132 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.132 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.132 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.132 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.133 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.133 I llama_model_loader: - type  f32:  194 tensors
0.00.024.134 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.134 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.134 I llama_model_loader: - type q6_K:   13 tensors
0.00.045.130 I llm_load_vocab: special tokens cache size = 25
0.00.050.931 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.934 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.934 I llm_load_print_meta: arch             = gptneox
0.00.050.934 I llm_load_print_meta: vocab type       = BPE
0.00.050.935 I llm_load_print_meta: n_vocab          = 50304
0.00.050.935 I llm_load_print_meta: n_merges         = 50009
0.00.050.935 I llm_load_print_meta: vocab_only       = 0
0.00.050.935 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.935 I llm_load_print_meta: n_embd           = 2048
0.00.050.936 I llm_load_print_meta: n_layer          = 24
0.00.050.939 I llm_load_print_meta: n_head           = 16
0.00.050.939 I llm_load_print_meta: n_head_kv        = 16
0.00.050.940 I llm_load_print_meta: n_rot            = 32
0.00.050.940 I llm_load_print_meta: n_swa            = 0
0.00.050.940 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.940 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.941 I llm_load_print_meta: n_gqa            = 1
0.00.050.942 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.942 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.943 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.943 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.943 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.944 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.944 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.953 I llm_load_print_meta: n_ff             = 8192
0.00.050.956 I llm_load_print_meta: n_expert         = 0
0.00.050.957 I llm_load_print_meta: n_expert_used    = 0
0.00.050.958 I llm_load_print_meta: causal attn      = 1
0.00.050.959 I llm_load_print_meta: pooling type     = 0
0.00.050.959 I llm_load_print_meta: rope type        = 2
0.00.050.959 I llm_load_print_meta: rope scaling     = linear
0.00.050.960 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.960 I llm_load_print_meta: freq_scale_train = 1
0.00.050.960 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.960 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.960 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.960 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.960 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.961 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.961 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.968 I llm_load_print_meta: model type       = 1.4B
0.00.050.968 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.050.969 I llm_load_print_meta: model params     = 1.41 B
0.00.050.970 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.050.970 I llm_load_print_meta: general.name     = 1.4B
0.00.050.970 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.971 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.971 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.971 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.972 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.974 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.974 I llm_load_print_meta: max token length = 1024
0.00.052.763 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.763 I llm_load_tensors: offloading output layer to GPU
0.00.052.763 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.768 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.052.769 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.054.043 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.044 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.044 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.044 I llama_new_context_with_model: n_batch       = 2048
0.00.054.045 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.045 I llama_new_context_with_model: flash_attn    = 0
0.00.054.045 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.045 I llama_new_context_with_model: freq_scale    = 1
0.00.054.046 I ggml_metal_init: allocating
0.00.054.049 I ggml_metal_init: found device: Apple M4
0.00.054.051 I ggml_metal_init: picking default device: Apple M4
0.00.054.636 I ggml_metal_init: using embedded metal library
0.00.056.607 I ggml_metal_init: GPU name:   Apple M4
0.00.056.608 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.609 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.609 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.611 I ggml_metal_init: simdgroup reduction   = true
0.00.056.611 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.611 I ggml_metal_init: has bfloat            = true
0.00.056.611 I ggml_metal_init: use bfloat            = true
0.00.056.612 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.614 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.084.990 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.997 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.016 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.117 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.118 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.118 I llama_new_context_with_model: graph nodes  = 967
0.00.086.119 I llama_new_context_with_model: graph splits = 2
0.00.086.132 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.685.745 I main: llama threadpool init, n_threads = 4
0.00.685.777 I 
0.00.685.794 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 1 | LLAMAFILE = 1 | 
0.00.685.794 I 
0.00.686.034 I sampler seed: 1234
0.00.686.038 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.686.058 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.686.059 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.686.059 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the greatest power to create the future we want to see. I believe that we are in a wonderful time when people from all over the world are coming together to create a better world. I believe that we are going

0.01.441.157 I llama_perf_sampler_print:    sampling time =       1.28 ms /    71 runs   (    0.02 ms per token, 55425.45 tokens per second)
0.01.441.158 I llama_perf_context_print:        load time =     676.82 ms
0.01.441.159 I llama_perf_context_print: prompt eval time =      36.48 ms /     7 tokens (    5.21 ms per token,   191.86 tokens per second)
0.01.441.159 I llama_perf_context_print:        eval time =     715.51 ms /    63 runs   (   11.36 ms per token,    88.05 tokens per second)
0.01.441.159 I llama_perf_context_print:       total time =     755.41 ms /    70 tokens
0.01.441.333 I ggml_metal_free: deallocating

real	0m1.457s
user	0m0.109s
sys	0m0.184s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.090 I build: 4163 (8f419181) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.710 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.692 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.014.697 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.698 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.699 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.699 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.700 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.700 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.701 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.701 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.701 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.702 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.702 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.702 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.703 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.706 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.706 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.706 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.580 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.648 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.445 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.446 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.447 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.447 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.447 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.447 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.023.448 I llama_model_loader: - type  f32:  194 tensors
0.00.023.448 I llama_model_loader: - type q4_K:   61 tensors
0.00.023.448 I llama_model_loader: - type q5_K:   24 tensors
0.00.023.448 I llama_model_loader: - type q6_K:   13 tensors
0.00.044.057 I llm_load_vocab: special tokens cache size = 25
0.00.050.149 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.151 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.151 I llm_load_print_meta: arch             = gptneox
0.00.050.152 I llm_load_print_meta: vocab type       = BPE
0.00.050.152 I llm_load_print_meta: n_vocab          = 50304
0.00.050.152 I llm_load_print_meta: n_merges         = 50009
0.00.050.152 I llm_load_print_meta: vocab_only       = 0
0.00.050.153 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.153 I llm_load_print_meta: n_embd           = 2048
0.00.050.153 I llm_load_print_meta: n_layer          = 24
0.00.050.155 I llm_load_print_meta: n_head           = 16
0.00.050.156 I llm_load_print_meta: n_head_kv        = 16
0.00.050.156 I llm_load_print_meta: n_rot            = 32
0.00.050.156 I llm_load_print_meta: n_swa            = 0
0.00.050.156 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.157 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.157 I llm_load_print_meta: n_gqa            = 1
0.00.050.158 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.159 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.159 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.160 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.160 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.160 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.160 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.161 I llm_load_print_meta: n_ff             = 8192
0.00.050.161 I llm_load_print_meta: n_expert         = 0
0.00.050.161 I llm_load_print_meta: n_expert_used    = 0
0.00.050.161 I llm_load_print_meta: causal attn      = 1
0.00.050.161 I llm_load_print_meta: pooling type     = 0
0.00.050.161 I llm_load_print_meta: rope type        = 2
0.00.050.162 I llm_load_print_meta: rope scaling     = linear
0.00.050.162 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.162 I llm_load_print_meta: freq_scale_train = 1
0.00.050.162 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.163 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.163 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.165 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.165 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.165 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.166 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.177 I llm_load_print_meta: model type       = 1.4B
0.00.050.178 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.050.178 I llm_load_print_meta: model params     = 1.41 B
0.00.050.179 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.050.179 I llm_load_print_meta: general.name     = 1.4B
0.00.050.180 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.180 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.180 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.180 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.181 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.182 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.182 I llm_load_print_meta: max token length = 1024
0.00.051.885 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.885 I llm_load_tensors: offloading output layer to GPU
0.00.051.886 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.895 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.051.896 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.052.768 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.769 I llama_new_context_with_model: n_ctx         = 128
0.00.052.769 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.769 I llama_new_context_with_model: n_batch       = 128
0.00.052.770 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.770 I llama_new_context_with_model: flash_attn    = 0
0.00.052.770 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.770 I llama_new_context_with_model: freq_scale    = 1
0.00.052.771 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.771 I ggml_metal_init: allocating
0.00.052.776 I ggml_metal_init: found device: Apple M4
0.00.052.778 I ggml_metal_init: picking default device: Apple M4
0.00.053.321 I ggml_metal_init: using embedded metal library
0.00.055.257 I ggml_metal_init: GPU name:   Apple M4
0.00.055.258 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.259 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.259 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.259 I ggml_metal_init: simdgroup reduction   = true
0.00.055.259 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.260 I ggml_metal_init: has bfloat            = true
0.00.055.260 I ggml_metal_init: use bfloat            = true
0.00.055.260 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.261 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.387 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.064.391 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.064.405 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.065.301 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.065.302 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.065.302 I llama_new_context_with_model: graph nodes  = 967
0.00.065.302 I llama_new_context_with_model: graph splits = 2
0.00.065.315 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.602.287 I 
0.00.602.327 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 1 | LLAMAFILE = 1 | 
0.00.602.359 I perplexity: tokenizing the input ..
0.00.610.251 I perplexity: tokenization took 7.891 ms
0.00.610.253 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.744.908 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.746.140 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.746.152 I llama_perf_context_print:        load time =     593.57 ms
0.00.746.153 I llama_perf_context_print: prompt eval time =     134.43 ms /   128 tokens (    1.05 ms per token,   952.16 tokens per second)
0.00.746.154 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.746.154 I llama_perf_context_print:       total time =     143.87 ms /   129 tokens
0.00.746.483 I ggml_metal_free: deallocating

real	0m0.758s
user	0m0.076s
sys	0m0.120s
```
- q5_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.033 I build: 4163 (8f419181) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.064 I main: llama backend init
0.00.000.066 I main: load the model and apply lora adapter, if any
0.00.009.950 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.542 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.546 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.548 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.549 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.551 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.551 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.551 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.552 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.552 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.553 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.553 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.553 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.554 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.554 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.556 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.556 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.556 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.333 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.410 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.146 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.147 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.147 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.148 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.148 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.148 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.149 I llama_model_loader: - type  f32:  194 tensors
0.00.025.149 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.149 I llama_model_loader: - type q6_K:   37 tensors
0.00.045.703 I llm_load_vocab: special tokens cache size = 25
0.00.051.848 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.852 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.852 I llm_load_print_meta: arch             = gptneox
0.00.051.852 I llm_load_print_meta: vocab type       = BPE
0.00.051.853 I llm_load_print_meta: n_vocab          = 50304
0.00.051.853 I llm_load_print_meta: n_merges         = 50009
0.00.051.853 I llm_load_print_meta: vocab_only       = 0
0.00.051.853 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.853 I llm_load_print_meta: n_embd           = 2048
0.00.051.853 I llm_load_print_meta: n_layer          = 24
0.00.051.857 I llm_load_print_meta: n_head           = 16
0.00.051.858 I llm_load_print_meta: n_head_kv        = 16
0.00.051.858 I llm_load_print_meta: n_rot            = 32
0.00.051.858 I llm_load_print_meta: n_swa            = 0
0.00.051.858 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.858 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.859 I llm_load_print_meta: n_gqa            = 1
0.00.051.860 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.860 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.861 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.862 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.862 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.862 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.862 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.863 I llm_load_print_meta: n_ff             = 8192
0.00.051.863 I llm_load_print_meta: n_expert         = 0
0.00.051.863 I llm_load_print_meta: n_expert_used    = 0
0.00.051.865 I llm_load_print_meta: causal attn      = 1
0.00.051.866 I llm_load_print_meta: pooling type     = 0
0.00.051.866 I llm_load_print_meta: rope type        = 2
0.00.051.867 I llm_load_print_meta: rope scaling     = linear
0.00.051.867 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.867 I llm_load_print_meta: freq_scale_train = 1
0.00.051.867 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.868 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.868 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.868 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.868 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.868 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.869 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.882 I llm_load_print_meta: model type       = 1.4B
0.00.051.882 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.051.882 I llm_load_print_meta: model params     = 1.41 B
0.00.051.883 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.051.883 I llm_load_print_meta: general.name     = 1.4B
0.00.051.883 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.883 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.883 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.894 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.894 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.894 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.895 I llm_load_print_meta: max token length = 1024
0.00.053.453 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.454 I llm_load_tensors: offloading output layer to GPU
0.00.053.454 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.464 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.053.466 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.054.319 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.320 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.320 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.320 I llama_new_context_with_model: n_batch       = 2048
0.00.054.321 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.321 I llama_new_context_with_model: flash_attn    = 0
0.00.054.321 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.321 I llama_new_context_with_model: freq_scale    = 1
0.00.054.322 I ggml_metal_init: allocating
0.00.054.326 I ggml_metal_init: found device: Apple M4
0.00.054.328 I ggml_metal_init: picking default device: Apple M4
0.00.054.897 I ggml_metal_init: using embedded metal library
0.00.056.860 I ggml_metal_init: GPU name:   Apple M4
0.00.056.862 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.863 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.863 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.864 I ggml_metal_init: simdgroup reduction   = true
0.00.056.866 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.867 I ggml_metal_init: has bfloat            = true
0.00.056.868 I ggml_metal_init: use bfloat            = true
0.00.056.868 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.869 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.084.939 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.947 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.970 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.085.914 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.085.915 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.085.916 I llama_new_context_with_model: graph nodes  = 967
0.00.085.916 I llama_new_context_with_model: graph splits = 2
0.00.085.930 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.764.577 I main: llama threadpool init, n_threads = 4
0.00.764.619 I 
0.00.764.646 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 1 | LLAMAFILE = 1 | 
0.00.764.647 I 
0.00.764.891 I sampler seed: 1234
0.00.764.896 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.764.908 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.764.908 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.764.908 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.611.069 I llama_perf_sampler_print:    sampling time =       1.40 ms /    71 runs   (    0.02 ms per token, 50823.19 tokens per second)
0.01.611.069 I llama_perf_context_print:        load time =     754.62 ms
0.01.611.070 I llama_perf_context_print: prompt eval time =      38.69 ms /     7 tokens (    5.53 ms per token,   180.93 tokens per second)
0.01.611.071 I llama_perf_context_print:        eval time =     804.54 ms /    63 runs   (   12.77 ms per token,    78.31 tokens per second)
0.01.611.073 I llama_perf_context_print:       total time =     846.50 ms /    70 tokens
0.01.611.270 I ggml_metal_free: deallocating

real	0m1.627s
user	0m0.107s
sys	0m0.204s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.091 I build: 4163 (8f419181) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.430 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.345 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.349 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.355 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.356 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.356 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.356 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.357 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.358 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.358 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.358 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.358 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.359 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.359 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.359 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.361 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.361 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.361 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.109 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.123 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.912 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.913 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.913 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.914 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.914 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.914 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.023.915 I llama_model_loader: - type  f32:  194 tensors
0.00.023.915 I llama_model_loader: - type q5_K:   61 tensors
0.00.023.915 I llama_model_loader: - type q6_K:   37 tensors
0.00.043.964 I llm_load_vocab: special tokens cache size = 25
0.00.050.134 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.139 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.139 I llm_load_print_meta: arch             = gptneox
0.00.050.139 I llm_load_print_meta: vocab type       = BPE
0.00.050.139 I llm_load_print_meta: n_vocab          = 50304
0.00.050.140 I llm_load_print_meta: n_merges         = 50009
0.00.050.140 I llm_load_print_meta: vocab_only       = 0
0.00.050.142 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.142 I llm_load_print_meta: n_embd           = 2048
0.00.050.142 I llm_load_print_meta: n_layer          = 24
0.00.050.167 I llm_load_print_meta: n_head           = 16
0.00.050.169 I llm_load_print_meta: n_head_kv        = 16
0.00.050.169 I llm_load_print_meta: n_rot            = 32
0.00.050.169 I llm_load_print_meta: n_swa            = 0
0.00.050.169 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.170 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.170 I llm_load_print_meta: n_gqa            = 1
0.00.050.171 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.171 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.172 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.172 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.172 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.173 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.173 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.173 I llm_load_print_meta: n_ff             = 8192
0.00.050.173 I llm_load_print_meta: n_expert         = 0
0.00.050.173 I llm_load_print_meta: n_expert_used    = 0
0.00.050.174 I llm_load_print_meta: causal attn      = 1
0.00.050.174 I llm_load_print_meta: pooling type     = 0
0.00.050.175 I llm_load_print_meta: rope type        = 2
0.00.050.176 I llm_load_print_meta: rope scaling     = linear
0.00.050.176 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.176 I llm_load_print_meta: freq_scale_train = 1
0.00.050.177 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.177 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.177 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.177 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.177 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.177 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.177 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.190 I llm_load_print_meta: model type       = 1.4B
0.00.050.192 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.050.192 I llm_load_print_meta: model params     = 1.41 B
0.00.050.193 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.050.193 I llm_load_print_meta: general.name     = 1.4B
0.00.050.193 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.193 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.193 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.194 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.195 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.195 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.195 I llm_load_print_meta: max token length = 1024
0.00.052.129 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.129 I llm_load_tensors: offloading output layer to GPU
0.00.052.129 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.140 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.052.140 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.053.141 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.142 I llama_new_context_with_model: n_ctx         = 128
0.00.053.142 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.142 I llama_new_context_with_model: n_batch       = 128
0.00.053.142 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.142 I llama_new_context_with_model: flash_attn    = 0
0.00.053.143 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.143 I llama_new_context_with_model: freq_scale    = 1
0.00.053.143 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.144 I ggml_metal_init: allocating
0.00.053.152 I ggml_metal_init: found device: Apple M4
0.00.053.154 I ggml_metal_init: picking default device: Apple M4
0.00.053.754 I ggml_metal_init: using embedded metal library
0.00.055.970 I ggml_metal_init: GPU name:   Apple M4
0.00.055.972 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.973 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.973 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.973 I ggml_metal_init: simdgroup reduction   = true
0.00.055.973 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.974 I ggml_metal_init: has bfloat            = true
0.00.055.974 I ggml_metal_init: use bfloat            = true
0.00.055.974 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.975 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.826 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.064.829 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.064.843 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.065.712 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.065.713 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.065.713 I llama_new_context_with_model: graph nodes  = 967
0.00.065.713 I llama_new_context_with_model: graph splits = 2
0.00.065.726 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.691.051 I 
0.00.691.110 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 1 | LLAMAFILE = 1 | 
0.00.691.145 I perplexity: tokenizing the input ..
0.00.699.514 I perplexity: tokenization took 8.367 ms
0.00.699.518 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.840.211 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.841.473 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.841.488 I llama_perf_context_print:        load time =     681.61 ms
0.00.841.489 I llama_perf_context_print: prompt eval time =     140.45 ms /   128 tokens (    1.10 ms per token,   911.35 tokens per second)
0.00.841.490 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.841.491 I llama_perf_context_print:       total time =     150.45 ms /   129 tokens
0.00.842.037 I ggml_metal_free: deallocating

real	0m0.856s
user	0m0.075s
sys	0m0.143s
```
- q6_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.036 I build: 4163 (8f419181) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.067 I main: llama backend init
0.00.000.069 I main: load the model and apply lora adapter, if any
0.00.008.703 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.027.835 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.027.839 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.027.840 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.027.841 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.027.841 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.027.841 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.027.841 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.027.842 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.027.842 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.027.843 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.027.843 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.027.843 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.027.844 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.027.844 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.027.847 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.027.847 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.027.847 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.031.719 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.032.841 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.036.665 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.036.666 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.036.667 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.036.667 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.036.667 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.036.668 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.036.668 I llama_model_loader: - type  f32:  194 tensors
0.00.036.669 I llama_model_loader: - type q6_K:   98 tensors
0.00.060.565 I llm_load_vocab: special tokens cache size = 25
0.00.067.437 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.067.440 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.067.440 I llm_load_print_meta: arch             = gptneox
0.00.067.441 I llm_load_print_meta: vocab type       = BPE
0.00.067.441 I llm_load_print_meta: n_vocab          = 50304
0.00.067.441 I llm_load_print_meta: n_merges         = 50009
0.00.067.441 I llm_load_print_meta: vocab_only       = 0
0.00.067.441 I llm_load_print_meta: n_ctx_train      = 2048
0.00.067.441 I llm_load_print_meta: n_embd           = 2048
0.00.067.442 I llm_load_print_meta: n_layer          = 24
0.00.067.445 I llm_load_print_meta: n_head           = 16
0.00.067.445 I llm_load_print_meta: n_head_kv        = 16
0.00.067.445 I llm_load_print_meta: n_rot            = 32
0.00.067.448 I llm_load_print_meta: n_swa            = 0
0.00.067.449 I llm_load_print_meta: n_embd_head_k    = 128
0.00.067.449 I llm_load_print_meta: n_embd_head_v    = 128
0.00.067.449 I llm_load_print_meta: n_gqa            = 1
0.00.067.450 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.067.451 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.067.451 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.067.451 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.067.452 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.067.452 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.067.453 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.067.454 I llm_load_print_meta: n_ff             = 8192
0.00.067.454 I llm_load_print_meta: n_expert         = 0
0.00.067.454 I llm_load_print_meta: n_expert_used    = 0
0.00.067.454 I llm_load_print_meta: causal attn      = 1
0.00.067.455 I llm_load_print_meta: pooling type     = 0
0.00.067.455 I llm_load_print_meta: rope type        = 2
0.00.067.455 I llm_load_print_meta: rope scaling     = linear
0.00.067.456 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.067.456 I llm_load_print_meta: freq_scale_train = 1
0.00.067.456 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.067.456 I llm_load_print_meta: rope_finetuned   = unknown
0.00.067.457 I llm_load_print_meta: ssm_d_conv       = 0
0.00.067.457 I llm_load_print_meta: ssm_d_inner      = 0
0.00.067.460 I llm_load_print_meta: ssm_d_state      = 0
0.00.067.460 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.067.460 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.067.471 I llm_load_print_meta: model type       = 1.4B
0.00.067.472 I llm_load_print_meta: model ftype      = Q6_K
0.00.067.472 I llm_load_print_meta: model params     = 1.41 B
0.00.067.472 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.067.473 I llm_load_print_meta: general.name     = 1.4B
0.00.067.473 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.067.473 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.067.473 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.067.473 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.067.473 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.067.474 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.067.474 I llm_load_print_meta: max token length = 1024
0.00.069.189 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.069.189 I llm_load_tensors: offloading output layer to GPU
0.00.069.189 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.069.199 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.069.200 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.070.143 I llama_new_context_with_model: n_seq_max     = 1
0.00.070.144 I llama_new_context_with_model: n_ctx         = 2048
0.00.070.144 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.070.144 I llama_new_context_with_model: n_batch       = 2048
0.00.070.145 I llama_new_context_with_model: n_ubatch      = 512
0.00.070.145 I llama_new_context_with_model: flash_attn    = 0
0.00.070.145 I llama_new_context_with_model: freq_base     = 10000.0
0.00.070.146 I llama_new_context_with_model: freq_scale    = 1
0.00.070.146 I ggml_metal_init: allocating
0.00.070.149 I ggml_metal_init: found device: Apple M4
0.00.070.151 I ggml_metal_init: picking default device: Apple M4
0.00.070.735 I ggml_metal_init: using embedded metal library
0.00.073.034 I ggml_metal_init: GPU name:   Apple M4
0.00.073.036 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.073.036 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.073.037 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.073.037 I ggml_metal_init: simdgroup reduction   = true
0.00.073.038 I ggml_metal_init: simdgroup matrix mul. = true
0.00.073.039 I ggml_metal_init: has bfloat            = true
0.00.073.039 I ggml_metal_init: use bfloat            = true
0.00.073.040 I ggml_metal_init: hasUnifiedMemory      = true
0.00.073.042 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.103.175 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.103.180 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.103.199 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.104.172 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.104.173 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.104.173 I llama_new_context_with_model: graph nodes  = 967
0.00.104.174 I llama_new_context_with_model: graph splits = 2
0.00.104.186 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.954.610 I main: llama threadpool init, n_threads = 4
0.00.954.644 I 
0.00.954.665 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 1 | LLAMAFILE = 1 | 
0.00.954.665 I 
0.00.954.896 I sampler seed: 1234
0.00.954.901 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.954.950 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.954.954 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.954.955 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.834.403 I llama_perf_sampler_print:    sampling time =       1.32 ms /    71 runs   (    0.02 ms per token, 53951.37 tokens per second)
0.01.834.404 I llama_perf_context_print:        load time =     945.90 ms
0.01.834.405 I llama_perf_context_print: prompt eval time =      38.56 ms /     7 tokens (    5.51 ms per token,   181.54 tokens per second)
0.01.834.405 I llama_perf_context_print:        eval time =     837.80 ms /    63 runs   (   13.30 ms per token,    75.20 tokens per second)
0.01.834.406 I llama_perf_context_print:       total time =     879.80 ms /    70 tokens
0.01.834.570 I ggml_metal_free: deallocating

real	0m1.849s
user	0m0.118s
sys	0m0.233s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.090 I build: 4163 (8f419181) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.320 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.047 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.051 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.057 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.057 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.058 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.058 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.058 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.061 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.061 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.062 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.062 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.062 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.063 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.066 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.068 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.069 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.069 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.781 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.841 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.617 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.619 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.619 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.619 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.619 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.620 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.023.620 I llama_model_loader: - type  f32:  194 tensors
0.00.023.621 I llama_model_loader: - type q6_K:   98 tensors
0.00.043.634 I llm_load_vocab: special tokens cache size = 25
0.00.049.553 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.555 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.556 I llm_load_print_meta: arch             = gptneox
0.00.049.556 I llm_load_print_meta: vocab type       = BPE
0.00.049.556 I llm_load_print_meta: n_vocab          = 50304
0.00.049.557 I llm_load_print_meta: n_merges         = 50009
0.00.049.557 I llm_load_print_meta: vocab_only       = 0
0.00.049.557 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.557 I llm_load_print_meta: n_embd           = 2048
0.00.049.557 I llm_load_print_meta: n_layer          = 24
0.00.049.560 I llm_load_print_meta: n_head           = 16
0.00.049.561 I llm_load_print_meta: n_head_kv        = 16
0.00.049.561 I llm_load_print_meta: n_rot            = 32
0.00.049.561 I llm_load_print_meta: n_swa            = 0
0.00.049.561 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.562 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.562 I llm_load_print_meta: n_gqa            = 1
0.00.049.563 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.564 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.566 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.566 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.566 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.566 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.566 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.567 I llm_load_print_meta: n_ff             = 8192
0.00.049.567 I llm_load_print_meta: n_expert         = 0
0.00.049.567 I llm_load_print_meta: n_expert_used    = 0
0.00.049.568 I llm_load_print_meta: causal attn      = 1
0.00.049.568 I llm_load_print_meta: pooling type     = 0
0.00.049.568 I llm_load_print_meta: rope type        = 2
0.00.049.576 I llm_load_print_meta: rope scaling     = linear
0.00.049.578 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.578 I llm_load_print_meta: freq_scale_train = 1
0.00.049.578 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.579 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.579 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.579 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.579 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.579 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.579 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.592 I llm_load_print_meta: model type       = 1.4B
0.00.049.592 I llm_load_print_meta: model ftype      = Q6_K
0.00.049.592 I llm_load_print_meta: model params     = 1.41 B
0.00.049.593 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.049.593 I llm_load_print_meta: general.name     = 1.4B
0.00.049.593 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.593 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.593 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.594 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.594 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.049.596 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.596 I llm_load_print_meta: max token length = 1024
0.00.051.580 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.580 I llm_load_tensors: offloading output layer to GPU
0.00.051.580 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.590 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.051.591 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.052.539 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.540 I llama_new_context_with_model: n_ctx         = 128
0.00.052.540 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.540 I llama_new_context_with_model: n_batch       = 128
0.00.052.541 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.541 I llama_new_context_with_model: flash_attn    = 0
0.00.052.541 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.541 I llama_new_context_with_model: freq_scale    = 1
0.00.052.542 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.542 I ggml_metal_init: allocating
0.00.052.545 I ggml_metal_init: found device: Apple M4
0.00.052.547 I ggml_metal_init: picking default device: Apple M4
0.00.053.078 I ggml_metal_init: using embedded metal library
0.00.055.035 I ggml_metal_init: GPU name:   Apple M4
0.00.055.037 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.037 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.037 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.038 I ggml_metal_init: simdgroup reduction   = true
0.00.055.038 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.038 I ggml_metal_init: has bfloat            = true
0.00.055.038 I ggml_metal_init: use bfloat            = true
0.00.055.039 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.039 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.986 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.063.989 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.064.004 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.064.867 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.064.868 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.064.868 I llama_new_context_with_model: graph nodes  = 967
0.00.064.868 I llama_new_context_with_model: graph splits = 2
0.00.064.881 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.685.806 I 
0.00.685.848 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 1 | LLAMAFILE = 1 | 
0.00.685.888 I perplexity: tokenizing the input ..
0.00.693.910 I perplexity: tokenization took 8.023 ms
0.00.693.914 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.834.350 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.835.544 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.835.559 I llama_perf_context_print:        load time =     676.48 ms
0.00.835.560 I llama_perf_context_print: prompt eval time =     140.21 ms /   128 tokens (    1.10 ms per token,   912.89 tokens per second)
0.00.835.560 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.835.561 I llama_perf_context_print:       total time =     149.76 ms /   129 tokens
0.00.835.862 I ggml_metal_free: deallocating

real	0m0.848s
user	0m0.075s
sys	0m0.128s
```
- save-load-state: 
```
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4163 (8f419181)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 'Ä'
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x11a70a0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x11a70a7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x11a70ad50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x11a70b300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x11a70b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x11a70be60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x11a70c410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x11a70c9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x11a70cf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x11a70d470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x11a70d970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x11a70de70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x11a70e990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x11a70f140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x11a70f950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x11a710070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x11a710790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x11a710eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x11a7115d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x11a711da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x11a7124c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x11a712be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x11a713300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x11a713ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x11a7142c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x11a714580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x11a714b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x11a715800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x11a715d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x11a716000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x11a7164a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x11a716760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x11a716ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x11a717530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x11a7177f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x11a717c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x11a718130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x11a7185d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x11a718a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x11a718f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x11a7193b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x11a719850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x11a719cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x11a71a190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x11a71a450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x11a71aa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x11a71b070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x11a71b990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x11a71bfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x11a71c5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x11a71cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x11a71d1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x11a71d7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x11a71ddf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x11a71e5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x11a71ea80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x11a71ef20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x11a71f1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x11a71f7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x11a71ffe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x11a7202a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x11a720740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x11a720be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x11a721080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x11a721520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x11a7219c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x11a721e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x11a722300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x11a7227a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x11a722c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x11a7230e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x11a723580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x11a723a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x11a723ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x11a724360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x11a724800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x11a724ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x11a725140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x11a7255e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x11a725a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x11a725f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x11a7263c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x11a726860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x11a726d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x11a7271a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x11a727640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x11a727ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x11a727f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x11a728420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x11a7288c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x11a728d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x11a729200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x11a7296a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x11a729b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x11a729fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x11a72a480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x11a72a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x11a71b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x11a72af70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x11a72b410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x11a72b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x11a72bd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x11a72c1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x11a72c690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x11a72cb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x11a72cfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x11a72d470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x11a72d910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x11a72ddb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x11a72e250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x11a72e6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x11a72eb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x11a72f030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x11a72f4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x11a72f970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x11a72fe10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x11a7302b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x11a730750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x11a730bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x11a731090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x11a731530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x11a7319d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x11a731e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x11a732310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x11a7327b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x11a732c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x11a7330f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x11a733590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x11a733a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x11a733ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x11a734370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x11a734810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x11a734cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x11a735150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x11a7355f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x11a735a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x11a735f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x11a7363d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x11a736870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x11a736d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x11a7371b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x11a737650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x11a737af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x11a737f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x11a738430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x11a7388d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x11a738d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x11a739210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x11a7396b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x11a739b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x11a739ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x11a73a490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x11a73a930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x11a73ae80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x11a73b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x11a73b920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x11a73be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x11a73c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x11a73c740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x11a73cd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x11a73d360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x11a73d970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x11a73df80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x11a73e770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x11a73ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x11a73f0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x11a73f550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x11a73fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x11a740250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x11a7407a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x11a740cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x11a741240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x11a741790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x11a741ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x11a742230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x11a742780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x11a742cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x11a743220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x11a743770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x11a743cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x11a744210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x11a744760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x11a744cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x11a745200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x11a745750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x11a745ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x11a7461f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x11a746740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x11a746c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x11a7471e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x11a747730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x11a747c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x11a7481d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x11a748720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x11a748c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x11a7491c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x11a749710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x11a749c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x11a74a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x11a74a700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x11a74ac50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x11a74b1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x11a74b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x11a74bc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x11a74c190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x11a74c6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x11a74cc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x11a74d180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x11a74d6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x11a74dc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x11a74e170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x11a74e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x11a74ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x11a74f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x11a74f6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x11a74fc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x11a750150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x11a7506a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x11a750bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x11a751140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x11a751690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x11a751be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x11a752130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x11a752680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x11a752b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x11a752fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x11a753460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x11a753900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x11a753da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x11a754240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x11a7546e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x11a754b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x11a755020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x11a7554c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x11a755960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x11a755e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x11a7562a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x11a7567f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x11a756f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x11a757630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x11a757d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x11a758470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x11a758730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x11a758d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x11a759350 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
0.00.141.527 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x10df04ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x10df04f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x10df053c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x10df05830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x10df05ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x10df06110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x10df06580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x10df069f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x10df06e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x10df07360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x10df077d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x10df07e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x10df08970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x10df09120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x10df09930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x10df0a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x10df0a770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x10df0ae90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x10df0b5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x10df0bd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x10df0c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x10df0cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x10df0d2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x10df0da00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x10df0e120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x10df0e3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x10df0e6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x10df0eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x10df0ef80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x10df0f3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x10df0f860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x10df0fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x10df10200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x10df104c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x10df10930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x10df10da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x10df11210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x10df11680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x10df11af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x10df11f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x10df123d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x10df12840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x10df12cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x10df13120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x10df13590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x10df13a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x10df13e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x10df142e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x10df14750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x10df14bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x10df15030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x10df154a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x10df15910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x10df15d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x10df161f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x10df16660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x10df16bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x10df170d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x10df17540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x10df179b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x10df17e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x10df18290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x10df18700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x10df18b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x10df18fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x11a604280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x11a6046f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x11a604cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x11a6051d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x11a6056e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x11a605bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x11a606100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x11a606610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x11a606b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x11a607030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x11a607540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x11a607a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x11a607f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x11a608470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x11a608980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x11a608e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x11a6093a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x11a6098b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x11a609dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x11a60a2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x11a60a7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x11a60acf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x11a60b200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x11a60b710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x11a60bc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x11a60c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x11a60c640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x11a60cb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x11a60d060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x11a60d570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x11a60da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x11a60df90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x11a60e4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x11a60e9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x11a60eec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x11a60f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x11a60f8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x11a60fdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x11a610300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x11a610810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x11a610d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x11a611230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x11a611740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x11a611c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x11a612160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x11a612660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x11a612b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x11a613070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x11a613580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x11a613a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x11a613fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x11a6144b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x11a6149c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x11a614ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x11a6153e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x11a6158f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x11a615e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x11a616310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x11a616820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x11a616d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x11a617240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x11a617750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x11a617c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x11a618170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x11a618680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x11a618b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x11a6190a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x11a6195b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x11a619ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x11a619fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x11a61a4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x11a61a9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x11a61af00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x11a61b410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x11a61b920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x11a61be30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x11a61c340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x11a61c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x11a61cd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x11a61d270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x11a61d780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x11a61dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x11a61e1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x11a61e6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x11a61ebc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x11a61f0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x11a61f5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x11a61faf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x11a620000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x11a6205b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x11a620b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x11a621110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x11a6216c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x11a621cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x11a6222e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x11a6228f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x11a622f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x11a623510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x11a623d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x11a6241a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x11a624640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x11a624ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x11a625290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x11a6257e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x11a625d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x11a626280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x11a6267d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x11a626d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x11a627270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x11a6277c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x11a627d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x11a628260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x11a6287b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x11a628d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x11a629250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x11a6297a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x11a629cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x11a62a240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x11a62a790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x11a62ace0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x11a62b230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x11a62b780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x11a62bcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x11a62c220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x11a62c770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x11a62ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x11a62d210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x11a62d760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x11a62dcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x11a62e200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x11a62e750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x11a62eca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x11a62f1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x11a62f740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x11a62fc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x11a6301e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x11a630730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x11a630c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x11a6311d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x11a631720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x11a631c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x11a6321c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x11a632710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x11a632c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x11a6331b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x11a633700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x11a633c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x11a6341a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x11a6346f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x11a634c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x11a635190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x11a6356e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x11a635c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x11a636180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x11a6366d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x11a636c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x11a637170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x11a6376c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x11a637c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x11a6380b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x11a638550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x11a6389f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x11a638e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x11a639330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x11a6397d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x11a639c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x11a63a110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x11a63a5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x11a63aa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x11a63aef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x11a63b390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x11a63b830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x11a63bd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x11a63c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x11a63cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x11a63d2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x11a63da00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x11a63dcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x11a63e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x11a63e8e0 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x11a6046e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x11a604b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x11a604fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x11a605430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x11a6058a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x11a605d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x11a606180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x11a6065f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x11a606a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x11a606ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x11a607340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x11a607920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x11a608210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x11a608990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x11a609170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x11a609860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x11a609f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x11a60a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x11a60ad30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x11a60b6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x11a60bda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x11a60c490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x11a60cb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x11a60d270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x11a60d960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x11a60ddd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x11a60e240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x11a60e6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x11a60eb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x11a60ef90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x11a60f400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x11a60f870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x11a60fce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x11a60ffa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x11a610410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x11a610880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x11a610cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x11a611160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x11a6115d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x11a611a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x11a611eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x11a612320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x11a612790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x11a612c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x11a613070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x11a6134e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x11a613950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x11a613dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x11a614230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x11a6146a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x11a614b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x11a614f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x11a6153f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x11a615860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x11a615cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x11a616140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x11a6165b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x11a616a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x11a616e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x11a617300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x11a617770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x11a617be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x11a618050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x11a6184c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x11a618930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x11a618da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x11a619210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x11a619680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x11a619af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x11a619f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x11a61a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x11a61a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x11a61acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x11a61b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x11a61b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x11a61ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x11a61be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x11a61c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x11a61c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x11a61cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x11a61d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x11a61d4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x11a61d910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x11a61dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x11a61e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x11a61e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x11a61ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x11a61ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x11a61f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x11a61f820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x11a61fc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x11a620100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x11a620570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x11a6209e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x11a620e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x11a6212c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x11a621730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x11a621ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x11a622010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x11a622480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x11a6228f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x11a622d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x11a6231d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x11a623640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x11a623ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x11a623f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x11a624390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x11a624800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x11a624c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x11a6250e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x11a625550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x11a6259c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x11a625e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x11a6262a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x11a626710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x11a626b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x11a626ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x11a627460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x11a6278d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x11a627d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x11a6281b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x11a628620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x11a628a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x11a628f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x11a629370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x11a6297e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x11a629c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x11a62a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x11a62a530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x11a62a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x11a62ae10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x11a62b280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x11a62b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x11a62bb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x11a62bfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x11a62c440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x11a62c8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x11a62cd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x11a62d190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x11a62d600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x11a62da70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x11a62dee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x11a62e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x11a62e7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x11a62ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x11a62f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x11a62f510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x11a62f980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x11a62fdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x11a630260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x11a6306d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x11a630b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x11a630fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x11a631420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x11a631890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x11a631d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x11a632170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x11a6325e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x11a632a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x11a632ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x11a633330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x11a6337a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x11a633c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x11a634080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x11a6344f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x11a634960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x11a634dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x11a635550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x11a6359c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x11a635e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x11a6362a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x11a636710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x11a636b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x11a636ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x11a637460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x11a6378d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x11a637d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x11a6381b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x11a638620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x11a638a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x11a638f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x11a639370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x11a6397e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x11a639c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x11a63a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x11a63a530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x11a63a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x11a63ae10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x11a63b280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x11a63b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x11a63bb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x11a63bfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x11a63c440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x11a63c8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x11a63cd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x11a63d190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x11a63d600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x11a63da70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x11a63dee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x11a63e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x11a63e7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x11a63ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x11a63f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x11a63f930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x11a63fe80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x11a6403d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x11a640920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x11a640e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x11a6413c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x11a641910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x11a641e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x11a6423b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x11a642900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x11a642e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x11a6433a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x11a6438f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x11a643e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x11a644390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x11a6448e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x11a644e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x11a645380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x11a6458d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x11a645e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x11a646370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x11a646810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x11a646cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x11a647150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x11a6475f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x11a647a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x11a647f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x11a6483d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x11a648870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x11a648d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x11a6491b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x11a649650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x11a649af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x11a649f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x11a64a4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x11a64ac00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x11a64b320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x11a64ba40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x11a64c160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x11a64c420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x11a64ca30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x11a64d040 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.708s
user	0m0.294s
sys	0m0.269s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4163 (8f419181)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 'Ä'
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13a70da60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13a70e170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13a70e720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13a70ecd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13a70f280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13a70f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13a70fde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13a710390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13a710940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13a710e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13a711340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13a711840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13a712360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13a712b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13a713320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13a713a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13a714160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13a714880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13a714fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13a715770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13a715e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13a7165b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13a716cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13a717570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13a717c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13a717f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13a718560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13a7191d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13a719710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13a7199d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13a719e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13a71a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13a71a9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13a71af00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13a71b1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13a71b660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13a71bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13a71bfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13a71c440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13a71c8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13a71cd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13a71d220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13a71d6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13a71db60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13a71de20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13a71e430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13a71ea40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13a71f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13a71f970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13a71ff80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13a720590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13a720ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13a7211b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13a7217c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13a721fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13a722450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13a7228f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13a722bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13a7231c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13a7239b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13a723c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13a724110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13a7245b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13a724a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13a724ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13a725390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13a725830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13a725cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13a726170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13a726610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13a726ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13a726f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13a7273f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13a727890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13a727d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13a7281d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13a728670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13a728b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13a728fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13a729450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13a7298f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13a729d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13a72a230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13a72a6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13a72ab70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13a72b010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13a72b4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13a72b950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13a72bdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13a72c290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13a72c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13a72cbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13a72d070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13a72d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13a72d9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13a72de50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13a72e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13a71f050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13a72e940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13a72ede0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13a72f280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13a72f720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13a72fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13a730060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13a730500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13a7309a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13a730e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13a7312e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13a731780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13a731c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13a7320c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13a732560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13a732a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13a732ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13a733340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13a7337e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13a733c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13a734120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13a7345c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13a734a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13a734f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13a7353a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13a735840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13a735ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13a736180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13a736620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13a736ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13a736f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13a737400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13a7378a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13a737d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13a7381e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13a738680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13a738b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13a738fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13a739460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13a739900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13a739da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13a73a240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13a73a6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13a73ab80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13a73b020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13a73b4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13a73b960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13a73be00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13a73c2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13a73c740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13a73cbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13a73d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13a73d520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13a73d9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13a73de60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13a73e300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13a73e850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13a73eda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13a73f2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13a73f840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13a73fb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13a740110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13a740720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13a740d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13a741340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13a741950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13a742140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13a7425e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13a742a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13a742f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13a7436d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13a743c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13a744170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13a7446c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13a744c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13a745160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13a7456b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13a745c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13a746150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13a7466a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13a746bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13a747140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13a747690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13a747be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13a748130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13a748680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13a748bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13a749120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13a749670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13a749bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13a74a110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13a74a660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13a74abb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13a74b100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13a74b650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13a74bba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13a74c0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13a74c640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13a74cb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13a74d0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13a74d630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13a74db80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13a74e0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13a74e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13a74eb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13a74f0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13a74f610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13a74fb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13a7500b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13a750600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13a750b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13a7510a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13a7515f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13a751b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13a752090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13a7525e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13a752b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13a753080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13a7535d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13a753b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13a754070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13a7545c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13a754b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13a755060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13a7555b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13a755b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13a756050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13a7564f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13a756990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13a756e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13a7572d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13a757770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13a757c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13a7580b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13a758550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13a7589f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13a758e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13a759330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13a7597d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13a759c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13a75a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13a75a8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13a75b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13a75b720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13a75be40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13a75c100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13a75c710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13a75cd20 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
0.00.088.589 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13b0078d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13b007d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13b0085c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13b008b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13b009060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13b0095b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13b009b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13b00a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13b00a4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13b00a990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13b00ae30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13b00b0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13b00bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13b00c290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13b00caa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13b00d1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13b00d8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13b00e000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13b00e720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13b00f0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13b00f7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13b00ff10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13b010630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13b010d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13b011470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13b011730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13b011d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13b012350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13b012960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13b013150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13b0135f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13b0138b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13b014140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13b014680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13b014940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13b014de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13b015280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13b015720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13b015bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13b016060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13b016500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13b0169a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13b016e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13b0172e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13b0175a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13b017bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13b0181c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13b0187d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13b018de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13b0193f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13b019a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13b01a010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13b01a620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13b01ac30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13b01b420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13b01b8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13b01bd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13b01c020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13b01c630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13b01ce20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13b01d2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13b01d760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13b01dc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13b01e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13b01e540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13b01e9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13b01ee80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13b01f320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13b01f7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13b01fc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13b020100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13b0205a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13b020a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13b020ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13b021380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13b021820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13b021cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13b022160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13b022600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13b022aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13b022f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13b0233e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13b023880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13b023d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13b0241c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13b024660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13b024b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13b024fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13b025440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13b0258e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13b025d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13b026220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13b0266c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13b026b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13b027000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13b0274a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13b027940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13b027de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13b028280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13b028720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13b028bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13b029060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13b029500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13b0299a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13b029e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13b02a2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13b02a780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13b02ac20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13b02b0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13b02b560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13b02ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13b02bea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13b02c340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13b02c7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13b02cc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13b02d120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13b02d5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13b02da60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13b02df00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13b02e3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13b02e840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13b02ece0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13b02f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13b02f620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13b02fac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13b02ff60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13b030400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13b0308a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13b030d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13b0311e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13b031680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13b031b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13b031fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13b032460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13b032900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13b032da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13b033240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13b0336e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13b033b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13b034020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13b0344c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13b034960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13b034e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13b0352a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13b035740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13b035be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13b036080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13b036520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13b0369c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13b036e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13b037300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13b0377a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13b037c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13b038190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13b0386e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13b038c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13b039180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13b039440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13b039a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13b03a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13b03a670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13b03ac80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13b03b290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13b03ba80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13b03bf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13b03c3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13b03c860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13b03d010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13b03d560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13b03dab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13b03e000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13b03e550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13b03eaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13b03eff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13b03f540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13b03fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13b03ffe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13b040530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13b040a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13b040fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13b041520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13b041a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13b041fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13b042510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13b042a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13b042fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13b043500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13b043a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13b043fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13b0444f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13b044a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13b044f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13b0454e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13b045a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13b045f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13b0464d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13b046a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13b046f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13b0474c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13b047a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13b047f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13b0484b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13b048a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13b048f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13b0494a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13b0499f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13b049f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13b04a490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13b04a9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13b04af30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13b04b480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13b04b9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13b04bf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13b04c470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13b04c9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13b04cf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13b04d460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13b04d9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13b04df00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13b04e450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13b04e9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13b04eef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13b04f440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13b04f990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13b04fe30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13b0502d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13b050770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13b050c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13b0510b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13b051550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13b0519f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13b051e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13b052330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13b0527d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13b052c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13b053110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13b0535b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13b053b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13b054220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13b054940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13b055060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13b055780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13b055a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13b056050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13b056660 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13b0078d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13b007d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13b0081b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13b008620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13b008a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13b008f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13b009370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13b0097e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13b009c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13b00a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13b00a530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13b00a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13b00b290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13b00ba10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13b00c1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13b00c8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13b00cfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13b00d6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13b00ddb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13b00e730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13b00ee20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13b00f510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13b00fc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13b0102f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13b0109e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13b010e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13b0112c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13b011730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13b011ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13b012010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13b012480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13b0128f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13b012d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13b013020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13b013490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13b013900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13b013d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13b0141e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13b014650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13b014ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13b014f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13b0153a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13b015810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13b015c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13b0160f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13b016560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13b0169d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13b016e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13b0172b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13b017720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13b017b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13b018000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13b018470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13b0188e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13b018d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13b0191c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13b019630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13b019aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13b019f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13b01a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13b01a7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13b01ac60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13b01b0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13b01b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13b01b9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13b01be20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13b01c290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13b01c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13b01cb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13b01cfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13b01d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13b01d8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13b01dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13b01e1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13b01e610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13b01ea80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13b01eef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13b01f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13b01f7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13b01fc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13b0200b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13b020520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13b020990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13b020e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13b021270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13b0216e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13b021b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13b021fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13b022430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13b0228a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13b022d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13b023180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13b0235f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13b023a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13b023ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13b024340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13b0247b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13b024c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13b025090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13b025500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13b025970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13b025de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13b026250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13b0266c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13b026b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13b026fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13b027410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13b027880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13b027cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13b028160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13b0285d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13b028a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13b028eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13b029320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13b029790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13b029c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13b02a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13b02a4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13b02a950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13b02adc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13b02b230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13b02b6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13b02bb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13b02bf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13b02c3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13b02c860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13b02ccd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13b02d140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13b02d5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13b02da20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13b02de90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13b02e300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13b02e770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13b02ebe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13b02f050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13b02f4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13b02f930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13b02fda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13b030210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13b030680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13b030af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13b030f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13b0313d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13b031840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13b031cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13b032120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13b032590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13b032a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13b032e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13b0332e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13b033750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13b033bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13b034030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13b0344a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13b034910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13b034d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13b0351f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13b035660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13b035ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13b035f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13b0363b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13b036820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13b036c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13b037100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13b037570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13b0379e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13b037e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13b0385d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13b038a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13b038eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13b039320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13b039790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13b039c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13b03a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13b03a4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13b03a950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13b03adc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13b03b230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13b03b6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13b03bb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13b03bf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13b03c3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13b03c860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13b03ccd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13b03d140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13b03d5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13b03da20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13b03de90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13b03e300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13b03e770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13b03ebe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13b03f050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13b03f4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13b03f930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13b03fda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13b040210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13b040680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13b040af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13b040f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13b0413d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13b041840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13b041cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13b042120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13b042590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13b042a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13b042e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13b0432e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13b043750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13b043bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13b044030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13b0444a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13b044910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13b044d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13b0451f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13b045660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13b045ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13b045f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13b0463b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13b046820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13b046c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13b047100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13b047570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13b0479e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13b047e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13b0482c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13b048730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13b048ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13b049010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13b049480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13b0498f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13b049d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13b04a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13b04a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13b04aab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13b04af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13b04b390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13b04b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13b04bc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13b04c360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13b04ca50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13b04d140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13b04d830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13b04dca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13b04e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13b04e580 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Our old friend, Zorro." "He


second run: The quick brown fox jumps over the lazy Dog." "Our old friend, Zorro." "He


single seq run: The quick brown fox jumps over the lazy Dog." "Our old friend, Zorro." "He

real	0m0.948s
user	0m0.236s
sys	0m0.124s
```
### ctest_with_model_debug

Runs ctest with model files in debug mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-debug
    Start 27: test-model-load-cancel
1/2 Test #27: test-model-load-cancel ...........   Passed    0.53 sec
    Start 28: test-autorelease
2/2 Test #28: test-autorelease .................   Passed    0.57 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   1.10 sec*proc (2 tests)

Total Test time (real) =   1.11 sec
        1.13 real         0.71 user         0.05 sys
```
### ctest_with_model_release

Runs ctest with model files in release mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-release
    Start 27: test-model-load-cancel
1/2 Test #27: test-model-load-cancel ...........   Passed    0.25 sec
    Start 28: test-autorelease
2/2 Test #28: test-autorelease .................   Passed    0.26 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   0.50 sec*proc (2 tests)

Total Test time (real) =   0.51 sec
        0.52 real         0.14 user         0.04 sys
```
