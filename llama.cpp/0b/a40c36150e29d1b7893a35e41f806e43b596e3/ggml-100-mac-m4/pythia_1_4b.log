Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:43 (message):
  OpenMP not found


-- Using llamafile
-- ARM detected
-- Performing Test COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- Using runtime weight conversion of Q4_0 to Q4_0_x_x to enable optimized GEMM/GEMV kernels
-- Including CPU backend
CMake Warning at ggml/src/ggml-amx/CMakeLists.txt:106 (message):
  AMX requires x86 and gcc version > 11.0.  Turning off GGML_AMX.


-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (1.1s)
-- Generating done (0.2s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m1.311s
user	0m0.552s
sys	0m0.782s
++ nproc
+ make -j10
[  0%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  0%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  1%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  2%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  2%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  3%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  3%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  4%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  5%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  6%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  6%] Built target sha1
[  6%] Built target sha256
[  6%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-aarch64.c.o
[  6%] Built target build_info
[  6%] Built target xxhash
[  7%] Linking CXX shared library libggml-base.dylib
[  7%] Built target ggml-base
[  8%] Generate assembly for embedded Metal library
Embedding Metal library
[  8%] Building C object ggml/src/ggml-cpu/CMakeFiles/ggml-cpu.dir/ggml-cpu-quants.c.o
[  9%] Building CXX object ggml/src/ggml-cpu/CMakeFiles/ggml-cpu.dir/llamafile/sgemm.cpp.o
[ 11%] Building C object ggml/src/ggml-cpu/CMakeFiles/ggml-cpu.dir/ggml-cpu-aarch64.c.o
[ 11%] Building CXX object ggml/src/ggml-cpu/CMakeFiles/ggml-cpu.dir/ggml-cpu.cpp.o
[ 11%] Building C object ggml/src/ggml-cpu/CMakeFiles/ggml-cpu.dir/ggml-cpu.c.o
[ 12%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[ 12%] Linking CXX shared library libggml-cpu.dylib
[ 12%] Linking CXX shared library libggml-blas.dylib
[ 12%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 13%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 13%] Built target ggml-blas
[ 13%] Built target ggml-cpu
[ 14%] Linking C shared library libggml-metal.dylib
[ 14%] Built target ggml-metal
[ 14%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 15%] Linking CXX shared library libggml.dylib
[ 15%] Built target ggml
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 17%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 17%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 20%] Linking CXX executable ../../bin/llama-gguf
[ 20%] Linking CXX executable ../../bin/llama-gguf-hash
[ 20%] Linking CXX shared library libllama.dylib
[ 20%] Built target llama-gguf-hash
[ 20%] Built target llama-gguf
[ 20%] Built target llama
[ 20%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 22%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 22%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 20%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 22%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 22%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 23%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 24%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 23%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 25%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 26%] Linking C executable ../bin/test-c
[ 26%] Linking CXX executable ../../bin/llama-quantize-stats
[ 27%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 27%] Linking CXX executable ../../bin/llama-simple-chat
[ 27%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 27%] Linking CXX executable ../../bin/llama-simple
[ 28%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 29%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 29%] Built target llava
[ 30%] Linking CXX static library libcommon.a
[ 30%] Built target llama-quantize-stats
[ 30%] Built target test-c
[ 30%] Built target llama-simple-chat
[ 31%] Linking CXX static library libllava_static.a
[ 31%] Linking CXX shared library libllava_shared.dylib
[ 31%] Built target llama-simple
[ 31%] Built target common
[ 31%] Built target llava_static
[ 31%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 32%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 32%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 32%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 32%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 33%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 34%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 34%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 34%] Built target llava_shared
[ 35%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 36%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 38%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 38%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 39%] Linking CXX executable ../bin/test-tokenizer-0
[ 39%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 39%] Linking CXX executable ../bin/test-quantize-perf
[ 40%] Linking CXX executable ../bin/test-sampling
[ 40%] Linking CXX executable ../bin/test-log
[ 41%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 42%] Linking CXX executable ../bin/test-quantize-fns
[ 43%] Linking CXX executable ../bin/test-arg-parser
[ 44%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 44%] Linking CXX executable ../bin/test-chat-template
[ 44%] Built target test-tokenizer-1-bpe
[ 44%] Built target test-tokenizer-1-spm
[ 44%] Built target test-tokenizer-0
[ 44%] Built target test-sampling
[ 45%] Built target test-log
[ 45%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 46%] Linking CXX executable ../bin/test-grammar-parser
[ 46%] Built target test-quantize-perf
[ 47%] Built target test-arg-parser
[ 47%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 47%] Built target test-quantize-fns
[ 47%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 47%] Built target test-chat-template
[ 48%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 49%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 49%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 52%] Linking CXX executable ../bin/test-barrier
[ 52%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 53%] Linking CXX executable ../bin/test-grammar-integration
[ 54%] Linking CXX executable ../bin/test-llama-grammar
[ 54%] Built target test-grammar-parser
[ 54%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 55%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 57%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 58%] Linking CXX executable ../bin/test-backend-ops
[ 59%] Linking CXX executable ../bin/test-rope
[ 60%] Linking CXX executable ../../bin/llama-cvector-generator
[ 61%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 60%] Linking CXX executable ../bin/test-model-load-cancel
[ 61%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 61%] Linking CXX executable ../bin/test-autorelease
[ 61%] Built target test-barrier
[ 61%] Built target test-grammar-integration
[ 61%] Built target test-llama-grammar
[ 62%] Linking CXX executable ../../bin/llama-batched-bench
[ 63%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 63%] Built target test-backend-ops
[ 63%] Built target test-rope
[ 63%] Built target test-model-load-cancel
[ 63%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 63%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 63%] Built target llama-cvector-generator
[ 63%] Built target test-json-schema-to-grammar
[ 63%] Built target test-autorelease
[ 64%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 64%] Linking CXX executable ../../bin/llama-batched
[ 65%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 66%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 67%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 68%] Linking CXX executable ../../bin/llama-embedding
[ 68%] Built target llama-batched-bench
[ 68%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 68%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 69%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 69%] Linking CXX executable ../../bin/llama-eval-callback
[ 69%] Linking CXX executable ../../bin/llama-export-lora
[ 69%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 70%] Linking CXX executable ../../bin/llama-gritlm
[ 70%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 71%] Linking CXX executable ../../bin/llama-imatrix
[ 72%] Linking CXX executable ../../bin/llama-gguf-split
[ 72%] Built target llama-batched
[ 72%] Built target llama-embedding
[ 73%] Linking CXX executable ../../bin/llama-infill
[ 73%] Built target llama-convert-llama2c-to-ggml
[ 73%] Built target llama-export-lora
[ 73%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 73%] Built target llama-gbnf-validator
[ 73%] Built target llama-eval-callback
[ 73%] Built target llama-gritlm
[ 73%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 74%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 74%] Built target llama-imatrix
[ 74%] Built target llama-gguf-split
[ 76%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 76%] Linking CXX executable ../../bin/llama-bench
[ 76%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 77%] Linking CXX executable ../../bin/llama-llava-cli
[ 77%] Built target llama-infill
[ 79%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 79%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 79%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 80%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 80%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 81%] Linking CXX executable ../../bin/llama-lookahead
[ 81%] Linking CXX executable ../../bin/llama-lookup
[ 81%] Linking CXX executable ../../bin/llama-lookup-create
[ 82%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 82%] Linking CXX executable ../../bin/llama-lookup-merge
[ 83%] Linking CXX executable ../../bin/llama-cli
[ 83%] Linking CXX executable ../../bin/llama-lookup-stats
[ 83%] Built target llama-bench
[ 83%] Built target llama-llava-cli
[ 83%] Built target llama-minicpmv-cli
[ 84%] Linking CXX executable ../../bin/llama-parallel
[ 84%] Built target llama-lookahead
[ 84%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 84%] Built target llama-lookup
[ 84%] Built target llama-cli
[ 84%] Built target llama-lookup-merge
[ 84%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 84%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 84%] Built target llama-lookup-create
[ 84%] Built target llama-lookup-stats
[ 85%] Generating loading.html.hpp
[ 86%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 86%] Generating completion.js.hpp
[ 87%] Linking CXX executable ../../bin/llama-passkey
[ 88%] Linking CXX executable ../../bin/llama-quantize
[ 89%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 90%] Linking CXX executable ../../bin/llama-perplexity
[ 90%] Built target llama-parallel
[ 91%] Generating deps_daisyui.min.css.hpp
[ 91%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 92%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 93%] Generating deps_markdown-it.js.hpp
[ 93%] Linking CXX executable ../../bin/llama-retrieval
[ 93%] Linking CXX executable ../../bin/llama-save-load-state
[ 93%] Linking CXX executable ../../bin/llama-speculative
[ 94%] Linking CXX executable ../../bin/llama-speculative-simple
[ 94%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 94%] Built target llama-passkey
[ 94%] Built target llama-quantize
[ 94%] Built target llama-perplexity
[ 95%] Generating deps_tailwindcss.js.hpp
[ 95%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 96%] Linking CXX executable ../../bin/llama-tokenize
[ 96%] Built target llama-retrieval
[ 96%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 96%] Built target llama-save-load-state
[ 96%] Built target llama-speculative
[ 96%] Generating deps_vue.esm-browser.js.hpp
[ 97%] Generating index.html.hpp
[ 97%] Built target llama-speculative-simple
[ 98%] Linking CXX executable ../../bin/llama-vdot
[ 99%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Built target llama-tokenize
[ 99%] Built target llama-vdot
[ 99%] Built target llama-q8dot
[100%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m2.637s
user	0m5.767s
sys	0m8.463s

main: quantize time =  4697.35 ms
main:    total time =  4697.35 ms

main: quantize time =  2028.20 ms
main:    total time =  2028.20 ms

main: quantize time =  2240.66 ms
main:    total time =  2240.66 ms

main: quantize time =  2429.79 ms
main:    total time =  2429.79 ms

main: quantize time =  2444.30 ms
main:    total time =  2444.30 ms

main: quantize time =  5126.16 ms
main:    total time =  5126.16 ms

main: quantize time =  5753.90 ms
main:    total time =  5753.90 ms

main: quantize time =  6891.45 ms
main:    total time =  6891.45 ms

main: quantize time =  5875.72 ms
main:    total time =  5875.72 ms

main: quantize time =  4279.36 ms
main:    total time =  4279.36 ms
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.111 I build: 4161 (0ba40c36) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.246 I main: llama backend init
0.00.000.253 I main: load the model and apply lora adapter, if any
0.00.029.383 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.041.441 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.041.467 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.041.471 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.041.472 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.041.477 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.041.477 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.041.478 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.041.480 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.041.481 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.041.482 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.041.482 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.041.483 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.041.484 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.041.485 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.041.491 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.041.491 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.041.492 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.050.569 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.052.690 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.059.845 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.059.848 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.059.848 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.059.849 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.059.849 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.059.850 I llama_model_loader: - type  f32:  194 tensors
0.00.059.850 I llama_model_loader: - type  f16:   98 tensors
0.00.090.011 I llm_load_vocab: special tokens cache size = 25
0.00.096.917 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.096.920 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.096.920 I llm_load_print_meta: arch             = gptneox
0.00.096.920 I llm_load_print_meta: vocab type       = BPE
0.00.096.921 I llm_load_print_meta: n_vocab          = 50304
0.00.096.921 I llm_load_print_meta: n_merges         = 50009
0.00.096.921 I llm_load_print_meta: vocab_only       = 0
0.00.096.921 I llm_load_print_meta: n_ctx_train      = 2048
0.00.096.921 I llm_load_print_meta: n_embd           = 2048
0.00.096.921 I llm_load_print_meta: n_layer          = 24
0.00.096.924 I llm_load_print_meta: n_head           = 16
0.00.096.925 I llm_load_print_meta: n_head_kv        = 16
0.00.096.925 I llm_load_print_meta: n_rot            = 32
0.00.096.925 I llm_load_print_meta: n_swa            = 0
0.00.096.925 I llm_load_print_meta: n_embd_head_k    = 128
0.00.096.925 I llm_load_print_meta: n_embd_head_v    = 128
0.00.096.926 I llm_load_print_meta: n_gqa            = 1
0.00.096.927 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.096.927 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.096.928 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.096.928 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.096.928 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.096.929 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.096.929 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.096.929 I llm_load_print_meta: n_ff             = 8192
0.00.096.930 I llm_load_print_meta: n_expert         = 0
0.00.096.930 I llm_load_print_meta: n_expert_used    = 0
0.00.096.930 I llm_load_print_meta: causal attn      = 1
0.00.096.930 I llm_load_print_meta: pooling type     = 0
0.00.096.930 I llm_load_print_meta: rope type        = 2
0.00.096.930 I llm_load_print_meta: rope scaling     = linear
0.00.096.931 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.096.931 I llm_load_print_meta: freq_scale_train = 1
0.00.096.931 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.096.931 I llm_load_print_meta: rope_finetuned   = unknown
0.00.096.931 I llm_load_print_meta: ssm_d_conv       = 0
0.00.096.932 I llm_load_print_meta: ssm_d_inner      = 0
0.00.096.932 I llm_load_print_meta: ssm_d_state      = 0
0.00.096.932 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.096.932 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.096.943 I llm_load_print_meta: model type       = 1.4B
0.00.096.944 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.096.944 I llm_load_print_meta: model params     = 1.41 B
0.00.096.945 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.096.945 I llm_load_print_meta: general.name     = 1.4B
0.00.096.945 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.096.945 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.096.945 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.096.946 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.096.946 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.096.948 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.096.948 I llm_load_print_meta: max token length = 1024
0.00.098.976 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.098.976 I llm_load_tensors: offloading output layer to GPU
0.00.098.977 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.098.993 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.098.994 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.099.914 I llama_new_context_with_model: n_seq_max     = 1
0.00.099.915 I llama_new_context_with_model: n_ctx         = 2048
0.00.099.916 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.099.916 I llama_new_context_with_model: n_batch       = 2048
0.00.099.916 I llama_new_context_with_model: n_ubatch      = 512
0.00.099.916 I llama_new_context_with_model: flash_attn    = 0
0.00.099.916 I llama_new_context_with_model: freq_base     = 10000.0
0.00.099.917 I llama_new_context_with_model: freq_scale    = 1
0.00.099.917 I ggml_metal_init: allocating
0.00.099.926 I ggml_metal_init: found device: Apple M4
0.00.099.929 I ggml_metal_init: picking default device: Apple M4
0.00.100.577 I ggml_metal_init: using embedded metal library
0.00.117.365 I ggml_metal_init: GPU name:   Apple M4
0.00.117.367 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.117.368 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.117.368 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.117.368 I ggml_metal_init: simdgroup reduction   = true
0.00.117.369 I ggml_metal_init: simdgroup matrix mul. = true
0.00.117.369 I ggml_metal_init: has bfloat            = true
0.00.117.369 I ggml_metal_init: use bfloat            = true
0.00.117.369 I ggml_metal_init: hasUnifiedMemory      = true
0.00.117.370 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.163.061 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.163.066 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.163.088 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.164.062 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.164.064 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.164.064 I llama_new_context_with_model: graph nodes  = 967
0.00.164.064 I llama_new_context_with_model: graph splits = 2
0.00.164.084 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.251.471 I main: llama threadpool init, n_threads = 4
0.00.251.502 I 
0.00.251.521 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
0.00.251.521 I 
0.00.251.596 I sampler seed: 1234
0.00.251.601 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.251.625 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.251.627 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.251.627 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling depressed, and I've had a hard time getting out of bed. I think the only thing I

0.02.051.199 I llama_perf_sampler_print:    sampling time =       1.24 ms /    71 runs   (    0.02 ms per token, 57304.28 tokens per second)
0.02.051.201 I llama_perf_context_print:        load time =     222.08 ms
0.02.051.201 I llama_perf_context_print: prompt eval time =      37.77 ms /     7 tokens (    5.40 ms per token,   185.33 tokens per second)
0.02.051.202 I llama_perf_context_print:        eval time =    1758.97 ms /    63 runs   (   27.92 ms per token,    35.82 tokens per second)
0.02.051.203 I llama_perf_context_print:       total time =    1799.73 ms /    70 tokens
0.02.051.381 I ggml_metal_free: deallocating

real	0m2.367s
user	0m0.147s
sys	0m0.098s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.039 I build: 4161 (0ba40c36) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.070 I main: llama backend init
0.00.000.072 I main: load the model and apply lora adapter, if any
0.00.009.394 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.031.998 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.032.004 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.032.006 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.032.009 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.032.010 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.032.010 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.032.010 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.032.012 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.032.012 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.032.012 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.032.012 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.032.013 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.032.013 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.032.013 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.032.015 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.032.016 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.032.016 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.035.977 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.037.234 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.041.730 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.041.732 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.041.732 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.041.733 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.041.733 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.041.733 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.041.734 I llama_model_loader: - type  f32:  194 tensors
0.00.041.734 I llama_model_loader: - type q8_0:   98 tensors
0.00.068.641 I llm_load_vocab: special tokens cache size = 25
0.00.076.878 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.076.883 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.076.883 I llm_load_print_meta: arch             = gptneox
0.00.076.884 I llm_load_print_meta: vocab type       = BPE
0.00.076.884 I llm_load_print_meta: n_vocab          = 50304
0.00.076.884 I llm_load_print_meta: n_merges         = 50009
0.00.076.885 I llm_load_print_meta: vocab_only       = 0
0.00.076.885 I llm_load_print_meta: n_ctx_train      = 2048
0.00.076.885 I llm_load_print_meta: n_embd           = 2048
0.00.076.885 I llm_load_print_meta: n_layer          = 24
0.00.076.894 I llm_load_print_meta: n_head           = 16
0.00.076.895 I llm_load_print_meta: n_head_kv        = 16
0.00.076.895 I llm_load_print_meta: n_rot            = 32
0.00.076.896 I llm_load_print_meta: n_swa            = 0
0.00.076.896 I llm_load_print_meta: n_embd_head_k    = 128
0.00.076.896 I llm_load_print_meta: n_embd_head_v    = 128
0.00.076.898 I llm_load_print_meta: n_gqa            = 1
0.00.076.899 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.076.900 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.076.901 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.076.902 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.076.902 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.076.902 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.076.903 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.076.904 I llm_load_print_meta: n_ff             = 8192
0.00.076.904 I llm_load_print_meta: n_expert         = 0
0.00.076.904 I llm_load_print_meta: n_expert_used    = 0
0.00.076.904 I llm_load_print_meta: causal attn      = 1
0.00.076.904 I llm_load_print_meta: pooling type     = 0
0.00.076.905 I llm_load_print_meta: rope type        = 2
0.00.076.905 I llm_load_print_meta: rope scaling     = linear
0.00.076.905 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.076.906 I llm_load_print_meta: freq_scale_train = 1
0.00.076.906 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.076.906 I llm_load_print_meta: rope_finetuned   = unknown
0.00.076.906 I llm_load_print_meta: ssm_d_conv       = 0
0.00.076.907 I llm_load_print_meta: ssm_d_inner      = 0
0.00.076.907 I llm_load_print_meta: ssm_d_state      = 0
0.00.076.907 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.076.907 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.076.922 I llm_load_print_meta: model type       = 1.4B
0.00.076.922 I llm_load_print_meta: model ftype      = Q8_0
0.00.076.922 I llm_load_print_meta: model params     = 1.41 B
0.00.076.924 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.076.925 I llm_load_print_meta: general.name     = 1.4B
0.00.076.925 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.076.925 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.076.925 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.076.925 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.076.926 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.076.926 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.076.926 I llm_load_print_meta: max token length = 1024
0.00.079.859 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.079.859 I llm_load_tensors: offloading output layer to GPU
0.00.079.859 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.079.869 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.079.871 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.081.124 I llama_new_context_with_model: n_seq_max     = 1
0.00.081.126 I llama_new_context_with_model: n_ctx         = 2048
0.00.081.126 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.081.126 I llama_new_context_with_model: n_batch       = 2048
0.00.081.126 I llama_new_context_with_model: n_ubatch      = 512
0.00.081.127 I llama_new_context_with_model: flash_attn    = 0
0.00.081.127 I llama_new_context_with_model: freq_base     = 10000.0
0.00.081.128 I llama_new_context_with_model: freq_scale    = 1
0.00.081.128 I ggml_metal_init: allocating
0.00.081.132 I ggml_metal_init: found device: Apple M4
0.00.081.135 I ggml_metal_init: picking default device: Apple M4
0.00.081.959 I ggml_metal_init: using embedded metal library
0.00.084.678 I ggml_metal_init: GPU name:   Apple M4
0.00.084.680 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.084.681 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.084.681 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.084.682 I ggml_metal_init: simdgroup reduction   = true
0.00.084.682 I ggml_metal_init: simdgroup matrix mul. = true
0.00.084.682 I ggml_metal_init: has bfloat            = true
0.00.084.682 I ggml_metal_init: use bfloat            = true
0.00.084.683 I ggml_metal_init: hasUnifiedMemory      = true
0.00.084.684 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.123.919 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.123.933 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.123.959 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.125.018 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.125.020 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.125.020 I llama_new_context_with_model: graph nodes  = 967
0.00.125.020 I llama_new_context_with_model: graph splits = 2
0.00.125.039 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.365.918 I main: llama threadpool init, n_threads = 4
0.01.365.948 I 
0.01.365.967 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
0.01.365.967 I 
0.01.366.173 I sampler seed: 1234
0.01.366.179 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.366.211 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.366.215 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.366.216 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.447.556 I llama_perf_sampler_print:    sampling time =       1.17 ms /    71 runs   (    0.02 ms per token, 60891.94 tokens per second)
0.02.447.557 I llama_perf_context_print:        load time =    1356.52 ms
0.02.447.558 I llama_perf_context_print: prompt eval time =      33.80 ms /     7 tokens (    4.83 ms per token,   207.07 tokens per second)
0.02.447.559 I llama_perf_context_print:        eval time =    1044.47 ms /    63 runs   (   16.58 ms per token,    60.32 tokens per second)
0.02.447.559 I llama_perf_context_print:       total time =    1081.64 ms /    70 tokens
0.02.447.728 I ggml_metal_free: deallocating

real	0m2.463s
user	0m0.123s
sys	0m0.297s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4161 (0ba40c36) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.073 I main: llama backend init
0.00.000.075 I main: load the model and apply lora adapter, if any
0.00.010.346 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.026.763 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.026.768 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.026.774 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.026.774 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.026.774 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.026.775 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.026.775 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.026.776 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.026.776 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.026.777 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.026.777 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.026.777 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.026.777 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.026.779 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.026.781 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.026.782 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.026.782 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.030.748 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.031.819 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.035.735 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.035.736 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.035.737 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.035.737 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.035.738 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.035.738 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.035.739 I llama_model_loader: - type  f32:  194 tensors
0.00.035.739 I llama_model_loader: - type q4_0:   97 tensors
0.00.035.739 I llama_model_loader: - type q6_K:    1 tensors
0.00.058.005 I llm_load_vocab: special tokens cache size = 25
0.00.064.198 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.064.202 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.064.202 I llm_load_print_meta: arch             = gptneox
0.00.064.202 I llm_load_print_meta: vocab type       = BPE
0.00.064.203 I llm_load_print_meta: n_vocab          = 50304
0.00.064.203 I llm_load_print_meta: n_merges         = 50009
0.00.064.203 I llm_load_print_meta: vocab_only       = 0
0.00.064.203 I llm_load_print_meta: n_ctx_train      = 2048
0.00.064.203 I llm_load_print_meta: n_embd           = 2048
0.00.064.203 I llm_load_print_meta: n_layer          = 24
0.00.064.207 I llm_load_print_meta: n_head           = 16
0.00.064.208 I llm_load_print_meta: n_head_kv        = 16
0.00.064.208 I llm_load_print_meta: n_rot            = 32
0.00.064.208 I llm_load_print_meta: n_swa            = 0
0.00.064.209 I llm_load_print_meta: n_embd_head_k    = 128
0.00.064.209 I llm_load_print_meta: n_embd_head_v    = 128
0.00.064.209 I llm_load_print_meta: n_gqa            = 1
0.00.064.210 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.064.211 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.064.211 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.064.212 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.064.212 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.064.212 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.064.212 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.064.213 I llm_load_print_meta: n_ff             = 8192
0.00.064.213 I llm_load_print_meta: n_expert         = 0
0.00.064.213 I llm_load_print_meta: n_expert_used    = 0
0.00.064.213 I llm_load_print_meta: causal attn      = 1
0.00.064.217 I llm_load_print_meta: pooling type     = 0
0.00.064.217 I llm_load_print_meta: rope type        = 2
0.00.064.217 I llm_load_print_meta: rope scaling     = linear
0.00.064.217 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.064.218 I llm_load_print_meta: freq_scale_train = 1
0.00.064.218 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.064.218 I llm_load_print_meta: rope_finetuned   = unknown
0.00.064.218 I llm_load_print_meta: ssm_d_conv       = 0
0.00.064.218 I llm_load_print_meta: ssm_d_inner      = 0
0.00.064.219 I llm_load_print_meta: ssm_d_state      = 0
0.00.064.219 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.064.219 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.064.230 I llm_load_print_meta: model type       = 1.4B
0.00.064.231 I llm_load_print_meta: model ftype      = Q4_0
0.00.064.232 I llm_load_print_meta: model params     = 1.41 B
0.00.064.233 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.064.233 I llm_load_print_meta: general.name     = 1.4B
0.00.064.233 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.064.235 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.064.235 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.064.235 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.064.235 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.064.236 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.064.236 I llm_load_print_meta: max token length = 1024
0.00.066.317 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.066.318 I llm_load_tensors: offloading output layer to GPU
0.00.066.318 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.066.327 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.066.329 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.067.238 I llama_new_context_with_model: n_seq_max     = 1
0.00.067.239 I llama_new_context_with_model: n_ctx         = 2048
0.00.067.239 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.067.239 I llama_new_context_with_model: n_batch       = 2048
0.00.067.239 I llama_new_context_with_model: n_ubatch      = 512
0.00.067.240 I llama_new_context_with_model: flash_attn    = 0
0.00.067.240 I llama_new_context_with_model: freq_base     = 10000.0
0.00.067.240 I llama_new_context_with_model: freq_scale    = 1
0.00.067.241 I ggml_metal_init: allocating
0.00.067.243 I ggml_metal_init: found device: Apple M4
0.00.067.245 I ggml_metal_init: picking default device: Apple M4
0.00.067.944 I ggml_metal_init: using embedded metal library
0.00.070.065 I ggml_metal_init: GPU name:   Apple M4
0.00.070.066 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.070.067 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.070.067 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.070.067 I ggml_metal_init: simdgroup reduction   = true
0.00.070.067 I ggml_metal_init: simdgroup matrix mul. = true
0.00.070.067 I ggml_metal_init: has bfloat            = true
0.00.070.068 I ggml_metal_init: use bfloat            = true
0.00.070.068 I ggml_metal_init: hasUnifiedMemory      = true
0.00.070.069 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.104.604 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.104.611 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.104.636 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.105.767 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.105.768 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.105.768 I llama_new_context_with_model: graph nodes  = 967
0.00.105.769 I llama_new_context_with_model: graph splits = 2
0.00.105.783 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.753.656 I main: llama threadpool init, n_threads = 4
0.00.753.688 I 
0.00.753.711 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
0.00.753.712 I 
0.00.753.864 I sampler seed: 1234
0.00.753.870 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.753.879 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.753.880 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.753.880 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.423.811 I llama_perf_sampler_print:    sampling time =       1.22 ms /    71 runs   (    0.02 ms per token, 58292.28 tokens per second)
0.01.423.812 I llama_perf_context_print:        load time =     743.31 ms
0.01.423.813 I llama_perf_context_print: prompt eval time =      33.00 ms /     7 tokens (    4.71 ms per token,   212.14 tokens per second)
0.01.423.814 I llama_perf_context_print:        eval time =     633.86 ms /    63 runs   (   10.06 ms per token,    99.39 tokens per second)
0.01.423.815 I llama_perf_context_print:       total time =     670.16 ms /    70 tokens
0.01.423.997 I ggml_metal_free: deallocating

real	0m1.439s
user	0m0.112s
sys	0m0.207s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.034 I build: 4161 (0ba40c36) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.064 I main: llama backend init
0.00.000.066 I main: load the model and apply lora adapter, if any
0.00.008.873 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.469 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.015.474 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.477 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.477 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.477 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.478 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.478 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.479 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.479 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.480 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.480 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.480 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.481 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.481 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.483 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.484 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.484 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.389 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.465 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.245 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.246 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.247 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.247 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.247 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.247 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.024.248 I llama_model_loader: - type  f32:  194 tensors
0.00.024.248 I llama_model_loader: - type q4_1:   97 tensors
0.00.024.249 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.425 I llm_load_vocab: special tokens cache size = 25
0.00.051.460 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.463 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.464 I llm_load_print_meta: arch             = gptneox
0.00.051.464 I llm_load_print_meta: vocab type       = BPE
0.00.051.464 I llm_load_print_meta: n_vocab          = 50304
0.00.051.464 I llm_load_print_meta: n_merges         = 50009
0.00.051.464 I llm_load_print_meta: vocab_only       = 0
0.00.051.465 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.465 I llm_load_print_meta: n_embd           = 2048
0.00.051.465 I llm_load_print_meta: n_layer          = 24
0.00.051.467 I llm_load_print_meta: n_head           = 16
0.00.051.468 I llm_load_print_meta: n_head_kv        = 16
0.00.051.468 I llm_load_print_meta: n_rot            = 32
0.00.051.469 I llm_load_print_meta: n_swa            = 0
0.00.051.469 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.469 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.470 I llm_load_print_meta: n_gqa            = 1
0.00.051.470 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.471 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.472 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.472 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.472 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.472 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.475 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.476 I llm_load_print_meta: n_ff             = 8192
0.00.051.476 I llm_load_print_meta: n_expert         = 0
0.00.051.476 I llm_load_print_meta: n_expert_used    = 0
0.00.051.477 I llm_load_print_meta: causal attn      = 1
0.00.051.477 I llm_load_print_meta: pooling type     = 0
0.00.051.477 I llm_load_print_meta: rope type        = 2
0.00.051.477 I llm_load_print_meta: rope scaling     = linear
0.00.051.478 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.479 I llm_load_print_meta: freq_scale_train = 1
0.00.051.480 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.480 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.480 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.480 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.480 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.480 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.480 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.492 I llm_load_print_meta: model type       = 1.4B
0.00.051.492 I llm_load_print_meta: model ftype      = Q4_1
0.00.051.493 I llm_load_print_meta: model params     = 1.41 B
0.00.051.493 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.051.493 I llm_load_print_meta: general.name     = 1.4B
0.00.051.493 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.494 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.494 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.494 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.494 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.494 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.495 I llm_load_print_meta: max token length = 1024
0.00.053.300 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.301 I llm_load_tensors: offloading output layer to GPU
0.00.053.301 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.310 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.053.312 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.054.215 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.216 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.216 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.216 I llama_new_context_with_model: n_batch       = 2048
0.00.054.217 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.217 I llama_new_context_with_model: flash_attn    = 0
0.00.054.217 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.217 I llama_new_context_with_model: freq_scale    = 1
0.00.054.218 I ggml_metal_init: allocating
0.00.054.224 I ggml_metal_init: found device: Apple M4
0.00.054.227 I ggml_metal_init: picking default device: Apple M4
0.00.054.773 I ggml_metal_init: using embedded metal library
0.00.056.713 I ggml_metal_init: GPU name:   Apple M4
0.00.056.714 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.715 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.715 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.715 I ggml_metal_init: simdgroup reduction   = true
0.00.056.715 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.715 I ggml_metal_init: has bfloat            = true
0.00.056.715 I ggml_metal_init: use bfloat            = true
0.00.056.716 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.716 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.083.189 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.083.195 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.083.215 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.084.166 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.084.167 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.084.167 I llama_new_context_with_model: graph nodes  = 967
0.00.084.167 I llama_new_context_with_model: graph splits = 2
0.00.084.182 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.795.383 I main: llama threadpool init, n_threads = 4
0.00.795.423 I 
0.00.795.446 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
0.00.795.446 I 
0.00.795.611 I sampler seed: 1234
0.00.795.615 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.795.625 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.795.627 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.795.627 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.510.729 I llama_perf_sampler_print:    sampling time =       1.11 ms /    71 runs   (    0.02 ms per token, 64137.31 tokens per second)
0.01.510.730 I llama_perf_context_print:        load time =     786.50 ms
0.01.510.731 I llama_perf_context_print: prompt eval time =      33.13 ms /     7 tokens (    4.73 ms per token,   211.30 tokens per second)
0.01.510.731 I llama_perf_context_print:        eval time =     679.06 ms /    63 runs   (   10.78 ms per token,    92.77 tokens per second)
0.01.510.732 I llama_perf_context_print:       total time =     715.35 ms /    70 tokens
0.01.510.911 I ggml_metal_free: deallocating

real	0m1.524s
user	0m0.107s
sys	0m0.209s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.042 I build: 4161 (0ba40c36) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.074 I main: llama backend init
0.00.000.076 I main: load the model and apply lora adapter, if any
0.00.009.899 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.312 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.016.316 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.317 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.318 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.318 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.318 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.323 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.324 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.324 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.325 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.325 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.325 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.326 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.326 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.327 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.329 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.329 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.359 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.465 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.181 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.182 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.183 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.183 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.183 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.184 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.025.184 I llama_model_loader: - type  f32:  194 tensors
0.00.025.184 I llama_model_loader: - type q5_0:   97 tensors
0.00.025.185 I llama_model_loader: - type q6_K:    1 tensors
0.00.046.404 I llm_load_vocab: special tokens cache size = 25
0.00.052.405 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.410 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.410 I llm_load_print_meta: arch             = gptneox
0.00.052.411 I llm_load_print_meta: vocab type       = BPE
0.00.052.411 I llm_load_print_meta: n_vocab          = 50304
0.00.052.411 I llm_load_print_meta: n_merges         = 50009
0.00.052.411 I llm_load_print_meta: vocab_only       = 0
0.00.052.412 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.412 I llm_load_print_meta: n_embd           = 2048
0.00.052.417 I llm_load_print_meta: n_layer          = 24
0.00.052.419 I llm_load_print_meta: n_head           = 16
0.00.052.420 I llm_load_print_meta: n_head_kv        = 16
0.00.052.420 I llm_load_print_meta: n_rot            = 32
0.00.052.420 I llm_load_print_meta: n_swa            = 0
0.00.052.420 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.421 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.421 I llm_load_print_meta: n_gqa            = 1
0.00.052.422 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.423 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.423 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.424 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.426 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.426 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.426 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.427 I llm_load_print_meta: n_ff             = 8192
0.00.052.427 I llm_load_print_meta: n_expert         = 0
0.00.052.427 I llm_load_print_meta: n_expert_used    = 0
0.00.052.427 I llm_load_print_meta: causal attn      = 1
0.00.052.427 I llm_load_print_meta: pooling type     = 0
0.00.052.427 I llm_load_print_meta: rope type        = 2
0.00.052.428 I llm_load_print_meta: rope scaling     = linear
0.00.052.428 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.428 I llm_load_print_meta: freq_scale_train = 1
0.00.052.429 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.429 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.429 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.429 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.429 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.429 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.429 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.441 I llm_load_print_meta: model type       = 1.4B
0.00.052.441 I llm_load_print_meta: model ftype      = Q5_0
0.00.052.442 I llm_load_print_meta: model params     = 1.41 B
0.00.052.442 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.052.443 I llm_load_print_meta: general.name     = 1.4B
0.00.052.443 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.443 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.443 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.443 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.444 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.052.444 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.444 I llm_load_print_meta: max token length = 1024
0.00.054.197 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.197 I llm_load_tensors: offloading output layer to GPU
0.00.054.197 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.207 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.054.208 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.055.107 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.107 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.108 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.108 I llama_new_context_with_model: n_batch       = 2048
0.00.055.108 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.108 I llama_new_context_with_model: flash_attn    = 0
0.00.055.109 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.109 I llama_new_context_with_model: freq_scale    = 1
0.00.055.109 I ggml_metal_init: allocating
0.00.055.113 I ggml_metal_init: found device: Apple M4
0.00.055.115 I ggml_metal_init: picking default device: Apple M4
0.00.055.661 I ggml_metal_init: using embedded metal library
0.00.057.558 I ggml_metal_init: GPU name:   Apple M4
0.00.057.559 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.560 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.560 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.560 I ggml_metal_init: simdgroup reduction   = true
0.00.057.560 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.560 I ggml_metal_init: has bfloat            = true
0.00.057.561 I ggml_metal_init: use bfloat            = true
0.00.057.561 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.562 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.084.854 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.860 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.881 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.085.818 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.085.820 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.085.820 I llama_new_context_with_model: graph nodes  = 967
0.00.085.820 I llama_new_context_with_model: graph splits = 2
0.00.085.841 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.810.338 I main: llama threadpool init, n_threads = 4
0.00.810.369 I 
0.00.810.404 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
0.00.810.405 I 
0.00.810.552 I sampler seed: 1234
0.00.810.558 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.810.567 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.810.568 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.810.568 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.586.261 I llama_perf_sampler_print:    sampling time =       1.26 ms /    71 runs   (    0.02 ms per token, 56393.96 tokens per second)
0.01.586.262 I llama_perf_context_print:        load time =     800.44 ms
0.01.586.263 I llama_perf_context_print: prompt eval time =      36.93 ms /     7 tokens (    5.28 ms per token,   189.55 tokens per second)
0.01.586.264 I llama_perf_context_print:        eval time =     735.68 ms /    63 runs   (   11.68 ms per token,    85.63 tokens per second)
0.01.586.267 I llama_perf_context_print:       total time =     775.93 ms /    70 tokens
0.01.586.449 I ggml_metal_free: deallocating

real	0m1.600s
user	0m0.107s
sys	0m0.231s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.036 I build: 4161 (0ba40c36) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.067 I main: llama backend init
0.00.000.070 I main: load the model and apply lora adapter, if any
0.00.009.244 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.797 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.015.801 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.805 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.806 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.806 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.806 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.807 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.807 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.808 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.808 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.808 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.810 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.811 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.811 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.813 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.813 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.813 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.647 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.706 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.486 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.487 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.488 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.488 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.488 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.488 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.489 I llama_model_loader: - type  f32:  194 tensors
0.00.024.489 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.490 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.836 I llm_load_vocab: special tokens cache size = 25
0.00.050.892 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.894 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.895 I llm_load_print_meta: arch             = gptneox
0.00.050.895 I llm_load_print_meta: vocab type       = BPE
0.00.050.895 I llm_load_print_meta: n_vocab          = 50304
0.00.050.895 I llm_load_print_meta: n_merges         = 50009
0.00.050.896 I llm_load_print_meta: vocab_only       = 0
0.00.050.896 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.896 I llm_load_print_meta: n_embd           = 2048
0.00.050.896 I llm_load_print_meta: n_layer          = 24
0.00.050.899 I llm_load_print_meta: n_head           = 16
0.00.050.900 I llm_load_print_meta: n_head_kv        = 16
0.00.050.900 I llm_load_print_meta: n_rot            = 32
0.00.050.900 I llm_load_print_meta: n_swa            = 0
0.00.050.901 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.901 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.901 I llm_load_print_meta: n_gqa            = 1
0.00.050.902 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.903 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.903 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.904 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.904 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.904 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.904 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.905 I llm_load_print_meta: n_ff             = 8192
0.00.050.906 I llm_load_print_meta: n_expert         = 0
0.00.050.906 I llm_load_print_meta: n_expert_used    = 0
0.00.050.906 I llm_load_print_meta: causal attn      = 1
0.00.050.906 I llm_load_print_meta: pooling type     = 0
0.00.050.906 I llm_load_print_meta: rope type        = 2
0.00.050.906 I llm_load_print_meta: rope scaling     = linear
0.00.050.907 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.907 I llm_load_print_meta: freq_scale_train = 1
0.00.050.907 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.908 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.908 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.908 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.908 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.908 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.908 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.920 I llm_load_print_meta: model type       = 1.4B
0.00.050.920 I llm_load_print_meta: model ftype      = Q5_1
0.00.050.920 I llm_load_print_meta: model params     = 1.41 B
0.00.050.921 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.050.921 I llm_load_print_meta: general.name     = 1.4B
0.00.050.921 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.921 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.922 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.922 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.922 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.922 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.922 I llm_load_print_meta: max token length = 1024
0.00.052.713 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.713 I llm_load_tensors: offloading output layer to GPU
0.00.052.713 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.723 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.052.724 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.053.600 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.600 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.601 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.601 I llama_new_context_with_model: n_batch       = 2048
0.00.053.601 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.601 I llama_new_context_with_model: flash_attn    = 0
0.00.053.602 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.602 I llama_new_context_with_model: freq_scale    = 1
0.00.053.602 I ggml_metal_init: allocating
0.00.053.605 I ggml_metal_init: found device: Apple M4
0.00.053.607 I ggml_metal_init: picking default device: Apple M4
0.00.054.152 I ggml_metal_init: using embedded metal library
0.00.056.077 I ggml_metal_init: GPU name:   Apple M4
0.00.056.078 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.078 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.079 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.079 I ggml_metal_init: simdgroup reduction   = true
0.00.056.079 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.079 I ggml_metal_init: has bfloat            = true
0.00.056.079 I ggml_metal_init: use bfloat            = true
0.00.056.080 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.080 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.085.138 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.143 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.163 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.146 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.147 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.147 I llama_new_context_with_model: graph nodes  = 967
0.00.086.148 I llama_new_context_with_model: graph splits = 2
0.00.086.170 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.846.068 I main: llama threadpool init, n_threads = 4
0.00.846.101 I 
0.00.846.124 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
0.00.846.124 I 
0.00.846.311 I sampler seed: 1234
0.00.846.316 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.846.325 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.846.326 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.846.326 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, literature and art.

If one's life is not happy, one can have no other way but

0.01.673.719 I llama_perf_sampler_print:    sampling time =       1.28 ms /    71 runs   (    0.02 ms per token, 55382.22 tokens per second)
0.01.673.720 I llama_perf_context_print:        load time =     836.82 ms
0.01.673.720 I llama_perf_context_print: prompt eval time =      37.01 ms /     7 tokens (    5.29 ms per token,   189.13 tokens per second)
0.01.673.721 I llama_perf_context_print:        eval time =     787.17 ms /    63 runs   (   12.49 ms per token,    80.03 tokens per second)
0.01.673.722 I llama_perf_context_print:       total time =     827.65 ms /    70 tokens
0.01.673.887 I ggml_metal_free: deallocating

real	0m1.687s
user	0m0.108s
sys	0m0.230s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.036 I build: 4161 (0ba40c36) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.067 I main: llama backend init
0.00.000.069 I main: load the model and apply lora adapter, if any
0.00.010.041 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.517 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.522 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.524 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.525 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.525 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.525 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.525 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.526 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.526 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.527 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.527 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.527 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.528 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.528 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.530 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.530 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.530 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.298 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.344 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.057 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.058 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.059 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.059 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.059 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.060 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.060 I llama_model_loader: - type  f32:  194 tensors
0.00.024.060 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.061 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.061 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.559 I llm_load_vocab: special tokens cache size = 25
0.00.050.539 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.542 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.543 I llm_load_print_meta: arch             = gptneox
0.00.050.543 I llm_load_print_meta: vocab type       = BPE
0.00.050.543 I llm_load_print_meta: n_vocab          = 50304
0.00.050.544 I llm_load_print_meta: n_merges         = 50009
0.00.050.544 I llm_load_print_meta: vocab_only       = 0
0.00.050.544 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.544 I llm_load_print_meta: n_embd           = 2048
0.00.050.544 I llm_load_print_meta: n_layer          = 24
0.00.050.547 I llm_load_print_meta: n_head           = 16
0.00.050.548 I llm_load_print_meta: n_head_kv        = 16
0.00.050.548 I llm_load_print_meta: n_rot            = 32
0.00.050.548 I llm_load_print_meta: n_swa            = 0
0.00.050.549 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.549 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.552 I llm_load_print_meta: n_gqa            = 1
0.00.050.553 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.554 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.554 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.555 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.555 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.555 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.555 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.556 I llm_load_print_meta: n_ff             = 8192
0.00.050.556 I llm_load_print_meta: n_expert         = 0
0.00.050.557 I llm_load_print_meta: n_expert_used    = 0
0.00.050.557 I llm_load_print_meta: causal attn      = 1
0.00.050.557 I llm_load_print_meta: pooling type     = 0
0.00.050.557 I llm_load_print_meta: rope type        = 2
0.00.050.557 I llm_load_print_meta: rope scaling     = linear
0.00.050.558 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.558 I llm_load_print_meta: freq_scale_train = 1
0.00.050.558 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.558 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.559 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.559 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.559 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.559 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.559 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.571 I llm_load_print_meta: model type       = 1.4B
0.00.050.571 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.050.571 I llm_load_print_meta: model params     = 1.41 B
0.00.050.572 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.050.572 I llm_load_print_meta: general.name     = 1.4B
0.00.050.572 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.573 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.573 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.573 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.573 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.573 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.573 I llm_load_print_meta: max token length = 1024
0.00.052.327 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.327 I llm_load_tensors: offloading output layer to GPU
0.00.052.327 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.337 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.052.338 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.053.244 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.245 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.245 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.245 I llama_new_context_with_model: n_batch       = 2048
0.00.053.246 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.246 I llama_new_context_with_model: flash_attn    = 0
0.00.053.246 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.247 I llama_new_context_with_model: freq_scale    = 1
0.00.053.247 I ggml_metal_init: allocating
0.00.053.253 I ggml_metal_init: found device: Apple M4
0.00.053.255 I ggml_metal_init: picking default device: Apple M4
0.00.053.805 I ggml_metal_init: using embedded metal library
0.00.055.763 I ggml_metal_init: GPU name:   Apple M4
0.00.055.765 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.765 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.766 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.766 I ggml_metal_init: simdgroup reduction   = true
0.00.055.766 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.766 I ggml_metal_init: has bfloat            = true
0.00.055.766 I ggml_metal_init: use bfloat            = true
0.00.055.767 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.767 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.083.547 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.083.554 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.083.574 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.084.544 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.084.545 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.084.546 I llama_new_context_with_model: graph nodes  = 967
0.00.084.546 I llama_new_context_with_model: graph splits = 2
0.00.084.567 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.523.652 I main: llama threadpool init, n_threads = 4
0.00.523.686 I 
0.00.523.709 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
0.00.523.709 I 
0.00.523.857 I sampler seed: 1234
0.00.523.863 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.523.873 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.523.874 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.523.874 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.212.663 I llama_perf_sampler_print:    sampling time =       1.15 ms /    71 runs   (    0.02 ms per token, 61900.61 tokens per second)
0.01.212.664 I llama_perf_context_print:        load time =     513.61 ms
0.01.212.665 I llama_perf_context_print: prompt eval time =      36.09 ms /     7 tokens (    5.16 ms per token,   193.94 tokens per second)
0.01.212.665 I llama_perf_context_print:        eval time =     649.77 ms /    63 runs   (   10.31 ms per token,    96.96 tokens per second)
0.01.212.666 I llama_perf_context_print:       total time =     689.01 ms /    70 tokens
0.01.212.833 I ggml_metal_free: deallocating

real	0m1.229s
user	0m0.108s
sys	0m0.154s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.036 I build: 4161 (0ba40c36) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.073 I main: llama backend init
0.00.000.075 I main: load the model and apply lora adapter, if any
0.00.009.347 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.187 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.192 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.198 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.199 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.199 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.200 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.200 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.201 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.201 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.202 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.202 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.202 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.203 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.203 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.205 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.205 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.206 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.234 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.298 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.124 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.125 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.125 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.126 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.126 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.126 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.127 I llama_model_loader: - type  f32:  194 tensors
0.00.025.127 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.127 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.128 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.128 I llama_model_loader: - type q6_K:    1 tensors
0.00.046.361 I llm_load_vocab: special tokens cache size = 25
0.00.052.324 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.327 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.327 I llm_load_print_meta: arch             = gptneox
0.00.052.327 I llm_load_print_meta: vocab type       = BPE
0.00.052.328 I llm_load_print_meta: n_vocab          = 50304
0.00.052.328 I llm_load_print_meta: n_merges         = 50009
0.00.052.328 I llm_load_print_meta: vocab_only       = 0
0.00.052.328 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.328 I llm_load_print_meta: n_embd           = 2048
0.00.052.329 I llm_load_print_meta: n_layer          = 24
0.00.052.331 I llm_load_print_meta: n_head           = 16
0.00.052.332 I llm_load_print_meta: n_head_kv        = 16
0.00.052.332 I llm_load_print_meta: n_rot            = 32
0.00.052.332 I llm_load_print_meta: n_swa            = 0
0.00.052.332 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.335 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.336 I llm_load_print_meta: n_gqa            = 1
0.00.052.337 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.338 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.338 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.339 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.339 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.339 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.339 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.340 I llm_load_print_meta: n_ff             = 8192
0.00.052.340 I llm_load_print_meta: n_expert         = 0
0.00.052.340 I llm_load_print_meta: n_expert_used    = 0
0.00.052.341 I llm_load_print_meta: causal attn      = 1
0.00.052.343 I llm_load_print_meta: pooling type     = 0
0.00.052.343 I llm_load_print_meta: rope type        = 2
0.00.052.343 I llm_load_print_meta: rope scaling     = linear
0.00.052.343 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.344 I llm_load_print_meta: freq_scale_train = 1
0.00.052.344 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.344 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.348 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.349 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.349 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.349 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.349 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.360 I llm_load_print_meta: model type       = 1.4B
0.00.052.360 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.052.361 I llm_load_print_meta: model params     = 1.41 B
0.00.052.361 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.052.361 I llm_load_print_meta: general.name     = 1.4B
0.00.052.362 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.362 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.362 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.362 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.363 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.052.363 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.363 I llm_load_print_meta: max token length = 1024
0.00.054.114 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.114 I llm_load_tensors: offloading output layer to GPU
0.00.054.114 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.124 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.054.125 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.054.956 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.957 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.957 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.957 I llama_new_context_with_model: n_batch       = 2048
0.00.054.957 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.957 I llama_new_context_with_model: flash_attn    = 0
0.00.054.958 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.958 I llama_new_context_with_model: freq_scale    = 1
0.00.054.958 I ggml_metal_init: allocating
0.00.054.961 I ggml_metal_init: found device: Apple M4
0.00.054.963 I ggml_metal_init: picking default device: Apple M4
0.00.055.498 I ggml_metal_init: using embedded metal library
0.00.057.474 I ggml_metal_init: GPU name:   Apple M4
0.00.057.475 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.476 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.476 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.476 I ggml_metal_init: simdgroup reduction   = true
0.00.057.476 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.477 I ggml_metal_init: has bfloat            = true
0.00.057.477 I ggml_metal_init: use bfloat            = true
0.00.057.477 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.478 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.083.916 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.083.925 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.083.945 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.084.911 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.084.913 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.084.913 I llama_new_context_with_model: graph nodes  = 967
0.00.084.913 I llama_new_context_with_model: graph splits = 2
0.00.084.934 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.636.882 I main: llama threadpool init, n_threads = 4
0.00.636.913 I 
0.00.636.957 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
0.00.636.959 I 
0.00.637.098 I sampler seed: 1234
0.00.637.104 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.637.134 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.637.143 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.637.143 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do that is most useful to my fellow men?'"

-Albert Einstein

"The way

0.01.379.457 I llama_perf_sampler_print:    sampling time =       1.20 ms /    71 runs   (    0.02 ms per token, 59364.55 tokens per second)
0.01.379.457 I llama_perf_context_print:        load time =     627.53 ms
0.01.379.458 I llama_perf_context_print: prompt eval time =      36.02 ms /     7 tokens (    5.15 ms per token,   194.31 tokens per second)
0.01.379.459 I llama_perf_context_print:        eval time =     703.28 ms /    63 runs   (   11.16 ms per token,    89.58 tokens per second)
0.01.379.459 I llama_perf_context_print:       total time =     742.58 ms /    70 tokens
0.01.379.633 I ggml_metal_free: deallocating

real	0m1.395s
user	0m0.108s
sys	0m0.184s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.035 I build: 4161 (0ba40c36) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.066 I main: llama backend init
0.00.000.068 I main: load the model and apply lora adapter, if any
0.00.009.581 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.940 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.945 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.946 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.951 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.952 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.952 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.953 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.954 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.954 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.954 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.955 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.955 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.955 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.956 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.957 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.958 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.958 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.889 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.919 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.689 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.690 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.691 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.691 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.691 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.692 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.692 I llama_model_loader: - type  f32:  194 tensors
0.00.025.693 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.693 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.693 I llama_model_loader: - type q6_K:   13 tensors
0.00.046.951 I llm_load_vocab: special tokens cache size = 25
0.00.053.032 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.053.035 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.053.036 I llm_load_print_meta: arch             = gptneox
0.00.053.036 I llm_load_print_meta: vocab type       = BPE
0.00.053.036 I llm_load_print_meta: n_vocab          = 50304
0.00.053.036 I llm_load_print_meta: n_merges         = 50009
0.00.053.037 I llm_load_print_meta: vocab_only       = 0
0.00.053.037 I llm_load_print_meta: n_ctx_train      = 2048
0.00.053.037 I llm_load_print_meta: n_embd           = 2048
0.00.053.037 I llm_load_print_meta: n_layer          = 24
0.00.053.040 I llm_load_print_meta: n_head           = 16
0.00.053.040 I llm_load_print_meta: n_head_kv        = 16
0.00.053.041 I llm_load_print_meta: n_rot            = 32
0.00.053.044 I llm_load_print_meta: n_swa            = 0
0.00.053.044 I llm_load_print_meta: n_embd_head_k    = 128
0.00.053.044 I llm_load_print_meta: n_embd_head_v    = 128
0.00.053.045 I llm_load_print_meta: n_gqa            = 1
0.00.053.046 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.053.046 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.053.053 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.053.054 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.053.054 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.053.055 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.053.055 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.053.057 I llm_load_print_meta: n_ff             = 8192
0.00.053.057 I llm_load_print_meta: n_expert         = 0
0.00.053.057 I llm_load_print_meta: n_expert_used    = 0
0.00.053.057 I llm_load_print_meta: causal attn      = 1
0.00.053.057 I llm_load_print_meta: pooling type     = 0
0.00.053.058 I llm_load_print_meta: rope type        = 2
0.00.053.058 I llm_load_print_meta: rope scaling     = linear
0.00.053.058 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.053.059 I llm_load_print_meta: freq_scale_train = 1
0.00.053.059 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.053.059 I llm_load_print_meta: rope_finetuned   = unknown
0.00.053.059 I llm_load_print_meta: ssm_d_conv       = 0
0.00.053.059 I llm_load_print_meta: ssm_d_inner      = 0
0.00.053.059 I llm_load_print_meta: ssm_d_state      = 0
0.00.053.059 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.053.059 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.053.072 I llm_load_print_meta: model type       = 1.4B
0.00.053.074 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.053.074 I llm_load_print_meta: model params     = 1.41 B
0.00.053.075 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.053.075 I llm_load_print_meta: general.name     = 1.4B
0.00.053.075 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.053.075 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.053.075 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.053.075 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.053.078 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.053.079 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.053.079 I llm_load_print_meta: max token length = 1024
0.00.054.855 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.856 I llm_load_tensors: offloading output layer to GPU
0.00.054.856 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.865 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.054.866 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.055.726 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.727 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.727 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.727 I llama_new_context_with_model: n_batch       = 2048
0.00.055.727 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.728 I llama_new_context_with_model: flash_attn    = 0
0.00.055.728 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.728 I llama_new_context_with_model: freq_scale    = 1
0.00.055.729 I ggml_metal_init: allocating
0.00.055.735 I ggml_metal_init: found device: Apple M4
0.00.055.738 I ggml_metal_init: picking default device: Apple M4
0.00.056.289 I ggml_metal_init: using embedded metal library
0.00.058.200 I ggml_metal_init: GPU name:   Apple M4
0.00.058.202 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.202 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.203 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.203 I ggml_metal_init: simdgroup reduction   = true
0.00.058.203 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.203 I ggml_metal_init: has bfloat            = true
0.00.058.203 I ggml_metal_init: use bfloat            = true
0.00.058.204 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.205 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.086.129 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.139 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.156 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.182 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.184 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.184 I llama_new_context_with_model: graph nodes  = 967
0.00.087.184 I llama_new_context_with_model: graph splits = 2
0.00.087.207 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.728.957 I main: llama threadpool init, n_threads = 4
0.00.728.997 I 
0.00.729.015 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
0.00.729.015 I 
0.00.729.164 I sampler seed: 1234
0.00.729.169 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.729.179 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.729.180 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.729.180 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the greatest power to create the future we want to see. I believe that we are in a wonderful time when people from all over the world are coming together to create a better world. I believe that we are going

0.01.474.060 I llama_perf_sampler_print:    sampling time =       1.22 ms /    71 runs   (    0.02 ms per token, 58053.97 tokens per second)
0.01.474.060 I llama_perf_context_print:        load time =     719.37 ms
0.01.474.061 I llama_perf_context_print: prompt eval time =      36.84 ms /     7 tokens (    5.26 ms per token,   190.02 tokens per second)
0.01.474.062 I llama_perf_context_print:        eval time =     705.01 ms /    63 runs   (   11.19 ms per token,    89.36 tokens per second)
0.01.474.062 I llama_perf_context_print:       total time =     745.11 ms /    70 tokens
0.01.474.229 I ggml_metal_free: deallocating

real	0m1.488s
user	0m0.108s
sys	0m0.205s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.036 I build: 4161 (0ba40c36) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.067 I main: llama backend init
0.00.000.069 I main: load the model and apply lora adapter, if any
0.00.008.371 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.761 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.014.766 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.767 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.768 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.768 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.769 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.769 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.771 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.772 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.772 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.772 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.773 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.773 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.777 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.779 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.779 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.780 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.212 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.560 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.938 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.939 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.939 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.939 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.940 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.940 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.941 I llama_model_loader: - type  f32:  194 tensors
0.00.024.941 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.941 I llama_model_loader: - type q6_K:   37 tensors
0.00.045.594 I llm_load_vocab: special tokens cache size = 25
0.00.051.734 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.737 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.737 I llm_load_print_meta: arch             = gptneox
0.00.051.737 I llm_load_print_meta: vocab type       = BPE
0.00.051.738 I llm_load_print_meta: n_vocab          = 50304
0.00.051.738 I llm_load_print_meta: n_merges         = 50009
0.00.051.738 I llm_load_print_meta: vocab_only       = 0
0.00.051.738 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.738 I llm_load_print_meta: n_embd           = 2048
0.00.051.739 I llm_load_print_meta: n_layer          = 24
0.00.051.741 I llm_load_print_meta: n_head           = 16
0.00.051.742 I llm_load_print_meta: n_head_kv        = 16
0.00.051.742 I llm_load_print_meta: n_rot            = 32
0.00.051.742 I llm_load_print_meta: n_swa            = 0
0.00.051.742 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.743 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.743 I llm_load_print_meta: n_gqa            = 1
0.00.051.744 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.745 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.747 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.747 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.747 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.747 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.747 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.748 I llm_load_print_meta: n_ff             = 8192
0.00.051.748 I llm_load_print_meta: n_expert         = 0
0.00.051.748 I llm_load_print_meta: n_expert_used    = 0
0.00.051.749 I llm_load_print_meta: causal attn      = 1
0.00.051.749 I llm_load_print_meta: pooling type     = 0
0.00.051.749 I llm_load_print_meta: rope type        = 2
0.00.051.749 I llm_load_print_meta: rope scaling     = linear
0.00.051.749 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.750 I llm_load_print_meta: freq_scale_train = 1
0.00.051.750 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.750 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.750 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.751 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.751 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.751 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.751 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.762 I llm_load_print_meta: model type       = 1.4B
0.00.051.763 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.051.763 I llm_load_print_meta: model params     = 1.41 B
0.00.051.763 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.051.764 I llm_load_print_meta: general.name     = 1.4B
0.00.051.764 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.764 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.764 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.764 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.765 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.767 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.767 I llm_load_print_meta: max token length = 1024
0.00.053.455 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.455 I llm_load_tensors: offloading output layer to GPU
0.00.053.455 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.465 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.053.466 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.054.322 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.322 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.323 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.323 I llama_new_context_with_model: n_batch       = 2048
0.00.054.323 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.323 I llama_new_context_with_model: flash_attn    = 0
0.00.054.323 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.324 I llama_new_context_with_model: freq_scale    = 1
0.00.054.324 I ggml_metal_init: allocating
0.00.054.329 I ggml_metal_init: found device: Apple M4
0.00.054.332 I ggml_metal_init: picking default device: Apple M4
0.00.054.879 I ggml_metal_init: using embedded metal library
0.00.056.757 I ggml_metal_init: GPU name:   Apple M4
0.00.056.758 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.759 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.759 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.759 I ggml_metal_init: simdgroup reduction   = true
0.00.056.759 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.759 I ggml_metal_init: has bfloat            = true
0.00.056.760 I ggml_metal_init: use bfloat            = true
0.00.056.760 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.761 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.083.746 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.083.755 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.083.782 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.084.737 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.084.739 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.084.739 I llama_new_context_with_model: graph nodes  = 967
0.00.084.739 I llama_new_context_with_model: graph splits = 2
0.00.084.761 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.805.504 I main: llama threadpool init, n_threads = 4
0.00.805.534 I 
0.00.805.580 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
0.00.805.582 I 
0.00.805.730 I sampler seed: 1234
0.00.805.735 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.805.777 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.805.779 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.805.779 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.635.953 I llama_perf_sampler_print:    sampling time =       1.14 ms /    71 runs   (    0.02 ms per token, 62062.94 tokens per second)
0.01.635.953 I llama_perf_context_print:        load time =     797.13 ms
0.01.635.956 I llama_perf_context_print: prompt eval time =      38.98 ms /     7 tokens (    5.57 ms per token,   179.60 tokens per second)
0.01.635.957 I llama_perf_context_print:        eval time =     788.28 ms /    63 runs   (   12.51 ms per token,    79.92 tokens per second)
0.01.635.958 I llama_perf_context_print:       total time =     830.45 ms /    70 tokens
0.01.636.126 I ggml_metal_free: deallocating

real	0m1.652s
user	0m0.109s
sys	0m0.220s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.035 I build: 4161 (0ba40c36) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.066 I main: llama backend init
0.00.000.068 I main: load the model and apply lora adapter, if any
0.00.009.786 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.998 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.002 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.008 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.008 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.009 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.009 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.009 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.010 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.011 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.011 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.012 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.012 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.012 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.013 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.014 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.015 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.015 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.833 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.869 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.695 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.697 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.697 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.697 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.698 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.698 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.699 I llama_model_loader: - type  f32:  194 tensors
0.00.024.699 I llama_model_loader: - type q6_K:   98 tensors
0.00.045.845 I llm_load_vocab: special tokens cache size = 25
0.00.051.904 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.907 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.907 I llm_load_print_meta: arch             = gptneox
0.00.051.907 I llm_load_print_meta: vocab type       = BPE
0.00.051.908 I llm_load_print_meta: n_vocab          = 50304
0.00.051.908 I llm_load_print_meta: n_merges         = 50009
0.00.051.908 I llm_load_print_meta: vocab_only       = 0
0.00.051.908 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.908 I llm_load_print_meta: n_embd           = 2048
0.00.051.909 I llm_load_print_meta: n_layer          = 24
0.00.051.911 I llm_load_print_meta: n_head           = 16
0.00.051.912 I llm_load_print_meta: n_head_kv        = 16
0.00.051.912 I llm_load_print_meta: n_rot            = 32
0.00.051.913 I llm_load_print_meta: n_swa            = 0
0.00.051.913 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.913 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.914 I llm_load_print_meta: n_gqa            = 1
0.00.051.914 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.915 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.916 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.916 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.916 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.916 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.916 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.917 I llm_load_print_meta: n_ff             = 8192
0.00.051.917 I llm_load_print_meta: n_expert         = 0
0.00.051.917 I llm_load_print_meta: n_expert_used    = 0
0.00.051.917 I llm_load_print_meta: causal attn      = 1
0.00.051.918 I llm_load_print_meta: pooling type     = 0
0.00.051.918 I llm_load_print_meta: rope type        = 2
0.00.051.918 I llm_load_print_meta: rope scaling     = linear
0.00.051.918 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.919 I llm_load_print_meta: freq_scale_train = 1
0.00.051.919 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.919 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.919 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.919 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.919 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.920 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.922 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.933 I llm_load_print_meta: model type       = 1.4B
0.00.051.933 I llm_load_print_meta: model ftype      = Q6_K
0.00.051.934 I llm_load_print_meta: model params     = 1.41 B
0.00.051.934 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.051.935 I llm_load_print_meta: general.name     = 1.4B
0.00.051.935 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.937 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.937 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.937 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.937 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.937 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.938 I llm_load_print_meta: max token length = 1024
0.00.053.653 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.654 I llm_load_tensors: offloading output layer to GPU
0.00.053.654 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.663 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.053.664 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.054.497 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.498 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.498 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.498 I llama_new_context_with_model: n_batch       = 2048
0.00.054.499 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.499 I llama_new_context_with_model: flash_attn    = 0
0.00.054.499 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.499 I llama_new_context_with_model: freq_scale    = 1
0.00.054.500 I ggml_metal_init: allocating
0.00.054.503 I ggml_metal_init: found device: Apple M4
0.00.054.505 I ggml_metal_init: picking default device: Apple M4
0.00.055.097 I ggml_metal_init: using embedded metal library
0.00.057.012 I ggml_metal_init: GPU name:   Apple M4
0.00.057.014 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.014 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.015 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.015 I ggml_metal_init: simdgroup reduction   = true
0.00.057.015 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.015 I ggml_metal_init: has bfloat            = true
0.00.057.015 I ggml_metal_init: use bfloat            = true
0.00.057.016 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.017 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.083.524 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.083.533 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.083.554 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.084.481 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.084.483 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.084.483 I llama_new_context_with_model: graph nodes  = 967
0.00.084.483 I llama_new_context_with_model: graph splits = 2
0.00.084.510 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.874.899 I main: llama threadpool init, n_threads = 4
0.00.874.932 I 
0.00.874.964 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
0.00.874.964 I 
0.00.875.108 I sampler seed: 1234
0.00.875.113 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.875.157 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.875.168 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.875.168 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.727.702 I llama_perf_sampler_print:    sampling time =       1.23 ms /    71 runs   (    0.02 ms per token, 57583.13 tokens per second)
0.01.727.703 I llama_perf_context_print:        load time =     865.11 ms
0.01.727.704 I llama_perf_context_print: prompt eval time =      38.91 ms /     7 tokens (    5.56 ms per token,   179.88 tokens per second)
0.01.727.704 I llama_perf_context_print:        eval time =     810.49 ms /    63 runs   (   12.86 ms per token,    77.73 tokens per second)
0.01.727.705 I llama_perf_context_print:       total time =     852.81 ms /    70 tokens
0.01.727.873 I ggml_metal_free: deallocating

real	0m1.742s
user	0m0.108s
sys	0m0.245s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.575 I build: 4161 (0ba40c36) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.024.163 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.037.873 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.037.878 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.037.880 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.037.880 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.037.882 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.037.883 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.037.883 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.037.884 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.037.884 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.037.885 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.037.885 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.037.886 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.037.886 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.037.886 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.037.889 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.037.889 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.037.889 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.045.178 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.047.189 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.053.686 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.053.688 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.053.688 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.053.689 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.053.689 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.053.690 I llama_model_loader: - type  f32:  194 tensors
0.00.053.690 I llama_model_loader: - type  f16:   98 tensors
0.00.081.344 I llm_load_vocab: special tokens cache size = 25
0.00.087.885 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.087.888 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.087.888 I llm_load_print_meta: arch             = gptneox
0.00.087.888 I llm_load_print_meta: vocab type       = BPE
0.00.087.888 I llm_load_print_meta: n_vocab          = 50304
0.00.087.889 I llm_load_print_meta: n_merges         = 50009
0.00.087.889 I llm_load_print_meta: vocab_only       = 0
0.00.087.889 I llm_load_print_meta: n_ctx_train      = 2048
0.00.087.889 I llm_load_print_meta: n_embd           = 2048
0.00.087.889 I llm_load_print_meta: n_layer          = 24
0.00.087.891 I llm_load_print_meta: n_head           = 16
0.00.087.896 I llm_load_print_meta: n_head_kv        = 16
0.00.087.897 I llm_load_print_meta: n_rot            = 32
0.00.087.897 I llm_load_print_meta: n_swa            = 0
0.00.087.897 I llm_load_print_meta: n_embd_head_k    = 128
0.00.087.897 I llm_load_print_meta: n_embd_head_v    = 128
0.00.087.898 I llm_load_print_meta: n_gqa            = 1
0.00.087.898 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.087.899 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.087.900 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.087.900 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.087.900 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.087.900 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.087.900 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.087.901 I llm_load_print_meta: n_ff             = 8192
0.00.087.901 I llm_load_print_meta: n_expert         = 0
0.00.087.901 I llm_load_print_meta: n_expert_used    = 0
0.00.087.901 I llm_load_print_meta: causal attn      = 1
0.00.087.901 I llm_load_print_meta: pooling type     = 0
0.00.087.902 I llm_load_print_meta: rope type        = 2
0.00.087.902 I llm_load_print_meta: rope scaling     = linear
0.00.087.902 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.087.902 I llm_load_print_meta: freq_scale_train = 1
0.00.087.903 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.087.903 I llm_load_print_meta: rope_finetuned   = unknown
0.00.087.903 I llm_load_print_meta: ssm_d_conv       = 0
0.00.087.903 I llm_load_print_meta: ssm_d_inner      = 0
0.00.087.903 I llm_load_print_meta: ssm_d_state      = 0
0.00.087.903 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.087.903 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.087.914 I llm_load_print_meta: model type       = 1.4B
0.00.087.915 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.087.915 I llm_load_print_meta: model params     = 1.41 B
0.00.087.916 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.087.916 I llm_load_print_meta: general.name     = 1.4B
0.00.087.917 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.087.917 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.087.917 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.087.917 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.087.917 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.087.918 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.087.918 I llm_load_print_meta: max token length = 1024
0.00.089.853 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.089.853 I llm_load_tensors: offloading output layer to GPU
0.00.089.853 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.089.862 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.089.864 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.090.730 I llama_new_context_with_model: n_seq_max     = 1
0.00.090.731 I llama_new_context_with_model: n_ctx         = 128
0.00.090.731 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.090.732 I llama_new_context_with_model: n_batch       = 128
0.00.090.732 I llama_new_context_with_model: n_ubatch      = 128
0.00.090.732 I llama_new_context_with_model: flash_attn    = 0
0.00.090.732 I llama_new_context_with_model: freq_base     = 10000.0
0.00.090.733 I llama_new_context_with_model: freq_scale    = 1
0.00.090.733 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.090.734 I ggml_metal_init: allocating
0.00.090.737 I ggml_metal_init: found device: Apple M4
0.00.090.739 I ggml_metal_init: picking default device: Apple M4
0.00.091.290 I ggml_metal_init: using embedded metal library
0.00.093.405 I ggml_metal_init: GPU name:   Apple M4
0.00.093.406 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.093.407 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.093.407 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.093.407 I ggml_metal_init: simdgroup reduction   = true
0.00.093.408 I ggml_metal_init: simdgroup matrix mul. = true
0.00.093.408 I ggml_metal_init: has bfloat            = true
0.00.093.408 I ggml_metal_init: use bfloat            = true
0.00.093.408 I ggml_metal_init: hasUnifiedMemory      = true
0.00.093.410 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.103.322 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.103.324 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.103.339 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.104.282 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.104.284 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.104.284 I llama_new_context_with_model: graph nodes  = 967
0.00.104.284 I llama_new_context_with_model: graph splits = 2
0.00.104.295 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.191.412 I 
0.01.191.462 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
0.01.191.524 I perplexity: tokenizing the input ..
0.01.203.501 I perplexity: tokenization took 11.978 ms
0.01.203.507 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.323.764 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.325.346 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.325.372 I llama_perf_context_print:        load time =    1167.24 ms
0.01.325.374 I llama_perf_context_print: prompt eval time =     120.00 ms /   128 tokens (    0.94 ms per token,  1066.68 tokens per second)
0.01.325.375 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.325.376 I llama_perf_context_print:       total time =     133.97 ms /   129 tokens
0.01.325.884 I ggml_metal_free: deallocating

real	0m1.518s
user	0m0.120s
sys	0m0.268s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.114 I build: 4161 (0ba40c36) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.705 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.018.375 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.018.380 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.382 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.383 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.383 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.383 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.384 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.385 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.385 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.386 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.386 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.386 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.387 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.387 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.388 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.389 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.389 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.375 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.751 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.029.725 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.029.728 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.029.728 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.029.728 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.029.729 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.029.729 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.029.729 I llama_model_loader: - type  f32:  194 tensors
0.00.029.730 I llama_model_loader: - type q8_0:   98 tensors
0.00.053.575 I llm_load_vocab: special tokens cache size = 25
0.00.059.727 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.059.730 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.059.731 I llm_load_print_meta: arch             = gptneox
0.00.059.731 I llm_load_print_meta: vocab type       = BPE
0.00.059.731 I llm_load_print_meta: n_vocab          = 50304
0.00.059.731 I llm_load_print_meta: n_merges         = 50009
0.00.059.731 I llm_load_print_meta: vocab_only       = 0
0.00.059.732 I llm_load_print_meta: n_ctx_train      = 2048
0.00.059.732 I llm_load_print_meta: n_embd           = 2048
0.00.059.732 I llm_load_print_meta: n_layer          = 24
0.00.059.735 I llm_load_print_meta: n_head           = 16
0.00.059.736 I llm_load_print_meta: n_head_kv        = 16
0.00.059.736 I llm_load_print_meta: n_rot            = 32
0.00.059.736 I llm_load_print_meta: n_swa            = 0
0.00.059.736 I llm_load_print_meta: n_embd_head_k    = 128
0.00.059.736 I llm_load_print_meta: n_embd_head_v    = 128
0.00.059.737 I llm_load_print_meta: n_gqa            = 1
0.00.059.738 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.059.738 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.059.739 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.059.739 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.059.739 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.059.739 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.059.739 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.059.740 I llm_load_print_meta: n_ff             = 8192
0.00.059.740 I llm_load_print_meta: n_expert         = 0
0.00.059.740 I llm_load_print_meta: n_expert_used    = 0
0.00.059.740 I llm_load_print_meta: causal attn      = 1
0.00.059.740 I llm_load_print_meta: pooling type     = 0
0.00.059.740 I llm_load_print_meta: rope type        = 2
0.00.059.741 I llm_load_print_meta: rope scaling     = linear
0.00.059.741 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.059.741 I llm_load_print_meta: freq_scale_train = 1
0.00.059.741 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.059.741 I llm_load_print_meta: rope_finetuned   = unknown
0.00.059.742 I llm_load_print_meta: ssm_d_conv       = 0
0.00.059.742 I llm_load_print_meta: ssm_d_inner      = 0
0.00.059.742 I llm_load_print_meta: ssm_d_state      = 0
0.00.059.742 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.059.742 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.059.753 I llm_load_print_meta: model type       = 1.4B
0.00.059.753 I llm_load_print_meta: model ftype      = Q8_0
0.00.059.753 I llm_load_print_meta: model params     = 1.41 B
0.00.059.754 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.059.754 I llm_load_print_meta: general.name     = 1.4B
0.00.059.754 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.059.754 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.059.755 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.059.755 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.059.755 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.059.755 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.059.755 I llm_load_print_meta: max token length = 1024
0.00.061.657 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.061.657 I llm_load_tensors: offloading output layer to GPU
0.00.061.657 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.061.666 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.061.668 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.062.546 I llama_new_context_with_model: n_seq_max     = 1
0.00.062.547 I llama_new_context_with_model: n_ctx         = 128
0.00.062.548 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.062.548 I llama_new_context_with_model: n_batch       = 128
0.00.062.548 I llama_new_context_with_model: n_ubatch      = 128
0.00.062.548 I llama_new_context_with_model: flash_attn    = 0
0.00.062.548 I llama_new_context_with_model: freq_base     = 10000.0
0.00.062.549 I llama_new_context_with_model: freq_scale    = 1
0.00.062.549 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.062.549 I ggml_metal_init: allocating
0.00.062.552 I ggml_metal_init: found device: Apple M4
0.00.062.553 I ggml_metal_init: picking default device: Apple M4
0.00.063.083 I ggml_metal_init: using embedded metal library
0.00.065.025 I ggml_metal_init: GPU name:   Apple M4
0.00.065.026 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.065.027 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.065.027 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.065.027 I ggml_metal_init: simdgroup reduction   = true
0.00.065.027 I ggml_metal_init: simdgroup matrix mul. = true
0.00.065.028 I ggml_metal_init: has bfloat            = true
0.00.065.028 I ggml_metal_init: use bfloat            = true
0.00.065.028 I ggml_metal_init: hasUnifiedMemory      = true
0.00.065.029 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.073.095 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.073.098 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.073.112 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.073.999 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.074.000 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.074.000 I llama_new_context_with_model: graph nodes  = 967
0.00.074.001 I llama_new_context_with_model: graph splits = 2
0.00.074.012 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.017.279 I 
0.01.017.297 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
0.01.017.309 I perplexity: tokenizing the input ..
0.01.024.922 I perplexity: tokenization took 7.612 ms
0.01.024.927 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.147.401 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.148.525 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.148.541 I llama_perf_context_print:        load time =    1006.57 ms
0.01.148.543 I llama_perf_context_print: prompt eval time =     122.25 ms /   128 tokens (    0.96 ms per token,  1046.99 tokens per second)
0.01.148.544 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.148.544 I llama_perf_context_print:       total time =     131.26 ms /   129 tokens
0.01.148.940 I ggml_metal_free: deallocating

real	0m1.164s
user	0m0.085s
sys	0m0.212s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.082 I build: 4161 (0ba40c36) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.115 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.852 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.014.856 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.858 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.863 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.864 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.864 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.864 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.865 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.865 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.866 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.866 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.866 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.867 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.867 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.869 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.869 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.869 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.677 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.740 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.519 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.520 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.521 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.521 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.521 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.522 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.023.522 I llama_model_loader: - type  f32:  194 tensors
0.00.023.522 I llama_model_loader: - type q4_0:   97 tensors
0.00.023.523 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.688 I llm_load_vocab: special tokens cache size = 25
0.00.049.679 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.682 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.682 I llm_load_print_meta: arch             = gptneox
0.00.049.682 I llm_load_print_meta: vocab type       = BPE
0.00.049.683 I llm_load_print_meta: n_vocab          = 50304
0.00.049.683 I llm_load_print_meta: n_merges         = 50009
0.00.049.683 I llm_load_print_meta: vocab_only       = 0
0.00.049.683 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.683 I llm_load_print_meta: n_embd           = 2048
0.00.049.683 I llm_load_print_meta: n_layer          = 24
0.00.049.686 I llm_load_print_meta: n_head           = 16
0.00.049.687 I llm_load_print_meta: n_head_kv        = 16
0.00.049.687 I llm_load_print_meta: n_rot            = 32
0.00.049.687 I llm_load_print_meta: n_swa            = 0
0.00.049.688 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.688 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.689 I llm_load_print_meta: n_gqa            = 1
0.00.049.689 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.690 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.690 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.691 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.693 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.693 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.693 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.694 I llm_load_print_meta: n_ff             = 8192
0.00.049.694 I llm_load_print_meta: n_expert         = 0
0.00.049.694 I llm_load_print_meta: n_expert_used    = 0
0.00.049.694 I llm_load_print_meta: causal attn      = 1
0.00.049.694 I llm_load_print_meta: pooling type     = 0
0.00.049.694 I llm_load_print_meta: rope type        = 2
0.00.049.694 I llm_load_print_meta: rope scaling     = linear
0.00.049.696 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.696 I llm_load_print_meta: freq_scale_train = 1
0.00.049.696 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.697 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.697 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.697 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.697 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.697 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.697 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.708 I llm_load_print_meta: model type       = 1.4B
0.00.049.709 I llm_load_print_meta: model ftype      = Q4_0
0.00.049.709 I llm_load_print_meta: model params     = 1.41 B
0.00.049.710 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.049.710 I llm_load_print_meta: general.name     = 1.4B
0.00.049.710 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.710 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.710 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.710 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.711 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.049.711 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.711 I llm_load_print_meta: max token length = 1024
0.00.051.413 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.413 I llm_load_tensors: offloading output layer to GPU
0.00.051.414 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.423 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.051.424 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.052.295 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.295 I llama_new_context_with_model: n_ctx         = 128
0.00.052.295 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.296 I llama_new_context_with_model: n_batch       = 128
0.00.052.296 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.296 I llama_new_context_with_model: flash_attn    = 0
0.00.052.296 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.297 I llama_new_context_with_model: freq_scale    = 1
0.00.052.297 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.297 I ggml_metal_init: allocating
0.00.052.303 I ggml_metal_init: found device: Apple M4
0.00.052.305 I ggml_metal_init: picking default device: Apple M4
0.00.052.853 I ggml_metal_init: using embedded metal library
0.00.054.770 I ggml_metal_init: GPU name:   Apple M4
0.00.054.771 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.772 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.772 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.772 I ggml_metal_init: simdgroup reduction   = true
0.00.054.772 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.772 I ggml_metal_init: has bfloat            = true
0.00.054.772 I ggml_metal_init: use bfloat            = true
0.00.054.773 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.774 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.853 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.063.856 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.063.870 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.064.759 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.064.760 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.064.761 I llama_new_context_with_model: graph nodes  = 967
0.00.064.761 I llama_new_context_with_model: graph splits = 2
0.00.064.773 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.648.653 I 
0.00.648.674 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
0.00.648.687 I perplexity: tokenizing the input ..
0.00.656.304 I perplexity: tokenization took 7.614 ms
0.00.656.308 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.779.246 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.780.334 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.780.353 I llama_perf_context_print:        load time =     639.53 ms
0.00.780.354 I llama_perf_context_print: prompt eval time =     122.72 ms /   128 tokens (    0.96 ms per token,  1043.04 tokens per second)
0.00.780.355 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.780.355 I llama_perf_context_print:       total time =     131.70 ms /   129 tokens
0.00.780.850 I ggml_metal_free: deallocating

real	0m0.794s
user	0m0.076s
sys	0m0.145s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.083 I build: 4161 (0ba40c36) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.989 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.710 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.014.714 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.716 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.717 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.717 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.717 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.718 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.718 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.719 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.719 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.721 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.722 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.722 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.722 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.724 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.725 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.725 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.494 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.546 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.332 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.334 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.334 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.334 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.335 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.335 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.023.335 I llama_model_loader: - type  f32:  194 tensors
0.00.023.336 I llama_model_loader: - type q4_1:   97 tensors
0.00.023.336 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.544 I llm_load_vocab: special tokens cache size = 25
0.00.049.526 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.529 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.530 I llm_load_print_meta: arch             = gptneox
0.00.049.530 I llm_load_print_meta: vocab type       = BPE
0.00.049.530 I llm_load_print_meta: n_vocab          = 50304
0.00.049.530 I llm_load_print_meta: n_merges         = 50009
0.00.049.531 I llm_load_print_meta: vocab_only       = 0
0.00.049.531 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.531 I llm_load_print_meta: n_embd           = 2048
0.00.049.531 I llm_load_print_meta: n_layer          = 24
0.00.049.533 I llm_load_print_meta: n_head           = 16
0.00.049.534 I llm_load_print_meta: n_head_kv        = 16
0.00.049.534 I llm_load_print_meta: n_rot            = 32
0.00.049.535 I llm_load_print_meta: n_swa            = 0
0.00.049.535 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.535 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.536 I llm_load_print_meta: n_gqa            = 1
0.00.049.536 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.537 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.537 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.538 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.538 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.538 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.538 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.539 I llm_load_print_meta: n_ff             = 8192
0.00.049.539 I llm_load_print_meta: n_expert         = 0
0.00.049.539 I llm_load_print_meta: n_expert_used    = 0
0.00.049.539 I llm_load_print_meta: causal attn      = 1
0.00.049.540 I llm_load_print_meta: pooling type     = 0
0.00.049.540 I llm_load_print_meta: rope type        = 2
0.00.049.541 I llm_load_print_meta: rope scaling     = linear
0.00.049.541 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.542 I llm_load_print_meta: freq_scale_train = 1
0.00.049.542 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.542 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.542 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.542 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.542 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.543 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.543 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.554 I llm_load_print_meta: model type       = 1.4B
0.00.049.554 I llm_load_print_meta: model ftype      = Q4_1
0.00.049.555 I llm_load_print_meta: model params     = 1.41 B
0.00.049.555 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.049.557 I llm_load_print_meta: general.name     = 1.4B
0.00.049.557 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.557 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.557 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.558 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.558 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.049.558 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.558 I llm_load_print_meta: max token length = 1024
0.00.051.291 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.292 I llm_load_tensors: offloading output layer to GPU
0.00.051.292 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.301 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.051.302 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.052.150 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.151 I llama_new_context_with_model: n_ctx         = 128
0.00.052.151 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.151 I llama_new_context_with_model: n_batch       = 128
0.00.052.152 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.152 I llama_new_context_with_model: flash_attn    = 0
0.00.052.152 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.152 I llama_new_context_with_model: freq_scale    = 1
0.00.052.153 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.153 I ggml_metal_init: allocating
0.00.052.156 I ggml_metal_init: found device: Apple M4
0.00.052.158 I ggml_metal_init: picking default device: Apple M4
0.00.052.718 I ggml_metal_init: using embedded metal library
0.00.054.614 I ggml_metal_init: GPU name:   Apple M4
0.00.054.616 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.616 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.617 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.617 I ggml_metal_init: simdgroup reduction   = true
0.00.054.617 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.617 I ggml_metal_init: has bfloat            = true
0.00.054.617 I ggml_metal_init: use bfloat            = true
0.00.054.618 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.618 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.744 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.063.746 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.063.760 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.064.652 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.064.653 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.064.654 I llama_new_context_with_model: graph nodes  = 967
0.00.064.654 I llama_new_context_with_model: graph splits = 2
0.00.064.666 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.712.792 I 
0.00.712.812 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
0.00.712.823 I perplexity: tokenizing the input ..
0.00.720.353 I perplexity: tokenization took 7.528 ms
0.00.720.358 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.843.392 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.844.514 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.844.526 I llama_perf_context_print:        load time =     703.80 ms
0.00.844.528 I llama_perf_context_print: prompt eval time =     122.81 ms /   128 tokens (    0.96 ms per token,  1042.30 tokens per second)
0.00.844.530 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.844.530 I llama_perf_context_print:       total time =     131.73 ms /   129 tokens
0.00.844.918 I ggml_metal_free: deallocating

real	0m0.857s
user	0m0.076s
sys	0m0.151s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.082 I build: 4161 (0ba40c36) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.396 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.147 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.016.151 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.153 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.153 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.154 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.154 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.154 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.155 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.156 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.156 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.156 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.157 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.158 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.158 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.160 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.160 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.160 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.972 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.017 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.776 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.777 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.777 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.777 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.778 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.778 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.024.778 I llama_model_loader: - type  f32:  194 tensors
0.00.024.779 I llama_model_loader: - type q5_0:   97 tensors
0.00.024.779 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.861 I llm_load_vocab: special tokens cache size = 25
0.00.050.893 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.896 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.897 I llm_load_print_meta: arch             = gptneox
0.00.050.897 I llm_load_print_meta: vocab type       = BPE
0.00.050.897 I llm_load_print_meta: n_vocab          = 50304
0.00.050.897 I llm_load_print_meta: n_merges         = 50009
0.00.050.897 I llm_load_print_meta: vocab_only       = 0
0.00.050.898 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.898 I llm_load_print_meta: n_embd           = 2048
0.00.050.898 I llm_load_print_meta: n_layer          = 24
0.00.050.901 I llm_load_print_meta: n_head           = 16
0.00.050.901 I llm_load_print_meta: n_head_kv        = 16
0.00.050.901 I llm_load_print_meta: n_rot            = 32
0.00.050.902 I llm_load_print_meta: n_swa            = 0
0.00.050.902 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.902 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.903 I llm_load_print_meta: n_gqa            = 1
0.00.050.903 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.904 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.906 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.906 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.906 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.906 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.906 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.907 I llm_load_print_meta: n_ff             = 8192
0.00.050.907 I llm_load_print_meta: n_expert         = 0
0.00.050.908 I llm_load_print_meta: n_expert_used    = 0
0.00.050.908 I llm_load_print_meta: causal attn      = 1
0.00.050.908 I llm_load_print_meta: pooling type     = 0
0.00.050.908 I llm_load_print_meta: rope type        = 2
0.00.050.910 I llm_load_print_meta: rope scaling     = linear
0.00.050.910 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.910 I llm_load_print_meta: freq_scale_train = 1
0.00.050.911 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.911 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.911 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.911 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.911 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.911 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.912 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.923 I llm_load_print_meta: model type       = 1.4B
0.00.050.924 I llm_load_print_meta: model ftype      = Q5_0
0.00.050.924 I llm_load_print_meta: model params     = 1.41 B
0.00.050.925 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.050.925 I llm_load_print_meta: general.name     = 1.4B
0.00.050.925 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.925 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.925 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.925 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.926 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.926 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.926 I llm_load_print_meta: max token length = 1024
0.00.052.673 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.674 I llm_load_tensors: offloading output layer to GPU
0.00.052.674 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.683 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.052.684 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.053.568 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.569 I llama_new_context_with_model: n_ctx         = 128
0.00.053.569 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.569 I llama_new_context_with_model: n_batch       = 128
0.00.053.569 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.570 I llama_new_context_with_model: flash_attn    = 0
0.00.053.570 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.570 I llama_new_context_with_model: freq_scale    = 1
0.00.053.571 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.571 I ggml_metal_init: allocating
0.00.053.574 I ggml_metal_init: found device: Apple M4
0.00.053.576 I ggml_metal_init: picking default device: Apple M4
0.00.054.126 I ggml_metal_init: using embedded metal library
0.00.056.032 I ggml_metal_init: GPU name:   Apple M4
0.00.056.034 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.034 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.035 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.035 I ggml_metal_init: simdgroup reduction   = true
0.00.056.035 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.035 I ggml_metal_init: has bfloat            = true
0.00.056.035 I ggml_metal_init: use bfloat            = true
0.00.056.036 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.036 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.104 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.106 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.120 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.008 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.009 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.009 I llama_new_context_with_model: graph nodes  = 967
0.00.066.010 I llama_new_context_with_model: graph splits = 2
0.00.066.021 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.730.366 I 
0.00.730.383 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
0.00.730.394 I perplexity: tokenizing the input ..
0.00.737.896 I perplexity: tokenization took 7.501 ms
0.00.737.899 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.873.118 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.874.210 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.874.224 I llama_perf_context_print:        load time =     719.97 ms
0.00.874.225 I llama_perf_context_print: prompt eval time =     135.00 ms /   128 tokens (    1.05 ms per token,   948.15 tokens per second)
0.00.874.225 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.874.226 I llama_perf_context_print:       total time =     143.86 ms /   129 tokens
0.00.874.549 I ggml_metal_free: deallocating

real	0m0.887s
user	0m0.076s
sys	0m0.169s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.085 I build: 4161 (0ba40c36) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.836 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.790 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.014.795 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.797 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.798 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.798 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.799 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.799 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.800 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.800 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.800 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.801 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.803 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.804 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.804 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.805 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.806 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.808 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.649 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.677 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.408 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.409 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.410 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.410 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.411 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.411 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.023.411 I llama_model_loader: - type  f32:  194 tensors
0.00.023.412 I llama_model_loader: - type q5_1:   97 tensors
0.00.023.412 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.355 I llm_load_vocab: special tokens cache size = 25
0.00.050.186 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.190 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.190 I llm_load_print_meta: arch             = gptneox
0.00.050.190 I llm_load_print_meta: vocab type       = BPE
0.00.050.191 I llm_load_print_meta: n_vocab          = 50304
0.00.050.191 I llm_load_print_meta: n_merges         = 50009
0.00.050.191 I llm_load_print_meta: vocab_only       = 0
0.00.050.191 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.191 I llm_load_print_meta: n_embd           = 2048
0.00.050.192 I llm_load_print_meta: n_layer          = 24
0.00.050.194 I llm_load_print_meta: n_head           = 16
0.00.050.195 I llm_load_print_meta: n_head_kv        = 16
0.00.050.195 I llm_load_print_meta: n_rot            = 32
0.00.050.196 I llm_load_print_meta: n_swa            = 0
0.00.050.196 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.197 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.198 I llm_load_print_meta: n_gqa            = 1
0.00.050.199 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.200 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.200 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.201 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.201 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.201 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.201 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.202 I llm_load_print_meta: n_ff             = 8192
0.00.050.202 I llm_load_print_meta: n_expert         = 0
0.00.050.202 I llm_load_print_meta: n_expert_used    = 0
0.00.050.202 I llm_load_print_meta: causal attn      = 1
0.00.050.202 I llm_load_print_meta: pooling type     = 0
0.00.050.203 I llm_load_print_meta: rope type        = 2
0.00.050.203 I llm_load_print_meta: rope scaling     = linear
0.00.050.203 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.204 I llm_load_print_meta: freq_scale_train = 1
0.00.050.204 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.204 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.204 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.204 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.205 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.205 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.205 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.214 I llm_load_print_meta: model type       = 1.4B
0.00.050.214 I llm_load_print_meta: model ftype      = Q5_1
0.00.050.215 I llm_load_print_meta: model params     = 1.41 B
0.00.050.216 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.050.216 I llm_load_print_meta: general.name     = 1.4B
0.00.050.216 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.216 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.216 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.216 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.217 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.217 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.217 I llm_load_print_meta: max token length = 1024
0.00.051.982 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.983 I llm_load_tensors: offloading output layer to GPU
0.00.051.983 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.992 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.051.993 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.052.869 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.869 I llama_new_context_with_model: n_ctx         = 128
0.00.052.870 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.870 I llama_new_context_with_model: n_batch       = 128
0.00.052.870 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.870 I llama_new_context_with_model: flash_attn    = 0
0.00.052.871 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.871 I llama_new_context_with_model: freq_scale    = 1
0.00.052.871 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.872 I ggml_metal_init: allocating
0.00.052.875 I ggml_metal_init: found device: Apple M4
0.00.052.877 I ggml_metal_init: picking default device: Apple M4
0.00.053.420 I ggml_metal_init: using embedded metal library
0.00.055.352 I ggml_metal_init: GPU name:   Apple M4
0.00.055.353 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.354 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.354 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.354 I ggml_metal_init: simdgroup reduction   = true
0.00.055.354 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.354 I ggml_metal_init: has bfloat            = true
0.00.055.355 I ggml_metal_init: use bfloat            = true
0.00.055.355 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.356 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.762 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.064.765 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.064.780 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.065.660 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.065.661 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.065.662 I llama_new_context_with_model: graph nodes  = 967
0.00.065.662 I llama_new_context_with_model: graph splits = 2
0.00.065.674 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.769.006 I 
0.00.769.027 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
0.00.769.040 I perplexity: tokenizing the input ..
0.00.776.475 I perplexity: tokenization took 7.434 ms
0.00.776.478 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.911.528 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.912.641 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.912.656 I llama_perf_context_print:        load time =     760.16 ms
0.00.912.657 I llama_perf_context_print: prompt eval time =     134.83 ms /   128 tokens (    1.05 ms per token,   949.34 tokens per second)
0.00.912.659 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.912.659 I llama_perf_context_print:       total time =     143.65 ms /   129 tokens
0.00.913.020 I ggml_metal_free: deallocating

real	0m0.925s
user	0m0.077s
sys	0m0.170s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.083 I build: 4161 (0ba40c36) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.899 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.348 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.353 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.354 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.356 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.356 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.357 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.357 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.358 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.358 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.358 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.359 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.359 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.359 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.360 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.363 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.363 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.363 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.109 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.122 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.855 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.856 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.857 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.857 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.857 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.857 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.023.858 I llama_model_loader: - type  f32:  194 tensors
0.00.023.858 I llama_model_loader: - type q2_K:   49 tensors
0.00.023.858 I llama_model_loader: - type q3_K:   48 tensors
0.00.023.859 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.945 I llm_load_vocab: special tokens cache size = 25
0.00.050.040 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.042 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.043 I llm_load_print_meta: arch             = gptneox
0.00.050.043 I llm_load_print_meta: vocab type       = BPE
0.00.050.043 I llm_load_print_meta: n_vocab          = 50304
0.00.050.043 I llm_load_print_meta: n_merges         = 50009
0.00.050.044 I llm_load_print_meta: vocab_only       = 0
0.00.050.044 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.044 I llm_load_print_meta: n_embd           = 2048
0.00.050.044 I llm_load_print_meta: n_layer          = 24
0.00.050.047 I llm_load_print_meta: n_head           = 16
0.00.050.047 I llm_load_print_meta: n_head_kv        = 16
0.00.050.048 I llm_load_print_meta: n_rot            = 32
0.00.050.048 I llm_load_print_meta: n_swa            = 0
0.00.050.048 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.048 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.049 I llm_load_print_meta: n_gqa            = 1
0.00.050.049 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.050 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.051 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.051 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.051 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.051 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.051 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.052 I llm_load_print_meta: n_ff             = 8192
0.00.050.052 I llm_load_print_meta: n_expert         = 0
0.00.050.052 I llm_load_print_meta: n_expert_used    = 0
0.00.050.053 I llm_load_print_meta: causal attn      = 1
0.00.050.053 I llm_load_print_meta: pooling type     = 0
0.00.050.053 I llm_load_print_meta: rope type        = 2
0.00.050.053 I llm_load_print_meta: rope scaling     = linear
0.00.050.054 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.054 I llm_load_print_meta: freq_scale_train = 1
0.00.050.054 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.054 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.054 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.055 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.055 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.055 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.055 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.066 I llm_load_print_meta: model type       = 1.4B
0.00.050.066 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.050.067 I llm_load_print_meta: model params     = 1.41 B
0.00.050.067 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.050.067 I llm_load_print_meta: general.name     = 1.4B
0.00.050.068 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.068 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.068 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.068 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.068 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.069 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.069 I llm_load_print_meta: max token length = 1024
0.00.051.787 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.787 I llm_load_tensors: offloading output layer to GPU
0.00.051.787 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.797 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.051.798 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.052.649 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.650 I llama_new_context_with_model: n_ctx         = 128
0.00.052.651 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.651 I llama_new_context_with_model: n_batch       = 128
0.00.052.651 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.651 I llama_new_context_with_model: flash_attn    = 0
0.00.052.651 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.652 I llama_new_context_with_model: freq_scale    = 1
0.00.052.652 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.652 I ggml_metal_init: allocating
0.00.052.658 I ggml_metal_init: found device: Apple M4
0.00.052.662 I ggml_metal_init: picking default device: Apple M4
0.00.053.190 I ggml_metal_init: using embedded metal library
0.00.055.120 I ggml_metal_init: GPU name:   Apple M4
0.00.055.121 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.122 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.122 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.122 I ggml_metal_init: simdgroup reduction   = true
0.00.055.122 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.122 I ggml_metal_init: has bfloat            = true
0.00.055.123 I ggml_metal_init: use bfloat            = true
0.00.055.123 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.124 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.358 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.064.361 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.064.376 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.065.248 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.065.249 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.065.250 I llama_new_context_with_model: graph nodes  = 967
0.00.065.250 I llama_new_context_with_model: graph splits = 2
0.00.065.262 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.454.995 I 
0.00.455.013 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
0.00.455.026 I perplexity: tokenizing the input ..
0.00.462.250 I perplexity: tokenization took 7.221 ms
0.00.462.253 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.594.725 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.595.822 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.595.839 I llama_perf_context_print:        load time =     445.09 ms
0.00.595.840 I llama_perf_context_print: prompt eval time =     132.25 ms /   128 tokens (    1.03 ms per token,   967.85 tokens per second)
0.00.595.841 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.595.842 I llama_perf_context_print:       total time =     140.84 ms /   129 tokens
0.00.596.331 I ggml_metal_free: deallocating

real	0m0.609s
user	0m0.076s
sys	0m0.096s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.085 I build: 4161 (0ba40c36) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.596 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.520 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.015.525 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.527 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.528 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.528 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.528 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.529 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.529 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.530 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.530 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.532 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.532 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.532 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.533 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.535 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.537 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.537 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.346 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.366 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.205 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.206 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.206 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.207 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.207 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.207 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.208 I llama_model_loader: - type  f32:  194 tensors
0.00.024.208 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.208 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.209 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.209 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.128 I llm_load_vocab: special tokens cache size = 25
0.00.051.197 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.200 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.201 I llm_load_print_meta: arch             = gptneox
0.00.051.201 I llm_load_print_meta: vocab type       = BPE
0.00.051.201 I llm_load_print_meta: n_vocab          = 50304
0.00.051.201 I llm_load_print_meta: n_merges         = 50009
0.00.051.202 I llm_load_print_meta: vocab_only       = 0
0.00.051.202 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.202 I llm_load_print_meta: n_embd           = 2048
0.00.051.202 I llm_load_print_meta: n_layer          = 24
0.00.051.205 I llm_load_print_meta: n_head           = 16
0.00.051.205 I llm_load_print_meta: n_head_kv        = 16
0.00.051.205 I llm_load_print_meta: n_rot            = 32
0.00.051.206 I llm_load_print_meta: n_swa            = 0
0.00.051.206 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.206 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.207 I llm_load_print_meta: n_gqa            = 1
0.00.051.207 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.208 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.209 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.209 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.209 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.209 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.210 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.210 I llm_load_print_meta: n_ff             = 8192
0.00.051.210 I llm_load_print_meta: n_expert         = 0
0.00.051.211 I llm_load_print_meta: n_expert_used    = 0
0.00.051.211 I llm_load_print_meta: causal attn      = 1
0.00.051.211 I llm_load_print_meta: pooling type     = 0
0.00.051.211 I llm_load_print_meta: rope type        = 2
0.00.051.211 I llm_load_print_meta: rope scaling     = linear
0.00.051.212 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.212 I llm_load_print_meta: freq_scale_train = 1
0.00.051.212 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.212 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.213 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.213 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.213 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.213 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.213 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.225 I llm_load_print_meta: model type       = 1.4B
0.00.051.225 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.051.225 I llm_load_print_meta: model params     = 1.41 B
0.00.051.226 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.051.226 I llm_load_print_meta: general.name     = 1.4B
0.00.051.226 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.226 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.226 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.227 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.227 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.227 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.227 I llm_load_print_meta: max token length = 1024
0.00.052.994 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.994 I llm_load_tensors: offloading output layer to GPU
0.00.052.995 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.004 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.053.005 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.053.923 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.925 I llama_new_context_with_model: n_ctx         = 128
0.00.053.925 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.925 I llama_new_context_with_model: n_batch       = 128
0.00.053.925 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.925 I llama_new_context_with_model: flash_attn    = 0
0.00.053.926 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.926 I llama_new_context_with_model: freq_scale    = 1
0.00.053.926 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.927 I ggml_metal_init: allocating
0.00.053.930 I ggml_metal_init: found device: Apple M4
0.00.053.932 I ggml_metal_init: picking default device: Apple M4
0.00.054.498 I ggml_metal_init: using embedded metal library
0.00.056.458 I ggml_metal_init: GPU name:   Apple M4
0.00.056.459 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.460 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.460 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.460 I ggml_metal_init: simdgroup reduction   = true
0.00.056.460 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.461 I ggml_metal_init: has bfloat            = true
0.00.056.461 I ggml_metal_init: use bfloat            = true
0.00.056.461 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.462 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.935 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.937 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.951 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.849 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.850 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.850 I llama_new_context_with_model: graph nodes  = 967
0.00.066.850 I llama_new_context_with_model: graph splits = 2
0.00.066.863 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.565.806 I 
0.00.565.823 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
0.00.565.834 I perplexity: tokenizing the input ..
0.00.573.238 I perplexity: tokenization took 7.4 ms
0.00.573.242 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.705.680 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.706.776 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.706.788 I llama_perf_context_print:        load time =     556.21 ms
0.00.706.788 I llama_perf_context_print: prompt eval time =     132.22 ms /   128 tokens (    1.03 ms per token,   968.08 tokens per second)
0.00.706.789 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.706.790 I llama_perf_context_print:       total time =     140.98 ms /   129 tokens
0.00.707.148 I ggml_metal_free: deallocating

real	0m0.719s
user	0m0.077s
sys	0m0.131s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.084 I build: 4161 (0ba40c36) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.260 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.940 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.014.945 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.946 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.947 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.947 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.947 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.948 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.949 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.949 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.950 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.950 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.950 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.950 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.951 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.952 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.953 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.953 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.720 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.731 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.517 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.518 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.518 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.519 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.519 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.519 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.023.520 I llama_model_loader: - type  f32:  194 tensors
0.00.023.520 I llama_model_loader: - type q4_K:   61 tensors
0.00.023.520 I llama_model_loader: - type q5_K:   24 tensors
0.00.023.521 I llama_model_loader: - type q6_K:   13 tensors
0.00.043.529 I llm_load_vocab: special tokens cache size = 25
0.00.049.589 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.592 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.593 I llm_load_print_meta: arch             = gptneox
0.00.049.593 I llm_load_print_meta: vocab type       = BPE
0.00.049.593 I llm_load_print_meta: n_vocab          = 50304
0.00.049.593 I llm_load_print_meta: n_merges         = 50009
0.00.049.593 I llm_load_print_meta: vocab_only       = 0
0.00.049.594 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.594 I llm_load_print_meta: n_embd           = 2048
0.00.049.594 I llm_load_print_meta: n_layer          = 24
0.00.049.596 I llm_load_print_meta: n_head           = 16
0.00.049.601 I llm_load_print_meta: n_head_kv        = 16
0.00.049.601 I llm_load_print_meta: n_rot            = 32
0.00.049.601 I llm_load_print_meta: n_swa            = 0
0.00.049.601 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.602 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.605 I llm_load_print_meta: n_gqa            = 1
0.00.049.605 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.606 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.607 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.607 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.607 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.608 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.608 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.609 I llm_load_print_meta: n_ff             = 8192
0.00.049.609 I llm_load_print_meta: n_expert         = 0
0.00.049.609 I llm_load_print_meta: n_expert_used    = 0
0.00.049.609 I llm_load_print_meta: causal attn      = 1
0.00.049.609 I llm_load_print_meta: pooling type     = 0
0.00.049.609 I llm_load_print_meta: rope type        = 2
0.00.049.610 I llm_load_print_meta: rope scaling     = linear
0.00.049.610 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.610 I llm_load_print_meta: freq_scale_train = 1
0.00.049.610 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.611 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.611 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.611 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.611 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.611 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.611 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.623 I llm_load_print_meta: model type       = 1.4B
0.00.049.623 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.049.623 I llm_load_print_meta: model params     = 1.41 B
0.00.049.624 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.049.624 I llm_load_print_meta: general.name     = 1.4B
0.00.049.624 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.624 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.625 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.625 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.625 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.049.625 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.626 I llm_load_print_meta: max token length = 1024
0.00.051.343 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.343 I llm_load_tensors: offloading output layer to GPU
0.00.051.343 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.352 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.051.353 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.052.220 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.221 I llama_new_context_with_model: n_ctx         = 128
0.00.052.221 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.221 I llama_new_context_with_model: n_batch       = 128
0.00.052.221 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.221 I llama_new_context_with_model: flash_attn    = 0
0.00.052.222 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.222 I llama_new_context_with_model: freq_scale    = 1
0.00.052.222 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.223 I ggml_metal_init: allocating
0.00.052.225 I ggml_metal_init: found device: Apple M4
0.00.052.227 I ggml_metal_init: picking default device: Apple M4
0.00.052.768 I ggml_metal_init: using embedded metal library
0.00.054.642 I ggml_metal_init: GPU name:   Apple M4
0.00.054.644 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.644 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.645 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.645 I ggml_metal_init: simdgroup reduction   = true
0.00.054.645 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.645 I ggml_metal_init: has bfloat            = true
0.00.054.645 I ggml_metal_init: use bfloat            = true
0.00.054.646 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.646 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.718 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.063.723 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.063.736 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.064.626 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.064.627 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.064.627 I llama_new_context_with_model: graph nodes  = 967
0.00.064.628 I llama_new_context_with_model: graph splits = 2
0.00.064.639 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.646.361 I 
0.00.646.377 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
0.00.646.387 I perplexity: tokenizing the input ..
0.00.653.811 I perplexity: tokenization took 7.423 ms
0.00.653.814 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.788.310 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.789.416 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.789.436 I llama_perf_context_print:        load time =     637.10 ms
0.00.789.437 I llama_perf_context_print: prompt eval time =     134.28 ms /   128 tokens (    1.05 ms per token,   953.24 tokens per second)
0.00.789.438 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.789.439 I llama_perf_context_print:       total time =     143.08 ms /   129 tokens
0.00.789.789 I ggml_metal_free: deallocating

real	0m0.803s
user	0m0.076s
sys	0m0.143s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.085 I build: 4161 (0ba40c36) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.229 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.997 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.002 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.003 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.004 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.004 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.005 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.005 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.006 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.006 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.007 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.007 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.007 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.008 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.008 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.010 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.010 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.010 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.791 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.812 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.618 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.619 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.619 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.619 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.620 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.620 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.023.621 I llama_model_loader: - type  f32:  194 tensors
0.00.023.621 I llama_model_loader: - type q5_K:   61 tensors
0.00.023.621 I llama_model_loader: - type q6_K:   37 tensors
0.00.044.511 I llm_load_vocab: special tokens cache size = 25
0.00.050.597 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.600 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.600 I llm_load_print_meta: arch             = gptneox
0.00.050.601 I llm_load_print_meta: vocab type       = BPE
0.00.050.601 I llm_load_print_meta: n_vocab          = 50304
0.00.050.601 I llm_load_print_meta: n_merges         = 50009
0.00.050.601 I llm_load_print_meta: vocab_only       = 0
0.00.050.601 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.602 I llm_load_print_meta: n_embd           = 2048
0.00.050.602 I llm_load_print_meta: n_layer          = 24
0.00.050.604 I llm_load_print_meta: n_head           = 16
0.00.050.605 I llm_load_print_meta: n_head_kv        = 16
0.00.050.605 I llm_load_print_meta: n_rot            = 32
0.00.050.606 I llm_load_print_meta: n_swa            = 0
0.00.050.606 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.606 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.607 I llm_load_print_meta: n_gqa            = 1
0.00.050.607 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.608 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.610 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.610 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.610 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.611 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.611 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.611 I llm_load_print_meta: n_ff             = 8192
0.00.050.612 I llm_load_print_meta: n_expert         = 0
0.00.050.612 I llm_load_print_meta: n_expert_used    = 0
0.00.050.612 I llm_load_print_meta: causal attn      = 1
0.00.050.612 I llm_load_print_meta: pooling type     = 0
0.00.050.612 I llm_load_print_meta: rope type        = 2
0.00.050.612 I llm_load_print_meta: rope scaling     = linear
0.00.050.613 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.613 I llm_load_print_meta: freq_scale_train = 1
0.00.050.614 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.614 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.614 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.614 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.614 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.614 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.615 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.626 I llm_load_print_meta: model type       = 1.4B
0.00.050.626 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.050.627 I llm_load_print_meta: model params     = 1.41 B
0.00.050.627 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.050.627 I llm_load_print_meta: general.name     = 1.4B
0.00.050.627 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.628 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.628 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.628 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.628 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.628 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.628 I llm_load_print_meta: max token length = 1024
0.00.052.386 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.386 I llm_load_tensors: offloading output layer to GPU
0.00.052.386 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.396 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.052.397 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.053.310 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.311 I llama_new_context_with_model: n_ctx         = 128
0.00.053.311 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.311 I llama_new_context_with_model: n_batch       = 128
0.00.053.312 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.312 I llama_new_context_with_model: flash_attn    = 0
0.00.053.312 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.312 I llama_new_context_with_model: freq_scale    = 1
0.00.053.313 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.313 I ggml_metal_init: allocating
0.00.053.319 I ggml_metal_init: found device: Apple M4
0.00.053.321 I ggml_metal_init: picking default device: Apple M4
0.00.053.851 I ggml_metal_init: using embedded metal library
0.00.055.769 I ggml_metal_init: GPU name:   Apple M4
0.00.055.771 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.771 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.771 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.772 I ggml_metal_init: simdgroup reduction   = true
0.00.055.772 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.772 I ggml_metal_init: has bfloat            = true
0.00.055.772 I ggml_metal_init: use bfloat            = true
0.00.055.773 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.773 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.656 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.064.659 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.064.674 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.065.530 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.065.531 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.065.531 I llama_new_context_with_model: graph nodes  = 967
0.00.065.531 I llama_new_context_with_model: graph splits = 2
0.00.065.543 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.730.044 I 
0.00.730.061 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
0.00.730.072 I perplexity: tokenizing the input ..
0.00.737.331 I perplexity: tokenization took 7.258 ms
0.00.737.336 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.878.495 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.879.581 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.879.594 I llama_perf_context_print:        load time =     720.81 ms
0.00.879.595 I llama_perf_context_print: prompt eval time =     140.94 ms /   128 tokens (    1.10 ms per token,   908.17 tokens per second)
0.00.879.596 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.879.597 I llama_perf_context_print:       total time =     149.55 ms /   129 tokens
0.00.879.982 I ggml_metal_free: deallocating

real	0m0.892s
user	0m0.076s
sys	0m0.165s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.083 I build: 4161 (0ba40c36) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.412 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.214 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.218 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.220 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.220 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.221 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.221 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.221 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.222 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.223 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.223 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.223 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.224 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.224 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.224 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.226 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.226 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.227 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.059 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.088 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.959 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.960 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.961 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.961 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.961 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.962 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.962 I llama_model_loader: - type  f32:  194 tensors
0.00.024.962 I llama_model_loader: - type q6_K:   98 tensors
0.00.045.943 I llm_load_vocab: special tokens cache size = 25
0.00.052.106 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.109 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.110 I llm_load_print_meta: arch             = gptneox
0.00.052.110 I llm_load_print_meta: vocab type       = BPE
0.00.052.110 I llm_load_print_meta: n_vocab          = 50304
0.00.052.110 I llm_load_print_meta: n_merges         = 50009
0.00.052.111 I llm_load_print_meta: vocab_only       = 0
0.00.052.111 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.111 I llm_load_print_meta: n_embd           = 2048
0.00.052.111 I llm_load_print_meta: n_layer          = 24
0.00.052.114 I llm_load_print_meta: n_head           = 16
0.00.052.115 I llm_load_print_meta: n_head_kv        = 16
0.00.052.115 I llm_load_print_meta: n_rot            = 32
0.00.052.115 I llm_load_print_meta: n_swa            = 0
0.00.052.115 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.116 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.116 I llm_load_print_meta: n_gqa            = 1
0.00.052.117 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.118 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.118 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.119 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.119 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.119 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.119 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.120 I llm_load_print_meta: n_ff             = 8192
0.00.052.120 I llm_load_print_meta: n_expert         = 0
0.00.052.120 I llm_load_print_meta: n_expert_used    = 0
0.00.052.120 I llm_load_print_meta: causal attn      = 1
0.00.052.120 I llm_load_print_meta: pooling type     = 0
0.00.052.120 I llm_load_print_meta: rope type        = 2
0.00.052.121 I llm_load_print_meta: rope scaling     = linear
0.00.052.124 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.124 I llm_load_print_meta: freq_scale_train = 1
0.00.052.124 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.124 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.124 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.125 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.125 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.125 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.125 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.136 I llm_load_print_meta: model type       = 1.4B
0.00.052.137 I llm_load_print_meta: model ftype      = Q6_K
0.00.052.137 I llm_load_print_meta: model params     = 1.41 B
0.00.052.137 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.052.137 I llm_load_print_meta: general.name     = 1.4B
0.00.052.138 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.138 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.138 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.138 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.138 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.052.139 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.139 I llm_load_print_meta: max token length = 1024
0.00.053.846 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.846 I llm_load_tensors: offloading output layer to GPU
0.00.053.846 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.855 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.053.857 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.054.690 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.691 I llama_new_context_with_model: n_ctx         = 128
0.00.054.691 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.691 I llama_new_context_with_model: n_batch       = 128
0.00.054.691 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.691 I llama_new_context_with_model: flash_attn    = 0
0.00.054.692 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.692 I llama_new_context_with_model: freq_scale    = 1
0.00.054.693 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.693 I ggml_metal_init: allocating
0.00.054.699 I ggml_metal_init: found device: Apple M4
0.00.054.701 I ggml_metal_init: picking default device: Apple M4
0.00.055.237 I ggml_metal_init: using embedded metal library
0.00.057.171 I ggml_metal_init: GPU name:   Apple M4
0.00.057.173 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.173 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.174 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.174 I ggml_metal_init: simdgroup reduction   = true
0.00.057.174 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.174 I ggml_metal_init: has bfloat            = true
0.00.057.174 I ggml_metal_init: use bfloat            = true
0.00.057.175 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.175 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.068 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.072 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.086 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.968 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.969 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.969 I llama_new_context_with_model: graph nodes  = 967
0.00.066.969 I llama_new_context_with_model: graph splits = 2
0.00.066.981 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.414.606 I 
0.00.414.633 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
0.00.414.646 I perplexity: tokenizing the input ..
0.00.421.827 I perplexity: tokenization took 7.18 ms
0.00.421.831 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.562.250 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.563.360 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.563.378 I llama_perf_context_print:        load time =     404.19 ms
0.00.563.379 I llama_perf_context_print: prompt eval time =     140.20 ms /   128 tokens (    1.10 ms per token,   912.99 tokens per second)
0.00.563.380 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.563.381 I llama_perf_context_print:       total time =     148.77 ms /   129 tokens
0.00.563.830 I ggml_metal_free: deallocating

real	0m0.577s
user	0m0.077s
sys	0m0.106s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.284 I build: 4161 (0ba40c36) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.018.590 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.030.565 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.030.573 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.030.576 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.030.577 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.030.578 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.030.579 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.030.579 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.030.581 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.030.581 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.030.582 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.030.586 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.030.586 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.030.587 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.030.587 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.030.590 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.030.591 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.030.593 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.038.642 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.040.569 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.047.261 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.047.263 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.047.264 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.047.264 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.047.265 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.047.266 I llama_model_loader: - type  f32:  194 tensors
0.00.047.266 I llama_model_loader: - type  f16:   98 tensors
0.00.075.595 I llm_load_vocab: special tokens cache size = 25
0.00.082.255 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.082.258 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.082.258 I llm_load_print_meta: arch             = gptneox
0.00.082.258 I llm_load_print_meta: vocab type       = BPE
0.00.082.259 I llm_load_print_meta: n_vocab          = 50304
0.00.082.259 I llm_load_print_meta: n_merges         = 50009
0.00.082.259 I llm_load_print_meta: vocab_only       = 0
0.00.082.259 I llm_load_print_meta: n_ctx_train      = 2048
0.00.082.259 I llm_load_print_meta: n_embd           = 2048
0.00.082.259 I llm_load_print_meta: n_layer          = 24
0.00.082.262 I llm_load_print_meta: n_head           = 16
0.00.082.262 I llm_load_print_meta: n_head_kv        = 16
0.00.082.262 I llm_load_print_meta: n_rot            = 32
0.00.082.263 I llm_load_print_meta: n_swa            = 0
0.00.082.263 I llm_load_print_meta: n_embd_head_k    = 128
0.00.082.263 I llm_load_print_meta: n_embd_head_v    = 128
0.00.082.264 I llm_load_print_meta: n_gqa            = 1
0.00.082.264 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.082.265 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.082.265 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.082.266 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.082.266 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.082.266 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.082.266 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.082.267 I llm_load_print_meta: n_ff             = 8192
0.00.082.267 I llm_load_print_meta: n_expert         = 0
0.00.082.267 I llm_load_print_meta: n_expert_used    = 0
0.00.082.267 I llm_load_print_meta: causal attn      = 1
0.00.082.267 I llm_load_print_meta: pooling type     = 0
0.00.082.267 I llm_load_print_meta: rope type        = 2
0.00.082.268 I llm_load_print_meta: rope scaling     = linear
0.00.082.268 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.082.268 I llm_load_print_meta: freq_scale_train = 1
0.00.082.268 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.082.268 I llm_load_print_meta: rope_finetuned   = unknown
0.00.082.268 I llm_load_print_meta: ssm_d_conv       = 0
0.00.082.269 I llm_load_print_meta: ssm_d_inner      = 0
0.00.082.269 I llm_load_print_meta: ssm_d_state      = 0
0.00.082.269 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.082.269 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.082.280 I llm_load_print_meta: model type       = 1.4B
0.00.082.280 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.082.282 I llm_load_print_meta: model params     = 1.41 B
0.00.082.282 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.082.282 I llm_load_print_meta: general.name     = 1.4B
0.00.082.282 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.082.283 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.082.283 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.082.283 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.082.283 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.082.283 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.082.284 I llm_load_print_meta: max token length = 1024
0.00.084.207 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.084.208 I llm_load_tensors: offloading output layer to GPU
0.00.084.208 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.084.217 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.084.218 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.085.100 I llama_new_context_with_model: n_seq_max     = 1
0.00.085.101 I llama_new_context_with_model: n_ctx         = 128
0.00.085.101 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.085.101 I llama_new_context_with_model: n_batch       = 128
0.00.085.101 I llama_new_context_with_model: n_ubatch      = 128
0.00.085.102 I llama_new_context_with_model: flash_attn    = 0
0.00.085.102 I llama_new_context_with_model: freq_base     = 10000.0
0.00.085.102 I llama_new_context_with_model: freq_scale    = 1
0.00.085.102 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.085.103 I ggml_metal_init: allocating
0.00.085.105 I ggml_metal_init: found device: Apple M4
0.00.085.107 I ggml_metal_init: picking default device: Apple M4
0.00.085.688 I ggml_metal_init: using embedded metal library
0.00.087.766 I ggml_metal_init: GPU name:   Apple M4
0.00.087.768 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.087.768 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.087.769 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.087.769 I ggml_metal_init: simdgroup reduction   = true
0.00.087.769 I ggml_metal_init: simdgroup matrix mul. = true
0.00.087.769 I ggml_metal_init: has bfloat            = true
0.00.087.769 I ggml_metal_init: use bfloat            = true
0.00.087.770 I ggml_metal_init: hasUnifiedMemory      = true
0.00.087.772 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.096.568 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.096.572 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.096.584 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.097.459 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.097.460 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.097.461 I llama_new_context_with_model: graph nodes  = 967
0.00.097.461 I llama_new_context_with_model: graph splits = 2
0.00.097.481 I 
0.00.097.510 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
0.00.097.511 I compute_imatrix: tokenizing the input ..
0.00.104.398 I compute_imatrix: tokenization took 6.887 ms
0.00.104.400 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.794.260 I compute_imatrix: 1.69 seconds per pass - ETA 0.02 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.797.347 I llama_perf_context_print:        load time =    1775.67 ms
0.01.797.348 I llama_perf_context_print: prompt eval time =    1689.34 ms /   128 tokens (   13.20 ms per token,    75.77 tokens per second)
0.01.797.349 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.797.350 I llama_perf_context_print:       total time =    1778.75 ms /   129 tokens
0.01.797.989 I ggml_metal_free: deallocating

real	0m1.989s
user	0m0.165s
sys	0m0.328s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4161 (0ba40c36)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 'Ä'
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x111e0a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x111e0a8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x111e0ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x111e0b420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x111e0b9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x111e0bf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x111e0c530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x111e0cae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x111e0d090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x111e0d590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x111e0da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x111e0df90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x111e0eab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x111e0f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x111e0fa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x111e10190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x111e108b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x111e10fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x111e116f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x111e11ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x111e125e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x111e12d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x111e13420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x111e13cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x111e143e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x111e146a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x111e14cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x111e15920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x111e15e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x111e16120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x111e165c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x111e16880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x111e17110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x111e17650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x111e17910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x111e17db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x111e18250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x111e186f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x111e18b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x111e19030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x111e194d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x111e19970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x111e19e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x111e1a2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x111e1a570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x111e1ab80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x111e1b190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x111e1bab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x111e1c0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x111e1c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x111e1cce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x111e1d2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x111e1d900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x111e1df10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x111e1e700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x111e1eba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x111e1f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x111e1f300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x111e1f910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x111e20100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x111e203c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x111e20860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x111e20d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x111e211a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x111e21640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x111e21ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x111e21f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x111e22420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x111e228c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x111e22d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x111e23200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x111e236a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x111e23b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x111e23fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x111e24480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x111e24920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x111e24dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x111e25260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x111e25700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x111e25ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x111e26040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x111e264e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x111e26980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x111e26e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x111e272c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x111e27760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x111e27c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x111e280a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x111e28540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x111e289e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x111e28e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x111e29320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x111e297c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x111e29c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x111e2a100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x111e2a5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x111e2aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x111e1b7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x111e2b090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x111e2b530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x111e2b9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x111e2be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x111e2c310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x111e2c7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x111e2cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x111e2d0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x111e2d590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x111e2da30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x111e2ded0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x111e2e370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x111e2e810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x111e2ecb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x111e2f150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x111e2f5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x111e2fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x111e2ff30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x111e303d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x111e30870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x111e30d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x111e311b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x111e31650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x111e31af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x111e31f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x111e32430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x111e328d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x111e32d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x111e33210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x111e336b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x111e33b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x111e33ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x111e34490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x111e34930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x111e34dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x111e35270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x111e35710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x111e35bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x111e36050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x111e364f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x111e36990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x111e36e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x111e372d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x111e37770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x111e37c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x111e380b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x111e38550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x111e389f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x111e38e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x111e39330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x111e397d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x111e39c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x111e3a110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x111e3a5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x111e3aa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x111e3afa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x111e3b4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x111e3ba40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x111e3bf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x111e3c250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x111e3c860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x111e3ce70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x111e3d480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x111e3da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x111e3e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x111e3e890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x111e3ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x111e3f1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x111e3f670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x111e3fe20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x111e40370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x111e408c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x111e40e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x111e41360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x111e418b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x111e41e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x111e42350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x111e428a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x111e42df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x111e43340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x111e43890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x111e43de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x111e44330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x111e44880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x111e44dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x111e45320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x111e45870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x111e45dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x111e46310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x111e46860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x111e46db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x111e47300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x111e47850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x111e47da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x111e482f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x111e48840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x111e48d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x111e492e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x111e49830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x111e49d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x111e4a2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x111e4a820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x111e4ad70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x111e4b2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x111e4b810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x111e4bd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x111e4c2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x111e4c800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x111e4cd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x111e4d2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x111e4d7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x111e4dd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x111e4e290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x111e4e7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x111e4ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x111e4f280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x111e4f7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x111e4fd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x111e50270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x111e507c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x111e50d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x111e51260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x111e517b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x111e51d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x111e52250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x111e527a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x111e52c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x111e530e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x111e53580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x111e53a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x111e53ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x111e54360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x111e54800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x111e54ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x111e55140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x111e555e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x111e55a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x111e55f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x111e563c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x111e56910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x111e57030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x111e57750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x111e57e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x111e58590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x111e58850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x111e58e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x111e59470 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
0.00.145.761 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x105a04dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x105a05240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x105a056b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x105a05b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x105a05f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x105a06400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x105a06870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x105a06ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x105a07150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x105a075c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x105a07a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x105a08120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x105a08c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x105a093f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x105a09c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x105a0a320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x105a0aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x105a0b160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x105a0b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x105a0bfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x105a0c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x105a0cdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x105a0d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x105a0dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x105a0e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x105a0e610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x105a0e8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x105a0ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x105a0f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x105a0f620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x105a0fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x105a0ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x105a10430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x105a106f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x105a10b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x105a10fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x105a11440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x105a118b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x105a11d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x105a12190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x105a12600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x105a12a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x105a12ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x105a13350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x105a137c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x105a13c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x105a140a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x105a14510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x105a14980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x105a14df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x105a15260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x105a156d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x105a15b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x105a15fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x105a16420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x105a16890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x105a16e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x105a17300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x105a17770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x105a17be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x105a18050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x105a184c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x105a18930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x105a18da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x105a19210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x105a19680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x105a19af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x105a19f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x105a1a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x105a1a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x105a1acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x105a1b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x105a1b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x105a1ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x105a1be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x105a1c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x105a1c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x105a1cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x105a1d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x105a1d4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x105a1d910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x105a1dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x105a1e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x105a1e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x105a1ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x105a1ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x105a1f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x105a1f820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x105a1fc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x105a20100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x105a20570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x105a209e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x105a20e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x105a212c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x105a21730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x105a21ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x105a22010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x105a22480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x105a228f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x105a22d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x105a231d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x105a23640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x105a23ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x105a23f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x105a24390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x105a24800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x105a24c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x105a250e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x105a25550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x105a259c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x105a25e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x105a262a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x105a26710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x105a26b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x105a26ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x105a27460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x105a278d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x105a27d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x105a281b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x105a28620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x105a28a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x105a28f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x105a29370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x105a297e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x105a29c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x105a2a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x105a2a530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x105a2a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x105a2ae10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x105a2b280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x105a2b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x105a2bb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x105a2bfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x105a2c440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x105a2c8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x105a2cd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x105a2d190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x105a2d600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x105a2da70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x105a2dee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x105a2e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x105a2e7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x105a2ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x105a2f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x105a2f510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x105a2f980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x105a2fdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x105a30260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x105a306d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x105a30b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x105a30fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x105a31420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x105a31890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x105a31d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x105a32170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x105a325e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x105a32a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x105a32ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x105a33330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x105a337a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x105a33c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x105a34080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x105a344f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x105a34960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x105a34dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x105a35240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x105a356b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x105a36240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x105a36500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x105a367c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x105a36c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x105a370a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x105a37510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x105a37980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x105a37df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x105a38260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x105a386d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x105a38b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x105a38fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x105a39420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x105a39890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x105a39d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x105a3a170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x105a3a5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x105a3aa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x105a3aec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x105a3b330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x105a3b7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x105a3bc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x105a3c080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x105a3c4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x105a3c960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x105a3cdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x105a3d240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x105a3d6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x105a3db20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x105a3df90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x105a3e400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x105a3e870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x105a3ece0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x105a3f150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x105a3f5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x105a3fa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x105a3fea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x105a40310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x105a40780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x105a40bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x105a41060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x105a414d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x105a41940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x105a41db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x105a42220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x105a42690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x105a42b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x105a42f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x105a433e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x105a43850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x105a43cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x105a44130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x105a445a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x105a44a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x105a44e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x105a452f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x105a45760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x105a45bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x105a46040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x105a464b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x105a46920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x105a46d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x105a47200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x105a47670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x105a47ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x105a47f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x105a483c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x105a48830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x105a48ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x105a49110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x105a49580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x105a4a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x105a4a7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x105a4af00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x105a4b620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x105a4b8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x105a4bba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x105a4c010 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x105a04ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x105a05150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x105a055c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x105a05a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x105a05ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x105a06310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x105a06780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x105a06bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x105a07060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x105a074d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x105a07940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x105a07f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x105a08810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x105a08f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x105a09770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x105a09e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x105a0a550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x105a0ac40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x105a0b330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x105a0bcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x105a0c3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x105a0ca90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x105a0d180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x105a0d870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x105a0df60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x105a0e3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x105a0e840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x105a0ecb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x105a0f120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x105a0f590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x105a0fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x105a0fe70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x105a102e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x105a105a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x105a10a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x105a10e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x105a112f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x105a11760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x105a11bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x105a12040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x105a124b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x105a12920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x105a12d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x105a13200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x105a13670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x105a13ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x105a13f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x105a143c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x105a14830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x105a14ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x105a15110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x105a15580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x105a159f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x105a15e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x105a162d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x105a16740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x105a16bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x105a17020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x105a17490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x105a17900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x105a17d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x105a181e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x105a18650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x105a18ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x105a18f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x105a193a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x105a19810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x105a19c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x105a1a0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x105a1a560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x105a1a9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x105a1ae40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x105a1b2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x105a1b720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x105a1bb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x105a1c000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x105a1c470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x105a1c8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x105a1cd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x105a1d1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x105a1d630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x105a1daa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x105a1df10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x105a1e380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x105a1e7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x105a1ec60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x105a1f0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x105a1f540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x105a1f9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x105a1fe20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x105a20290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x105a20700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x105a20b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x105a20fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x105a21450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x105a218c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x105a21d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x105a221a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x105a22610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x105a22a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x105a22ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x105a23360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x105a237d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x105a23c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x105a240b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x105a24520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x105a24990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x105a24e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x105a25270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x105a256e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x105a25b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x105a25fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x105a26430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x105a268a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x105a26d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x105a27180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x105a275f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x105a27a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x105a27ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x105a28340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x105a287b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x105a28c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x105a29090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x105a29500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x105a29970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x105a29de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x105a2a250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x105a2a6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x105a2ab30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x105a2afa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x105a2b410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x105a2b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x105a2bcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x105a2c160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x105a2c5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x105a2ca40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x105a2ceb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x105a2d320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x105a2d790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x105a2dc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x105a2e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x105a2e4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x105a2e950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x105a2edc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x105a2f230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x105a2f6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x105a2fb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x105a2ff80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x105a303f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x105a30860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x105a30cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x105a31140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x105a315b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x105a31a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x105a31e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x105a32300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x105a32770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x105a32be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x105a33050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x105a334c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x105a33930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x105a33da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x105a34210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x105a34680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x105a34af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x105a34f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x105a353d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x105a35b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x105a35fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x105a36430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x105a368a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x105a36d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x105a37180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x105a375f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x105a37a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x105a37ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x105a38340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x105a387b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x105a38c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x105a39090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x105a39500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x105a39970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x105a39de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x105a3a250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x105a3a6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x105a3ab30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x105a3afa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x105a3b410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x105a3b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x105a3bcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x105a3c160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x105a3c5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x105a3ca40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x105a3ceb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x105a3d320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x105a3d790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x105a3dc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x105a3e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x105a3e4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x105a3e950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x105a3edc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x105a3f230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x105a3f6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x105a3fb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x105a3ff80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x105a403f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x105a40860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x105a40cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x105a41140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x105a415b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x105a41a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x105a41e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x105a42300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x105a42770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x105a42be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x105a43050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x105a434c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x105a43930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x105a43da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x105a44210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x105a44680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x105a44af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x105a44f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x105a453d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x105a45840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x105a45cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x105a46120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x105a46590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x105a46a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x105a46e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x105a472e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x105a47750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x105a47bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x105a48030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x105a484a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x105a48910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x105a48d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x105a491f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x105a498e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x105a49fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x105a4a6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x105a4adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x105a4b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x105a4b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x105a4bb00 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.827s
user	0m0.306s
sys	0m0.316s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4161 (0ba40c36)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 'Ä'
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x129f0d170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x129f0d880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x129f0de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x129f0e3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x129f0e990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x129f0ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x129f0f4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x129f0faa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x129f10050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x129f10550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x129f10a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x129f10f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x129f11a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x129f12220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x129f12a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x129f13150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x129f13870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x129f13f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x129f146b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x129f14e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x129f155a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x129f15cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x129f163e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x129f16c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x129f173a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x129f17660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x129f17c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x129f188e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x129f18e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x129f190e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x129f19580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x129f19840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x129f1a0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x129f1a610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x129f1a8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x129f1ad70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x129f1b210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x129f1b6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x129f1bb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x129f1bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x129f1c490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x129f1c930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x129f1cdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x129f1d270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x129f1d530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x129f1db40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x129f1e150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x129f1ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x129f1f080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x129f1f690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x129f1fca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x129f202b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x129f208c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x129f20ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x129f216c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x129f21b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x129f22000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x129f222c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x129f228d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x129f230c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x129f23380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x129f23820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x129f23cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x129f24160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x129f24600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x129f24aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x129f24f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x129f253e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x129f25880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x129f25d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x129f261c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x129f26660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x129f26b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x129f26fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x129f27440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x129f278e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x129f27d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x129f28220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x129f286c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x129f28b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x129f29000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x129f294a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x129f29940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x129f29de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x129f2a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x129f2a720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x129f2abc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x129f2b060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x129f2b500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x129f2b9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x129f2be40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x129f2c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x129f2c780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x129f2cc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x129f2d0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x129f2d560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x129f2da00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x129f1e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x129f2e050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x129f2e4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x129f2e990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x129f2ee30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x129f2f2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x129f2f770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x129f2fc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x129f300b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x129f30550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x129f309f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x129f30e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x129f31330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x129f317d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x129f31c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x129f32110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x129f325b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x129f32a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x129f32ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x129f33390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x129f33830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x129f33cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x129f34170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x129f34610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x129f34ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x129f34f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x129f353f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x129f35890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x129f35d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x129f361d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x129f36670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x129f36b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x129f36fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x129f37450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x129f378f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x129f37d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x129f38230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x129f386d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x129f38b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x129f39010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x129f394b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x129f39950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x129f39df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x129f3a290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x129f3a730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x129f3abd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x129f3b070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x129f3b510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x129f3b9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x129f3be50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x129f3c2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x129f3c790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x129f3cc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x129f3d0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x129f3d570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x129f3da10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x129f3df60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x129f3e4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x129f3ea00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x129f3ef50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x129f3f210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x129f3f820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x129f3fe30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x129f40440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x129f40a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x129f41060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x129f41850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x129f41cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x129f42190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x129f42630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x129f42de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x129f43330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x129f43880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x129f43dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x129f44320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x129f44870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x129f44dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x129f45310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x129f45860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x129f45db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x129f46300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x129f46850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x129f46da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x129f472f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x129f47840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x129f47d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x129f482e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x129f48830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x129f48d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x129f492d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x129f49820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x129f49d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x129f4a2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x129f4a810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x129f4ad60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x129f4b2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x129f4b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x129f4bd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x129f4c2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x129f4c7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x129f4cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x129f4d290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x129f4d7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x129f4dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x129f4e280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x129f4e7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x129f4ed20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x129f4f270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x129f4f7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x129f4fd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x129f50260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x129f507b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x129f50d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x129f51250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x129f517a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x129f51cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x129f52240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x129f52790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x129f52ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x129f53230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x129f53780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x129f53cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x129f54220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x129f54770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x129f54cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x129f55210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x129f55760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x129f55c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x129f560a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x129f56540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x129f569e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x129f56e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x129f57320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x129f577c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x129f57c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x129f58100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x129f585a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x129f58a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x129f58ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x129f59380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x129f598d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x129f59ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x129f5a710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x129f5ae30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x129f5b550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x129f5b810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x129f5be20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x129f5c430 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
0.00.088.482 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12b004ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12b004f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12b0053c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12b005830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12b005ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12b006110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12b006580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12b0069f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12b006e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12b0072d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12b007740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12b007e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12b008940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12b0090f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12b009900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12b00a020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12b00a740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12b00ae60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12b00b580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12b00bd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12b00c470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12b00cb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12b00d2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12b00d9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12b00e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12b00e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12b00e670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12b00eae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12b00ef50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12b00f3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12b00f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12b00fd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12b0101d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12b010490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12b010900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12b010d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12b0111e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12b011650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12b011ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12b011f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12b0123a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12b012810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12b012c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12b0130f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12b013560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12b0139d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12b013e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12b0142b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12b014720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12b014b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12b015000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12b015470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12b0158e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12b015d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12b0161c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12b016630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12b016ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12b0170a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12b017510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12b017980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12b017df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12b018260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12b0186d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12b018b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12b018fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12b019420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12b019890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12b019d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12b01a170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12b01a5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12b01aa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12b01aec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12b01b330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12b01b7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12b01bc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12b01c080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12b01c4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12b01c960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12b01cdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12b01d240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12b01d6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12b01db20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12b01df90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12b01e400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12b01e870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12b01ece0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12b01f150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12b01f5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12b01fa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12b01fea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12b020310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12b020780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12b020bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12b021060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12b0214d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12b021940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12b021db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12b022220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12b022690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12b022b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12b022f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12b0233e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12b023850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12b023cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12b024130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12b0245a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12b024a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12b024e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12b0252f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12b025760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12b025bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12b026040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12b0264b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12b026920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12b026d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12b027200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12b027670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12b027ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12b027f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12b0283c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12b028830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12b028ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12b029110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12b029580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12b0299f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12b029e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12b02a2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12b02a740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12b02abb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12b02b020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12b02b490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12b02b900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12b02bd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12b02c1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12b02c650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12b02cac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12b02cf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12b02d3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12b02d810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12b02dc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12b02e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12b02e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12b02e9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12b02ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12b02f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12b02f720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12b02fb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12b030000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12b030470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12b0308e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12b030d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12b0311c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12b031630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12b031aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12b031f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12b032380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12b0327f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12b032c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12b0330d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12b033540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12b0339b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12b033e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12b034290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12b034700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12b034b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12b034fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12b035450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12b035fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12b0362a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12b036560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12b0369d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12b036e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12b0372b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12b037720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12b037b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12b038000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12b038470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12b0388e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12b038d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12b0391c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12b039630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12b039aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12b039f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12b03a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12b03a7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12b03ac60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12b03b0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12b03b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12b03b9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12b03be20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12b03c290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12b03c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12b03cb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12b03cfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12b03d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12b03d8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12b03dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12b03e1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12b03e610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12b03ea80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12b03eef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12b03f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12b03f7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12b03fc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12b0400b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12b040520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12b040990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12b040e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12b041270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12b0416e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12b041b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12b041fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12b042430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12b0428a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12b042d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12b043180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12b0435f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12b043a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12b043ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12b044340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12b0447b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12b044c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12b045090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12b045500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12b045970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12b045de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12b046250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12b0466c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12b046b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12b046fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12b047410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12b047880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12b047cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12b048160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12b0485d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12b048a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12b048eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12b049320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12b049e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12b04a580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12b04aca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12b04b3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12b04b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12b04b940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12b04bdb0 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x129f0de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x129f0ef30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x129f0e3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x129f0e990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x129f0d170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x129f4c210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x129f4c4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x129f4c940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x129f4cdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x129f4d220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x129f4d690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x129f4db00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x129f4e3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x129f4eb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x129f4f350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x129f4fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x129f50130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x129f50820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x129f50f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x129f51890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x129f51f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x129f52670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x129f52d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x129f53450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x129f53b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x129f53fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x129f54420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x129f54890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x129f54d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x129f55170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x129f555e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x129f55a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x129f55ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x129f56180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x129f565f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x129f56a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x129f56ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x129f57340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x129f577b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x129f57c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x129f58090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x129f58500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x129f58970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x129f58de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x129f59250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x129f596c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x129f59b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x129f59fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x129f5a410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x129f5a880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x129f5acf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x129f5b160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x129f5b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x129f5ba40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x129f5beb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x129f5c320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x129f1a600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x129f1aa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x129f1aee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x129f1b350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x129f1b7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x129f1bc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x129f1c0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x129f1c510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x129f1c980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x129f1cdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x129f1d260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x129f1d6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x129f1db40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x129f1dfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x129f1e420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x129f1e890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x129f1ed00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x129f1f170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x129f1f5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x129f1fa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x129f1fec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x129f20330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x129f207a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x129f20c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x129f21080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x129f214f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x129f21960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x129f21dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x129f22240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x129f226b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x129f22b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x129f22f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x129f23400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x129f23870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x129f23ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x129f24150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x129f245c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x129f24a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x129f24ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x129f25310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x129f25780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x129f25bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x129f26060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x129f264d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x129f26940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x129f26db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x129f27220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x129f27690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x129f27b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x129f27f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x129f283e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x129f28850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x129f28cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x129f29130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x129f295a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x129f29a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x129f29e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x129f2a2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x129f2a760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x129f2abd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x129f2b040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x129f2b4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x129f2b920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x129f2bd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x129f2c200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x129f2c670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x129f2cae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x129f2cf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x129f2d3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x129f2d830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x129f2dca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x129f2e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x129f2e580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x129f2e9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x129f2ee60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x129f2f2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x129f2f740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x129f2fbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x129f30020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x129f30490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x129f30900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x129f30d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x129f311e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x129f31650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x129f31ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x129f31f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x129f323a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x129f32810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x129f32c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x129f330f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x129f33560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x129f339d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x129f33e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x129f342b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x129f34720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x129f34b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x129f35000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x129f35470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x129f358e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x129f35d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x129f361c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x129f36630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x129f36aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x129f36f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x129f37380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x129f377f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x129f37c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x129f380d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x129f38540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x129f389b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x129f38e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x129f395a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x129f39a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x129f39e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x129f3a2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x129f3a760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x129f3abd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x129f3b040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x129f3b4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x129f3b920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x129f3bd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x129f3c200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x129f3c670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x129f3cae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x129f3cf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x129f3d3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x129f3d830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x129f3dca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x129f3e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x129f3e580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x129f3e9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x129f3ee60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x129f3f2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x129f3f740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x129f3fbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x129f40020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x129f40490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x129f40900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x129f40d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x129f411e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x129f41650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x129f41ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x129f41f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x129f423a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x129f42810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x129f42c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x129f430f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x129f43560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x129f439d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x129f43e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x129f442b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x129f44720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x129f44b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x129f45000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x129f45470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x129f458e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x129f45d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x129f461c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x129f46630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x129f46aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x129f46f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x129f47380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x129f477f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x129f47c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x129f480d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x129f48540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x129f489b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x129f48e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x129f49290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x129f49700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x129f49b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x129f49fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x129f4a450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x129f4a8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x129f4ad30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x129f4b1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x129f4b610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x129f4ba80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x129f18e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x129f19280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x129f196f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x129f19b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x129f10510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x129f10c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x129f112f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x129f119e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x129f11e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x129f122c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x129f12730 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Our old friend, Zorro." "He


second run: The quick brown fox jumps over the lazy Dog." "Our old friend, Zorro." "He


single seq run: The quick brown fox jumps over the lazy Dog." "Our old friend, Zorro." "He

real	0m0.924s
user	0m0.238s
sys	0m0.122s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
