Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:318 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- ARM -mcpu not found, -mcpu=native will be used
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod - Success
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sve
-- Performing Test GGML_MACHINE_SUPPORTS_sve - Failed
-- Performing Test GGML_MACHINE_SUPPORTS_nosve
-- Performing Test GGML_MACHINE_SUPPORTS_nosve - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sme
-- Performing Test GGML_MACHINE_SUPPORTS_sme - Success
-- ARM feature DOTPROD enabled
-- ARM feature MATMUL_INT8 enabled
-- ARM feature FMA enabled
-- ARM feature FP16_VECTOR_ARITHMETIC enabled
-- ARM feature SME enabled
-- Adding CPU backend variant ggml-cpu: -mcpu=native+dotprod+i8mm+nosve+sme 
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (2.8s)
-- Generating done (0.2s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m3.022s
user	0m1.045s
sys	0m1.438s
++ nproc
+ make -j10
[  0%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  0%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  1%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  2%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  2%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  2%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  4%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  4%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  4%] Built target build_info
[  5%] Built target sha1
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o
[  5%] Built target xxhash
[  5%] Built target sha256
[  5%] Linking CXX shared library ../../bin/libggml-base.dylib
[  5%] Built target ggml-base
[  5%] Generate assembly for embedded Metal library
Embedding Metal library
[  7%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[  7%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[ 10%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 10%] Linking CXX shared library ../../../bin/libggml-blas.dylib
[ 11%] Linking CXX shared library ../../bin/libggml-cpu.dylib
[ 11%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 12%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 12%] Built target ggml-blas
[ 12%] Built target ggml-cpu
[ 13%] Linking C shared library ../../../bin/libggml-metal.dylib
[ 13%] Built target ggml-metal
[ 13%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 14%] Linking CXX shared library ../../bin/libggml.dylib
[ 14%] Built target ggml
[ 14%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 14%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o
[ 14%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o
[ 17%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o
[ 19%] Linking CXX executable ../../bin/llama-gguf
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o
[ 20%] Linking CXX executable ../../bin/llama-gguf-hash
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 25%] Built target llama-gguf
[ 25%] Linking CXX shared library ../bin/libllama.dylib
[ 25%] Built target llama-gguf-hash
[ 25%] Built target llama
[ 25%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 25%] Building CXX object common/CMakeFiles/common.dir/chat.cpp.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 27%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 27%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 27%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 28%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 29%] Linking CXX executable ../../bin/llama-quantize-stats
[ 30%] Linking CXX executable ../../bin/llama-simple
[ 30%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 31%] Linking C executable ../bin/test-c
[ 31%] Building CXX object common/CMakeFiles/common.dir/llguidance.cpp.o
[ 32%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 32%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 32%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 33%] Linking CXX executable ../../bin/llama-simple-chat
[ 34%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 34%] Built target llava
[ 35%] Linking CXX static library libcommon.a
[ 35%] Built target llama-simple
[ 35%] Built target test-c
[ 35%] Built target llama-quantize-stats
[ 35%] Built target llama-simple-chat
[ 36%] Linking CXX shared library ../../bin/libllava_shared.dylib
[ 36%] Linking CXX static library libllava_static.a
[ 36%] Built target llava_static
[ 36%] Built target llava_shared
[ 36%] Built target common
[ 37%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-chat.dir/test-chat.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 41%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 41%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 41%] Linking CXX executable ../bin/test-tokenizer-0
[ 41%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 43%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 45%] Building CXX object tests/CMakeFiles/test-chat.dir/get-model.cpp.o
[ 46%] Linking CXX executable ../bin/test-grammar-integration
[ 46%] Linking CXX executable ../bin/test-sampling
[ 47%] Linking CXX executable ../bin/test-grammar-parser
[ 47%] Linking CXX executable ../bin/test-log
[ 47%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 48%] Linking CXX executable ../bin/test-llama-grammar
[ 48%] Linking CXX executable ../bin/test-chat
[ 48%] Built target test-json-schema-to-grammar
[ 48%] Built target test-tokenizer-1-bpe
[ 48%] Built target test-llama-grammar
[ 48%] Built target test-sampling
[ 48%] Built target test-grammar-integration
[ 48%] Built target test-tokenizer-1-spm
[ 48%] Built target test-tokenizer-0
[ 48%] Built target test-log
[ 48%] Built target test-chat
[ 49%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 49%] Built target test-grammar-parser
[ 49%] Building CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 57%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 59%] Linking CXX executable ../bin/test-model-load-cancel
[ 59%] Linking CXX executable ../bin/test-arg-parser
[ 59%] Linking CXX executable ../bin/test-gguf
[ 60%] Linking CXX executable ../bin/test-chat-template
[ 60%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 61%] Linking CXX executable ../bin/test-backend-ops
[ 62%] Linking CXX executable ../bin/test-barrier
[ 62%] Linking CXX executable ../bin/test-autorelease
[ 62%] Linking CXX executable ../bin/test-quantize-perf
[ 62%] Linking CXX executable ../bin/test-quantize-fns
[ 63%] Linking CXX executable ../bin/test-rope
[ 63%] Built target test-backend-ops
[ 63%] Built target test-model-load-cancel
[ 63%] Built target test-gguf
[ 63%] Built target test-chat-template
[ 63%] Built target test-autorelease
[ 63%] Built target test-barrier
[ 63%] Built target test-quantize-perf
[ 63%] Built target test-arg-parser
[ 63%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 63%] Built target test-quantize-fns
[ 64%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 64%] Built target test-rope
[ 65%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 65%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 65%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 66%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 66%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 66%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 67%] Linking CXX executable ../../bin/llama-batched-bench
[ 67%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 67%] Linking CXX executable ../../bin/llama-embedding
[ 68%] Linking CXX executable ../../bin/llama-batched
[ 68%] Linking CXX executable ../../bin/llama-eval-callback
[ 69%] Linking CXX executable ../../bin/llama-gguf-split
[ 69%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 69%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 71%] Linking CXX executable ../../bin/llama-imatrix
[ 71%] Linking CXX executable ../../bin/llama-gritlm
[ 72%] Linking CXX executable ../../bin/llama-infill
[ 73%] Linking CXX executable ../../bin/llama-bench
[ 73%] Built target llama-gbnf-validator
[ 73%] Built target llama-gguf-split
[ 73%] Built target llama-eval-callback
[ 73%] Built target llama-batched
[ 73%] Built target llama-batched-bench
[ 73%] Built target llama-embedding
[ 73%] Built target llama-gritlm
[ 73%] Built target llama-imatrix
[ 73%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 73%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 73%] Built target llama-bench
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 74%] Built target llama-infill
[ 75%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 75%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 76%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 77%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 78%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 78%] Linking CXX executable ../../bin/llama-lookup
[ 79%] Linking CXX executable ../../bin/llama-lookahead
[ 79%] Linking CXX executable ../../bin/llama-lookup-create
[ 80%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 80%] Linking CXX executable ../../bin/llama-lookup-merge
[ 80%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 80%] Linking CXX executable ../../bin/llama-cli
[ 80%] Linking CXX executable ../../bin/llama-lookup-stats
[ 80%] Linking CXX executable ../../bin/llama-parallel
[ 80%] Linking CXX executable ../../bin/llama-passkey
[ 80%] Linking CXX executable ../../bin/llama-perplexity
[ 81%] Linking CXX executable ../../bin/llama-quantize
[ 81%] Built target llama-lookup
[ 81%] Built target llama-lookahead
[ 81%] Built target llama-lookup-merge
[ 81%] Built target llama-lookup-create
[ 81%] Built target llama-cli
[ 81%] Generating loading.html.hpp
[ 81%] Built target llama-parallel
[ 81%] Built target llama-lookup-stats
[ 81%] Built target llama-passkey
[ 81%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 81%] Built target llama-perplexity
[ 83%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 83%] Building CXX object examples/run/CMakeFiles/llama-run.dir/linenoise.cpp/linenoise.cpp.o
[ 83%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 83%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 83%] Built target llama-quantize
[ 84%] Generating index.html.gz.hpp
[ 84%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 85%] Linking CXX executable ../../bin/llama-retrieval
[ 85%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 85%] Linking CXX executable ../../bin/llama-save-load-state
[ 85%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 86%] Linking CXX executable ../../bin/llama-speculative
[ 86%] Linking CXX executable ../../bin/llama-run
[ 87%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 88%] Linking CXX executable ../../bin/llama-speculative-simple
[ 89%] Linking CXX executable ../../bin/llama-tokenize
[ 91%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 91%] Linking CXX executable ../../bin/llama-tts
[ 91%] Linking CXX executable ../../bin/llama-gen-docs
[ 91%] Built target llama-save-load-state
[ 91%] Built target llama-speculative
[ 91%] Built target llama-retrieval
[ 91%] Built target llama-run
[ 91%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 91%] Built target llama-tokenize
[ 91%] Built target llama-speculative-simple
[ 92%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 93%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 93%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 94%] Built target llama-tts
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-llava-clip-quantize-cli.dir/clip-quantize-cli.cpp.o
[ 94%] Built target llama-gen-docs
[ 94%] Linking CXX executable ../../bin/llama-export-lora
[ 94%] Linking CXX executable ../../bin/llama-cvector-generator
[ 95%] Linking CXX executable ../../bin/llama-llava-cli
[ 95%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 95%] Built target llama-convert-llama2c-to-ggml
[ 96%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 97%] Linking CXX executable ../../bin/llama-llava-clip-quantize-cli
[ 97%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 97%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 98%] Linking CXX executable ../../bin/llama-vdot
[ 99%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Built target llama-export-lora
[ 99%] Built target llama-cvector-generator
[ 99%] Built target llama-llava-cli
[ 99%] Built target llama-minicpmv-cli
[ 99%] Built target llama-llava-clip-quantize-cli
[ 99%] Built target llama-qwen2vl-cli
[ 99%] Built target llama-q8dot
[ 99%] Built target llama-vdot
[ 99%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m3.250s
user	0m6.450s
sys	0m10.469s

main: quantize time =  4981.37 ms
main:    total time =  4981.37 ms

main: quantize time =  1839.70 ms
main:    total time =  1839.70 ms

main: quantize time =  1812.07 ms
main:    total time =  1812.07 ms

main: quantize time =  2318.07 ms
main:    total time =  2318.07 ms

main: quantize time =  2704.77 ms
main:    total time =  2704.77 ms

main: quantize time =  4753.17 ms
main:    total time =  4753.17 ms

main: quantize time =  5523.73 ms
main:    total time =  5523.73 ms

main: quantize time =  6641.74 ms
main:    total time =  6641.74 ms

main: quantize time =  5725.83 ms
main:    total time =  5725.83 ms

main: quantize time =  4727.36 ms
main:    total time =  4727.36 ms
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.173 I build: 4793 (70680c48) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.340 I main: llama backend init
0.00.000.347 I main: load the model and apply lora adapter, if any
0.00.050.668 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.063.383 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.063.402 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.063.406 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.063.406 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.063.407 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.063.408 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.063.408 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.063.426 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.063.427 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.063.428 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.063.428 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.063.429 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.063.429 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.063.430 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.063.435 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.063.436 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.063.437 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.072.002 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.074.226 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.082.291 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.082.296 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.082.296 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.082.297 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.082.297 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.082.299 I llama_model_loader: - type  f32:  194 tensors
0.00.082.299 I llama_model_loader: - type  f16:   98 tensors
0.00.082.301 I print_info: file format = GGUF V3 (latest)
0.00.082.302 I print_info: file type   = all F32 (guessed)
0.00.082.304 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.096.872 I load: special tokens cache size = 25
0.00.105.610 I load: token to piece cache size = 0.2984 MB
0.00.105.613 I print_info: arch             = gptneox
0.00.105.614 I print_info: vocab_only       = 0
0.00.105.614 I print_info: n_ctx_train      = 2048
0.00.105.614 I print_info: n_embd           = 2048
0.00.105.614 I print_info: n_layer          = 24
0.00.105.617 I print_info: n_head           = 16
0.00.105.618 I print_info: n_head_kv        = 16
0.00.105.618 I print_info: n_rot            = 32
0.00.105.619 I print_info: n_swa            = 0
0.00.105.619 I print_info: n_embd_head_k    = 128
0.00.105.619 I print_info: n_embd_head_v    = 128
0.00.105.620 I print_info: n_gqa            = 1
0.00.105.621 I print_info: n_embd_k_gqa     = 2048
0.00.105.621 I print_info: n_embd_v_gqa     = 2048
0.00.105.622 I print_info: f_norm_eps       = 1.0e-05
0.00.105.622 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.105.623 I print_info: f_clamp_kqv      = 0.0e+00
0.00.105.623 I print_info: f_max_alibi_bias = 0.0e+00
0.00.105.623 I print_info: f_logit_scale    = 0.0e+00
0.00.105.626 I print_info: n_ff             = 8192
0.00.105.626 I print_info: n_expert         = 0
0.00.105.626 I print_info: n_expert_used    = 0
0.00.105.627 I print_info: causal attn      = 1
0.00.105.627 I print_info: pooling type     = 0
0.00.105.627 I print_info: rope type        = 2
0.00.105.629 I print_info: rope scaling     = linear
0.00.105.629 I print_info: freq_base_train  = 10000.0
0.00.105.629 I print_info: freq_scale_train = 1
0.00.105.629 I print_info: n_ctx_orig_yarn  = 2048
0.00.105.630 I print_info: rope_finetuned   = unknown
0.00.105.630 I print_info: ssm_d_conv       = 0
0.00.105.630 I print_info: ssm_d_inner      = 0
0.00.105.630 I print_info: ssm_d_state      = 0
0.00.105.630 I print_info: ssm_dt_rank      = 0
0.00.105.630 I print_info: ssm_dt_b_c_rms   = 0
0.00.105.631 I print_info: model type       = 1.4B
0.00.105.631 I print_info: model params     = 1.41 B
0.00.105.631 I print_info: general.name     = 1.4B
0.00.105.631 I print_info: vocab type       = BPE
0.00.105.632 I print_info: n_vocab          = 50304
0.00.105.632 I print_info: n_merges         = 50009
0.00.105.632 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.105.633 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.105.633 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.105.633 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.105.635 I print_info: LF token         = 187 'Ċ'
0.00.105.635 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.105.635 I print_info: max token length = 1024
0.00.105.636 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.147.152 I load_tensors: offloading 24 repeating layers to GPU
0.00.147.156 I load_tensors: offloading output layer to GPU
0.00.147.156 I load_tensors: offloaded 25/25 layers to GPU
0.00.147.182 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.147.184 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.147.695 I llama_init_from_model: n_seq_max     = 1
0.00.147.696 I llama_init_from_model: n_ctx         = 2048
0.00.147.696 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.147.696 I llama_init_from_model: n_batch       = 2048
0.00.147.696 I llama_init_from_model: n_ubatch      = 512
0.00.147.697 I llama_init_from_model: flash_attn    = 0
0.00.147.697 I llama_init_from_model: freq_base     = 10000.0
0.00.147.697 I llama_init_from_model: freq_scale    = 1
0.00.147.700 I ggml_metal_init: allocating
0.00.147.751 I ggml_metal_init: found device: Apple M4
0.00.147.760 I ggml_metal_init: picking default device: Apple M4
0.00.148.436 I ggml_metal_init: using embedded metal library
0.00.160.584 I ggml_metal_init: GPU name:   Apple M4
0.00.160.586 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.160.586 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.160.586 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.160.587 I ggml_metal_init: simdgroup reduction   = true
0.00.160.587 I ggml_metal_init: simdgroup matrix mul. = true
0.00.160.587 I ggml_metal_init: has residency sets    = true
0.00.160.587 I ggml_metal_init: has bfloat            = true
0.00.160.587 I ggml_metal_init: use bfloat            = true
0.00.160.588 I ggml_metal_init: hasUnifiedMemory      = true
0.00.160.588 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.186.234 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.215.047 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.215.054 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.215.099 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.219.320 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.219.322 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.219.322 I llama_init_from_model: graph nodes  = 967
0.00.219.322 I llama_init_from_model: graph splits = 2
0.00.219.329 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.219.445 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.219.445 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.285.447 I main: llama threadpool init, n_threads = 4
0.00.285.490 I 
0.00.285.525 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.285.526 I 
0.00.285.706 I sampler seed: 1234
0.00.285.711 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.285.735 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.285.737 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.285.737 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.122.668 I llama_perf_sampler_print:    sampling time =       1.27 ms /    71 runs   (    0.02 ms per token, 56037.88 tokens per second)
0.02.122.668 I llama_perf_context_print:        load time =     233.86 ms
0.02.122.669 I llama_perf_context_print: prompt eval time =      43.67 ms /     7 tokens (    6.24 ms per token,   160.29 tokens per second)
0.02.122.670 I llama_perf_context_print:        eval time =    1790.28 ms /    63 runs   (   28.42 ms per token,    35.19 tokens per second)
0.02.122.670 I llama_perf_context_print:       total time =    1838.13 ms /    70 tokens
0.02.122.901 I ggml_metal_free: deallocating

real	0m2.451s
user	0m0.132s
sys	0m0.139s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.051 I build: 4793 (70680c48) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.092 I main: llama backend init
0.00.000.094 I main: load the model and apply lora adapter, if any
0.00.009.951 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.026.225 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.026.233 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.026.235 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.026.236 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.026.236 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.026.236 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.026.237 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.026.238 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.026.238 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.026.239 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.026.239 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.026.239 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.026.240 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.026.240 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.026.242 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.026.244 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.026.246 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.030.181 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.031.207 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.035.085 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.035.086 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.035.087 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.035.087 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.035.087 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.035.088 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.035.088 I llama_model_loader: - type  f32:  194 tensors
0.00.035.088 I llama_model_loader: - type q8_0:   98 tensors
0.00.035.089 I print_info: file format = GGUF V3 (latest)
0.00.035.090 I print_info: file type   = Q8_0
0.00.035.092 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.044.336 I load: special tokens cache size = 25
0.00.050.676 I load: token to piece cache size = 0.2984 MB
0.00.050.680 I print_info: arch             = gptneox
0.00.050.680 I print_info: vocab_only       = 0
0.00.050.681 I print_info: n_ctx_train      = 2048
0.00.050.681 I print_info: n_embd           = 2048
0.00.050.681 I print_info: n_layer          = 24
0.00.050.687 I print_info: n_head           = 16
0.00.050.688 I print_info: n_head_kv        = 16
0.00.050.688 I print_info: n_rot            = 32
0.00.050.688 I print_info: n_swa            = 0
0.00.050.688 I print_info: n_embd_head_k    = 128
0.00.050.688 I print_info: n_embd_head_v    = 128
0.00.050.689 I print_info: n_gqa            = 1
0.00.050.690 I print_info: n_embd_k_gqa     = 2048
0.00.050.691 I print_info: n_embd_v_gqa     = 2048
0.00.050.691 I print_info: f_norm_eps       = 1.0e-05
0.00.050.692 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.692 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.692 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.693 I print_info: f_logit_scale    = 0.0e+00
0.00.050.694 I print_info: n_ff             = 8192
0.00.050.697 I print_info: n_expert         = 0
0.00.050.697 I print_info: n_expert_used    = 0
0.00.050.697 I print_info: causal attn      = 1
0.00.050.697 I print_info: pooling type     = 0
0.00.050.697 I print_info: rope type        = 2
0.00.050.698 I print_info: rope scaling     = linear
0.00.050.698 I print_info: freq_base_train  = 10000.0
0.00.050.698 I print_info: freq_scale_train = 1
0.00.050.698 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.698 I print_info: rope_finetuned   = unknown
0.00.050.699 I print_info: ssm_d_conv       = 0
0.00.050.699 I print_info: ssm_d_inner      = 0
0.00.050.699 I print_info: ssm_d_state      = 0
0.00.050.699 I print_info: ssm_dt_rank      = 0
0.00.050.699 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.699 I print_info: model type       = 1.4B
0.00.050.700 I print_info: model params     = 1.41 B
0.00.050.700 I print_info: general.name     = 1.4B
0.00.050.701 I print_info: vocab type       = BPE
0.00.050.701 I print_info: n_vocab          = 50304
0.00.050.701 I print_info: n_merges         = 50009
0.00.050.701 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.701 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.702 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.702 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.702 I print_info: LF token         = 187 'Ċ'
0.00.050.702 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.702 I print_info: max token length = 1024
0.00.050.704 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.089.949 I load_tensors: offloading 24 repeating layers to GPU
0.01.089.954 I load_tensors: offloading output layer to GPU
0.01.089.955 I load_tensors: offloaded 25/25 layers to GPU
0.01.089.981 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.01.089.983 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.01.090.901 I llama_init_from_model: n_seq_max     = 1
0.01.090.902 I llama_init_from_model: n_ctx         = 2048
0.01.090.903 I llama_init_from_model: n_ctx_per_seq = 2048
0.01.090.903 I llama_init_from_model: n_batch       = 2048
0.01.090.903 I llama_init_from_model: n_ubatch      = 512
0.01.090.903 I llama_init_from_model: flash_attn    = 0
0.01.090.904 I llama_init_from_model: freq_base     = 10000.0
0.01.090.905 I llama_init_from_model: freq_scale    = 1
0.01.090.906 I ggml_metal_init: allocating
0.01.090.915 I ggml_metal_init: found device: Apple M4
0.01.090.921 I ggml_metal_init: picking default device: Apple M4
0.01.092.204 I ggml_metal_init: using embedded metal library
0.01.097.444 I ggml_metal_init: GPU name:   Apple M4
0.01.097.447 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.097.447 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.097.448 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.097.449 I ggml_metal_init: simdgroup reduction   = true
0.01.097.449 I ggml_metal_init: simdgroup matrix mul. = true
0.01.097.449 I ggml_metal_init: has residency sets    = true
0.01.097.449 I ggml_metal_init: has bfloat            = true
0.01.097.449 I ggml_metal_init: use bfloat            = true
0.01.097.450 I ggml_metal_init: hasUnifiedMemory      = true
0.01.097.451 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.113.910 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.165.508 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.01.165.514 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.01.165.562 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.169.823 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.01.169.825 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.01.169.825 I llama_init_from_model: graph nodes  = 967
0.01.169.825 I llama_init_from_model: graph splits = 2
0.01.169.831 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.01.169.966 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.169.967 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.225.510 I main: llama threadpool init, n_threads = 4
0.01.225.558 I 
0.01.225.582 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.225.582 I 
0.01.225.733 I sampler seed: 1234
0.01.225.738 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.225.749 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.225.749 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.225.751 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.324.499 I llama_perf_sampler_print:    sampling time =       1.32 ms /    71 runs   (    0.02 ms per token, 53910.40 tokens per second)
0.02.324.500 I llama_perf_context_print:        load time =    1214.82 ms
0.02.324.501 I llama_perf_context_print: prompt eval time =      49.15 ms /     7 tokens (    7.02 ms per token,   142.42 tokens per second)
0.02.324.501 I llama_perf_context_print:        eval time =    1046.70 ms /    63 runs   (   16.61 ms per token,    60.19 tokens per second)
0.02.324.502 I llama_perf_context_print:       total time =    1099.72 ms /    70 tokens
0.02.324.753 I ggml_metal_free: deallocating

real	0m2.344s
user	0m0.110s
sys	0m0.269s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.076 I build: 4793 (70680c48) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.132 I main: llama backend init
0.00.000.137 I main: load the model and apply lora adapter, if any
0.00.017.267 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.033.300 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.033.306 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.033.308 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.033.309 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.033.309 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.033.315 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.033.315 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.033.319 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.033.319 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.033.319 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.033.320 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.033.320 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.033.321 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.033.322 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.033.325 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.033.326 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.033.326 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.038.981 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.040.477 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.045.853 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.045.855 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.045.856 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.045.856 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.045.856 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.045.857 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.045.858 I llama_model_loader: - type  f32:  194 tensors
0.00.045.858 I llama_model_loader: - type q4_0:   97 tensors
0.00.045.859 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.863 I print_info: file format = GGUF V3 (latest)
0.00.045.864 I print_info: file type   = Q4_0
0.00.045.865 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.060.190 I load: special tokens cache size = 25
0.00.074.179 I load: token to piece cache size = 0.2984 MB
0.00.074.185 I print_info: arch             = gptneox
0.00.074.186 I print_info: vocab_only       = 0
0.00.074.186 I print_info: n_ctx_train      = 2048
0.00.074.187 I print_info: n_embd           = 2048
0.00.074.187 I print_info: n_layer          = 24
0.00.074.191 I print_info: n_head           = 16
0.00.074.193 I print_info: n_head_kv        = 16
0.00.074.193 I print_info: n_rot            = 32
0.00.074.193 I print_info: n_swa            = 0
0.00.074.194 I print_info: n_embd_head_k    = 128
0.00.074.194 I print_info: n_embd_head_v    = 128
0.00.074.198 I print_info: n_gqa            = 1
0.00.074.199 I print_info: n_embd_k_gqa     = 2048
0.00.074.200 I print_info: n_embd_v_gqa     = 2048
0.00.074.201 I print_info: f_norm_eps       = 1.0e-05
0.00.074.203 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.074.204 I print_info: f_clamp_kqv      = 0.0e+00
0.00.074.204 I print_info: f_max_alibi_bias = 0.0e+00
0.00.074.204 I print_info: f_logit_scale    = 0.0e+00
0.00.074.205 I print_info: n_ff             = 8192
0.00.074.206 I print_info: n_expert         = 0
0.00.074.206 I print_info: n_expert_used    = 0
0.00.074.206 I print_info: causal attn      = 1
0.00.074.206 I print_info: pooling type     = 0
0.00.074.209 I print_info: rope type        = 2
0.00.074.211 I print_info: rope scaling     = linear
0.00.074.212 I print_info: freq_base_train  = 10000.0
0.00.074.212 I print_info: freq_scale_train = 1
0.00.074.212 I print_info: n_ctx_orig_yarn  = 2048
0.00.074.213 I print_info: rope_finetuned   = unknown
0.00.074.213 I print_info: ssm_d_conv       = 0
0.00.074.213 I print_info: ssm_d_inner      = 0
0.00.074.213 I print_info: ssm_d_state      = 0
0.00.074.213 I print_info: ssm_dt_rank      = 0
0.00.074.214 I print_info: ssm_dt_b_c_rms   = 0
0.00.074.214 I print_info: model type       = 1.4B
0.00.074.214 I print_info: model params     = 1.41 B
0.00.074.215 I print_info: general.name     = 1.4B
0.00.074.216 I print_info: vocab type       = BPE
0.00.074.216 I print_info: n_vocab          = 50304
0.00.074.216 I print_info: n_merges         = 50009
0.00.074.216 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.074.217 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.074.217 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.074.217 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.074.218 I print_info: LF token         = 187 'Ċ'
0.00.074.223 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.074.224 I print_info: max token length = 1024
0.00.074.224 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.569.196 I load_tensors: offloading 24 repeating layers to GPU
0.00.569.212 I load_tensors: offloading output layer to GPU
0.00.569.213 I load_tensors: offloaded 25/25 layers to GPU
0.00.569.249 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.569.251 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.570.684 I llama_init_from_model: n_seq_max     = 1
0.00.570.688 I llama_init_from_model: n_ctx         = 2048
0.00.570.688 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.570.689 I llama_init_from_model: n_batch       = 2048
0.00.570.689 I llama_init_from_model: n_ubatch      = 512
0.00.570.690 I llama_init_from_model: flash_attn    = 0
0.00.570.691 I llama_init_from_model: freq_base     = 10000.0
0.00.570.692 I llama_init_from_model: freq_scale    = 1
0.00.570.695 I ggml_metal_init: allocating
0.00.570.769 I ggml_metal_init: found device: Apple M4
0.00.570.782 I ggml_metal_init: picking default device: Apple M4
0.00.572.729 I ggml_metal_init: using embedded metal library
0.00.579.444 I ggml_metal_init: GPU name:   Apple M4
0.00.579.448 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.579.449 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.579.450 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.579.450 I ggml_metal_init: simdgroup reduction   = true
0.00.579.450 I ggml_metal_init: simdgroup matrix mul. = true
0.00.579.451 I ggml_metal_init: has residency sets    = true
0.00.579.451 I ggml_metal_init: has bfloat            = true
0.00.579.451 I ggml_metal_init: use bfloat            = true
0.00.579.452 I ggml_metal_init: hasUnifiedMemory      = true
0.00.579.454 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.598.189 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.651.564 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.651.570 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.651.608 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.656.120 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.656.122 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.656.122 I llama_init_from_model: graph nodes  = 967
0.00.656.122 I llama_init_from_model: graph splits = 2
0.00.656.128 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.656.243 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.656.244 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.713.802 I main: llama threadpool init, n_threads = 4
0.00.713.846 I 
0.00.713.870 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.713.870 I 
0.00.714.040 I sampler seed: 1234
0.00.714.045 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.714.068 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.714.069 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.714.069 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.411.489 I llama_perf_sampler_print:    sampling time =       1.43 ms /    71 runs   (    0.02 ms per token, 49650.35 tokens per second)
0.01.411.491 I llama_perf_context_print:        load time =     695.78 ms
0.01.411.492 I llama_perf_context_print: prompt eval time =      49.62 ms /     7 tokens (    7.09 ms per token,   141.07 tokens per second)
0.01.411.492 I llama_perf_context_print:        eval time =     644.85 ms /    63 runs   (   10.24 ms per token,    97.70 tokens per second)
0.01.411.493 I llama_perf_context_print:       total time =     698.43 ms /    70 tokens
0.01.411.724 I ggml_metal_free: deallocating

real	0m1.454s
user	0m0.131s
sys	0m0.203s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4793 (70680c48) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.082 I main: llama backend init
0.00.000.084 I main: load the model and apply lora adapter, if any
0.00.008.841 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.023.796 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.023.800 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.023.802 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.023.802 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.023.804 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.023.804 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.023.805 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.023.806 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.023.806 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.023.808 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.023.808 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.023.809 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.023.809 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.023.809 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.023.814 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.023.814 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.023.815 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.027.659 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.028.663 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.032.442 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.032.443 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.032.443 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.032.444 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.032.444 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.032.444 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.032.445 I llama_model_loader: - type  f32:  194 tensors
0.00.032.445 I llama_model_loader: - type q4_1:   97 tensors
0.00.032.445 I llama_model_loader: - type q6_K:    1 tensors
0.00.032.446 I print_info: file format = GGUF V3 (latest)
0.00.032.447 I print_info: file type   = Q4_1
0.00.032.448 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.040.712 I load: special tokens cache size = 25
0.00.047.190 I load: token to piece cache size = 0.2984 MB
0.00.047.193 I print_info: arch             = gptneox
0.00.047.193 I print_info: vocab_only       = 0
0.00.047.194 I print_info: n_ctx_train      = 2048
0.00.047.194 I print_info: n_embd           = 2048
0.00.047.194 I print_info: n_layer          = 24
0.00.047.197 I print_info: n_head           = 16
0.00.047.198 I print_info: n_head_kv        = 16
0.00.047.198 I print_info: n_rot            = 32
0.00.047.198 I print_info: n_swa            = 0
0.00.047.198 I print_info: n_embd_head_k    = 128
0.00.047.198 I print_info: n_embd_head_v    = 128
0.00.047.199 I print_info: n_gqa            = 1
0.00.047.200 I print_info: n_embd_k_gqa     = 2048
0.00.047.201 I print_info: n_embd_v_gqa     = 2048
0.00.047.201 I print_info: f_norm_eps       = 1.0e-05
0.00.047.202 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.047.202 I print_info: f_clamp_kqv      = 0.0e+00
0.00.047.202 I print_info: f_max_alibi_bias = 0.0e+00
0.00.047.202 I print_info: f_logit_scale    = 0.0e+00
0.00.047.207 I print_info: n_ff             = 8192
0.00.047.207 I print_info: n_expert         = 0
0.00.047.208 I print_info: n_expert_used    = 0
0.00.047.208 I print_info: causal attn      = 1
0.00.047.208 I print_info: pooling type     = 0
0.00.047.209 I print_info: rope type        = 2
0.00.047.211 I print_info: rope scaling     = linear
0.00.047.211 I print_info: freq_base_train  = 10000.0
0.00.047.211 I print_info: freq_scale_train = 1
0.00.047.211 I print_info: n_ctx_orig_yarn  = 2048
0.00.047.212 I print_info: rope_finetuned   = unknown
0.00.047.212 I print_info: ssm_d_conv       = 0
0.00.047.212 I print_info: ssm_d_inner      = 0
0.00.047.212 I print_info: ssm_d_state      = 0
0.00.047.212 I print_info: ssm_dt_rank      = 0
0.00.047.212 I print_info: ssm_dt_b_c_rms   = 0
0.00.047.213 I print_info: model type       = 1.4B
0.00.047.213 I print_info: model params     = 1.41 B
0.00.047.213 I print_info: general.name     = 1.4B
0.00.047.214 I print_info: vocab type       = BPE
0.00.047.214 I print_info: n_vocab          = 50304
0.00.047.214 I print_info: n_merges         = 50009
0.00.047.214 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.047.215 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.047.215 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.047.215 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.047.215 I print_info: LF token         = 187 'Ċ'
0.00.047.215 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.047.215 I print_info: max token length = 1024
0.00.047.216 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.570.708 I load_tensors: offloading 24 repeating layers to GPU
0.00.570.718 I load_tensors: offloading output layer to GPU
0.00.570.719 I load_tensors: offloaded 25/25 layers to GPU
0.00.570.755 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.570.758 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.572.359 I llama_init_from_model: n_seq_max     = 1
0.00.572.362 I llama_init_from_model: n_ctx         = 2048
0.00.572.363 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.572.363 I llama_init_from_model: n_batch       = 2048
0.00.572.364 I llama_init_from_model: n_ubatch      = 512
0.00.572.364 I llama_init_from_model: flash_attn    = 0
0.00.572.366 I llama_init_from_model: freq_base     = 10000.0
0.00.572.367 I llama_init_from_model: freq_scale    = 1
0.00.572.371 I ggml_metal_init: allocating
0.00.572.418 I ggml_metal_init: found device: Apple M4
0.00.572.430 I ggml_metal_init: picking default device: Apple M4
0.00.574.267 I ggml_metal_init: using embedded metal library
0.00.580.184 I ggml_metal_init: GPU name:   Apple M4
0.00.580.189 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.580.189 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.580.190 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.580.191 I ggml_metal_init: simdgroup reduction   = true
0.00.580.191 I ggml_metal_init: simdgroup matrix mul. = true
0.00.580.191 I ggml_metal_init: has residency sets    = true
0.00.580.192 I ggml_metal_init: has bfloat            = true
0.00.580.192 I ggml_metal_init: use bfloat            = true
0.00.580.193 I ggml_metal_init: hasUnifiedMemory      = true
0.00.580.194 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.599.514 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.652.483 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.652.490 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.652.524 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.656.641 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.656.643 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.656.643 I llama_init_from_model: graph nodes  = 967
0.00.656.643 I llama_init_from_model: graph splits = 2
0.00.656.649 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.656.782 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.656.783 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.710.030 I main: llama threadpool init, n_threads = 4
0.00.710.074 I 
0.00.710.098 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.710.098 I 
0.00.710.263 I sampler seed: 1234
0.00.710.267 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.710.277 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.710.282 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.710.282 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.444.445 I llama_perf_sampler_print:    sampling time =       1.27 ms /    71 runs   (    0.02 ms per token, 56082.15 tokens per second)
0.01.444.446 I llama_perf_context_print:        load time =     700.46 ms
0.01.444.448 I llama_perf_context_print: prompt eval time =      48.75 ms /     7 tokens (    6.96 ms per token,   143.60 tokens per second)
0.01.444.449 I llama_perf_context_print:        eval time =     682.72 ms /    63 runs   (   10.84 ms per token,    92.28 tokens per second)
0.01.444.450 I llama_perf_context_print:       total time =     735.15 ms /    70 tokens
0.01.444.711 I ggml_metal_free: deallocating

real	0m1.461s
user	0m0.110s
sys	0m0.191s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.049 I build: 4793 (70680c48) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.089 I main: llama backend init
0.00.000.091 I main: load the model and apply lora adapter, if any
0.00.008.949 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.026.155 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.026.159 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.026.161 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.026.161 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.026.161 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.026.162 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.026.162 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.026.163 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.026.163 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.026.164 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.026.164 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.026.165 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.026.165 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.026.166 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.026.167 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.026.167 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.026.168 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.030.110 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.031.166 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.035.015 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.035.016 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.035.017 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.035.017 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.035.017 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.035.018 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.035.018 I llama_model_loader: - type  f32:  194 tensors
0.00.035.018 I llama_model_loader: - type q5_0:   97 tensors
0.00.035.018 I llama_model_loader: - type q6_K:    1 tensors
0.00.035.019 I print_info: file format = GGUF V3 (latest)
0.00.035.019 I print_info: file type   = Q5_0
0.00.035.020 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.043.742 I load: special tokens cache size = 25
0.00.050.552 I load: token to piece cache size = 0.2984 MB
0.00.050.555 I print_info: arch             = gptneox
0.00.050.556 I print_info: vocab_only       = 0
0.00.050.556 I print_info: n_ctx_train      = 2048
0.00.050.556 I print_info: n_embd           = 2048
0.00.050.556 I print_info: n_layer          = 24
0.00.050.559 I print_info: n_head           = 16
0.00.050.560 I print_info: n_head_kv        = 16
0.00.050.560 I print_info: n_rot            = 32
0.00.050.560 I print_info: n_swa            = 0
0.00.050.560 I print_info: n_embd_head_k    = 128
0.00.050.561 I print_info: n_embd_head_v    = 128
0.00.050.561 I print_info: n_gqa            = 1
0.00.050.562 I print_info: n_embd_k_gqa     = 2048
0.00.050.563 I print_info: n_embd_v_gqa     = 2048
0.00.050.563 I print_info: f_norm_eps       = 1.0e-05
0.00.050.564 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.564 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.564 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.564 I print_info: f_logit_scale    = 0.0e+00
0.00.050.565 I print_info: n_ff             = 8192
0.00.050.565 I print_info: n_expert         = 0
0.00.050.565 I print_info: n_expert_used    = 0
0.00.050.565 I print_info: causal attn      = 1
0.00.050.565 I print_info: pooling type     = 0
0.00.050.565 I print_info: rope type        = 2
0.00.050.566 I print_info: rope scaling     = linear
0.00.050.566 I print_info: freq_base_train  = 10000.0
0.00.050.566 I print_info: freq_scale_train = 1
0.00.050.566 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.567 I print_info: rope_finetuned   = unknown
0.00.050.567 I print_info: ssm_d_conv       = 0
0.00.050.567 I print_info: ssm_d_inner      = 0
0.00.050.567 I print_info: ssm_d_state      = 0
0.00.050.567 I print_info: ssm_dt_rank      = 0
0.00.050.567 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.568 I print_info: model type       = 1.4B
0.00.050.569 I print_info: model params     = 1.41 B
0.00.050.569 I print_info: general.name     = 1.4B
0.00.050.569 I print_info: vocab type       = BPE
0.00.050.570 I print_info: n_vocab          = 50304
0.00.050.570 I print_info: n_merges         = 50009
0.00.050.570 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.570 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.570 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.570 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.571 I print_info: LF token         = 187 'Ċ'
0.00.050.571 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.571 I print_info: max token length = 1024
0.00.050.573 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.692.694 I load_tensors: offloading 24 repeating layers to GPU
0.00.692.708 I load_tensors: offloading output layer to GPU
0.00.692.709 I load_tensors: offloaded 25/25 layers to GPU
0.00.692.747 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.692.748 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.694.476 I llama_init_from_model: n_seq_max     = 1
0.00.694.478 I llama_init_from_model: n_ctx         = 2048
0.00.694.479 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.694.480 I llama_init_from_model: n_batch       = 2048
0.00.694.480 I llama_init_from_model: n_ubatch      = 512
0.00.694.480 I llama_init_from_model: flash_attn    = 0
0.00.694.483 I llama_init_from_model: freq_base     = 10000.0
0.00.694.483 I llama_init_from_model: freq_scale    = 1
0.00.694.486 I ggml_metal_init: allocating
0.00.694.596 I ggml_metal_init: found device: Apple M4
0.00.694.609 I ggml_metal_init: picking default device: Apple M4
0.00.696.553 I ggml_metal_init: using embedded metal library
0.00.703.509 I ggml_metal_init: GPU name:   Apple M4
0.00.703.514 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.703.515 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.703.515 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.703.516 I ggml_metal_init: simdgroup reduction   = true
0.00.703.516 I ggml_metal_init: simdgroup matrix mul. = true
0.00.703.517 I ggml_metal_init: has residency sets    = true
0.00.703.517 I ggml_metal_init: has bfloat            = true
0.00.703.518 I ggml_metal_init: use bfloat            = true
0.00.703.519 I ggml_metal_init: hasUnifiedMemory      = true
0.00.703.520 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.722.209 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.778.152 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.778.158 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.778.195 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.782.184 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.782.186 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.782.187 I llama_init_from_model: graph nodes  = 967
0.00.782.187 I llama_init_from_model: graph splits = 2
0.00.782.192 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.782.307 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.782.308 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.843.038 I main: llama threadpool init, n_threads = 4
0.00.843.083 I 
0.00.843.108 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.843.109 I 
0.00.843.259 I sampler seed: 1234
0.00.843.264 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.843.275 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.843.275 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.843.275 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.637.484 I llama_perf_sampler_print:    sampling time =       1.34 ms /    71 runs   (    0.02 ms per token, 52906.11 tokens per second)
0.01.637.485 I llama_perf_context_print:        load time =     833.34 ms
0.01.637.485 I llama_perf_context_print: prompt eval time =      52.99 ms /     7 tokens (    7.57 ms per token,   132.10 tokens per second)
0.01.637.487 I llama_perf_context_print:        eval time =     738.41 ms /    63 runs   (   11.72 ms per token,    85.32 tokens per second)
0.01.637.487 I llama_perf_context_print:       total time =     795.19 ms /    70 tokens
0.01.637.771 I ggml_metal_free: deallocating

real	0m1.655s
user	0m0.112s
sys	0m0.212s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4793 (70680c48) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.078 I main: llama backend init
0.00.000.080 I main: load the model and apply lora adapter, if any
0.00.017.417 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.024.869 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.024.874 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.024.875 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.024.876 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.024.876 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.024.877 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.024.877 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.024.878 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.024.878 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.024.878 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.024.879 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.024.879 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.024.879 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.024.880 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.024.881 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.024.882 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.024.882 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.029.145 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.030.340 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.034.782 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.034.783 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.034.784 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.034.784 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.034.784 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.034.785 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.034.785 I llama_model_loader: - type  f32:  194 tensors
0.00.034.785 I llama_model_loader: - type q5_1:   97 tensors
0.00.034.786 I llama_model_loader: - type q6_K:    1 tensors
0.00.034.786 I print_info: file format = GGUF V3 (latest)
0.00.034.787 I print_info: file type   = Q5_1
0.00.034.787 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.044.355 I load: special tokens cache size = 25
0.00.051.776 I load: token to piece cache size = 0.2984 MB
0.00.051.780 I print_info: arch             = gptneox
0.00.051.780 I print_info: vocab_only       = 0
0.00.051.781 I print_info: n_ctx_train      = 2048
0.00.051.781 I print_info: n_embd           = 2048
0.00.051.781 I print_info: n_layer          = 24
0.00.051.784 I print_info: n_head           = 16
0.00.051.785 I print_info: n_head_kv        = 16
0.00.051.786 I print_info: n_rot            = 32
0.00.051.786 I print_info: n_swa            = 0
0.00.051.788 I print_info: n_embd_head_k    = 128
0.00.051.788 I print_info: n_embd_head_v    = 128
0.00.051.789 I print_info: n_gqa            = 1
0.00.051.790 I print_info: n_embd_k_gqa     = 2048
0.00.051.791 I print_info: n_embd_v_gqa     = 2048
0.00.051.791 I print_info: f_norm_eps       = 1.0e-05
0.00.051.797 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.797 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.797 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.798 I print_info: f_logit_scale    = 0.0e+00
0.00.051.799 I print_info: n_ff             = 8192
0.00.051.799 I print_info: n_expert         = 0
0.00.051.799 I print_info: n_expert_used    = 0
0.00.051.799 I print_info: causal attn      = 1
0.00.051.801 I print_info: pooling type     = 0
0.00.051.801 I print_info: rope type        = 2
0.00.051.802 I print_info: rope scaling     = linear
0.00.051.802 I print_info: freq_base_train  = 10000.0
0.00.051.802 I print_info: freq_scale_train = 1
0.00.051.803 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.803 I print_info: rope_finetuned   = unknown
0.00.051.803 I print_info: ssm_d_conv       = 0
0.00.051.803 I print_info: ssm_d_inner      = 0
0.00.051.803 I print_info: ssm_d_state      = 0
0.00.051.804 I print_info: ssm_dt_rank      = 0
0.00.051.804 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.804 I print_info: model type       = 1.4B
0.00.051.805 I print_info: model params     = 1.41 B
0.00.051.813 I print_info: general.name     = 1.4B
0.00.051.816 I print_info: vocab type       = BPE
0.00.051.817 I print_info: n_vocab          = 50304
0.00.051.817 I print_info: n_merges         = 50009
0.00.051.817 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.818 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.818 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.818 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.818 I print_info: LF token         = 187 'Ċ'
0.00.051.820 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.820 I print_info: max token length = 1024
0.00.051.821 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.772.521 I load_tensors: offloading 24 repeating layers to GPU
0.00.772.536 I load_tensors: offloading output layer to GPU
0.00.772.537 I load_tensors: offloaded 25/25 layers to GPU
0.00.772.577 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.772.579 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.774.018 I llama_init_from_model: n_seq_max     = 1
0.00.774.021 I llama_init_from_model: n_ctx         = 2048
0.00.774.022 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.774.022 I llama_init_from_model: n_batch       = 2048
0.00.774.023 I llama_init_from_model: n_ubatch      = 512
0.00.774.023 I llama_init_from_model: flash_attn    = 0
0.00.774.025 I llama_init_from_model: freq_base     = 10000.0
0.00.774.025 I llama_init_from_model: freq_scale    = 1
0.00.774.028 I ggml_metal_init: allocating
0.00.774.128 I ggml_metal_init: found device: Apple M4
0.00.774.141 I ggml_metal_init: picking default device: Apple M4
0.00.775.960 I ggml_metal_init: using embedded metal library
0.00.782.525 I ggml_metal_init: GPU name:   Apple M4
0.00.782.531 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.782.531 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.782.532 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.782.533 I ggml_metal_init: simdgroup reduction   = true
0.00.782.533 I ggml_metal_init: simdgroup matrix mul. = true
0.00.782.533 I ggml_metal_init: has residency sets    = true
0.00.782.534 I ggml_metal_init: has bfloat            = true
0.00.782.534 I ggml_metal_init: use bfloat            = true
0.00.782.535 I ggml_metal_init: hasUnifiedMemory      = true
0.00.782.537 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.801.050 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.859.125 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.859.131 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.859.167 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.864.535 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.864.538 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.864.538 I llama_init_from_model: graph nodes  = 967
0.00.864.539 I llama_init_from_model: graph splits = 2
0.00.864.544 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.864.674 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.864.674 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.926.367 I main: llama threadpool init, n_threads = 4
0.00.926.411 I 
0.00.926.435 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.926.437 I 
0.00.926.621 I sampler seed: 1234
0.00.926.626 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.926.662 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.926.664 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.926.664 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.782.632 I llama_perf_sampler_print:    sampling time =       1.49 ms /    71 runs   (    0.02 ms per token, 47747.14 tokens per second)
0.01.782.633 I llama_perf_context_print:        load time =     908.24 ms
0.01.782.634 I llama_perf_context_print: prompt eval time =      52.34 ms /     7 tokens (    7.48 ms per token,   133.74 tokens per second)
0.01.782.634 I llama_perf_context_print:        eval time =     801.10 ms /    63 runs   (   12.72 ms per token,    78.64 tokens per second)
0.01.782.635 I llama_perf_context_print:       total time =     856.97 ms /    70 tokens
0.01.782.881 I ggml_metal_free: deallocating

real	0m1.810s
user	0m0.116s
sys	0m0.236s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.047 I build: 4793 (70680c48) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.085 I main: llama backend init
0.00.000.088 I main: load the model and apply lora adapter, if any
0.00.009.435 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.858 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.865 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.867 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.867 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.868 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.868 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.868 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.869 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.871 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.871 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.872 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.872 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.875 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.875 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.877 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.878 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.878 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.944 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.010 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.003 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.005 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.005 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.005 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.006 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.006 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.026.007 I llama_model_loader: - type  f32:  194 tensors
0.00.026.007 I llama_model_loader: - type q2_K:   49 tensors
0.00.026.007 I llama_model_loader: - type q3_K:   48 tensors
0.00.026.008 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.008 I print_info: file format = GGUF V3 (latest)
0.00.026.009 I print_info: file type   = Q2_K - Medium
0.00.026.010 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.034.614 I load: special tokens cache size = 25
0.00.040.818 I load: token to piece cache size = 0.2984 MB
0.00.040.822 I print_info: arch             = gptneox
0.00.040.822 I print_info: vocab_only       = 0
0.00.040.822 I print_info: n_ctx_train      = 2048
0.00.040.822 I print_info: n_embd           = 2048
0.00.040.822 I print_info: n_layer          = 24
0.00.040.827 I print_info: n_head           = 16
0.00.040.827 I print_info: n_head_kv        = 16
0.00.040.828 I print_info: n_rot            = 32
0.00.040.828 I print_info: n_swa            = 0
0.00.040.828 I print_info: n_embd_head_k    = 128
0.00.040.828 I print_info: n_embd_head_v    = 128
0.00.040.829 I print_info: n_gqa            = 1
0.00.040.829 I print_info: n_embd_k_gqa     = 2048
0.00.040.832 I print_info: n_embd_v_gqa     = 2048
0.00.040.833 I print_info: f_norm_eps       = 1.0e-05
0.00.040.833 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.833 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.833 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.833 I print_info: f_logit_scale    = 0.0e+00
0.00.040.834 I print_info: n_ff             = 8192
0.00.040.834 I print_info: n_expert         = 0
0.00.040.834 I print_info: n_expert_used    = 0
0.00.040.834 I print_info: causal attn      = 1
0.00.040.834 I print_info: pooling type     = 0
0.00.040.835 I print_info: rope type        = 2
0.00.040.835 I print_info: rope scaling     = linear
0.00.040.836 I print_info: freq_base_train  = 10000.0
0.00.040.837 I print_info: freq_scale_train = 1
0.00.040.837 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.837 I print_info: rope_finetuned   = unknown
0.00.040.837 I print_info: ssm_d_conv       = 0
0.00.040.837 I print_info: ssm_d_inner      = 0
0.00.040.837 I print_info: ssm_d_state      = 0
0.00.040.837 I print_info: ssm_dt_rank      = 0
0.00.040.838 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.838 I print_info: model type       = 1.4B
0.00.040.838 I print_info: model params     = 1.41 B
0.00.040.838 I print_info: general.name     = 1.4B
0.00.040.839 I print_info: vocab type       = BPE
0.00.040.839 I print_info: n_vocab          = 50304
0.00.040.839 I print_info: n_merges         = 50009
0.00.040.839 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.839 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.840 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.840 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.840 I print_info: LF token         = 187 'Ċ'
0.00.040.840 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.840 I print_info: max token length = 1024
0.00.040.841 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.622.210 I load_tensors: offloading 24 repeating layers to GPU
0.00.622.219 I load_tensors: offloading output layer to GPU
0.00.622.219 I load_tensors: offloaded 25/25 layers to GPU
0.00.622.250 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.622.252 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.623.613 I llama_init_from_model: n_seq_max     = 1
0.00.623.619 I llama_init_from_model: n_ctx         = 2048
0.00.623.620 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.623.620 I llama_init_from_model: n_batch       = 2048
0.00.623.621 I llama_init_from_model: n_ubatch      = 512
0.00.623.621 I llama_init_from_model: flash_attn    = 0
0.00.623.623 I llama_init_from_model: freq_base     = 10000.0
0.00.623.624 I llama_init_from_model: freq_scale    = 1
0.00.623.626 I ggml_metal_init: allocating
0.00.623.718 I ggml_metal_init: found device: Apple M4
0.00.623.731 I ggml_metal_init: picking default device: Apple M4
0.00.625.490 I ggml_metal_init: using embedded metal library
0.00.630.735 I ggml_metal_init: GPU name:   Apple M4
0.00.630.743 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.630.744 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.630.744 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.630.745 I ggml_metal_init: simdgroup reduction   = true
0.00.630.745 I ggml_metal_init: simdgroup matrix mul. = true
0.00.630.745 I ggml_metal_init: has residency sets    = true
0.00.630.746 I ggml_metal_init: has bfloat            = true
0.00.630.746 I ggml_metal_init: use bfloat            = true
0.00.630.748 I ggml_metal_init: hasUnifiedMemory      = true
0.00.630.751 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.646.145 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.680.370 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.680.378 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.680.412 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.685.311 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.685.314 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.685.314 I llama_init_from_model: graph nodes  = 967
0.00.685.315 I llama_init_from_model: graph splits = 2
0.00.685.321 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.685.451 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.685.452 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.746.026 I main: llama threadpool init, n_threads = 4
0.00.746.073 I 
0.00.746.096 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.746.097 I 
0.00.746.272 I sampler seed: 1234
0.00.746.276 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.746.287 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.746.288 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.746.288 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.433.401 I llama_perf_sampler_print:    sampling time =       1.35 ms /    71 runs   (    0.02 ms per token, 52475.98 tokens per second)
0.01.433.402 I llama_perf_context_print:        load time =     735.84 ms
0.01.433.403 I llama_perf_context_print: prompt eval time =      44.15 ms /     7 tokens (    6.31 ms per token,   158.55 tokens per second)
0.01.433.404 I llama_perf_context_print:        eval time =     640.08 ms /    63 runs   (   10.16 ms per token,    98.43 tokens per second)
0.01.433.405 I llama_perf_context_print:       total time =     688.12 ms /    70 tokens
0.01.433.644 I ggml_metal_free: deallocating

real	0m1.452s
user	0m0.107s
sys	0m0.139s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.047 I build: 4793 (70680c48) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.084 I main: llama backend init
0.00.000.086 I main: load the model and apply lora adapter, if any
0.00.008.683 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.134 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.139 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.141 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.141 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.142 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.142 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.142 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.143 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.144 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.144 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.145 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.145 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.145 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.146 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.147 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.148 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.148 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.001 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.049 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.868 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.870 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.870 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.870 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.871 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.871 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.871 I llama_model_loader: - type  f32:  194 tensors
0.00.024.872 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.872 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.872 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.872 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.873 I print_info: file format = GGUF V3 (latest)
0.00.024.874 I print_info: file type   = Q3_K - Medium
0.00.024.874 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.032.763 I load: special tokens cache size = 25
0.00.038.541 I load: token to piece cache size = 0.2984 MB
0.00.038.544 I print_info: arch             = gptneox
0.00.038.544 I print_info: vocab_only       = 0
0.00.038.545 I print_info: n_ctx_train      = 2048
0.00.038.545 I print_info: n_embd           = 2048
0.00.038.545 I print_info: n_layer          = 24
0.00.038.548 I print_info: n_head           = 16
0.00.038.548 I print_info: n_head_kv        = 16
0.00.038.549 I print_info: n_rot            = 32
0.00.038.549 I print_info: n_swa            = 0
0.00.038.549 I print_info: n_embd_head_k    = 128
0.00.038.550 I print_info: n_embd_head_v    = 128
0.00.038.551 I print_info: n_gqa            = 1
0.00.038.552 I print_info: n_embd_k_gqa     = 2048
0.00.038.553 I print_info: n_embd_v_gqa     = 2048
0.00.038.553 I print_info: f_norm_eps       = 1.0e-05
0.00.038.553 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.554 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.554 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.554 I print_info: f_logit_scale    = 0.0e+00
0.00.038.555 I print_info: n_ff             = 8192
0.00.038.563 I print_info: n_expert         = 0
0.00.038.566 I print_info: n_expert_used    = 0
0.00.038.566 I print_info: causal attn      = 1
0.00.038.566 I print_info: pooling type     = 0
0.00.038.566 I print_info: rope type        = 2
0.00.038.567 I print_info: rope scaling     = linear
0.00.038.567 I print_info: freq_base_train  = 10000.0
0.00.038.567 I print_info: freq_scale_train = 1
0.00.038.567 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.568 I print_info: rope_finetuned   = unknown
0.00.038.568 I print_info: ssm_d_conv       = 0
0.00.038.569 I print_info: ssm_d_inner      = 0
0.00.038.569 I print_info: ssm_d_state      = 0
0.00.038.569 I print_info: ssm_dt_rank      = 0
0.00.038.569 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.569 I print_info: model type       = 1.4B
0.00.038.570 I print_info: model params     = 1.41 B
0.00.038.570 I print_info: general.name     = 1.4B
0.00.038.570 I print_info: vocab type       = BPE
0.00.038.570 I print_info: n_vocab          = 50304
0.00.038.571 I print_info: n_merges         = 50009
0.00.038.571 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.571 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.571 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.572 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.573 I print_info: LF token         = 187 'Ċ'
0.00.038.573 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.573 I print_info: max token length = 1024
0.00.038.573 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.467.158 I load_tensors: offloading 24 repeating layers to GPU
0.00.467.172 I load_tensors: offloading output layer to GPU
0.00.467.173 I load_tensors: offloaded 25/25 layers to GPU
0.00.467.207 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.467.208 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.468.724 I llama_init_from_model: n_seq_max     = 1
0.00.468.729 I llama_init_from_model: n_ctx         = 2048
0.00.468.730 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.468.731 I llama_init_from_model: n_batch       = 2048
0.00.468.731 I llama_init_from_model: n_ubatch      = 512
0.00.468.731 I llama_init_from_model: flash_attn    = 0
0.00.468.733 I llama_init_from_model: freq_base     = 10000.0
0.00.468.733 I llama_init_from_model: freq_scale    = 1
0.00.468.735 I ggml_metal_init: allocating
0.00.468.789 I ggml_metal_init: found device: Apple M4
0.00.468.802 I ggml_metal_init: picking default device: Apple M4
0.00.470.709 I ggml_metal_init: using embedded metal library
0.00.476.431 I ggml_metal_init: GPU name:   Apple M4
0.00.476.437 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.476.438 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.476.438 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.476.439 I ggml_metal_init: simdgroup reduction   = true
0.00.476.439 I ggml_metal_init: simdgroup matrix mul. = true
0.00.476.440 I ggml_metal_init: has residency sets    = true
0.00.476.440 I ggml_metal_init: has bfloat            = true
0.00.476.440 I ggml_metal_init: use bfloat            = true
0.00.476.441 I ggml_metal_init: hasUnifiedMemory      = true
0.00.476.443 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.496.603 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.557.432 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.557.438 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.557.474 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.562.071 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.562.073 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.562.073 I llama_init_from_model: graph nodes  = 967
0.00.562.073 I llama_init_from_model: graph splits = 2
0.00.562.079 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.562.200 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.562.201 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.621.023 I main: llama threadpool init, n_threads = 4
0.00.621.065 I 
0.00.621.087 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.621.088 I 
0.00.621.265 I sampler seed: 1234
0.00.621.272 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.621.322 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.621.325 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.621.325 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.370.720 I llama_perf_sampler_print:    sampling time =       1.41 ms /    71 runs   (    0.02 ms per token, 50247.70 tokens per second)
0.01.370.720 I llama_perf_context_print:        load time =     611.61 ms
0.01.370.722 I llama_perf_context_print: prompt eval time =      50.19 ms /     7 tokens (    7.17 ms per token,   139.48 tokens per second)
0.01.370.722 I llama_perf_context_print:        eval time =     696.20 ms /    63 runs   (   11.05 ms per token,    90.49 tokens per second)
0.01.370.723 I llama_perf_context_print:       total time =     750.42 ms /    70 tokens
0.01.370.923 I ggml_metal_free: deallocating

real	0m1.392s
user	0m0.112s
sys	0m0.203s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.043 I build: 4793 (70680c48) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.083 I main: llama backend init
0.00.000.085 I main: load the model and apply lora adapter, if any
0.00.015.972 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.023.869 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.023.874 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.023.876 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.023.877 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.023.877 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.023.878 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.023.878 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.023.879 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.023.879 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.023.880 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.023.880 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.023.880 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.023.881 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.023.881 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.023.882 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.023.882 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.023.883 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.028.133 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.029.293 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.033.514 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.033.515 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.033.516 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.033.516 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.033.517 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.033.517 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.033.517 I llama_model_loader: - type  f32:  194 tensors
0.00.033.518 I llama_model_loader: - type q4_K:   61 tensors
0.00.033.518 I llama_model_loader: - type q5_K:   24 tensors
0.00.033.518 I llama_model_loader: - type q6_K:   13 tensors
0.00.033.518 I print_info: file format = GGUF V3 (latest)
0.00.033.519 I print_info: file type   = Q4_K - Medium
0.00.033.520 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.043.587 I load: special tokens cache size = 25
0.00.051.363 I load: token to piece cache size = 0.2984 MB
0.00.051.366 I print_info: arch             = gptneox
0.00.051.367 I print_info: vocab_only       = 0
0.00.051.367 I print_info: n_ctx_train      = 2048
0.00.051.367 I print_info: n_embd           = 2048
0.00.051.367 I print_info: n_layer          = 24
0.00.051.370 I print_info: n_head           = 16
0.00.051.371 I print_info: n_head_kv        = 16
0.00.051.371 I print_info: n_rot            = 32
0.00.051.371 I print_info: n_swa            = 0
0.00.051.371 I print_info: n_embd_head_k    = 128
0.00.051.371 I print_info: n_embd_head_v    = 128
0.00.051.372 I print_info: n_gqa            = 1
0.00.051.373 I print_info: n_embd_k_gqa     = 2048
0.00.051.374 I print_info: n_embd_v_gqa     = 2048
0.00.051.376 I print_info: f_norm_eps       = 1.0e-05
0.00.051.376 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.376 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.377 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.377 I print_info: f_logit_scale    = 0.0e+00
0.00.051.377 I print_info: n_ff             = 8192
0.00.051.378 I print_info: n_expert         = 0
0.00.051.378 I print_info: n_expert_used    = 0
0.00.051.378 I print_info: causal attn      = 1
0.00.051.378 I print_info: pooling type     = 0
0.00.051.378 I print_info: rope type        = 2
0.00.051.378 I print_info: rope scaling     = linear
0.00.051.379 I print_info: freq_base_train  = 10000.0
0.00.051.379 I print_info: freq_scale_train = 1
0.00.051.379 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.380 I print_info: rope_finetuned   = unknown
0.00.051.380 I print_info: ssm_d_conv       = 0
0.00.051.380 I print_info: ssm_d_inner      = 0
0.00.051.380 I print_info: ssm_d_state      = 0
0.00.051.380 I print_info: ssm_dt_rank      = 0
0.00.051.380 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.381 I print_info: model type       = 1.4B
0.00.051.381 I print_info: model params     = 1.41 B
0.00.051.381 I print_info: general.name     = 1.4B
0.00.051.382 I print_info: vocab type       = BPE
0.00.051.382 I print_info: n_vocab          = 50304
0.00.051.382 I print_info: n_merges         = 50009
0.00.051.383 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.383 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.383 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.383 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.385 I print_info: LF token         = 187 'Ċ'
0.00.051.386 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.386 I print_info: max token length = 1024
0.00.051.386 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.529.599 I load_tensors: offloading 24 repeating layers to GPU
0.00.529.615 I load_tensors: offloading output layer to GPU
0.00.529.616 I load_tensors: offloaded 25/25 layers to GPU
0.00.529.655 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.529.656 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.531.248 I llama_init_from_model: n_seq_max     = 1
0.00.531.250 I llama_init_from_model: n_ctx         = 2048
0.00.531.251 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.531.251 I llama_init_from_model: n_batch       = 2048
0.00.531.252 I llama_init_from_model: n_ubatch      = 512
0.00.531.252 I llama_init_from_model: flash_attn    = 0
0.00.531.254 I llama_init_from_model: freq_base     = 10000.0
0.00.531.254 I llama_init_from_model: freq_scale    = 1
0.00.531.256 I ggml_metal_init: allocating
0.00.531.339 I ggml_metal_init: found device: Apple M4
0.00.531.354 I ggml_metal_init: picking default device: Apple M4
0.00.533.231 I ggml_metal_init: using embedded metal library
0.00.540.158 I ggml_metal_init: GPU name:   Apple M4
0.00.540.165 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.540.166 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.540.166 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.540.167 I ggml_metal_init: simdgroup reduction   = true
0.00.540.167 I ggml_metal_init: simdgroup matrix mul. = true
0.00.540.167 I ggml_metal_init: has residency sets    = true
0.00.540.167 I ggml_metal_init: has bfloat            = true
0.00.540.168 I ggml_metal_init: use bfloat            = true
0.00.540.169 I ggml_metal_init: hasUnifiedMemory      = true
0.00.540.180 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.558.911 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.616.735 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.616.741 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.616.777 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.621.456 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.621.458 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.621.458 I llama_init_from_model: graph nodes  = 967
0.00.621.459 I llama_init_from_model: graph splits = 2
0.00.621.464 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.621.592 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.621.592 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.677.401 I main: llama threadpool init, n_threads = 4
0.00.677.445 I 
0.00.677.468 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.677.469 I 
0.00.677.642 I sampler seed: 1234
0.00.677.647 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.677.670 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.677.672 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.677.672 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.442.094 I llama_perf_sampler_print:    sampling time =       1.36 ms /    71 runs   (    0.02 ms per token, 52052.79 tokens per second)
0.01.442.095 I llama_perf_context_print:        load time =     660.68 ms
0.01.442.096 I llama_perf_context_print: prompt eval time =      47.55 ms /     7 tokens (    6.79 ms per token,   147.22 tokens per second)
0.01.442.097 I llama_perf_context_print:        eval time =     713.98 ms /    63 runs   (   11.33 ms per token,    88.24 tokens per second)
0.01.442.098 I llama_perf_context_print:       total time =     765.44 ms /    70 tokens
0.01.442.357 I ggml_metal_free: deallocating

real	0m1.470s
user	0m0.116s
sys	0m0.203s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.044 I build: 4793 (70680c48) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.080 I main: llama backend init
0.00.000.082 I main: load the model and apply lora adapter, if any
0.00.008.746 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.528 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.533 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.534 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.535 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.535 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.536 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.536 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.537 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.537 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.538 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.538 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.538 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.539 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.539 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.540 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.541 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.541 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.346 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.356 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.133 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.134 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.134 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.134 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.135 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.135 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.136 I llama_model_loader: - type  f32:  194 tensors
0.00.024.136 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.136 I llama_model_loader: - type q6_K:   37 tensors
0.00.024.137 I print_info: file format = GGUF V3 (latest)
0.00.024.137 I print_info: file type   = Q5_K - Medium
0.00.024.138 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.032.084 I load: special tokens cache size = 25
0.00.037.961 I load: token to piece cache size = 0.2984 MB
0.00.037.963 I print_info: arch             = gptneox
0.00.037.964 I print_info: vocab_only       = 0
0.00.037.964 I print_info: n_ctx_train      = 2048
0.00.037.964 I print_info: n_embd           = 2048
0.00.037.964 I print_info: n_layer          = 24
0.00.037.967 I print_info: n_head           = 16
0.00.037.968 I print_info: n_head_kv        = 16
0.00.037.968 I print_info: n_rot            = 32
0.00.037.968 I print_info: n_swa            = 0
0.00.037.969 I print_info: n_embd_head_k    = 128
0.00.037.970 I print_info: n_embd_head_v    = 128
0.00.037.972 I print_info: n_gqa            = 1
0.00.037.973 I print_info: n_embd_k_gqa     = 2048
0.00.037.974 I print_info: n_embd_v_gqa     = 2048
0.00.037.975 I print_info: f_norm_eps       = 1.0e-05
0.00.037.975 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.037.975 I print_info: f_clamp_kqv      = 0.0e+00
0.00.037.975 I print_info: f_max_alibi_bias = 0.0e+00
0.00.037.976 I print_info: f_logit_scale    = 0.0e+00
0.00.037.976 I print_info: n_ff             = 8192
0.00.037.976 I print_info: n_expert         = 0
0.00.037.977 I print_info: n_expert_used    = 0
0.00.037.977 I print_info: causal attn      = 1
0.00.037.977 I print_info: pooling type     = 0
0.00.037.977 I print_info: rope type        = 2
0.00.037.977 I print_info: rope scaling     = linear
0.00.037.978 I print_info: freq_base_train  = 10000.0
0.00.037.978 I print_info: freq_scale_train = 1
0.00.037.978 I print_info: n_ctx_orig_yarn  = 2048
0.00.037.978 I print_info: rope_finetuned   = unknown
0.00.037.978 I print_info: ssm_d_conv       = 0
0.00.037.979 I print_info: ssm_d_inner      = 0
0.00.037.979 I print_info: ssm_d_state      = 0
0.00.037.983 I print_info: ssm_dt_rank      = 0
0.00.037.983 I print_info: ssm_dt_b_c_rms   = 0
0.00.037.985 I print_info: model type       = 1.4B
0.00.037.985 I print_info: model params     = 1.41 B
0.00.037.985 I print_info: general.name     = 1.4B
0.00.037.986 I print_info: vocab type       = BPE
0.00.037.986 I print_info: n_vocab          = 50304
0.00.037.986 I print_info: n_merges         = 50009
0.00.037.986 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.037.986 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.037.987 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.037.987 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.037.987 I print_info: LF token         = 187 'Ċ'
0.00.037.987 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.037.987 I print_info: max token length = 1024
0.00.037.988 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.589.066 I load_tensors: offloading 24 repeating layers to GPU
0.00.589.070 I load_tensors: offloading output layer to GPU
0.00.589.071 I load_tensors: offloaded 25/25 layers to GPU
0.00.589.096 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.589.097 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.590.645 I llama_init_from_model: n_seq_max     = 1
0.00.590.647 I llama_init_from_model: n_ctx         = 2048
0.00.590.647 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.590.648 I llama_init_from_model: n_batch       = 2048
0.00.590.648 I llama_init_from_model: n_ubatch      = 512
0.00.590.649 I llama_init_from_model: flash_attn    = 0
0.00.590.650 I llama_init_from_model: freq_base     = 10000.0
0.00.590.650 I llama_init_from_model: freq_scale    = 1
0.00.590.651 I ggml_metal_init: allocating
0.00.590.682 I ggml_metal_init: found device: Apple M4
0.00.590.695 I ggml_metal_init: picking default device: Apple M4
0.00.592.209 I ggml_metal_init: using embedded metal library
0.00.598.277 I ggml_metal_init: GPU name:   Apple M4
0.00.598.281 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.598.281 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.598.282 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.598.283 I ggml_metal_init: simdgroup reduction   = true
0.00.598.283 I ggml_metal_init: simdgroup matrix mul. = true
0.00.598.283 I ggml_metal_init: has residency sets    = true
0.00.598.283 I ggml_metal_init: has bfloat            = true
0.00.598.284 I ggml_metal_init: use bfloat            = true
0.00.598.284 I ggml_metal_init: hasUnifiedMemory      = true
0.00.598.286 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.615.644 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.667.770 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.667.781 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.667.833 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.671.985 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.671.987 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.671.987 I llama_init_from_model: graph nodes  = 967
0.00.671.988 I llama_init_from_model: graph splits = 2
0.00.671.993 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.672.125 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.672.125 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.735.767 I main: llama threadpool init, n_threads = 4
0.00.735.832 I 
0.00.735.857 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.735.857 I 
0.00.736.028 I sampler seed: 1234
0.00.736.033 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.736.053 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.736.053 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.736.053 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.587.729 I llama_perf_sampler_print:    sampling time =       1.33 ms /    71 runs   (    0.02 ms per token, 53544.49 tokens per second)
0.01.587.731 I llama_perf_context_print:        load time =     726.27 ms
0.01.587.733 I llama_perf_context_print: prompt eval time =      52.64 ms /     7 tokens (    7.52 ms per token,   132.97 tokens per second)
0.01.587.733 I llama_perf_context_print:        eval time =     796.15 ms /    63 runs   (   12.64 ms per token,    79.13 tokens per second)
0.01.587.734 I llama_perf_context_print:       total time =     852.71 ms /    70 tokens
0.01.587.958 I ggml_metal_free: deallocating

real	0m1.604s
user	0m0.108s
sys	0m0.208s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.046 I build: 4793 (70680c48) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.083 I main: llama backend init
0.00.000.085 I main: load the model and apply lora adapter, if any
0.00.009.188 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.831 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.835 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.837 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.838 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.838 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.839 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.839 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.840 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.840 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.841 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.843 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.843 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.844 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.844 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.845 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.846 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.846 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.637 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.614 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.362 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.364 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.364 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.364 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.365 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.365 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.365 I llama_model_loader: - type  f32:  194 tensors
0.00.024.366 I llama_model_loader: - type q6_K:   98 tensors
0.00.024.366 I print_info: file format = GGUF V3 (latest)
0.00.024.367 I print_info: file type   = Q6_K
0.00.024.368 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.032.260 I load: special tokens cache size = 25
0.00.038.335 I load: token to piece cache size = 0.2984 MB
0.00.038.338 I print_info: arch             = gptneox
0.00.038.338 I print_info: vocab_only       = 0
0.00.038.339 I print_info: n_ctx_train      = 2048
0.00.038.339 I print_info: n_embd           = 2048
0.00.038.339 I print_info: n_layer          = 24
0.00.038.341 I print_info: n_head           = 16
0.00.038.342 I print_info: n_head_kv        = 16
0.00.038.342 I print_info: n_rot            = 32
0.00.038.342 I print_info: n_swa            = 0
0.00.038.343 I print_info: n_embd_head_k    = 128
0.00.038.344 I print_info: n_embd_head_v    = 128
0.00.038.345 I print_info: n_gqa            = 1
0.00.038.346 I print_info: n_embd_k_gqa     = 2048
0.00.038.347 I print_info: n_embd_v_gqa     = 2048
0.00.038.347 I print_info: f_norm_eps       = 1.0e-05
0.00.038.348 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.348 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.348 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.348 I print_info: f_logit_scale    = 0.0e+00
0.00.038.349 I print_info: n_ff             = 8192
0.00.038.349 I print_info: n_expert         = 0
0.00.038.349 I print_info: n_expert_used    = 0
0.00.038.349 I print_info: causal attn      = 1
0.00.038.350 I print_info: pooling type     = 0
0.00.038.350 I print_info: rope type        = 2
0.00.038.350 I print_info: rope scaling     = linear
0.00.038.350 I print_info: freq_base_train  = 10000.0
0.00.038.351 I print_info: freq_scale_train = 1
0.00.038.351 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.351 I print_info: rope_finetuned   = unknown
0.00.038.351 I print_info: ssm_d_conv       = 0
0.00.038.351 I print_info: ssm_d_inner      = 0
0.00.038.351 I print_info: ssm_d_state      = 0
0.00.038.352 I print_info: ssm_dt_rank      = 0
0.00.038.352 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.352 I print_info: model type       = 1.4B
0.00.038.352 I print_info: model params     = 1.41 B
0.00.038.353 I print_info: general.name     = 1.4B
0.00.038.353 I print_info: vocab type       = BPE
0.00.038.353 I print_info: n_vocab          = 50304
0.00.038.355 I print_info: n_merges         = 50009
0.00.038.356 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.356 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.356 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.356 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.356 I print_info: LF token         = 187 'Ċ'
0.00.038.357 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.357 I print_info: max token length = 1024
0.00.038.357 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.635.404 I load_tensors: offloading 24 repeating layers to GPU
0.00.635.407 I load_tensors: offloading output layer to GPU
0.00.635.408 I load_tensors: offloaded 25/25 layers to GPU
0.00.635.432 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.635.434 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.636.892 I llama_init_from_model: n_seq_max     = 1
0.00.636.894 I llama_init_from_model: n_ctx         = 2048
0.00.636.894 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.636.895 I llama_init_from_model: n_batch       = 2048
0.00.636.895 I llama_init_from_model: n_ubatch      = 512
0.00.636.895 I llama_init_from_model: flash_attn    = 0
0.00.636.896 I llama_init_from_model: freq_base     = 10000.0
0.00.636.897 I llama_init_from_model: freq_scale    = 1
0.00.636.898 I ggml_metal_init: allocating
0.00.636.947 I ggml_metal_init: found device: Apple M4
0.00.636.958 I ggml_metal_init: picking default device: Apple M4
0.00.638.438 I ggml_metal_init: using embedded metal library
0.00.644.534 I ggml_metal_init: GPU name:   Apple M4
0.00.644.538 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.644.539 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.644.540 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.644.540 I ggml_metal_init: simdgroup reduction   = true
0.00.644.540 I ggml_metal_init: simdgroup matrix mul. = true
0.00.644.541 I ggml_metal_init: has residency sets    = true
0.00.644.541 I ggml_metal_init: has bfloat            = true
0.00.644.541 I ggml_metal_init: use bfloat            = true
0.00.644.542 I ggml_metal_init: hasUnifiedMemory      = true
0.00.644.544 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.662.023 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.715.393 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.715.400 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.715.485 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.719.962 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.719.964 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.719.964 I llama_init_from_model: graph nodes  = 967
0.00.719.965 I llama_init_from_model: graph splits = 2
0.00.719.970 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.720.095 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.720.096 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.784.843 I main: llama threadpool init, n_threads = 4
0.00.784.898 I 
0.00.784.921 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.784.921 I 
0.00.785.102 I sampler seed: 1234
0.00.785.107 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.785.118 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.785.119 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.785.119 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.666.909 I llama_perf_sampler_print:    sampling time =       1.38 ms /    71 runs   (    0.02 ms per token, 51412.02 tokens per second)
0.01.666.910 I llama_perf_context_print:        load time =     774.92 ms
0.01.666.910 I llama_perf_context_print: prompt eval time =      57.61 ms /     7 tokens (    8.23 ms per token,   121.50 tokens per second)
0.01.666.911 I llama_perf_context_print:        eval time =     821.21 ms /    63 runs   (   13.04 ms per token,    76.72 tokens per second)
0.01.666.911 I llama_perf_context_print:       total time =     882.80 ms /    70 tokens
0.01.667.164 I ggml_metal_free: deallocating

real	0m1.685s
user	0m0.108s
sys	0m0.212s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.564 I build: 4793 (70680c48) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.024.280 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.038.846 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.038.855 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.038.865 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.038.866 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.038.867 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.038.867 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.038.868 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.038.870 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.038.870 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.038.871 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.038.872 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.038.872 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.038.873 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.038.874 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.038.877 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.038.878 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.038.878 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.046.763 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.048.630 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.055.008 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.055.009 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.055.010 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.055.010 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.055.011 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.055.011 I llama_model_loader: - type  f32:  194 tensors
0.00.055.012 I llama_model_loader: - type  f16:   98 tensors
0.00.055.013 I print_info: file format = GGUF V3 (latest)
0.00.055.014 I print_info: file type   = all F32 (guessed)
0.00.055.015 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.066.849 I load: special tokens cache size = 25
0.00.074.459 I load: token to piece cache size = 0.2984 MB
0.00.074.462 I print_info: arch             = gptneox
0.00.074.462 I print_info: vocab_only       = 0
0.00.074.463 I print_info: n_ctx_train      = 2048
0.00.074.463 I print_info: n_embd           = 2048
0.00.074.463 I print_info: n_layer          = 24
0.00.074.466 I print_info: n_head           = 16
0.00.074.467 I print_info: n_head_kv        = 16
0.00.074.467 I print_info: n_rot            = 32
0.00.074.467 I print_info: n_swa            = 0
0.00.074.468 I print_info: n_embd_head_k    = 128
0.00.074.468 I print_info: n_embd_head_v    = 128
0.00.074.468 I print_info: n_gqa            = 1
0.00.074.469 I print_info: n_embd_k_gqa     = 2048
0.00.074.470 I print_info: n_embd_v_gqa     = 2048
0.00.074.470 I print_info: f_norm_eps       = 1.0e-05
0.00.074.472 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.074.472 I print_info: f_clamp_kqv      = 0.0e+00
0.00.074.472 I print_info: f_max_alibi_bias = 0.0e+00
0.00.074.472 I print_info: f_logit_scale    = 0.0e+00
0.00.074.473 I print_info: n_ff             = 8192
0.00.074.473 I print_info: n_expert         = 0
0.00.074.474 I print_info: n_expert_used    = 0
0.00.074.474 I print_info: causal attn      = 1
0.00.074.474 I print_info: pooling type     = 0
0.00.074.474 I print_info: rope type        = 2
0.00.074.474 I print_info: rope scaling     = linear
0.00.074.477 I print_info: freq_base_train  = 10000.0
0.00.074.477 I print_info: freq_scale_train = 1
0.00.074.477 I print_info: n_ctx_orig_yarn  = 2048
0.00.074.477 I print_info: rope_finetuned   = unknown
0.00.074.477 I print_info: ssm_d_conv       = 0
0.00.074.478 I print_info: ssm_d_inner      = 0
0.00.074.478 I print_info: ssm_d_state      = 0
0.00.074.478 I print_info: ssm_dt_rank      = 0
0.00.074.478 I print_info: ssm_dt_b_c_rms   = 0
0.00.074.478 I print_info: model type       = 1.4B
0.00.074.478 I print_info: model params     = 1.41 B
0.00.074.479 I print_info: general.name     = 1.4B
0.00.074.479 I print_info: vocab type       = BPE
0.00.074.479 I print_info: n_vocab          = 50304
0.00.074.480 I print_info: n_merges         = 50009
0.00.074.480 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.074.480 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.074.480 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.074.480 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.074.481 I print_info: LF token         = 187 'Ċ'
0.00.074.481 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.074.481 I print_info: max token length = 1024
0.00.074.481 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.378.202 I load_tensors: offloading 24 repeating layers to GPU
0.01.378.209 I load_tensors: offloading output layer to GPU
0.01.378.210 I load_tensors: offloaded 25/25 layers to GPU
0.01.378.236 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.378.238 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.01.379.225 I llama_init_from_model: n_seq_max     = 1
0.01.379.226 I llama_init_from_model: n_ctx         = 128
0.01.379.226 I llama_init_from_model: n_ctx_per_seq = 128
0.01.379.226 I llama_init_from_model: n_batch       = 128
0.01.379.227 I llama_init_from_model: n_ubatch      = 128
0.01.379.227 I llama_init_from_model: flash_attn    = 0
0.01.379.227 I llama_init_from_model: freq_base     = 10000.0
0.01.379.228 I llama_init_from_model: freq_scale    = 1
0.01.379.228 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.379.229 I ggml_metal_init: allocating
0.01.379.261 I ggml_metal_init: found device: Apple M4
0.01.379.268 I ggml_metal_init: picking default device: Apple M4
0.01.380.328 I ggml_metal_init: using embedded metal library
0.01.384.132 I ggml_metal_init: GPU name:   Apple M4
0.01.384.135 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.384.135 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.384.136 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.384.136 I ggml_metal_init: simdgroup reduction   = true
0.01.384.136 I ggml_metal_init: simdgroup matrix mul. = true
0.01.384.136 I ggml_metal_init: has residency sets    = true
0.01.384.137 I ggml_metal_init: has bfloat            = true
0.01.384.137 I ggml_metal_init: use bfloat            = true
0.01.384.137 I ggml_metal_init: hasUnifiedMemory      = true
0.01.384.138 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.394.850 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.396.546 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.396.551 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.396.575 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.398.269 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.398.270 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.398.270 I llama_init_from_model: graph nodes  = 967
0.01.398.271 I llama_init_from_model: graph splits = 2
0.01.398.272 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.398.272 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.433.067 I 
0.01.433.106 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.433.112 I perplexity: tokenizing the input ..
0.01.438.293 I perplexity: tokenization took 5.18 ms
0.01.438.298 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.556.502 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.557.847 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.557.876 I llama_perf_context_print:        load time =    1408.78 ms
0.01.557.877 I llama_perf_context_print: prompt eval time =     117.94 ms /   128 tokens (    0.92 ms per token,  1085.31 tokens per second)
0.01.557.878 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.557.878 I llama_perf_context_print:       total time =     124.81 ms /   129 tokens
0.01.558.233 I ggml_metal_free: deallocating

real	0m1.745s
user	0m0.096s
sys	0m0.250s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.106 I build: 4793 (70680c48) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.185 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.531 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.016.538 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.545 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.546 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.546 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.547 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.547 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.548 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.548 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.550 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.550 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.550 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.551 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.552 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.554 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.555 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.555 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.446 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.504 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.394 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.395 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.396 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.396 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.396 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.397 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.025.397 I llama_model_loader: - type  f32:  194 tensors
0.00.025.398 I llama_model_loader: - type q8_0:   98 tensors
0.00.025.398 I print_info: file format = GGUF V3 (latest)
0.00.025.399 I print_info: file type   = Q8_0
0.00.025.400 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.033.854 I load: special tokens cache size = 25
0.00.039.996 I load: token to piece cache size = 0.2984 MB
0.00.040.000 I print_info: arch             = gptneox
0.00.040.001 I print_info: vocab_only       = 0
0.00.040.001 I print_info: n_ctx_train      = 2048
0.00.040.001 I print_info: n_embd           = 2048
0.00.040.001 I print_info: n_layer          = 24
0.00.040.006 I print_info: n_head           = 16
0.00.040.007 I print_info: n_head_kv        = 16
0.00.040.007 I print_info: n_rot            = 32
0.00.040.007 I print_info: n_swa            = 0
0.00.040.007 I print_info: n_embd_head_k    = 128
0.00.040.007 I print_info: n_embd_head_v    = 128
0.00.040.008 I print_info: n_gqa            = 1
0.00.040.009 I print_info: n_embd_k_gqa     = 2048
0.00.040.012 I print_info: n_embd_v_gqa     = 2048
0.00.040.013 I print_info: f_norm_eps       = 1.0e-05
0.00.040.013 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.013 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.014 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.014 I print_info: f_logit_scale    = 0.0e+00
0.00.040.014 I print_info: n_ff             = 8192
0.00.040.014 I print_info: n_expert         = 0
0.00.040.015 I print_info: n_expert_used    = 0
0.00.040.015 I print_info: causal attn      = 1
0.00.040.015 I print_info: pooling type     = 0
0.00.040.015 I print_info: rope type        = 2
0.00.040.015 I print_info: rope scaling     = linear
0.00.040.016 I print_info: freq_base_train  = 10000.0
0.00.040.016 I print_info: freq_scale_train = 1
0.00.040.016 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.016 I print_info: rope_finetuned   = unknown
0.00.040.016 I print_info: ssm_d_conv       = 0
0.00.040.016 I print_info: ssm_d_inner      = 0
0.00.040.017 I print_info: ssm_d_state      = 0
0.00.040.017 I print_info: ssm_dt_rank      = 0
0.00.040.017 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.017 I print_info: model type       = 1.4B
0.00.040.017 I print_info: model params     = 1.41 B
0.00.040.018 I print_info: general.name     = 1.4B
0.00.040.018 I print_info: vocab type       = BPE
0.00.040.020 I print_info: n_vocab          = 50304
0.00.040.020 I print_info: n_merges         = 50009
0.00.040.020 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.020 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.020 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.020 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.021 I print_info: LF token         = 187 'Ċ'
0.00.040.021 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.021 I print_info: max token length = 1024
0.00.040.022 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.808.291 I load_tensors: offloading 24 repeating layers to GPU
0.00.808.295 I load_tensors: offloading output layer to GPU
0.00.808.296 I load_tensors: offloaded 25/25 layers to GPU
0.00.808.323 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.808.324 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.809.463 I llama_init_from_model: n_seq_max     = 1
0.00.809.465 I llama_init_from_model: n_ctx         = 128
0.00.809.465 I llama_init_from_model: n_ctx_per_seq = 128
0.00.809.466 I llama_init_from_model: n_batch       = 128
0.00.809.466 I llama_init_from_model: n_ubatch      = 128
0.00.809.466 I llama_init_from_model: flash_attn    = 0
0.00.809.467 I llama_init_from_model: freq_base     = 10000.0
0.00.809.468 I llama_init_from_model: freq_scale    = 1
0.00.809.468 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.809.470 I ggml_metal_init: allocating
0.00.809.526 I ggml_metal_init: found device: Apple M4
0.00.809.536 I ggml_metal_init: picking default device: Apple M4
0.00.810.581 I ggml_metal_init: using embedded metal library
0.00.814.746 I ggml_metal_init: GPU name:   Apple M4
0.00.814.749 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.814.749 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.814.755 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.814.755 I ggml_metal_init: simdgroup reduction   = true
0.00.814.755 I ggml_metal_init: simdgroup matrix mul. = true
0.00.814.756 I ggml_metal_init: has residency sets    = true
0.00.814.756 I ggml_metal_init: has bfloat            = true
0.00.814.756 I ggml_metal_init: use bfloat            = true
0.00.814.757 I ggml_metal_init: hasUnifiedMemory      = true
0.00.814.758 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.828.096 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.830.038 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.830.041 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.830.068 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.832.057 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.832.058 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.832.059 I llama_init_from_model: graph nodes  = 967
0.00.832.059 I llama_init_from_model: graph splits = 2
0.00.832.060 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.832.060 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.857.351 I 
0.00.857.384 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.857.386 I perplexity: tokenizing the input ..
0.00.861.330 I perplexity: tokenization took 3.942 ms
0.00.861.333 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.999.046 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.000.519 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.000.544 I llama_perf_context_print:        load time =     848.16 ms
0.01.000.544 I llama_perf_context_print: prompt eval time =     137.47 ms /   128 tokens (    1.07 ms per token,   931.11 tokens per second)
0.01.000.545 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.000.546 I llama_perf_context_print:       total time =     143.19 ms /   129 tokens
0.01.000.945 I ggml_metal_free: deallocating

real	0m1.017s
user	0m0.070s
sys	0m0.162s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.104 I build: 4793 (70680c48) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.264 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.642 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.017.649 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.657 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.658 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.658 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.658 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.658 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.659 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.659 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.660 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.660 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.660 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.661 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.661 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.663 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.664 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.665 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.390 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.407 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.142 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.144 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.144 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.144 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.145 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.145 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.026.145 I llama_model_loader: - type  f32:  194 tensors
0.00.026.146 I llama_model_loader: - type q4_0:   97 tensors
0.00.026.146 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.147 I print_info: file format = GGUF V3 (latest)
0.00.026.147 I print_info: file type   = Q4_0
0.00.026.148 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.034.369 I load: special tokens cache size = 25
0.00.040.532 I load: token to piece cache size = 0.2984 MB
0.00.040.536 I print_info: arch             = gptneox
0.00.040.536 I print_info: vocab_only       = 0
0.00.040.537 I print_info: n_ctx_train      = 2048
0.00.040.537 I print_info: n_embd           = 2048
0.00.040.537 I print_info: n_layer          = 24
0.00.040.541 I print_info: n_head           = 16
0.00.040.542 I print_info: n_head_kv        = 16
0.00.040.542 I print_info: n_rot            = 32
0.00.040.542 I print_info: n_swa            = 0
0.00.040.543 I print_info: n_embd_head_k    = 128
0.00.040.544 I print_info: n_embd_head_v    = 128
0.00.040.545 I print_info: n_gqa            = 1
0.00.040.545 I print_info: n_embd_k_gqa     = 2048
0.00.040.546 I print_info: n_embd_v_gqa     = 2048
0.00.040.546 I print_info: f_norm_eps       = 1.0e-05
0.00.040.547 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.548 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.548 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.549 I print_info: f_logit_scale    = 0.0e+00
0.00.040.550 I print_info: n_ff             = 8192
0.00.040.550 I print_info: n_expert         = 0
0.00.040.550 I print_info: n_expert_used    = 0
0.00.040.550 I print_info: causal attn      = 1
0.00.040.551 I print_info: pooling type     = 0
0.00.040.551 I print_info: rope type        = 2
0.00.040.551 I print_info: rope scaling     = linear
0.00.040.551 I print_info: freq_base_train  = 10000.0
0.00.040.552 I print_info: freq_scale_train = 1
0.00.040.552 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.552 I print_info: rope_finetuned   = unknown
0.00.040.552 I print_info: ssm_d_conv       = 0
0.00.040.552 I print_info: ssm_d_inner      = 0
0.00.040.552 I print_info: ssm_d_state      = 0
0.00.040.553 I print_info: ssm_dt_rank      = 0
0.00.040.553 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.553 I print_info: model type       = 1.4B
0.00.040.553 I print_info: model params     = 1.41 B
0.00.040.554 I print_info: general.name     = 1.4B
0.00.040.554 I print_info: vocab type       = BPE
0.00.040.555 I print_info: n_vocab          = 50304
0.00.040.555 I print_info: n_merges         = 50009
0.00.040.556 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.556 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.556 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.556 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.556 I print_info: LF token         = 187 'Ċ'
0.00.040.557 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.557 I print_info: max token length = 1024
0.00.040.557 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.536.680 I load_tensors: offloading 24 repeating layers to GPU
0.00.536.693 I load_tensors: offloading output layer to GPU
0.00.536.694 I load_tensors: offloaded 25/25 layers to GPU
0.00.536.734 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.536.736 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.538.358 I llama_init_from_model: n_seq_max     = 1
0.00.538.360 I llama_init_from_model: n_ctx         = 128
0.00.538.361 I llama_init_from_model: n_ctx_per_seq = 128
0.00.538.361 I llama_init_from_model: n_batch       = 128
0.00.538.361 I llama_init_from_model: n_ubatch      = 128
0.00.538.362 I llama_init_from_model: flash_attn    = 0
0.00.538.365 I llama_init_from_model: freq_base     = 10000.0
0.00.538.365 I llama_init_from_model: freq_scale    = 1
0.00.538.366 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.538.368 I ggml_metal_init: allocating
0.00.538.447 I ggml_metal_init: found device: Apple M4
0.00.538.461 I ggml_metal_init: picking default device: Apple M4
0.00.540.261 I ggml_metal_init: using embedded metal library
0.00.546.649 I ggml_metal_init: GPU name:   Apple M4
0.00.546.659 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.546.660 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.546.661 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.546.662 I ggml_metal_init: simdgroup reduction   = true
0.00.546.662 I ggml_metal_init: simdgroup matrix mul. = true
0.00.546.662 I ggml_metal_init: has residency sets    = true
0.00.546.663 I ggml_metal_init: has bfloat            = true
0.00.546.663 I ggml_metal_init: use bfloat            = true
0.00.546.664 I ggml_metal_init: hasUnifiedMemory      = true
0.00.546.669 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.565.951 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.569.532 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.569.538 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.569.587 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.572.829 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.572.830 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.572.831 I llama_init_from_model: graph nodes  = 967
0.00.572.831 I llama_init_from_model: graph splits = 2
0.00.572.834 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.572.834 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.596.841 I 
0.00.596.911 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.596.918 I perplexity: tokenizing the input ..
0.00.604.391 I perplexity: tokenization took 7.469 ms
0.00.604.399 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.727.765 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.729.074 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.729.100 I llama_perf_context_print:        load time =     586.57 ms
0.00.729.101 I llama_perf_context_print: prompt eval time =     122.50 ms /   128 tokens (    0.96 ms per token,  1044.91 tokens per second)
0.00.729.102 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.729.102 I llama_perf_context_print:       total time =     132.26 ms /   129 tokens
0.00.729.475 I ggml_metal_free: deallocating

real	0m0.745s
user	0m0.081s
sys	0m0.124s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.104 I build: 4793 (70680c48) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.079 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.507 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.513 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.515 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.516 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.516 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.517 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.517 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.518 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.518 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.519 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.519 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.519 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.522 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.523 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.525 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.525 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.525 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.463 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.516 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.391 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.393 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.393 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.394 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.394 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.394 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.025.395 I llama_model_loader: - type  f32:  194 tensors
0.00.025.395 I llama_model_loader: - type q4_1:   97 tensors
0.00.025.396 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.396 I print_info: file format = GGUF V3 (latest)
0.00.025.397 I print_info: file type   = Q4_1
0.00.025.398 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.033.888 I load: special tokens cache size = 25
0.00.040.060 I load: token to piece cache size = 0.2984 MB
0.00.040.064 I print_info: arch             = gptneox
0.00.040.065 I print_info: vocab_only       = 0
0.00.040.065 I print_info: n_ctx_train      = 2048
0.00.040.065 I print_info: n_embd           = 2048
0.00.040.065 I print_info: n_layer          = 24
0.00.040.070 I print_info: n_head           = 16
0.00.040.070 I print_info: n_head_kv        = 16
0.00.040.070 I print_info: n_rot            = 32
0.00.040.070 I print_info: n_swa            = 0
0.00.040.071 I print_info: n_embd_head_k    = 128
0.00.040.073 I print_info: n_embd_head_v    = 128
0.00.040.073 I print_info: n_gqa            = 1
0.00.040.074 I print_info: n_embd_k_gqa     = 2048
0.00.040.075 I print_info: n_embd_v_gqa     = 2048
0.00.040.077 I print_info: f_norm_eps       = 1.0e-05
0.00.040.077 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.077 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.077 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.077 I print_info: f_logit_scale    = 0.0e+00
0.00.040.078 I print_info: n_ff             = 8192
0.00.040.078 I print_info: n_expert         = 0
0.00.040.078 I print_info: n_expert_used    = 0
0.00.040.078 I print_info: causal attn      = 1
0.00.040.079 I print_info: pooling type     = 0
0.00.040.079 I print_info: rope type        = 2
0.00.040.079 I print_info: rope scaling     = linear
0.00.040.080 I print_info: freq_base_train  = 10000.0
0.00.040.081 I print_info: freq_scale_train = 1
0.00.040.081 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.081 I print_info: rope_finetuned   = unknown
0.00.040.081 I print_info: ssm_d_conv       = 0
0.00.040.081 I print_info: ssm_d_inner      = 0
0.00.040.081 I print_info: ssm_d_state      = 0
0.00.040.082 I print_info: ssm_dt_rank      = 0
0.00.040.082 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.082 I print_info: model type       = 1.4B
0.00.040.082 I print_info: model params     = 1.41 B
0.00.040.082 I print_info: general.name     = 1.4B
0.00.040.083 I print_info: vocab type       = BPE
0.00.040.083 I print_info: n_vocab          = 50304
0.00.040.083 I print_info: n_merges         = 50009
0.00.040.083 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.085 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.085 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.085 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.085 I print_info: LF token         = 187 'Ċ'
0.00.040.086 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.086 I print_info: max token length = 1024
0.00.040.086 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.606.846 I load_tensors: offloading 24 repeating layers to GPU
0.00.606.859 I load_tensors: offloading output layer to GPU
0.00.606.860 I load_tensors: offloaded 25/25 layers to GPU
0.00.606.897 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.606.898 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.608.613 I llama_init_from_model: n_seq_max     = 1
0.00.608.616 I llama_init_from_model: n_ctx         = 128
0.00.608.617 I llama_init_from_model: n_ctx_per_seq = 128
0.00.608.617 I llama_init_from_model: n_batch       = 128
0.00.608.618 I llama_init_from_model: n_ubatch      = 128
0.00.608.618 I llama_init_from_model: flash_attn    = 0
0.00.608.620 I llama_init_from_model: freq_base     = 10000.0
0.00.608.621 I llama_init_from_model: freq_scale    = 1
0.00.608.622 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.608.624 I ggml_metal_init: allocating
0.00.608.705 I ggml_metal_init: found device: Apple M4
0.00.608.719 I ggml_metal_init: picking default device: Apple M4
0.00.610.503 I ggml_metal_init: using embedded metal library
0.00.616.673 I ggml_metal_init: GPU name:   Apple M4
0.00.616.681 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.616.682 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.616.683 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.616.683 I ggml_metal_init: simdgroup reduction   = true
0.00.616.684 I ggml_metal_init: simdgroup matrix mul. = true
0.00.616.684 I ggml_metal_init: has residency sets    = true
0.00.616.684 I ggml_metal_init: has bfloat            = true
0.00.616.685 I ggml_metal_init: use bfloat            = true
0.00.616.686 I ggml_metal_init: hasUnifiedMemory      = true
0.00.616.700 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.635.862 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.639.316 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.639.320 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.639.373 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.642.552 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.642.554 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.642.555 I llama_init_from_model: graph nodes  = 967
0.00.642.555 I llama_init_from_model: graph splits = 2
0.00.642.558 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.642.558 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.667.057 I 
0.00.667.127 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.667.133 I perplexity: tokenizing the input ..
0.00.673.948 I perplexity: tokenization took 6.814 ms
0.00.673.954 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.806.741 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.808.076 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.808.104 I llama_perf_context_print:        load time =     657.97 ms
0.00.808.106 I llama_perf_context_print: prompt eval time =     132.48 ms /   128 tokens (    1.03 ms per token,   966.18 tokens per second)
0.00.808.107 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.808.107 I llama_perf_context_print:       total time =     141.05 ms /   129 tokens
0.00.808.533 I ggml_metal_free: deallocating

real	0m0.823s
user	0m0.080s
sys	0m0.116s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.105 I build: 4793 (70680c48) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.807 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.362 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.016.369 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.376 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.376 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.377 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.377 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.377 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.378 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.379 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.379 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.379 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.380 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.380 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.380 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.382 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.382 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.382 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.242 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.334 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.182 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.184 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.184 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.184 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.185 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.185 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.025.185 I llama_model_loader: - type  f32:  194 tensors
0.00.025.186 I llama_model_loader: - type q5_0:   97 tensors
0.00.025.186 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.187 I print_info: file format = GGUF V3 (latest)
0.00.025.187 I print_info: file type   = Q5_0
0.00.025.189 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.033.848 I load: special tokens cache size = 25
0.00.039.991 I load: token to piece cache size = 0.2984 MB
0.00.039.995 I print_info: arch             = gptneox
0.00.039.995 I print_info: vocab_only       = 0
0.00.039.996 I print_info: n_ctx_train      = 2048
0.00.039.996 I print_info: n_embd           = 2048
0.00.039.996 I print_info: n_layer          = 24
0.00.040.000 I print_info: n_head           = 16
0.00.040.001 I print_info: n_head_kv        = 16
0.00.040.001 I print_info: n_rot            = 32
0.00.040.001 I print_info: n_swa            = 0
0.00.040.001 I print_info: n_embd_head_k    = 128
0.00.040.002 I print_info: n_embd_head_v    = 128
0.00.040.002 I print_info: n_gqa            = 1
0.00.040.003 I print_info: n_embd_k_gqa     = 2048
0.00.040.005 I print_info: n_embd_v_gqa     = 2048
0.00.040.006 I print_info: f_norm_eps       = 1.0e-05
0.00.040.006 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.006 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.007 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.008 I print_info: f_logit_scale    = 0.0e+00
0.00.040.008 I print_info: n_ff             = 8192
0.00.040.008 I print_info: n_expert         = 0
0.00.040.008 I print_info: n_expert_used    = 0
0.00.040.009 I print_info: causal attn      = 1
0.00.040.010 I print_info: pooling type     = 0
0.00.040.010 I print_info: rope type        = 2
0.00.040.011 I print_info: rope scaling     = linear
0.00.040.011 I print_info: freq_base_train  = 10000.0
0.00.040.011 I print_info: freq_scale_train = 1
0.00.040.011 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.012 I print_info: rope_finetuned   = unknown
0.00.040.012 I print_info: ssm_d_conv       = 0
0.00.040.012 I print_info: ssm_d_inner      = 0
0.00.040.012 I print_info: ssm_d_state      = 0
0.00.040.012 I print_info: ssm_dt_rank      = 0
0.00.040.012 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.012 I print_info: model type       = 1.4B
0.00.040.014 I print_info: model params     = 1.41 B
0.00.040.014 I print_info: general.name     = 1.4B
0.00.040.014 I print_info: vocab type       = BPE
0.00.040.014 I print_info: n_vocab          = 50304
0.00.040.015 I print_info: n_merges         = 50009
0.00.040.015 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.015 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.015 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.015 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.015 I print_info: LF token         = 187 'Ċ'
0.00.040.016 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.016 I print_info: max token length = 1024
0.00.040.016 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.653.624 I load_tensors: offloading 24 repeating layers to GPU
0.00.653.641 I load_tensors: offloading output layer to GPU
0.00.653.642 I load_tensors: offloaded 25/25 layers to GPU
0.00.653.677 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.653.679 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.655.316 I llama_init_from_model: n_seq_max     = 1
0.00.655.320 I llama_init_from_model: n_ctx         = 128
0.00.655.321 I llama_init_from_model: n_ctx_per_seq = 128
0.00.655.321 I llama_init_from_model: n_batch       = 128
0.00.655.321 I llama_init_from_model: n_ubatch      = 128
0.00.655.322 I llama_init_from_model: flash_attn    = 0
0.00.655.324 I llama_init_from_model: freq_base     = 10000.0
0.00.655.324 I llama_init_from_model: freq_scale    = 1
0.00.655.325 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.655.328 I ggml_metal_init: allocating
0.00.655.386 I ggml_metal_init: found device: Apple M4
0.00.655.400 I ggml_metal_init: picking default device: Apple M4
0.00.656.867 I ggml_metal_init: using embedded metal library
0.00.663.201 I ggml_metal_init: GPU name:   Apple M4
0.00.663.205 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.663.206 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.663.206 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.663.207 I ggml_metal_init: simdgroup reduction   = true
0.00.663.207 I ggml_metal_init: simdgroup matrix mul. = true
0.00.663.207 I ggml_metal_init: has residency sets    = true
0.00.663.208 I ggml_metal_init: has bfloat            = true
0.00.663.208 I ggml_metal_init: use bfloat            = true
0.00.663.209 I ggml_metal_init: hasUnifiedMemory      = true
0.00.663.212 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.680.197 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.683.655 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.683.661 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.683.709 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.686.957 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.686.958 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.686.959 I llama_init_from_model: graph nodes  = 967
0.00.686.959 I llama_init_from_model: graph splits = 2
0.00.686.962 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.686.962 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.715.022 I 
0.00.715.098 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.715.107 I perplexity: tokenizing the input ..
0.00.722.200 I perplexity: tokenization took 7.09 ms
0.00.722.206 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.857.928 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.859.280 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.859.318 I llama_perf_context_print:        load time =     706.21 ms
0.00.859.319 I llama_perf_context_print: prompt eval time =     134.78 ms /   128 tokens (    1.05 ms per token,   949.66 tokens per second)
0.00.859.320 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.859.320 I llama_perf_context_print:       total time =     144.30 ms /   129 tokens
0.00.859.736 I ggml_metal_free: deallocating

real	0m0.874s
user	0m0.080s
sys	0m0.145s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.099 I build: 4793 (70680c48) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.989 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.254 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.017.260 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.262 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.262 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.263 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.263 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.263 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.264 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.265 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.265 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.266 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.266 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.269 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.269 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.271 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.271 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.272 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.105 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.137 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.898 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.900 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.900 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.900 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.901 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.901 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.902 I llama_model_loader: - type  f32:  194 tensors
0.00.025.902 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.903 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.903 I print_info: file format = GGUF V3 (latest)
0.00.025.904 I print_info: file type   = Q5_1
0.00.025.905 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.034.186 I load: special tokens cache size = 25
0.00.040.316 I load: token to piece cache size = 0.2984 MB
0.00.040.320 I print_info: arch             = gptneox
0.00.040.320 I print_info: vocab_only       = 0
0.00.040.321 I print_info: n_ctx_train      = 2048
0.00.040.321 I print_info: n_embd           = 2048
0.00.040.321 I print_info: n_layer          = 24
0.00.040.326 I print_info: n_head           = 16
0.00.040.326 I print_info: n_head_kv        = 16
0.00.040.326 I print_info: n_rot            = 32
0.00.040.327 I print_info: n_swa            = 0
0.00.040.327 I print_info: n_embd_head_k    = 128
0.00.040.327 I print_info: n_embd_head_v    = 128
0.00.040.328 I print_info: n_gqa            = 1
0.00.040.328 I print_info: n_embd_k_gqa     = 2048
0.00.040.329 I print_info: n_embd_v_gqa     = 2048
0.00.040.330 I print_info: f_norm_eps       = 1.0e-05
0.00.040.330 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.330 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.330 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.331 I print_info: f_logit_scale    = 0.0e+00
0.00.040.331 I print_info: n_ff             = 8192
0.00.040.331 I print_info: n_expert         = 0
0.00.040.331 I print_info: n_expert_used    = 0
0.00.040.331 I print_info: causal attn      = 1
0.00.040.332 I print_info: pooling type     = 0
0.00.040.332 I print_info: rope type        = 2
0.00.040.332 I print_info: rope scaling     = linear
0.00.040.332 I print_info: freq_base_train  = 10000.0
0.00.040.332 I print_info: freq_scale_train = 1
0.00.040.333 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.333 I print_info: rope_finetuned   = unknown
0.00.040.333 I print_info: ssm_d_conv       = 0
0.00.040.333 I print_info: ssm_d_inner      = 0
0.00.040.333 I print_info: ssm_d_state      = 0
0.00.040.333 I print_info: ssm_dt_rank      = 0
0.00.040.333 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.334 I print_info: model type       = 1.4B
0.00.040.334 I print_info: model params     = 1.41 B
0.00.040.334 I print_info: general.name     = 1.4B
0.00.040.335 I print_info: vocab type       = BPE
0.00.040.335 I print_info: n_vocab          = 50304
0.00.040.335 I print_info: n_merges         = 50009
0.00.040.335 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.335 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.335 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.336 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.338 I print_info: LF token         = 187 'Ċ'
0.00.040.338 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.338 I print_info: max token length = 1024
0.00.040.340 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.664.274 I load_tensors: offloading 24 repeating layers to GPU
0.00.664.282 I load_tensors: offloading output layer to GPU
0.00.664.282 I load_tensors: offloaded 25/25 layers to GPU
0.00.664.315 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.664.318 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.665.880 I llama_init_from_model: n_seq_max     = 1
0.00.665.882 I llama_init_from_model: n_ctx         = 128
0.00.665.883 I llama_init_from_model: n_ctx_per_seq = 128
0.00.665.883 I llama_init_from_model: n_batch       = 128
0.00.665.884 I llama_init_from_model: n_ubatch      = 128
0.00.665.884 I llama_init_from_model: flash_attn    = 0
0.00.665.886 I llama_init_from_model: freq_base     = 10000.0
0.00.665.886 I llama_init_from_model: freq_scale    = 1
0.00.665.887 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.665.889 I ggml_metal_init: allocating
0.00.665.960 I ggml_metal_init: found device: Apple M4
0.00.665.975 I ggml_metal_init: picking default device: Apple M4
0.00.667.574 I ggml_metal_init: using embedded metal library
0.00.673.643 I ggml_metal_init: GPU name:   Apple M4
0.00.673.647 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.673.647 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.673.648 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.673.648 I ggml_metal_init: simdgroup reduction   = true
0.00.673.649 I ggml_metal_init: simdgroup matrix mul. = true
0.00.673.649 I ggml_metal_init: has residency sets    = true
0.00.673.649 I ggml_metal_init: has bfloat            = true
0.00.673.649 I ggml_metal_init: use bfloat            = true
0.00.673.651 I ggml_metal_init: hasUnifiedMemory      = true
0.00.673.653 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.690.355 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.693.813 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.693.818 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.693.884 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.696.987 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.696.989 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.696.989 I llama_init_from_model: graph nodes  = 967
0.00.696.990 I llama_init_from_model: graph splits = 2
0.00.696.992 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.696.992 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.729.310 I 
0.00.729.395 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.729.401 I perplexity: tokenizing the input ..
0.00.736.685 I perplexity: tokenization took 7.28 ms
0.00.736.692 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.883.617 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.884.972 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.884.996 I llama_perf_context_print:        load time =     719.31 ms
0.00.884.997 I llama_perf_context_print: prompt eval time =     145.99 ms /   128 tokens (    1.14 ms per token,   876.78 tokens per second)
0.00.884.998 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.884.998 I llama_perf_context_print:       total time =     155.69 ms /   129 tokens
0.00.885.396 I ggml_metal_free: deallocating

real	0m0.901s
user	0m0.079s
sys	0m0.139s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.104 I build: 4793 (70680c48) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.952 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.051 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.057 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.059 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.059 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.060 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.060 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.060 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.061 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.064 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.064 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.065 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.065 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.065 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.066 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.067 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.068 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.068 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.004 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.066 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.973 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.974 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.975 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.975 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.976 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.976 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.977 I llama_model_loader: - type  f32:  194 tensors
0.00.024.977 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.977 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.977 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.978 I print_info: file format = GGUF V3 (latest)
0.00.024.979 I print_info: file type   = Q2_K - Medium
0.00.024.980 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.033.422 I load: special tokens cache size = 25
0.00.039.456 I load: token to piece cache size = 0.2984 MB
0.00.039.460 I print_info: arch             = gptneox
0.00.039.460 I print_info: vocab_only       = 0
0.00.039.461 I print_info: n_ctx_train      = 2048
0.00.039.461 I print_info: n_embd           = 2048
0.00.039.461 I print_info: n_layer          = 24
0.00.039.466 I print_info: n_head           = 16
0.00.039.466 I print_info: n_head_kv        = 16
0.00.039.467 I print_info: n_rot            = 32
0.00.039.467 I print_info: n_swa            = 0
0.00.039.467 I print_info: n_embd_head_k    = 128
0.00.039.467 I print_info: n_embd_head_v    = 128
0.00.039.468 I print_info: n_gqa            = 1
0.00.039.469 I print_info: n_embd_k_gqa     = 2048
0.00.039.469 I print_info: n_embd_v_gqa     = 2048
0.00.039.470 I print_info: f_norm_eps       = 1.0e-05
0.00.039.470 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.471 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.471 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.471 I print_info: f_logit_scale    = 0.0e+00
0.00.039.471 I print_info: n_ff             = 8192
0.00.039.472 I print_info: n_expert         = 0
0.00.039.472 I print_info: n_expert_used    = 0
0.00.039.472 I print_info: causal attn      = 1
0.00.039.472 I print_info: pooling type     = 0
0.00.039.472 I print_info: rope type        = 2
0.00.039.472 I print_info: rope scaling     = linear
0.00.039.473 I print_info: freq_base_train  = 10000.0
0.00.039.473 I print_info: freq_scale_train = 1
0.00.039.473 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.473 I print_info: rope_finetuned   = unknown
0.00.039.473 I print_info: ssm_d_conv       = 0
0.00.039.474 I print_info: ssm_d_inner      = 0
0.00.039.474 I print_info: ssm_d_state      = 0
0.00.039.474 I print_info: ssm_dt_rank      = 0
0.00.039.474 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.474 I print_info: model type       = 1.4B
0.00.039.477 I print_info: model params     = 1.41 B
0.00.039.477 I print_info: general.name     = 1.4B
0.00.039.477 I print_info: vocab type       = BPE
0.00.039.477 I print_info: n_vocab          = 50304
0.00.039.477 I print_info: n_merges         = 50009
0.00.039.478 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.478 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.478 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.478 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.480 I print_info: LF token         = 187 'Ċ'
0.00.039.480 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.480 I print_info: max token length = 1024
0.00.039.480 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.358.406 I load_tensors: offloading 24 repeating layers to GPU
0.00.358.422 I load_tensors: offloading output layer to GPU
0.00.358.423 I load_tensors: offloaded 25/25 layers to GPU
0.00.358.458 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.358.460 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.360.221 I llama_init_from_model: n_seq_max     = 1
0.00.360.224 I llama_init_from_model: n_ctx         = 128
0.00.360.225 I llama_init_from_model: n_ctx_per_seq = 128
0.00.360.225 I llama_init_from_model: n_batch       = 128
0.00.360.226 I llama_init_from_model: n_ubatch      = 128
0.00.360.226 I llama_init_from_model: flash_attn    = 0
0.00.360.228 I llama_init_from_model: freq_base     = 10000.0
0.00.360.229 I llama_init_from_model: freq_scale    = 1
0.00.360.229 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.360.245 I ggml_metal_init: allocating
0.00.360.343 I ggml_metal_init: found device: Apple M4
0.00.360.358 I ggml_metal_init: picking default device: Apple M4
0.00.362.260 I ggml_metal_init: using embedded metal library
0.00.367.812 I ggml_metal_init: GPU name:   Apple M4
0.00.367.827 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.367.827 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.367.828 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.367.829 I ggml_metal_init: simdgroup reduction   = true
0.00.367.829 I ggml_metal_init: simdgroup matrix mul. = true
0.00.367.829 I ggml_metal_init: has residency sets    = true
0.00.367.829 I ggml_metal_init: has bfloat            = true
0.00.367.830 I ggml_metal_init: use bfloat            = true
0.00.367.831 I ggml_metal_init: hasUnifiedMemory      = true
0.00.367.843 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.388.646 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.392.233 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.392.239 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.392.288 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.395.564 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.395.565 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.395.566 I llama_init_from_model: graph nodes  = 967
0.00.395.567 I llama_init_from_model: graph splits = 2
0.00.395.570 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.395.570 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.429.137 I 
0.00.429.229 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.429.248 I perplexity: tokenizing the input ..
0.00.436.171 I perplexity: tokenization took 6.92 ms
0.00.436.179 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.577.734 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.579.076 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.579.107 I llama_perf_context_print:        load time =     420.18 ms
0.00.579.108 I llama_perf_context_print: prompt eval time =     140.65 ms /   128 tokens (    1.10 ms per token,   910.06 tokens per second)
0.00.579.110 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.579.111 I llama_perf_context_print:       total time =     149.97 ms /   129 tokens
0.00.579.499 I ggml_metal_free: deallocating

real	0m0.594s
user	0m0.082s
sys	0m0.087s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.110 I build: 4793 (70680c48) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.865 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.727 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.015.733 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.740 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.740 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.740 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.741 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.741 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.742 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.742 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.743 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.743 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.743 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.744 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.744 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.746 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.746 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.746 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.698 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.764 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.660 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.662 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.662 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.662 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.663 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.663 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.664 I llama_model_loader: - type  f32:  194 tensors
0.00.024.664 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.664 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.665 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.665 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.666 I print_info: file format = GGUF V3 (latest)
0.00.024.666 I print_info: file type   = Q3_K - Medium
0.00.024.667 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.033.330 I load: special tokens cache size = 25
0.00.039.450 I load: token to piece cache size = 0.2984 MB
0.00.039.454 I print_info: arch             = gptneox
0.00.039.455 I print_info: vocab_only       = 0
0.00.039.455 I print_info: n_ctx_train      = 2048
0.00.039.455 I print_info: n_embd           = 2048
0.00.039.455 I print_info: n_layer          = 24
0.00.039.460 I print_info: n_head           = 16
0.00.039.461 I print_info: n_head_kv        = 16
0.00.039.461 I print_info: n_rot            = 32
0.00.039.461 I print_info: n_swa            = 0
0.00.039.461 I print_info: n_embd_head_k    = 128
0.00.039.461 I print_info: n_embd_head_v    = 128
0.00.039.462 I print_info: n_gqa            = 1
0.00.039.463 I print_info: n_embd_k_gqa     = 2048
0.00.039.463 I print_info: n_embd_v_gqa     = 2048
0.00.039.464 I print_info: f_norm_eps       = 1.0e-05
0.00.039.464 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.464 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.465 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.465 I print_info: f_logit_scale    = 0.0e+00
0.00.039.465 I print_info: n_ff             = 8192
0.00.039.465 I print_info: n_expert         = 0
0.00.039.466 I print_info: n_expert_used    = 0
0.00.039.466 I print_info: causal attn      = 1
0.00.039.466 I print_info: pooling type     = 0
0.00.039.466 I print_info: rope type        = 2
0.00.039.466 I print_info: rope scaling     = linear
0.00.039.466 I print_info: freq_base_train  = 10000.0
0.00.039.467 I print_info: freq_scale_train = 1
0.00.039.467 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.467 I print_info: rope_finetuned   = unknown
0.00.039.467 I print_info: ssm_d_conv       = 0
0.00.039.467 I print_info: ssm_d_inner      = 0
0.00.039.467 I print_info: ssm_d_state      = 0
0.00.039.467 I print_info: ssm_dt_rank      = 0
0.00.039.469 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.469 I print_info: model type       = 1.4B
0.00.039.469 I print_info: model params     = 1.41 B
0.00.039.469 I print_info: general.name     = 1.4B
0.00.039.470 I print_info: vocab type       = BPE
0.00.039.470 I print_info: n_vocab          = 50304
0.00.039.470 I print_info: n_merges         = 50009
0.00.039.470 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.470 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.471 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.471 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.471 I print_info: LF token         = 187 'Ċ'
0.00.039.471 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.471 I print_info: max token length = 1024
0.00.039.472 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.457.125 I load_tensors: offloading 24 repeating layers to GPU
0.00.457.134 I load_tensors: offloading output layer to GPU
0.00.457.135 I load_tensors: offloaded 25/25 layers to GPU
0.00.457.164 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.457.166 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.458.822 I llama_init_from_model: n_seq_max     = 1
0.00.458.825 I llama_init_from_model: n_ctx         = 128
0.00.458.826 I llama_init_from_model: n_ctx_per_seq = 128
0.00.458.826 I llama_init_from_model: n_batch       = 128
0.00.458.827 I llama_init_from_model: n_ubatch      = 128
0.00.458.827 I llama_init_from_model: flash_attn    = 0
0.00.458.829 I llama_init_from_model: freq_base     = 10000.0
0.00.458.829 I llama_init_from_model: freq_scale    = 1
0.00.458.830 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.458.832 I ggml_metal_init: allocating
0.00.458.897 I ggml_metal_init: found device: Apple M4
0.00.458.911 I ggml_metal_init: picking default device: Apple M4
0.00.460.889 I ggml_metal_init: using embedded metal library
0.00.466.928 I ggml_metal_init: GPU name:   Apple M4
0.00.466.936 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.466.937 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.466.938 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.466.938 I ggml_metal_init: simdgroup reduction   = true
0.00.466.939 I ggml_metal_init: simdgroup matrix mul. = true
0.00.466.939 I ggml_metal_init: has residency sets    = true
0.00.466.939 I ggml_metal_init: has bfloat            = true
0.00.466.940 I ggml_metal_init: use bfloat            = true
0.00.466.942 I ggml_metal_init: hasUnifiedMemory      = true
0.00.466.950 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.489.000 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.492.716 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.492.724 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.492.784 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.496.072 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.496.075 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.496.075 I llama_init_from_model: graph nodes  = 967
0.00.496.076 I llama_init_from_model: graph splits = 2
0.00.496.079 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.496.079 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.524.479 I 
0.00.524.562 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.524.569 I perplexity: tokenizing the input ..
0.00.531.651 I perplexity: tokenization took 7.079 ms
0.00.531.659 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.664.358 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.665.688 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.665.722 I llama_perf_context_print:        load time =     515.61 ms
0.00.665.723 I llama_perf_context_print: prompt eval time =     131.80 ms /   128 tokens (    1.03 ms per token,   971.17 tokens per second)
0.00.665.724 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.665.725 I llama_perf_context_print:       total time =     141.25 ms /   129 tokens
0.00.666.107 I ggml_metal_free: deallocating

real	0m0.679s
user	0m0.083s
sys	0m0.124s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.107 I build: 4793 (70680c48) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.950 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.765 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.772 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.774 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.774 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.774 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.775 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.775 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.776 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.776 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.777 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.777 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.778 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.778 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.778 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.780 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.781 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.781 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.642 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.719 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.657 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.659 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.659 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.659 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.661 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.662 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.662 I llama_model_loader: - type  f32:  194 tensors
0.00.025.662 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.663 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.663 I llama_model_loader: - type q6_K:   13 tensors
0.00.025.663 I print_info: file format = GGUF V3 (latest)
0.00.025.664 I print_info: file type   = Q4_K - Medium
0.00.025.665 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.033.819 I load: special tokens cache size = 25
0.00.039.993 I load: token to piece cache size = 0.2984 MB
0.00.039.997 I print_info: arch             = gptneox
0.00.039.997 I print_info: vocab_only       = 0
0.00.039.998 I print_info: n_ctx_train      = 2048
0.00.039.998 I print_info: n_embd           = 2048
0.00.039.998 I print_info: n_layer          = 24
0.00.040.002 I print_info: n_head           = 16
0.00.040.003 I print_info: n_head_kv        = 16
0.00.040.003 I print_info: n_rot            = 32
0.00.040.003 I print_info: n_swa            = 0
0.00.040.003 I print_info: n_embd_head_k    = 128
0.00.040.003 I print_info: n_embd_head_v    = 128
0.00.040.004 I print_info: n_gqa            = 1
0.00.040.004 I print_info: n_embd_k_gqa     = 2048
0.00.040.005 I print_info: n_embd_v_gqa     = 2048
0.00.040.005 I print_info: f_norm_eps       = 1.0e-05
0.00.040.006 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.006 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.006 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.006 I print_info: f_logit_scale    = 0.0e+00
0.00.040.007 I print_info: n_ff             = 8192
0.00.040.007 I print_info: n_expert         = 0
0.00.040.007 I print_info: n_expert_used    = 0
0.00.040.007 I print_info: causal attn      = 1
0.00.040.007 I print_info: pooling type     = 0
0.00.040.007 I print_info: rope type        = 2
0.00.040.008 I print_info: rope scaling     = linear
0.00.040.009 I print_info: freq_base_train  = 10000.0
0.00.040.009 I print_info: freq_scale_train = 1
0.00.040.009 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.010 I print_info: rope_finetuned   = unknown
0.00.040.010 I print_info: ssm_d_conv       = 0
0.00.040.010 I print_info: ssm_d_inner      = 0
0.00.040.010 I print_info: ssm_d_state      = 0
0.00.040.010 I print_info: ssm_dt_rank      = 0
0.00.040.010 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.011 I print_info: model type       = 1.4B
0.00.040.011 I print_info: model params     = 1.41 B
0.00.040.011 I print_info: general.name     = 1.4B
0.00.040.012 I print_info: vocab type       = BPE
0.00.040.012 I print_info: n_vocab          = 50304
0.00.040.012 I print_info: n_merges         = 50009
0.00.040.012 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.012 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.012 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.013 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.013 I print_info: LF token         = 187 'Ċ'
0.00.040.013 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.013 I print_info: max token length = 1024
0.00.040.014 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.526.898 I load_tensors: offloading 24 repeating layers to GPU
0.00.526.912 I load_tensors: offloading output layer to GPU
0.00.526.912 I load_tensors: offloaded 25/25 layers to GPU
0.00.526.947 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.526.949 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.528.598 I llama_init_from_model: n_seq_max     = 1
0.00.528.601 I llama_init_from_model: n_ctx         = 128
0.00.528.601 I llama_init_from_model: n_ctx_per_seq = 128
0.00.528.602 I llama_init_from_model: n_batch       = 128
0.00.528.602 I llama_init_from_model: n_ubatch      = 128
0.00.528.602 I llama_init_from_model: flash_attn    = 0
0.00.528.604 I llama_init_from_model: freq_base     = 10000.0
0.00.528.605 I llama_init_from_model: freq_scale    = 1
0.00.528.606 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.528.608 I ggml_metal_init: allocating
0.00.528.707 I ggml_metal_init: found device: Apple M4
0.00.528.721 I ggml_metal_init: picking default device: Apple M4
0.00.530.518 I ggml_metal_init: using embedded metal library
0.00.536.926 I ggml_metal_init: GPU name:   Apple M4
0.00.536.936 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.536.937 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.536.938 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.536.939 I ggml_metal_init: simdgroup reduction   = true
0.00.536.939 I ggml_metal_init: simdgroup matrix mul. = true
0.00.536.939 I ggml_metal_init: has residency sets    = true
0.00.536.940 I ggml_metal_init: has bfloat            = true
0.00.536.940 I ggml_metal_init: use bfloat            = true
0.00.536.941 I ggml_metal_init: hasUnifiedMemory      = true
0.00.536.945 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.556.218 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.559.799 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.559.809 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.559.861 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.563.340 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.563.341 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.563.342 I llama_init_from_model: graph nodes  = 967
0.00.563.342 I llama_init_from_model: graph splits = 2
0.00.563.345 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.563.346 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.590.379 I 
0.00.590.463 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.590.470 I perplexity: tokenizing the input ..
0.00.597.424 I perplexity: tokenization took 6.951 ms
0.00.597.443 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.743.850 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.745.195 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.745.220 I llama_perf_context_print:        load time =     580.42 ms
0.00.745.222 I llama_perf_context_print: prompt eval time =     145.46 ms /   128 tokens (    1.14 ms per token,   880.00 tokens per second)
0.00.745.222 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.745.223 I llama_perf_context_print:       total time =     154.85 ms /   129 tokens
0.00.745.589 I ggml_metal_free: deallocating

real	0m0.761s
user	0m0.080s
sys	0m0.125s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.094 I build: 4793 (70680c48) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.868 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.762 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.768 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.775 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.775 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.776 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.776 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.776 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.777 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.778 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.778 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.778 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.779 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.779 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.780 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.781 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.782 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.782 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.659 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.691 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.582 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.583 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.583 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.584 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.584 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.585 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.585 I llama_model_loader: - type  f32:  194 tensors
0.00.024.586 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.586 I llama_model_loader: - type q6_K:   37 tensors
0.00.024.587 I print_info: file format = GGUF V3 (latest)
0.00.024.587 I print_info: file type   = Q5_K - Medium
0.00.024.591 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.033.047 I load: special tokens cache size = 25
0.00.038.929 I load: token to piece cache size = 0.2984 MB
0.00.038.934 I print_info: arch             = gptneox
0.00.038.934 I print_info: vocab_only       = 0
0.00.038.935 I print_info: n_ctx_train      = 2048
0.00.038.935 I print_info: n_embd           = 2048
0.00.038.935 I print_info: n_layer          = 24
0.00.038.939 I print_info: n_head           = 16
0.00.038.940 I print_info: n_head_kv        = 16
0.00.038.940 I print_info: n_rot            = 32
0.00.038.940 I print_info: n_swa            = 0
0.00.038.940 I print_info: n_embd_head_k    = 128
0.00.038.941 I print_info: n_embd_head_v    = 128
0.00.038.941 I print_info: n_gqa            = 1
0.00.038.942 I print_info: n_embd_k_gqa     = 2048
0.00.038.943 I print_info: n_embd_v_gqa     = 2048
0.00.038.943 I print_info: f_norm_eps       = 1.0e-05
0.00.038.944 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.944 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.944 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.944 I print_info: f_logit_scale    = 0.0e+00
0.00.038.945 I print_info: n_ff             = 8192
0.00.038.945 I print_info: n_expert         = 0
0.00.038.945 I print_info: n_expert_used    = 0
0.00.038.945 I print_info: causal attn      = 1
0.00.038.945 I print_info: pooling type     = 0
0.00.038.945 I print_info: rope type        = 2
0.00.038.946 I print_info: rope scaling     = linear
0.00.038.946 I print_info: freq_base_train  = 10000.0
0.00.038.948 I print_info: freq_scale_train = 1
0.00.038.948 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.948 I print_info: rope_finetuned   = unknown
0.00.038.948 I print_info: ssm_d_conv       = 0
0.00.038.948 I print_info: ssm_d_inner      = 0
0.00.038.948 I print_info: ssm_d_state      = 0
0.00.038.948 I print_info: ssm_dt_rank      = 0
0.00.038.949 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.949 I print_info: model type       = 1.4B
0.00.038.949 I print_info: model params     = 1.41 B
0.00.038.949 I print_info: general.name     = 1.4B
0.00.038.950 I print_info: vocab type       = BPE
0.00.038.950 I print_info: n_vocab          = 50304
0.00.038.951 I print_info: n_merges         = 50009
0.00.038.951 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.951 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.952 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.952 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.952 I print_info: LF token         = 187 'Ċ'
0.00.038.952 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.953 I print_info: max token length = 1024
0.00.038.953 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.587.878 I load_tensors: offloading 24 repeating layers to GPU
0.00.587.893 I load_tensors: offloading output layer to GPU
0.00.587.894 I load_tensors: offloaded 25/25 layers to GPU
0.00.587.930 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.587.932 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.589.752 I llama_init_from_model: n_seq_max     = 1
0.00.589.754 I llama_init_from_model: n_ctx         = 128
0.00.589.755 I llama_init_from_model: n_ctx_per_seq = 128
0.00.589.755 I llama_init_from_model: n_batch       = 128
0.00.589.755 I llama_init_from_model: n_ubatch      = 128
0.00.589.756 I llama_init_from_model: flash_attn    = 0
0.00.589.758 I llama_init_from_model: freq_base     = 10000.0
0.00.589.759 I llama_init_from_model: freq_scale    = 1
0.00.589.759 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.589.764 I ggml_metal_init: allocating
0.00.589.851 I ggml_metal_init: found device: Apple M4
0.00.589.866 I ggml_metal_init: picking default device: Apple M4
0.00.591.664 I ggml_metal_init: using embedded metal library
0.00.598.494 I ggml_metal_init: GPU name:   Apple M4
0.00.598.499 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.598.500 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.598.501 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.598.501 I ggml_metal_init: simdgroup reduction   = true
0.00.598.502 I ggml_metal_init: simdgroup matrix mul. = true
0.00.598.502 I ggml_metal_init: has residency sets    = true
0.00.598.502 I ggml_metal_init: has bfloat            = true
0.00.598.502 I ggml_metal_init: use bfloat            = true
0.00.598.504 I ggml_metal_init: hasUnifiedMemory      = true
0.00.598.507 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.616.394 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.619.914 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.619.921 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.619.975 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.623.277 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.623.279 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.623.280 I llama_init_from_model: graph nodes  = 967
0.00.623.280 I llama_init_from_model: graph splits = 2
0.00.623.283 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.623.283 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.653.281 I 
0.00.653.369 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.653.379 I perplexity: tokenizing the input ..
0.00.660.696 I perplexity: tokenization took 7.315 ms
0.00.660.707 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.797.897 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.799.233 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.799.256 I llama_perf_context_print:        load time =     644.40 ms
0.00.799.257 I llama_perf_context_print: prompt eval time =     136.64 ms /   128 tokens (    1.07 ms per token,   936.75 tokens per second)
0.00.799.257 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.799.258 I llama_perf_context_print:       total time =     145.98 ms /   129 tokens
0.00.799.614 I ggml_metal_free: deallocating

real	0m0.813s
user	0m0.080s
sys	0m0.138s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.138 I build: 4793 (70680c48) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.917 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.445 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.451 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.453 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.453 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.454 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.454 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.454 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.461 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.461 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.461 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.462 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.462 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.462 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.463 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.464 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.465 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.465 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.318 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.393 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.231 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.233 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.233 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.234 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.234 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.234 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.235 I llama_model_loader: - type  f32:  194 tensors
0.00.024.235 I llama_model_loader: - type q6_K:   98 tensors
0.00.024.236 I print_info: file format = GGUF V3 (latest)
0.00.024.237 I print_info: file type   = Q6_K
0.00.024.238 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.032.941 I load: special tokens cache size = 25
0.00.038.920 I load: token to piece cache size = 0.2984 MB
0.00.038.924 I print_info: arch             = gptneox
0.00.038.925 I print_info: vocab_only       = 0
0.00.038.925 I print_info: n_ctx_train      = 2048
0.00.038.925 I print_info: n_embd           = 2048
0.00.038.925 I print_info: n_layer          = 24
0.00.038.930 I print_info: n_head           = 16
0.00.038.930 I print_info: n_head_kv        = 16
0.00.038.931 I print_info: n_rot            = 32
0.00.038.931 I print_info: n_swa            = 0
0.00.038.931 I print_info: n_embd_head_k    = 128
0.00.038.934 I print_info: n_embd_head_v    = 128
0.00.038.934 I print_info: n_gqa            = 1
0.00.038.935 I print_info: n_embd_k_gqa     = 2048
0.00.038.936 I print_info: n_embd_v_gqa     = 2048
0.00.038.936 I print_info: f_norm_eps       = 1.0e-05
0.00.038.937 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.937 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.937 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.937 I print_info: f_logit_scale    = 0.0e+00
0.00.038.939 I print_info: n_ff             = 8192
0.00.038.939 I print_info: n_expert         = 0
0.00.038.939 I print_info: n_expert_used    = 0
0.00.038.939 I print_info: causal attn      = 1
0.00.038.939 I print_info: pooling type     = 0
0.00.038.939 I print_info: rope type        = 2
0.00.038.941 I print_info: rope scaling     = linear
0.00.038.941 I print_info: freq_base_train  = 10000.0
0.00.038.941 I print_info: freq_scale_train = 1
0.00.038.941 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.942 I print_info: rope_finetuned   = unknown
0.00.038.942 I print_info: ssm_d_conv       = 0
0.00.038.942 I print_info: ssm_d_inner      = 0
0.00.038.942 I print_info: ssm_d_state      = 0
0.00.038.942 I print_info: ssm_dt_rank      = 0
0.00.038.942 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.942 I print_info: model type       = 1.4B
0.00.038.943 I print_info: model params     = 1.41 B
0.00.038.943 I print_info: general.name     = 1.4B
0.00.038.944 I print_info: vocab type       = BPE
0.00.038.944 I print_info: n_vocab          = 50304
0.00.038.944 I print_info: n_merges         = 50009
0.00.038.944 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.944 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.945 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.945 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.945 I print_info: LF token         = 187 'Ċ'
0.00.038.945 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.945 I print_info: max token length = 1024
0.00.038.946 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.614.231 I load_tensors: offloading 24 repeating layers to GPU
0.00.614.246 I load_tensors: offloading output layer to GPU
0.00.614.247 I load_tensors: offloaded 25/25 layers to GPU
0.00.614.289 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.614.290 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.616.009 I llama_init_from_model: n_seq_max     = 1
0.00.616.011 I llama_init_from_model: n_ctx         = 128
0.00.616.012 I llama_init_from_model: n_ctx_per_seq = 128
0.00.616.012 I llama_init_from_model: n_batch       = 128
0.00.616.012 I llama_init_from_model: n_ubatch      = 128
0.00.616.013 I llama_init_from_model: flash_attn    = 0
0.00.616.015 I llama_init_from_model: freq_base     = 10000.0
0.00.616.015 I llama_init_from_model: freq_scale    = 1
0.00.616.016 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.616.019 I ggml_metal_init: allocating
0.00.616.115 I ggml_metal_init: found device: Apple M4
0.00.616.129 I ggml_metal_init: picking default device: Apple M4
0.00.617.912 I ggml_metal_init: using embedded metal library
0.00.624.094 I ggml_metal_init: GPU name:   Apple M4
0.00.624.099 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.624.099 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.624.100 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.624.101 I ggml_metal_init: simdgroup reduction   = true
0.00.624.101 I ggml_metal_init: simdgroup matrix mul. = true
0.00.624.101 I ggml_metal_init: has residency sets    = true
0.00.624.101 I ggml_metal_init: has bfloat            = true
0.00.624.101 I ggml_metal_init: use bfloat            = true
0.00.624.102 I ggml_metal_init: hasUnifiedMemory      = true
0.00.624.112 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.641.914 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.645.320 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.645.324 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.645.368 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.648.474 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.648.476 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.648.476 I llama_init_from_model: graph nodes  = 967
0.00.648.477 I llama_init_from_model: graph splits = 2
0.00.648.479 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.648.480 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.685.748 I 
0.00.685.834 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.685.841 I perplexity: tokenizing the input ..
0.00.692.794 I perplexity: tokenization took 6.95 ms
0.00.692.802 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.824.420 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.825.755 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.825.776 I llama_perf_context_print:        load time =     676.82 ms
0.00.825.776 I llama_perf_context_print: prompt eval time =     130.72 ms /   128 tokens (    1.02 ms per token,   979.16 tokens per second)
0.00.825.777 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.825.778 I llama_perf_context_print:       total time =     140.03 ms /   129 tokens
0.00.826.176 I ggml_metal_free: deallocating

real	0m0.841s
user	0m0.080s
sys	0m0.148s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.277 I build: 4793 (70680c48) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.023.601 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.036.513 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.036.522 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.036.524 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.036.525 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.036.526 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.036.526 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.036.526 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.036.528 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.036.529 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.036.529 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.036.530 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.036.530 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.036.530 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.036.531 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.036.534 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.036.534 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.036.535 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.045.507 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.047.824 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.054.854 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.054.856 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.054.856 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.054.857 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.054.857 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.054.858 I llama_model_loader: - type  f32:  194 tensors
0.00.054.858 I llama_model_loader: - type  f16:   98 tensors
0.00.054.859 I print_info: file format = GGUF V3 (latest)
0.00.054.860 I print_info: file type   = all F32 (guessed)
0.00.054.862 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.067.925 I load: special tokens cache size = 25
0.00.076.392 I load: token to piece cache size = 0.2984 MB
0.00.076.395 I print_info: arch             = gptneox
0.00.076.395 I print_info: vocab_only       = 0
0.00.076.396 I print_info: n_ctx_train      = 2048
0.00.076.396 I print_info: n_embd           = 2048
0.00.076.396 I print_info: n_layer          = 24
0.00.076.399 I print_info: n_head           = 16
0.00.076.401 I print_info: n_head_kv        = 16
0.00.076.401 I print_info: n_rot            = 32
0.00.076.401 I print_info: n_swa            = 0
0.00.076.401 I print_info: n_embd_head_k    = 128
0.00.076.401 I print_info: n_embd_head_v    = 128
0.00.076.402 I print_info: n_gqa            = 1
0.00.076.403 I print_info: n_embd_k_gqa     = 2048
0.00.076.404 I print_info: n_embd_v_gqa     = 2048
0.00.076.404 I print_info: f_norm_eps       = 1.0e-05
0.00.076.404 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.076.408 I print_info: f_clamp_kqv      = 0.0e+00
0.00.076.408 I print_info: f_max_alibi_bias = 0.0e+00
0.00.076.408 I print_info: f_logit_scale    = 0.0e+00
0.00.076.409 I print_info: n_ff             = 8192
0.00.076.410 I print_info: n_expert         = 0
0.00.076.410 I print_info: n_expert_used    = 0
0.00.076.411 I print_info: causal attn      = 1
0.00.076.411 I print_info: pooling type     = 0
0.00.076.411 I print_info: rope type        = 2
0.00.076.411 I print_info: rope scaling     = linear
0.00.076.412 I print_info: freq_base_train  = 10000.0
0.00.076.412 I print_info: freq_scale_train = 1
0.00.076.412 I print_info: n_ctx_orig_yarn  = 2048
0.00.076.413 I print_info: rope_finetuned   = unknown
0.00.076.413 I print_info: ssm_d_conv       = 0
0.00.076.413 I print_info: ssm_d_inner      = 0
0.00.076.413 I print_info: ssm_d_state      = 0
0.00.076.413 I print_info: ssm_dt_rank      = 0
0.00.076.413 I print_info: ssm_dt_b_c_rms   = 0
0.00.076.418 I print_info: model type       = 1.4B
0.00.076.418 I print_info: model params     = 1.41 B
0.00.076.418 I print_info: general.name     = 1.4B
0.00.076.419 I print_info: vocab type       = BPE
0.00.076.419 I print_info: n_vocab          = 50304
0.00.076.419 I print_info: n_merges         = 50009
0.00.076.420 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.076.420 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.076.421 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.076.421 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.076.422 I print_info: LF token         = 187 'Ċ'
0.00.076.422 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.076.422 I print_info: max token length = 1024
0.00.076.422 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.305.325 I load_tensors: offloading 24 repeating layers to GPU
0.01.305.333 I load_tensors: offloading output layer to GPU
0.01.305.335 I load_tensors: offloaded 25/25 layers to GPU
0.01.305.362 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.305.363 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.01.306.169 I llama_init_from_model: n_seq_max     = 1
0.01.306.170 I llama_init_from_model: n_ctx         = 128
0.01.306.171 I llama_init_from_model: n_ctx_per_seq = 128
0.01.306.171 I llama_init_from_model: n_batch       = 128
0.01.306.171 I llama_init_from_model: n_ubatch      = 128
0.01.306.171 I llama_init_from_model: flash_attn    = 0
0.01.306.172 I llama_init_from_model: freq_base     = 10000.0
0.01.306.172 I llama_init_from_model: freq_scale    = 1
0.01.306.173 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.306.173 I ggml_metal_init: allocating
0.01.306.204 I ggml_metal_init: found device: Apple M4
0.01.306.209 I ggml_metal_init: picking default device: Apple M4
0.01.307.257 I ggml_metal_init: using embedded metal library
0.01.311.103 I ggml_metal_init: GPU name:   Apple M4
0.01.311.106 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.311.106 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.311.107 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.311.107 I ggml_metal_init: simdgroup reduction   = true
0.01.311.107 I ggml_metal_init: simdgroup matrix mul. = true
0.01.311.107 I ggml_metal_init: has residency sets    = true
0.01.311.107 I ggml_metal_init: has bfloat            = true
0.01.311.107 I ggml_metal_init: use bfloat            = true
0.01.311.108 I ggml_metal_init: hasUnifiedMemory      = true
0.01.311.109 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.321.935 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.323.705 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.323.707 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.323.733 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.325.412 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.325.414 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.325.414 I llama_init_from_model: graph nodes  = 967
0.01.325.414 I llama_init_from_model: graph splits = 2
0.01.325.415 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.325.416 I 
0.01.325.454 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.325.455 I compute_imatrix: tokenizing the input ..
0.01.329.489 I compute_imatrix: tokenization took 4.033 ms
0.01.329.490 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.595.286 I compute_imatrix: 0.27 seconds per pass - ETA 0.00 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.597.957 I llama_perf_context_print:        load time =    1571.68 ms
0.01.597.958 I llama_perf_context_print: prompt eval time =     264.05 ms /   128 tokens (    2.06 ms per token,   484.75 tokens per second)
0.01.597.959 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.597.959 I llama_perf_context_print:       total time =    1574.35 ms /   129 tokens
0.01.598.500 I ggml_metal_free: deallocating

real	0m1.787s
user	0m0.127s
sys	0m0.247s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4793 (70680c48)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 'Ċ'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x129d05a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x129d06160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x129d06710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x129d06cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x129d07270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x129d07820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x129d07dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x129d08380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x129d08930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x129d08e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x129d09330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x129d09830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x129d0a350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x129d0ab00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x129d0b310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x129d0ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x129d0c150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x129d0c870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x129d0cf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x129d0d760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x129d0de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x129d0e5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x129d0ecc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x129d0f560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x129d0fc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x129d0ff40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x129d10550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x129d111c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x129d11700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x129d119c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x129d11e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x129d12120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x129d129b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x129d12ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x129d131b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x129d13650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x129d13af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x129d13f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x129d14430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x129d148d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x129d14d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x129d15210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x129d156b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x129d15b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x129d15e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x129d16420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x129d16a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x129d17350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x129d17960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x129d17f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x129d18580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x129d18b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x129d191a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x129d197b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x129d19fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x129d1a440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x129d1a8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x129d1aba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x129d1b1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x129d1b9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x129d1bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x129d1c100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x129d1c5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x129d1ca40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x129d1cee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x129d1d380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x129d1d820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x129d1dcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x129d1e160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x129d1e600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x129d1eaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x129d1ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x129d1f3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x129d1f930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x129d1fe80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x129d203d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x129d20920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x129d20e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x129d213c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x129d21910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x129d21e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x129d223b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x129d22900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x129d22e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x129d233a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x129d238f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x129d23e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x129d24390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x129d248e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x129d24e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x129d25380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x129d258d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x129d25e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x129d26370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x129d268c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x129d26e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x129d27360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x129d17040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x129d277d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x129d27f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x129d284d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x129d28a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x129d28f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x129d294c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x129d29a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x129d29f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x129d2a4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x129d2aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x129d2af50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x129d2b4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x129d2b9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x129d2bf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x129d2c490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x129d2c930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x129d2cdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x129d2d270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x129d2d710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x129d2dbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x129d2e050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x129d2e4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x129d2e990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x129d2ee30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x129d2f2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x129d2f770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x129d2fc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x129d300b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x129d30550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x129d309f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x129d30e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x129d31330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x129d317d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x129d31c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x129d32110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x129d325b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x129d32a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x129d32ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x129d33390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x129d33830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x129d33cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x129d34170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x129d34610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x129d34ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x129d34f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x129d353f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x129d35890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x129d35d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x129d361d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x129d36670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x129d36b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x129d36fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x129d37450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x129d378f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x129d37d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x129d38230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x129d386d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x129d38b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x129d39010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x129d394b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x129d39950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x129d39df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x129d3a290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x129d3a730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x129d3abd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x129d3b070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x129d3b510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x129d3b9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x129d3be50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x129d3c2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x129d3c790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x129d3cc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x129d3d0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x129d3d570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x129d3da10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x129d3deb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x129d3e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x129d3e7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x129d3ec90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x129d3f130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x129d3f5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x129d3fa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x129d3ff10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x129d403b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x129d40850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x129d40cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x129d41190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x129d41630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x129d41ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x129d41f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x129d42410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x129d428b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x129d42d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x129d431f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x129d43690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x129d43be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x129d44130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x129d44680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x129d44bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x129d44e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x129d454a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x129d45ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x129d460c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x129d468b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x129d46d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x129d47010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x129d47620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x129d47c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x129d48420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x129d488c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x129d48d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x129d49200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x129d499b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x129d49f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x129d4a450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x129d4a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x129d4aef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x129d4b440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x129d4b990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x129d4bee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x129d4c430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x129d4c980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x129d4ced0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x129d4d420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x129d4d970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x129d4dec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x129d4e410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x129d4e960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x129d4eeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x129d4f400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x129d4f950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x129d4fea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x129d503f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x129d50940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x129d50e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x129d513e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x129d51930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x129d51e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x129d523d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x129d52920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x129d52e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x129d533c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x129d53910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x129d53e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x129d543b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x129d54900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x129d54e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x129d553a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x129d558f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x129d55e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x129d56390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x129d568e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x129d56e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x129d57380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x129d578d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x129d57e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x129d58370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x129d588c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x129d58e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x129d59360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x129d598b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x129d59e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x129d5a350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x129d5a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x129d5adf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x129d5b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x129d5b890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x129d5bde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x129d5c330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x129d5c7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x129d5cc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x129d5d110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x129d5d5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x129d5da50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x129d5def0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x129d5e390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x129d5e830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x129d5ecd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x129d5f170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x129d5f610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x129d5fab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x129d5ff50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x129d603f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x129d60890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x129d60d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x129d611d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x129d61670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x129d61b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x129d61fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x129d62450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x129d628f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x129d62d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x129d63230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x129d636d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x129d63c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x129d64340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x129d64a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x129d65180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x129d658a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x129d65b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x129d66350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x129d66610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x129d66c20 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.675.621 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.675.625 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x117704ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x117704f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x1177053c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x117705830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x117705ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x117706110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x117706580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x1177069f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x117706e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x1177073e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x117707850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x117707ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x1177089f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x1177091a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x1177099b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x11770a0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x11770a7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x11770af10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x11770b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x11770be00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x11770c520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x11770cc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x11770d360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x11770da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x11770e1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x11770e460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x11770e720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x11770eb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x11770f000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x11770f470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x11770f8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x11770fe10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x117710280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x117710540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1177109b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x117710e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x117711290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x117711700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x117711b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x117711fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x117712450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x1177128c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x117712d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x1177131a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x117713610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x117713a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x117713ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x117714360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x1177147d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x117714c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x1177150b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x117715520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x117715990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x117715e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x117716270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x1177166e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x117716c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x117717150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x1177175c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x117717a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x117717ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x117718310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x117718780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x117718bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x117719060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x1177194d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x117719940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x117719db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x11771a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x11771a690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x11771ab00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x11771af70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x11771b3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x11771b850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x11771bcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x11771c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x11771c5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x11771ca10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x11771ce80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x11771d2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x11771d760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x11771dbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x11771e040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x11771e4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x11771e920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x11771ed90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x11771f200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x11771f670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x11771fae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x11771ff50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x1177203c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x117720830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x117720ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x117721110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x117721580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x1177219f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x117721e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x1177222d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x117722740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x117722bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x117723020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x117723490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x117723900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x117723d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x1177241e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x117724650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x117724ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x117724f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x1177253a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x117725810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x117725c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x1177260f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x117726560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x1177269d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x117726e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x1177272b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x117727720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x117727b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x117728000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x117728470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x1177288e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x117728d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x1177291c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x117729630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x117729aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x117729f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x11772a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x11772a7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x11772ac60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x11772b0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x11772b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x11772b9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x11772be20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x11772c290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x11772c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x11772cb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x11772cfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x11772d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x11772d8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x11772dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x11772e1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x11772e610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x11772ea80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x11772eef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x11772f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x11772f7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x11772fc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x1177300b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x117730520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x117730990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x117730e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x117731270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x1177316e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x117731b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x117731fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x117732430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x1177328a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x117732d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x117733180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x1177335f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x117733a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x117733ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x117734340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x1177347b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x117734c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x117735090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x117735cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x117735f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x117736240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x1177366b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x117736b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x117736f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x117737400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x117737870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x117737ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x117738150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x1177385c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x117738a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x117738ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x117739310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x117739780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x117739bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x11773a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x11773a4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x11773a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x11773adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x11773b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x11773b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x11773bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x11773bf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x11773c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x11773c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x11773ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x11773d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x11773d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x11773da10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x11773de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x11773e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x11773e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x11773ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x11773f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x11773f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x11773fa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x11773ff20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x117740390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x117740800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x117740c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x1177410e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x117741600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x117741b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x117742680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x117742940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x117742f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x1177434c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x117743a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x117744040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x117744600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x117744bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x117745180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x117745740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x117745d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x1177462c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x117746880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x117746e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x117747400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x1177479c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x117747f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x117748540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x117748b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x1177490c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x117749680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x117749c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x11774a200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x11774a7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x11774ad80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x11774b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x11774b900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x11774bec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x11774c480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x11774ca40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x11774d000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x11774d5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x11774db80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x11774e140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x11774e700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x11774ecc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x11774f280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x11774f840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x11774fe00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x1177503c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x117750980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x117750f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x117751500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x117751ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x117752080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x117752640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x117752c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x1177531c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x117753780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x117753d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x117754300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x1177548c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x117754e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x117755440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x117755a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x117755fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x117756580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x117756b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x117757040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x117757540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x117757a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x117757f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x117758440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x117758940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x117758e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x117759340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x117759840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x117759d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x11775a240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x11775a740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x11775ac40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x11775b140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x11775b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x11775bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x11775c040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x11775c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x11775ca40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x11775cf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x11775d440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x11775d940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x11775de40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x11775e340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x11775e840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x11775f250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x11775f970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x117760090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1177607b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x117760a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x117761260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x117761520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x117761b30 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x129c0cc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x129c0d090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x129c0d910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x129c0de60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x129c0e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x129c0e900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x129c0ee50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x129c0f3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x129c0f840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x129c0fce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x129c10180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x129c10440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x129c10e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x129c115e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x129c11df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x129c12510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x129c12c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x129c13350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x129c13a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x129c14420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x129c14b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x129c15260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x129c15980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x129c160a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x129c167c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x129c16a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x129c17090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x129c176a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x129c17cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x129c184a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x129c18940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x129c18c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x129c19490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x129c199d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x129c19c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x129c1a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x129c1a5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x129c1aa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x129c1af10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x129c1b3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x129c1b850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x129c1bcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x129c1c190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x129c1c630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x129c1c8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x129c1cf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x129c1d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x129c1db20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x129c1e130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x129c1e740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x129c1ed50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x129c1f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x129c1f970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x129c1ff80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x129c20770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x129c20c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x129c210b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x129c21370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x129c21980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x129c22170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x129c22610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x129c22ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x129c22f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x129c233f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x129c23890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x129c23d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x129c241d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x129c24670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x129c24b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x129c24fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x129c25450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x129c258f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x129c25d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x129c262e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x129c26830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x129d45760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x129d472d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x129d668d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x129d45150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x129d45d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x129d18e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x129d18840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x129d1ae60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x129d478e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x129d10200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x129d16cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x129d17610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x129d17c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x129d166e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x129d160d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x129d19460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x129d18230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x129d0f200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x129d09af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x129d05080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x129d19a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x129d1b470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x129d27a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x129d65e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x129d123e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x129d126a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x129d47ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x129d46380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x129d10810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x129d10ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x129d10d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x129d67080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x129d67340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x129d67600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x129d678c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x129d67b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x129d67e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x129d68100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x129d683c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x129d68680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x129d68940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x129d68c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x129d68ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x129d69180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x129d69440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x129d69700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x129d699c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x129d69c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x129d69f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x129d6a200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x129d6a4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x129d6a780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x129d6aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x129d6ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x129d6afc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x129d6b280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x129d6b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x129d6b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x129d6bac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x129d6bd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x129d6c040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x129d6c300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x129d6c5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x129d6c880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x129d6cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x129d6ce00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x129d6d0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x129d6d380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x129d6d640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x129d6d900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x129d6dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x129d6de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x129d6e140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x129d6e400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x129d6e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x129d6e980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x129d6ec40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x129d6ef00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x129d6f1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x129d6f480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x129d6f740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x129d6fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x129d6fcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x129d6ff80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x129d70240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x129d70500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x129d707c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x129d70a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x129d70d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x129d71000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x129d712c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x129d71580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x129d71840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x129d71b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x129d71dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x129d72080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x129d72340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x129d72600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x129d728c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x129d72b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x129d72e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x129d73100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x129d733c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x129d73680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x129d73940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x129d73c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x129d73ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x129d74180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x129d74440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x129d74700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x129d749c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x129d74c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x129d74f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x129d75200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x129d754c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x129d75780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x129d75a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x129d75d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x129d75fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x129d76280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x129d76540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x129d76800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x129d76ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x129d76d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x129d77040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x129d77300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x129d775c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x129d77880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x129d77b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x129d77e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x129d780c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x129d78380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x129d78640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x129d78900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x129d78bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x129d79190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x129d79450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x129d79710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x129d799d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x129d79c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x129d79f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x129d7a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x129d7a4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x129d7a790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x129d7aa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x129d7ad10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x129d7afd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x129d7b290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x129d7b550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x129d7b810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x129d7bad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x129d7bd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x129d7c050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x129d7c310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x129d7c5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x129d7c890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x129d7cb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x129d7ce10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x129d7d0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x129d7d390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x129d7d650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x129d7d910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x129d7dbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x129d7de90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x129d7e150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x129d7e410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x129d7e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x129d7e990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x129d7ec50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x129d7ef10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x129d7f1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x129d7f490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x129d7f750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x129d7fa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x129d7fcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x129d7ff90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x129d80250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x129d80510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x129d807d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x129d80a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x129d80d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x129d81010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x129d812d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x129d81590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x129d81850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x129d81b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x129d81dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x129d82090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x129d82350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x129d82610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x129d828d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x129d82b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x129d82e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x129d83110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x129d833d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x129d83690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x129d83950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x129d83c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x129d83ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x129d84190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x129d84450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x129d84710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x129d849d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x129d84c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x129d84f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x129d85210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x129d854d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x129d85790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x129d85a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x129d85d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x129d85fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x129d86290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x129d86550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x129d86810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x129d86ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x129d86d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x129d87050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x129d87310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x129d875d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x129d87890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x129d87b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x129d87e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x129d880d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x129d88390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x129d88650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x129d88910 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.713s
user	0m0.277s
sys	0m0.333s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4793 (70680c48)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 'Ċ'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x148f0d630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x148f0dd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x148f0e320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x148f0e8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x148f0ee80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x148f0f430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x148f0f9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x148f0ff90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x148f10540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x148f10a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x148f10f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x148f11440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x148f11f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x148f12710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x148f12f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x148f13640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x148f13d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x148f14480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x148f14ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x148f15370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x148f15a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x148f161b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x148f168d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x148f17170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x148f17890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x148f17b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x148f18160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x148f18dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x148f19310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x148f195d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x148f19a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x148f19d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x148f1a5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x148f1ab00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x148f1adc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x148f1b260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x148f1b700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x148f1bba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x148f1c040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x148f1c4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x148f1c980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x148f1ce20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x148f1d2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x148f1d760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x148f1da20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x148f1e030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x148f1e640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x148f1ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x148f1f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x148f1fb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x148f20190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x148f207a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x148f20db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x148f213c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x148f21bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x148f22050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x148f224f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x148f227b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x148f22dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x148f235b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x148f23870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x148f23d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x148f241b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x148f24650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x148f24af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x148f24f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x148f25430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x148f258d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x148f25d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x148f26210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x148f266b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x148f26b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x148f26ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x148f27540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x148f27a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x148f27fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x148f28530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x148f28a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x148f28fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x148f29520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x148f29a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x148f29fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x148f2a510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x148f2aa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x148f2afb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x148f2b500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x148f2ba50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x148f2bfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x148f2c4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x148f2ca40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x148f2cf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x148f2d4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x148f2da30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x148f2df80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x148f2e4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x148f2ea20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x148f2ef70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x148f1ec50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x148f2f3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x148f2fb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x148f300e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x148f30630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x148f30b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x148f310d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x148f31620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x148f31b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x148f320c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x148f32610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x148f32b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x148f330b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x148f33600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x148f33b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x148f340a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x148f34540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x148f349e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x148f34e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x148f35320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x148f357c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x148f35c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x148f36100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x148f365a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x148f36a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x148f36ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x148f37380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x148f37820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x148f37cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x148f38160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x148f38600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x148f38aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x148f38f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x148f393e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x148f39880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x148f39d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x148f3a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x148f3a660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x148f3ab00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x148f3afa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x148f3b440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x148f3b8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x148f3bd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x148f3c220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x148f3c6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x148f3cb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x148f3d000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x148f3d4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x148f3d940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x148f3dde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x148f3e280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x148f3e720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x148f3ebc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x148f3f060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x148f3f500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x148f3f9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x148f3fe40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x148f402e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x148f40780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x148f40c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x148f410c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x148f41560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x148f41a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x148f41ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x148f42340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x148f427e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x148f42c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x148f43120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x148f435c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x148f43a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x148f43f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x148f443a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x148f44840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x148f44ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x148f45180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x148f45620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x148f45ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x148f45f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x148f46400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x148f468a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x148f46d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x148f471e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x148f47680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x148f47b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x148f47fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x148f48460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x148f48900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x148f48da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x148f49240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x148f496e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x148f49b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x148f4a020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x148f4a4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x148f4a960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x148f4ae00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x148f4b2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x148f4b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x148f4bd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x148f4c290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x148f4c7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x148f4caa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x148f4d0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x148f4d6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x148f4dcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x148f4e4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x148f4e960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x148f4ec20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x148f4f230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x148f4f840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x148f50030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x148f504d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x148f50970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x148f50e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x148f515c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x148f51b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x148f52060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x148f525b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x148f52b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x148f53050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x148f535a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x148f53af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x148f54040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x148f54590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x148f54ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x148f55030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x148f55580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x148f55ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x148f56020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x148f56570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x148f56ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x148f57010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x148f57560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x148f57ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x148f58000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x148f58550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x148f58aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x148f58ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x148f59540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x148f59a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x148f59fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x148f5a530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x148f5aa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x148f5afd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x148f5b520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x148f5ba70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x148f5bfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x148f5c510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x148f5ca60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x148f5cfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x148f5d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x148f5da50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x148f5dfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x148f5e4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x148f5ea40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x148f5ef90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x148f5f4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x148f5fa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x148f5ff80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x148f604d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x148f60a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x148f60f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x148f614c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x148f61a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x148f61f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x148f624b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x148f62a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x148f62f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x148f634a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x148f639f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x148f63f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x148f643e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x148f64880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x148f64d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x148f651c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x148f65660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x148f65b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x148f65fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x148f66440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x148f668e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x148f66d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x148f67220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x148f676c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x148f67b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x148f68000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x148f684a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x148f68940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x148f68de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x148f69280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x148f69720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x148f69bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x148f6a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x148f6a500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x148f6a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x148f6ae40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x148f6b2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x148f6b830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x148f6bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x148f6c670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x148f6cd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x148f6d4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x148f6d770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x148f6df60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x148f6e220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x148f6e830 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.096.451 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.096.455 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14a804b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14a805000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14a805470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14a8058e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14a805d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14a8061c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14a806630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14a806aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14a806f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14a807380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14a8077f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14a807ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14a808a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14a8091b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14a8099c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14a80a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14a80a800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14a80af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14a80b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14a80bd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14a80c490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14a80cbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14a80d2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14a80d9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14a80e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14a80e3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14a80e690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14a80eb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14a80ef70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14a80f3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14a80f850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14a80fd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14a8101f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14a8104b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14a810920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14a810d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14a811200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14a811670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14a811ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14a811f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14a8123c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14a812830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14a812ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14a813110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14a813580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14a8139f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14a813e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14a8142d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14a814740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14a814bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14a815020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14a815490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14a815900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14a815d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14a8161e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14a816650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14a816bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14a8170c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14a817530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14a8179a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14a817e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14a818280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14a8186f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14a818b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14a818fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14a819440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14a8198b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14a819d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14a81a190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14a81a600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14a81aa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14a81aee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14a81b350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14a81b7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14a81bc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14a81c0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14a81c510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14a81c980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14a81cdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14a81d260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14a81d6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x14a81db40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x14a81dfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x14a81e420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x14a81e890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x14a81ed00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x14a81f170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x14a81f5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x14a81fa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x14a81fec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x14a820330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x14a8207a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x14a820c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x14a821080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x14a8214f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x14a821960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x14a821dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x14a822240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x14a8226b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x14a822b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x14a822f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x14a823400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x14a823870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x14a823ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x14a824150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x14a8245c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x14a824a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x14a824ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x14a825310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x14a825780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x14a825bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x14a826060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x14a8264d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14a826940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14a826db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14a827220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14a827690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14a827b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14a827f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14a8283e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14a828850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14a828cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14a829130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14a8295a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14a829a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14a829e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14a82a2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14a82a760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14a82abd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14a82b040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14a82b4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14a82b920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14a82bd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14a82c200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14a82c670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14a82cae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14a82cf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14a82d3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14a82d830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14a82dca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14a82e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14a82e580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14a82e9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14a82ee60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14a82f2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14a82f740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14a82fbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14a830020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14a830490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14a830900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14a830d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14a8311e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14a831650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14a831ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14a831f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14a8323a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14a832810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14a832c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14a8330f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14a833560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14a8339d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14a833e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14a8342b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14a834720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14a834b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14a835000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14a835c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14a835ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14a8361b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14a836620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14a836a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14a836f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14a837370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14a8377e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14a837c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14a8380c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14a838530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14a8389a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14a838e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14a839280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14a8396f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14a839b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14a839fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14a83a440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14a83a8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14a83ad20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14a83b190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14a83b600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14a83ba70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14a83bee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14a83c350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14a83c7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14a83cc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14a83d0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14a83d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14a83d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14a83ddf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14a83e260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14a83e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14a83eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14a83efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14a83f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14a83f980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14a83fe90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14a840300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x14a840770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14a840be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14a841050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14a841570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14a841a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14a8425f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14a8428b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14a842e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14a843430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14a8439f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14a843fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14a844570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14a844b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14a8450f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14a8456b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14a845c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14a846230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14a8467f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14a846db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14a847370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14a847930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14a847ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14a8484b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14a848a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14a849030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14a8495f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14a849bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14a84a170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14a84a730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14a84acf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14a84b2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14a84b870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14a84be30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14a84c3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14a84c9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14a84cf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14a84d530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14a84daf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14a84e0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14a84e670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14a84ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14a84f1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14a84f7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14a84fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14a850330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14a8508f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14a850eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14a851470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14a851a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14a851ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14a8525b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14a852b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14a853130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14a8536f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14a853cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14a854270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14a854830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14a854df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14a8553b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14a855970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14a855f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14a8564f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x14a856ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x14a856fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14a8574b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14a8579b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14a857eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14a8583b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14a8588b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14a858db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14a8592b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14a8597b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14a859cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14a85a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14a85a6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14a85abb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14a85b0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x14a85b5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x14a85bab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x14a85bfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x14a85c4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x14a85c9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x14a85ceb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x14a85d3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x14a85d8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x14a85ddb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x14a85e2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14a85e7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14a85f1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14a85f8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14a860000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14a860720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14a8609e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x14a8611d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14a861490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14a861aa0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x148e08840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x148e08cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x148e09120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x148e09590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x148e09a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x148e09e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x148e0a2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x148e0a750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x148e0abc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x148e0b0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x148e0b560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x148e0bbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x148e0c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x148e0ceb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x148e0d6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x148e0dde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x148e0e500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x148e0ec20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x148e0f340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x148e0fb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x148e10230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x148e10950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x148e11070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x148e11790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x148e11eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x148e12170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x148e12430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x148e128a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x148e12d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x148e13180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x148e13680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x148e13b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x148e14000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x148e142c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x148e14730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x148e14ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x148e15100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x148e15600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x148e15b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x148e16000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x148e16500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x148e16a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x148e16f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x148e17400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x148e17900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x148e17d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x148e181e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x148e18650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x148e18ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x148e18f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x148e193a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x148e19810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x148e19c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x148e1a0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x148e1a560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x148e1ad30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x148e1b1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x148e1b490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x148e1baa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x148e1c290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x148e1c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x148e1cbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x148e1d070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x148e1d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x148e1d9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x148e1de50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x148e1e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x148e1e790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x148e1ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x148e1f0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x148e1f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x148e1fa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x148e1feb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x148e20400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x148e20950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x148e20ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x148e213f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x148e21940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x148e21e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x148e223e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x148e22930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x148e22e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x148e233d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x148e23920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x148e23e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x148e243c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x148e24910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x148e24e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x148e253b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x148e25900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x148e25e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x148e263a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x148e268f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x148e26e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x148e27390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x148e278e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x148e27e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x148e28380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x148e288d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x148e28e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x148e29370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x148e298c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x148e29e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x148e2a360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x148e2a8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x148e2ae00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x148e2b350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x148e2b8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x148e2bdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x148e2c340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x148e2c890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x148e2cde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x148e2d330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x148e2d7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x148e2dc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x148e2e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x148e2e5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x148e2ea50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x148e2eef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x148e2f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x148e2f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x148e2fcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x148e30170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x148e30610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x148e30ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x148e30f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x148e313f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x148e31890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x148e31d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x148e321d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x148e32670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x148e32b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x148e32fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x148e33450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x148e338f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x148e33d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x148e34230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x148e346d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x148e34b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x148e35010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x148e354b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x148e35950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x148e35df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x148e36290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x148e36730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x148e36bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x148e37070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x148e37510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x148e379b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x148e37e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x148e382f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x148e38790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x148e38c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x148e390d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x148e39570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x148e39a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x148e39eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x148e3a350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x148e3a7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x148e3ac90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x148e3b130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x148e3b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x148e3ba70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x148e3bf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x148e3c3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x148e3c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x148e3ccf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x148e3d190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x148e3d630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x148e3dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x148e3df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x148e3e410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x148e3e8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x148e3ed50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x148e3f1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x148e3f690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x148e3fb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x148e3ffd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x148e40470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x148e40910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x148e40db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x148e41250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x148e416f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x148e41b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x148e42030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x148e424d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x148e42970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x148e42e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x148e432b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x148e43750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x148e43bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x148e44090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x148e44530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x148e44a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x148e44fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x148e45520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x148e45a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x148e45d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x148e46340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x148e46950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x148e46f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x148e47750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x148e47bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x148e47eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x148e484c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x148e48ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x148e492c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x148e49760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x148e49c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x148e4a0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x148e4a850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x148e4ada0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x148e4b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x148e4b840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x148e4bd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x148e4c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x148e4c830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x148e4cd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x148e4d2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x148e4d820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x148e4dd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x148e4e2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x148e4e810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x148e4ed60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x148e4f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x148e4f800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x148e4fd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x148e502a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x148e507f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x148e50d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x148e51290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x148e517e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x148e51d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x148e52280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x148e527d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x148e52d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x148e53270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x148e537c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x148e53d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x148e54260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x148e547b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x148e54d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x148e55250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x148e557a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x148e55cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x148e56240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x148e56790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x148e56ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x148e57230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x148e57780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x148e57cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x148e58220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x148e58770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x148e58cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x148e59210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x148e59760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x148e59cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x148e5a200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x148e5a750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x148e5aca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x148e5b1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x148e5b740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x148e5bc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x148e5c1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x148e5c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x148e5cc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x148e5d1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x148e5d670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x148e5db10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x148e5dfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x148e5e450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x148e5e8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x148e5ed90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x148e5f230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x148e5f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x148e5fb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x148e60010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x148e604b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x148e60950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x148e60df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x148e61290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x148e61730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x148e61bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x148e62070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x148e62510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x148e629b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x148e62e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x148e632f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x148e63790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x148e63c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x148e640d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x148e64570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x148e64ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x148e651e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x148e65900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x148e66020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x148e66740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x148e66a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x148e671f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x148e674b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x148e67ac0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.958s
user	0m0.231s
sys	0m0.188s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
