Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:312 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- ARM -mcpu not found, -mcpu=native will be used
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod - Success
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sve
-- Performing Test GGML_MACHINE_SUPPORTS_sve - Failed
-- ARM feature DOTPROD enabled
-- ARM feature MATMUL_INT8 enabled
-- ARM feature FMA enabled
-- ARM feature FP16_VECTOR_ARITHMETIC enabled
-- Adding CPU backend variant ggml-cpu: -mcpu=native+dotprod+i8mm+nosve 
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (2.3s)
-- Generating done (0.2s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m2.606s
user	0m0.911s
sys	0m1.236s
++ nproc
+ make -j10
[  0%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  0%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  0%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  1%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  2%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  2%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  4%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  5%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  5%] Built target build_info
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o
[  5%] Built target sha1
[  5%] Built target sha256
[  5%] Built target xxhash
[  6%] Linking CXX shared library ../../bin/libggml-base.dylib
[  6%] Built target ggml-base
[  6%] Generate assembly for embedded Metal library
Embedding Metal library
[  6%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[  6%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[  6%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[  7%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 11%] Linking CXX shared library ../../../bin/libggml-blas.dylib
[ 12%] Linking CXX shared library ../../bin/libggml-cpu.dylib
[ 12%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 13%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 13%] Built target ggml-blas
[ 13%] Built target ggml-cpu
[ 13%] Linking C shared library ../../../bin/libggml-metal.dylib
[ 13%] Built target ggml-metal
[ 13%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 14%] Linking CXX shared library ../../bin/libggml.dylib
[ 14%] Built target ggml
[ 14%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o
[ 15%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 20%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o
[ 21%] Linking CXX executable ../../bin/llama-gguf
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 24%] Linking CXX executable ../../bin/llama-gguf-hash
[ 25%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 25%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 26%] Linking CXX shared library ../bin/libllama.dylib
[ 26%] Built target llama-gguf
[ 26%] Built target llama-gguf-hash
[ 26%] Built target llama
[ 26%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 26%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 27%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 29%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 29%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 29%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 30%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 31%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 32%] Linking C executable ../bin/test-c
[ 32%] Linking CXX executable ../../bin/llama-simple
[ 32%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 34%] Linking CXX executable ../../bin/llama-quantize-stats
[ 34%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 35%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 35%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 35%] Linking CXX executable ../../bin/llama-simple-chat
[ 36%] Linking CXX static library libcommon.a
[ 36%] Built target llava
[ 36%] Built target llama-simple
[ 36%] Built target test-c
[ 36%] Built target llama-quantize-stats
[ 36%] Built target llama-simple-chat
[ 37%] Linking CXX static library libllava_static.a
[ 37%] Linking CXX shared library ../../bin/libllava_shared.dylib
[ 37%] Built target common
[ 37%] Built target llava_static
[ 37%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 40%] Built target llava_shared
[ 41%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 45%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 45%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 46%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 46%] Linking CXX executable ../bin/test-tokenizer-0
[ 46%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 46%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 46%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 46%] Linking CXX executable ../bin/test-grammar-integration
[ 46%] Linking CXX executable ../bin/test-llama-grammar
[ 46%] Linking CXX executable ../bin/test-grammar-parser
[ 46%] Linking CXX executable ../bin/test-sampling
[ 46%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 47%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 48%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 49%] Linking CXX executable ../bin/test-log
[ 49%] Built target test-tokenizer-1-spm
[ 49%] Built target test-tokenizer-0
[ 49%] Built target test-tokenizer-1-bpe
[ 49%] Linking CXX executable ../bin/test-arg-parser
[ 49%] Built target test-sampling
[ 49%] Built target test-llama-grammar
[ 49%] Built target test-json-schema-to-grammar
[ 49%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 49%] Built target test-grammar-parser
[ 49%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o
[ 50%] Built target test-grammar-integration
[ 50%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 50%] Built target test-log
[ 51%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 56%] Built target test-arg-parser
[ 56%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 56%] Linking CXX executable ../bin/test-chat-template
[ 57%] Linking CXX executable ../bin/test-autorelease
[ 57%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 58%] Linking CXX executable ../bin/test-gguf
[ 58%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 58%] Linking CXX executable ../bin/test-model-load-cancel
[ 58%] Linking CXX executable ../bin/test-backend-ops
[ 59%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 60%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 61%] Linking CXX executable ../bin/test-quantize-fns
[ 62%] Linking CXX executable ../bin/test-barrier
[ 62%] Linking CXX executable ../bin/test-quantize-perf
[ 62%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 62%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 62%] Built target test-autorelease
[ 62%] Built target test-chat-template
[ 62%] Built target test-backend-ops
[ 62%] Built target test-gguf
[ 62%] Built target test-model-load-cancel
[ 62%] Built target test-quantize-fns
[ 63%] Linking CXX executable ../../bin/llama-batched-bench
[ 63%] Built target test-barrier
[ 64%] Linking CXX executable ../bin/test-rope
[ 64%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 65%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 65%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 66%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 66%] Built target test-quantize-perf
[ 67%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 68%] Linking CXX executable ../../bin/llama-batched
[ 69%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 70%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 70%] Linking CXX executable ../../bin/llama-embedding
[ 70%] Linking CXX executable ../../bin/llama-eval-callback
[ 70%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 71%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 71%] Linking CXX executable ../../bin/llama-gguf-split
[ 71%] Built target test-rope
[ 71%] Built target llama-batched-bench
[ 71%] Linking CXX executable ../../bin/llama-gritlm
[ 71%] Linking CXX executable ../../bin/llama-imatrix
[ 71%] Linking CXX executable ../../bin/llama-infill
[ 71%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 72%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 72%] Built target llama-batched
[ 72%] Built target llama-gbnf-validator
[ 72%] Built target llama-eval-callback
[ 72%] Built target llama-gguf-split
[ 72%] Built target llama-embedding
[ 72%] Built target llama-gritlm
[ 73%] Linking CXX executable ../../bin/llama-bench
[ 73%] Linking CXX executable ../../bin/llama-lookahead
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 75%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 76%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 77%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 77%] Built target llama-imatrix
[ 77%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 77%] Built target llama-infill
[ 77%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 77%] Linking CXX executable ../../bin/llama-lookup-create
[ 77%] Linking CXX executable ../../bin/llama-lookup-stats
[ 77%] Linking CXX executable ../../bin/llama-lookup-merge
[ 77%] Linking CXX executable ../../bin/llama-lookup
[ 78%] Linking CXX executable ../../bin/llama-cli
[ 78%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 78%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 78%] Built target llama-lookahead
[ 79%] Linking CXX executable ../../bin/llama-parallel
[ 79%] Built target llama-bench
[ 80%] Linking CXX executable ../../bin/llama-passkey
[ 81%] Linking CXX executable ../../bin/llama-perplexity
[ 81%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 81%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 81%] Built target llama-lookup-merge
[ 81%] Built target llama-lookup-stats
[ 81%] Built target llama-lookup
[ 81%] Built target llama-cli
[ 81%] Built target llama-lookup-create
[ 81%] Generating loading.html.hpp
[ 82%] Linking CXX executable ../../bin/llama-quantize
[ 82%] Built target llama-parallel
[ 83%] Generating index.html.gz.hpp
[ 84%] Linking CXX executable ../../bin/llama-retrieval
[ 85%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 85%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 86%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 86%] Built target llama-perplexity
[ 86%] Built target llama-passkey
[ 87%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 87%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 87%] Linking CXX executable ../../bin/llama-save-load-state
[ 87%] Linking CXX executable ../../bin/llama-speculative
[ 88%] Building CXX object examples/run/CMakeFiles/llama-run.dir/linenoise.cpp/linenoise.cpp.o
[ 88%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 89%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 89%] Linking CXX executable ../../bin/llama-speculative-simple
[ 89%] Built target llama-quantize
[ 90%] Linking CXX executable ../../bin/llama-tokenize
[ 90%] Built target llama-retrieval
[ 90%] Linking CXX executable ../../bin/llama-run
[ 91%] Linking CXX executable ../../bin/llama-tts
[ 91%] Linking CXX executable ../../bin/llama-gen-docs
[ 91%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 91%] Built target llama-speculative
[ 91%] Built target llama-save-load-state
[ 91%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 91%] Built target llama-speculative-simple
[ 91%] Built target llama-tokenize
[ 92%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 93%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 93%] Built target llama-run
[ 93%] Built target llama-gen-docs
[ 93%] Built target llama-tts
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 95%] Linking CXX executable ../../bin/llama-cvector-generator
[ 96%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 96%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 96%] Linking CXX executable ../../bin/llama-export-lora
[ 96%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 96%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 96%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 96%] Linking CXX executable ../../bin/llama-llava-cli
[ 96%] Built target llama-convert-llama2c-to-ggml
[ 97%] Linking CXX executable ../../bin/llama-vdot
[ 98%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 99%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Built target llama-cvector-generator
[ 99%] Built target llama-export-lora
[ 99%] Built target llama-minicpmv-cli
[ 99%] Built target llama-llava-cli
[ 99%] Built target llama-vdot
[ 99%] Built target llama-qwen2vl-cli
[ 99%] Built target llama-q8dot
[100%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m3.134s
user	0m6.168s
sys	0m9.233s

main: quantize time =  5246.34 ms
main:    total time =  5246.34 ms

main: quantize time =  2583.50 ms
main:    total time =  2583.50 ms

main: quantize time =  1348.91 ms
main:    total time =  1348.91 ms

main: quantize time =  2425.39 ms
main:    total time =  2425.39 ms

main: quantize time =  3361.80 ms
main:    total time =  3361.80 ms

main: quantize time =  4956.14 ms
main:    total time =  4956.14 ms

main: quantize time =  5979.94 ms
main:    total time =  5979.94 ms

main: quantize time =  7127.79 ms
main:    total time =  7127.79 ms

main: quantize time =  5985.50 ms
main:    total time =  5985.50 ms

main: quantize time =  4554.34 ms
main:    total time =  4554.34 ms
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.159 I build: 4569 (2b8525d5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.344 I main: llama backend init
0.00.000.357 I main: load the model and apply lora adapter, if any
0.00.052.035 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.065.357 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.065.373 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.065.377 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.065.377 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.065.378 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.065.379 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.065.379 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.065.383 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.065.384 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.065.384 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.065.385 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.065.386 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.065.387 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.065.388 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.065.392 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.065.393 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.065.394 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.074.522 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.076.804 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.084.667 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.084.671 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.084.671 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.084.672 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.084.672 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.084.673 I llama_model_loader: - type  f32:  194 tensors
0.00.084.673 I llama_model_loader: - type  f16:   98 tensors
0.00.084.675 I print_info: file format = GGUF V3 (latest)
0.00.084.676 I print_info: file type   = all F32 (guessed)
0.00.084.678 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.098.288 I load: special tokens cache size = 25
0.00.106.953 I load: token to piece cache size = 0.2984 MB
0.00.106.956 I print_info: arch             = gptneox
0.00.106.957 I print_info: vocab_only       = 0
0.00.106.957 I print_info: n_ctx_train      = 2048
0.00.106.957 I print_info: n_embd           = 2048
0.00.106.957 I print_info: n_layer          = 24
0.00.106.961 I print_info: n_head           = 16
0.00.106.962 I print_info: n_head_kv        = 16
0.00.106.962 I print_info: n_rot            = 32
0.00.106.962 I print_info: n_swa            = 0
0.00.106.962 I print_info: n_embd_head_k    = 128
0.00.106.963 I print_info: n_embd_head_v    = 128
0.00.106.963 I print_info: n_gqa            = 1
0.00.106.964 I print_info: n_embd_k_gqa     = 2048
0.00.106.965 I print_info: n_embd_v_gqa     = 2048
0.00.106.966 I print_info: f_norm_eps       = 1.0e-05
0.00.106.966 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.106.966 I print_info: f_clamp_kqv      = 0.0e+00
0.00.106.967 I print_info: f_max_alibi_bias = 0.0e+00
0.00.106.967 I print_info: f_logit_scale    = 0.0e+00
0.00.106.968 I print_info: n_ff             = 8192
0.00.106.968 I print_info: n_expert         = 0
0.00.106.968 I print_info: n_expert_used    = 0
0.00.106.968 I print_info: causal attn      = 1
0.00.106.968 I print_info: pooling type     = 0
0.00.106.968 I print_info: rope type        = 2
0.00.106.969 I print_info: rope scaling     = linear
0.00.106.969 I print_info: freq_base_train  = 10000.0
0.00.106.969 I print_info: freq_scale_train = 1
0.00.106.970 I print_info: n_ctx_orig_yarn  = 2048
0.00.106.970 I print_info: rope_finetuned   = unknown
0.00.106.970 I print_info: ssm_d_conv       = 0
0.00.106.970 I print_info: ssm_d_inner      = 0
0.00.106.972 I print_info: ssm_d_state      = 0
0.00.106.972 I print_info: ssm_dt_rank      = 0
0.00.106.972 I print_info: ssm_dt_b_c_rms   = 0
0.00.106.972 I print_info: model type       = 1.4B
0.00.106.973 I print_info: model params     = 1.41 B
0.00.106.973 I print_info: general.name     = 1.4B
0.00.106.973 I print_info: vocab type       = BPE
0.00.106.973 I print_info: n_vocab          = 50304
0.00.106.974 I print_info: n_merges         = 50009
0.00.106.974 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.106.976 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.106.977 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.106.977 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.106.977 I print_info: LF token         = 128 'Ä'
0.00.106.977 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.106.978 I print_info: max token length = 1024
0.00.161.270 I load_tensors: offloading 24 repeating layers to GPU
0.00.161.273 I load_tensors: offloading output layer to GPU
0.00.161.274 I load_tensors: offloaded 25/25 layers to GPU
0.00.161.298 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.161.300 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.161.820 I llama_init_from_model: n_seq_max     = 1
0.00.161.821 I llama_init_from_model: n_ctx         = 2048
0.00.161.822 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.161.822 I llama_init_from_model: n_batch       = 2048
0.00.161.822 I llama_init_from_model: n_ubatch      = 512
0.00.161.822 I llama_init_from_model: flash_attn    = 0
0.00.161.823 I llama_init_from_model: freq_base     = 10000.0
0.00.161.823 I llama_init_from_model: freq_scale    = 1
0.00.161.824 I ggml_metal_init: allocating
0.00.161.848 I ggml_metal_init: found device: Apple M4
0.00.161.853 I ggml_metal_init: picking default device: Apple M4
0.00.162.448 I ggml_metal_init: using embedded metal library
0.00.182.891 I ggml_metal_init: GPU name:   Apple M4
0.00.182.893 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.182.894 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.182.894 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.182.895 I ggml_metal_init: simdgroup reduction   = true
0.00.182.895 I ggml_metal_init: simdgroup matrix mul. = true
0.00.182.895 I ggml_metal_init: has residency sets    = true
0.00.182.895 I ggml_metal_init: has bfloat            = true
0.00.182.895 I ggml_metal_init: use bfloat            = true
0.00.182.896 I ggml_metal_init: hasUnifiedMemory      = true
0.00.182.897 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.249.717 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.282.733 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.282.740 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.282.764 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.286.299 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.286.300 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.286.301 I llama_init_from_model: graph nodes  = 967
0.00.286.301 I llama_init_from_model: graph splits = 2
0.00.286.307 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.286.436 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.286.437 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.351.880 I main: llama threadpool init, n_threads = 4
0.00.351.925 I 
0.00.351.959 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.351.960 I 
0.00.352.138 I sampler seed: 1234
0.00.352.142 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.352.167 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.352.169 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.352.169 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.178.842 I llama_perf_sampler_print:    sampling time =       1.21 ms /    71 runs   (    0.02 ms per token, 58677.69 tokens per second)
0.02.178.843 I llama_perf_context_print:        load time =     298.84 ms
0.02.178.843 I llama_perf_context_print: prompt eval time =      43.67 ms /     7 tokens (    6.24 ms per token,   160.29 tokens per second)
0.02.178.845 I llama_perf_context_print:        eval time =    1780.02 ms /    63 runs   (   28.25 ms per token,    35.39 tokens per second)
0.02.178.845 I llama_perf_context_print:       total time =    1827.95 ms /    70 tokens
0.02.179.050 I ggml_metal_free: deallocating

real	0m2.451s
user	0m0.134s
sys	0m0.146s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.047 I build: 4569 (2b8525d5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.087 I main: llama backend init
0.00.000.089 I main: load the model and apply lora adapter, if any
0.00.009.892 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.026.695 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.026.701 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.026.705 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.026.706 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.026.706 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.026.706 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.026.707 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.026.709 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.026.710 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.026.712 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.026.712 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.026.713 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.026.713 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.026.713 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.026.718 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.026.718 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.026.718 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.030.592 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.031.639 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.035.478 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.035.480 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.035.480 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.035.481 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.035.481 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.035.481 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.035.482 I llama_model_loader: - type  f32:  194 tensors
0.00.035.483 I llama_model_loader: - type q8_0:   98 tensors
0.00.035.484 I print_info: file format = GGUF V3 (latest)
0.00.035.488 I print_info: file type   = Q8_0
0.00.035.489 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.044.803 I load: special tokens cache size = 25
0.00.051.383 I load: token to piece cache size = 0.2984 MB
0.00.051.387 I print_info: arch             = gptneox
0.00.051.387 I print_info: vocab_only       = 0
0.00.051.388 I print_info: n_ctx_train      = 2048
0.00.051.388 I print_info: n_embd           = 2048
0.00.051.388 I print_info: n_layer          = 24
0.00.051.396 I print_info: n_head           = 16
0.00.051.397 I print_info: n_head_kv        = 16
0.00.051.397 I print_info: n_rot            = 32
0.00.051.397 I print_info: n_swa            = 0
0.00.051.397 I print_info: n_embd_head_k    = 128
0.00.051.397 I print_info: n_embd_head_v    = 128
0.00.051.398 I print_info: n_gqa            = 1
0.00.051.399 I print_info: n_embd_k_gqa     = 2048
0.00.051.401 I print_info: n_embd_v_gqa     = 2048
0.00.051.402 I print_info: f_norm_eps       = 1.0e-05
0.00.051.403 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.403 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.403 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.403 I print_info: f_logit_scale    = 0.0e+00
0.00.051.404 I print_info: n_ff             = 8192
0.00.051.406 I print_info: n_expert         = 0
0.00.051.406 I print_info: n_expert_used    = 0
0.00.051.406 I print_info: causal attn      = 1
0.00.051.406 I print_info: pooling type     = 0
0.00.051.406 I print_info: rope type        = 2
0.00.051.407 I print_info: rope scaling     = linear
0.00.051.407 I print_info: freq_base_train  = 10000.0
0.00.051.407 I print_info: freq_scale_train = 1
0.00.051.408 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.408 I print_info: rope_finetuned   = unknown
0.00.051.408 I print_info: ssm_d_conv       = 0
0.00.051.408 I print_info: ssm_d_inner      = 0
0.00.051.408 I print_info: ssm_d_state      = 0
0.00.051.408 I print_info: ssm_dt_rank      = 0
0.00.051.408 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.409 I print_info: model type       = 1.4B
0.00.051.409 I print_info: model params     = 1.41 B
0.00.051.409 I print_info: general.name     = 1.4B
0.00.051.411 I print_info: vocab type       = BPE
0.00.051.411 I print_info: n_vocab          = 50304
0.00.051.411 I print_info: n_merges         = 50009
0.00.051.412 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.412 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.412 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.412 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.412 I print_info: LF token         = 128 'Ä'
0.00.051.413 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.413 I print_info: max token length = 1024
0.01.214.077 I load_tensors: offloading 24 repeating layers to GPU
0.01.214.081 I load_tensors: offloading output layer to GPU
0.01.214.082 I load_tensors: offloaded 25/25 layers to GPU
0.01.214.108 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.01.214.109 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.01.215.138 I llama_init_from_model: n_seq_max     = 1
0.01.215.140 I llama_init_from_model: n_ctx         = 2048
0.01.215.140 I llama_init_from_model: n_ctx_per_seq = 2048
0.01.215.141 I llama_init_from_model: n_batch       = 2048
0.01.215.141 I llama_init_from_model: n_ubatch      = 512
0.01.215.141 I llama_init_from_model: flash_attn    = 0
0.01.215.142 I llama_init_from_model: freq_base     = 10000.0
0.01.215.143 I llama_init_from_model: freq_scale    = 1
0.01.215.143 I ggml_metal_init: allocating
0.01.215.153 I ggml_metal_init: found device: Apple M4
0.01.215.161 I ggml_metal_init: picking default device: Apple M4
0.01.216.442 I ggml_metal_init: using embedded metal library
0.01.221.734 I ggml_metal_init: GPU name:   Apple M4
0.01.221.737 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.221.738 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.221.739 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.221.740 I ggml_metal_init: simdgroup reduction   = true
0.01.221.740 I ggml_metal_init: simdgroup matrix mul. = true
0.01.221.740 I ggml_metal_init: has residency sets    = true
0.01.221.740 I ggml_metal_init: has bfloat            = true
0.01.221.741 I ggml_metal_init: use bfloat            = true
0.01.221.741 I ggml_metal_init: hasUnifiedMemory      = true
0.01.221.742 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.238.444 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.290.725 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.01.290.731 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.01.290.754 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.295.340 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.01.295.342 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.01.295.342 I llama_init_from_model: graph nodes  = 967
0.01.295.342 I llama_init_from_model: graph splits = 2
0.01.295.350 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.01.295.481 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.295.482 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.353.271 I main: llama threadpool init, n_threads = 4
0.01.353.316 I 
0.01.353.340 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.353.342 I 
0.01.353.511 I sampler seed: 1234
0.01.353.516 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.353.559 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.353.561 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.353.561 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.445.018 I llama_perf_sampler_print:    sampling time =       1.35 ms /    71 runs   (    0.02 ms per token, 52475.98 tokens per second)
0.02.445.019 I llama_perf_context_print:        load time =    1342.51 ms
0.02.445.020 I llama_perf_context_print: prompt eval time =      49.41 ms /     7 tokens (    7.06 ms per token,   141.68 tokens per second)
0.02.445.020 I llama_perf_context_print:        eval time =    1039.05 ms /    63 runs   (   16.49 ms per token,    60.63 tokens per second)
0.02.445.020 I llama_perf_context_print:       total time =    1092.61 ms /    70 tokens
0.02.445.253 I ggml_metal_free: deallocating

real	0m2.465s
user	0m0.110s
sys	0m0.268s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.046 I build: 4569 (2b8525d5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.085 I main: llama backend init
0.00.000.087 I main: load the model and apply lora adapter, if any
0.00.019.047 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.036.805 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.036.811 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.036.817 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.036.817 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.036.818 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.036.818 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.036.818 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.036.821 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.036.821 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.036.822 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.036.822 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.036.822 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.036.825 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.036.826 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.036.828 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.036.828 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.036.829 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.041.398 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.042.632 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.047.065 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.047.067 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.047.067 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.047.067 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.047.068 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.047.068 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.047.069 I llama_model_loader: - type  f32:  194 tensors
0.00.047.069 I llama_model_loader: - type q4_0:   97 tensors
0.00.047.069 I llama_model_loader: - type q6_K:    1 tensors
0.00.047.070 I print_info: file format = GGUF V3 (latest)
0.00.047.071 I print_info: file type   = Q4_0
0.00.047.072 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.057.726 I load: special tokens cache size = 25
0.00.066.029 I load: token to piece cache size = 0.2984 MB
0.00.066.033 I print_info: arch             = gptneox
0.00.066.033 I print_info: vocab_only       = 0
0.00.066.033 I print_info: n_ctx_train      = 2048
0.00.066.033 I print_info: n_embd           = 2048
0.00.066.034 I print_info: n_layer          = 24
0.00.066.037 I print_info: n_head           = 16
0.00.066.038 I print_info: n_head_kv        = 16
0.00.066.039 I print_info: n_rot            = 32
0.00.066.039 I print_info: n_swa            = 0
0.00.066.039 I print_info: n_embd_head_k    = 128
0.00.066.039 I print_info: n_embd_head_v    = 128
0.00.066.040 I print_info: n_gqa            = 1
0.00.066.041 I print_info: n_embd_k_gqa     = 2048
0.00.066.042 I print_info: n_embd_v_gqa     = 2048
0.00.066.043 I print_info: f_norm_eps       = 1.0e-05
0.00.066.043 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.066.044 I print_info: f_clamp_kqv      = 0.0e+00
0.00.066.044 I print_info: f_max_alibi_bias = 0.0e+00
0.00.066.046 I print_info: f_logit_scale    = 0.0e+00
0.00.066.047 I print_info: n_ff             = 8192
0.00.066.047 I print_info: n_expert         = 0
0.00.066.048 I print_info: n_expert_used    = 0
0.00.066.048 I print_info: causal attn      = 1
0.00.066.049 I print_info: pooling type     = 0
0.00.066.050 I print_info: rope type        = 2
0.00.066.050 I print_info: rope scaling     = linear
0.00.066.050 I print_info: freq_base_train  = 10000.0
0.00.066.051 I print_info: freq_scale_train = 1
0.00.066.051 I print_info: n_ctx_orig_yarn  = 2048
0.00.066.051 I print_info: rope_finetuned   = unknown
0.00.066.051 I print_info: ssm_d_conv       = 0
0.00.066.051 I print_info: ssm_d_inner      = 0
0.00.066.051 I print_info: ssm_d_state      = 0
0.00.066.052 I print_info: ssm_dt_rank      = 0
0.00.066.052 I print_info: ssm_dt_b_c_rms   = 0
0.00.066.052 I print_info: model type       = 1.4B
0.00.066.054 I print_info: model params     = 1.41 B
0.00.066.054 I print_info: general.name     = 1.4B
0.00.066.055 I print_info: vocab type       = BPE
0.00.066.055 I print_info: n_vocab          = 50304
0.00.066.055 I print_info: n_merges         = 50009
0.00.066.057 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.066.057 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.066.058 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.066.058 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.066.058 I print_info: LF token         = 128 'Ä'
0.00.066.058 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.066.059 I print_info: max token length = 1024
0.00.640.531 I load_tensors: offloading 24 repeating layers to GPU
0.00.640.545 I load_tensors: offloading output layer to GPU
0.00.640.545 I load_tensors: offloaded 25/25 layers to GPU
0.00.640.580 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.640.581 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.641.812 I llama_init_from_model: n_seq_max     = 1
0.00.641.816 I llama_init_from_model: n_ctx         = 2048
0.00.641.817 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.641.817 I llama_init_from_model: n_batch       = 2048
0.00.641.818 I llama_init_from_model: n_ubatch      = 512
0.00.641.818 I llama_init_from_model: flash_attn    = 0
0.00.641.821 I llama_init_from_model: freq_base     = 10000.0
0.00.641.822 I llama_init_from_model: freq_scale    = 1
0.00.641.825 I ggml_metal_init: allocating
0.00.641.900 I ggml_metal_init: found device: Apple M4
0.00.641.913 I ggml_metal_init: picking default device: Apple M4
0.00.643.708 I ggml_metal_init: using embedded metal library
0.00.650.243 I ggml_metal_init: GPU name:   Apple M4
0.00.650.248 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.650.249 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.650.250 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.650.250 I ggml_metal_init: simdgroup reduction   = true
0.00.650.251 I ggml_metal_init: simdgroup matrix mul. = true
0.00.650.251 I ggml_metal_init: has residency sets    = true
0.00.650.251 I ggml_metal_init: has bfloat            = true
0.00.650.252 I ggml_metal_init: use bfloat            = true
0.00.650.253 I ggml_metal_init: hasUnifiedMemory      = true
0.00.650.263 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.668.751 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.734.208 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.734.214 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.734.237 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.738.607 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.738.608 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.738.609 I llama_init_from_model: graph nodes  = 967
0.00.738.609 I llama_init_from_model: graph splits = 2
0.00.738.616 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.738.747 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.738.748 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.796.448 I main: llama threadpool init, n_threads = 4
0.00.796.500 I 
0.00.796.522 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.796.523 I 
0.00.796.672 I sampler seed: 1234
0.00.796.677 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.796.735 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.796.737 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.796.737 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.474.045 I llama_perf_sampler_print:    sampling time =       1.37 ms /    71 runs   (    0.02 ms per token, 51787.02 tokens per second)
0.01.474.046 I llama_perf_context_print:        load time =     776.52 ms
0.01.474.047 I llama_perf_context_print: prompt eval time =      45.09 ms /     7 tokens (    6.44 ms per token,   155.23 tokens per second)
0.01.474.047 I llama_perf_context_print:        eval time =     629.35 ms /    63 runs   (    9.99 ms per token,   100.10 tokens per second)
0.01.474.048 I llama_perf_context_print:       total time =     678.47 ms /    70 tokens
0.01.474.290 I ggml_metal_free: deallocating

real	0m1.500s
user	0m0.117s
sys	0m0.220s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.042 I build: 4569 (2b8525d5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.078 I main: llama backend init
0.00.000.080 I main: load the model and apply lora adapter, if any
0.00.009.286 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.277 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.017.282 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.284 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.284 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.285 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.285 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.285 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.286 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.286 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.287 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.287 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.287 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.288 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.288 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.291 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.291 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.292 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.032 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.071 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.817 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.818 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.819 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.819 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.819 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.820 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.025.820 I llama_model_loader: - type  f32:  194 tensors
0.00.025.820 I llama_model_loader: - type q4_1:   97 tensors
0.00.025.821 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.821 I print_info: file format = GGUF V3 (latest)
0.00.025.822 I print_info: file type   = Q4_1
0.00.025.823 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.033.704 I load: special tokens cache size = 25
0.00.039.425 I load: token to piece cache size = 0.2984 MB
0.00.039.427 I print_info: arch             = gptneox
0.00.039.428 I print_info: vocab_only       = 0
0.00.039.428 I print_info: n_ctx_train      = 2048
0.00.039.428 I print_info: n_embd           = 2048
0.00.039.428 I print_info: n_layer          = 24
0.00.039.431 I print_info: n_head           = 16
0.00.039.432 I print_info: n_head_kv        = 16
0.00.039.432 I print_info: n_rot            = 32
0.00.039.432 I print_info: n_swa            = 0
0.00.039.433 I print_info: n_embd_head_k    = 128
0.00.039.433 I print_info: n_embd_head_v    = 128
0.00.039.433 I print_info: n_gqa            = 1
0.00.039.434 I print_info: n_embd_k_gqa     = 2048
0.00.039.435 I print_info: n_embd_v_gqa     = 2048
0.00.039.436 I print_info: f_norm_eps       = 1.0e-05
0.00.039.436 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.436 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.436 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.436 I print_info: f_logit_scale    = 0.0e+00
0.00.039.437 I print_info: n_ff             = 8192
0.00.039.437 I print_info: n_expert         = 0
0.00.039.437 I print_info: n_expert_used    = 0
0.00.039.438 I print_info: causal attn      = 1
0.00.039.438 I print_info: pooling type     = 0
0.00.039.439 I print_info: rope type        = 2
0.00.039.441 I print_info: rope scaling     = linear
0.00.039.442 I print_info: freq_base_train  = 10000.0
0.00.039.442 I print_info: freq_scale_train = 1
0.00.039.442 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.443 I print_info: rope_finetuned   = unknown
0.00.039.443 I print_info: ssm_d_conv       = 0
0.00.039.443 I print_info: ssm_d_inner      = 0
0.00.039.443 I print_info: ssm_d_state      = 0
0.00.039.443 I print_info: ssm_dt_rank      = 0
0.00.039.443 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.443 I print_info: model type       = 1.4B
0.00.039.444 I print_info: model params     = 1.41 B
0.00.039.444 I print_info: general.name     = 1.4B
0.00.039.445 I print_info: vocab type       = BPE
0.00.039.445 I print_info: n_vocab          = 50304
0.00.039.445 I print_info: n_merges         = 50009
0.00.039.445 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.445 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.445 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.446 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.447 I print_info: LF token         = 128 'Ä'
0.00.039.448 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.449 I print_info: max token length = 1024
0.00.633.258 I load_tensors: offloading 24 repeating layers to GPU
0.00.633.274 I load_tensors: offloading output layer to GPU
0.00.633.275 I load_tensors: offloaded 25/25 layers to GPU
0.00.633.309 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.633.310 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.634.647 I llama_init_from_model: n_seq_max     = 1
0.00.634.652 I llama_init_from_model: n_ctx         = 2048
0.00.634.653 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.634.653 I llama_init_from_model: n_batch       = 2048
0.00.634.653 I llama_init_from_model: n_ubatch      = 512
0.00.634.654 I llama_init_from_model: flash_attn    = 0
0.00.634.657 I llama_init_from_model: freq_base     = 10000.0
0.00.634.657 I llama_init_from_model: freq_scale    = 1
0.00.634.664 I ggml_metal_init: allocating
0.00.634.744 I ggml_metal_init: found device: Apple M4
0.00.634.759 I ggml_metal_init: picking default device: Apple M4
0.00.636.583 I ggml_metal_init: using embedded metal library
0.00.643.114 I ggml_metal_init: GPU name:   Apple M4
0.00.643.119 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.643.120 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.643.121 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.643.121 I ggml_metal_init: simdgroup reduction   = true
0.00.643.122 I ggml_metal_init: simdgroup matrix mul. = true
0.00.643.122 I ggml_metal_init: has residency sets    = true
0.00.643.122 I ggml_metal_init: has bfloat            = true
0.00.643.123 I ggml_metal_init: use bfloat            = true
0.00.643.124 I ggml_metal_init: hasUnifiedMemory      = true
0.00.643.134 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.661.632 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.714.918 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.714.925 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.714.948 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.719.120 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.719.123 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.719.123 I llama_init_from_model: graph nodes  = 967
0.00.719.123 I llama_init_from_model: graph splits = 2
0.00.719.134 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.719.266 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.719.267 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.774.795 I main: llama threadpool init, n_threads = 4
0.00.774.836 I 
0.00.774.860 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.774.862 I 
0.00.775.033 I sampler seed: 1234
0.00.775.038 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.775.081 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.775.084 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.775.084 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.500.913 I llama_perf_sampler_print:    sampling time =       1.29 ms /    71 runs   (    0.02 ms per token, 55124.22 tokens per second)
0.01.500.913 I llama_perf_context_print:        load time =     764.61 ms
0.01.500.914 I llama_perf_context_print: prompt eval time =      49.10 ms /     7 tokens (    7.01 ms per token,   142.57 tokens per second)
0.01.500.915 I llama_perf_context_print:        eval time =     673.90 ms /    63 runs   (   10.70 ms per token,    93.48 tokens per second)
0.01.500.916 I llama_perf_context_print:       total time =     727.02 ms /    70 tokens
0.01.501.193 I ggml_metal_free: deallocating

real	0m1.517s
user	0m0.109s
sys	0m0.197s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4569 (2b8525d5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.077 I main: llama backend init
0.00.000.079 I main: load the model and apply lora adapter, if any
0.00.010.148 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.919 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.017.924 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.930 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.930 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.931 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.931 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.931 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.934 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.934 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.935 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.935 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.936 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.936 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.936 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.938 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.938 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.938 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.765 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.803 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.612 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.614 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.614 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.614 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.614 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.615 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.026.615 I llama_model_loader: - type  f32:  194 tensors
0.00.026.616 I llama_model_loader: - type q5_0:   97 tensors
0.00.026.616 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.616 I print_info: file format = GGUF V3 (latest)
0.00.026.617 I print_info: file type   = Q5_0
0.00.026.618 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.034.383 I load: special tokens cache size = 25
0.00.040.293 I load: token to piece cache size = 0.2984 MB
0.00.040.296 I print_info: arch             = gptneox
0.00.040.297 I print_info: vocab_only       = 0
0.00.040.297 I print_info: n_ctx_train      = 2048
0.00.040.297 I print_info: n_embd           = 2048
0.00.040.297 I print_info: n_layer          = 24
0.00.040.300 I print_info: n_head           = 16
0.00.040.300 I print_info: n_head_kv        = 16
0.00.040.301 I print_info: n_rot            = 32
0.00.040.301 I print_info: n_swa            = 0
0.00.040.301 I print_info: n_embd_head_k    = 128
0.00.040.301 I print_info: n_embd_head_v    = 128
0.00.040.302 I print_info: n_gqa            = 1
0.00.040.303 I print_info: n_embd_k_gqa     = 2048
0.00.040.303 I print_info: n_embd_v_gqa     = 2048
0.00.040.304 I print_info: f_norm_eps       = 1.0e-05
0.00.040.304 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.306 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.307 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.307 I print_info: f_logit_scale    = 0.0e+00
0.00.040.307 I print_info: n_ff             = 8192
0.00.040.308 I print_info: n_expert         = 0
0.00.040.308 I print_info: n_expert_used    = 0
0.00.040.308 I print_info: causal attn      = 1
0.00.040.308 I print_info: pooling type     = 0
0.00.040.308 I print_info: rope type        = 2
0.00.040.309 I print_info: rope scaling     = linear
0.00.040.309 I print_info: freq_base_train  = 10000.0
0.00.040.309 I print_info: freq_scale_train = 1
0.00.040.309 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.310 I print_info: rope_finetuned   = unknown
0.00.040.310 I print_info: ssm_d_conv       = 0
0.00.040.310 I print_info: ssm_d_inner      = 0
0.00.040.310 I print_info: ssm_d_state      = 0
0.00.040.310 I print_info: ssm_dt_rank      = 0
0.00.040.310 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.311 I print_info: model type       = 1.4B
0.00.040.311 I print_info: model params     = 1.41 B
0.00.040.311 I print_info: general.name     = 1.4B
0.00.040.312 I print_info: vocab type       = BPE
0.00.040.312 I print_info: n_vocab          = 50304
0.00.040.312 I print_info: n_merges         = 50009
0.00.040.313 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.313 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.313 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.313 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.313 I print_info: LF token         = 128 'Ä'
0.00.040.315 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.315 I print_info: max token length = 1024
0.00.702.710 I load_tensors: offloading 24 repeating layers to GPU
0.00.702.726 I load_tensors: offloading output layer to GPU
0.00.702.726 I load_tensors: offloaded 25/25 layers to GPU
0.00.702.761 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.702.762 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.704.099 I llama_init_from_model: n_seq_max     = 1
0.00.704.106 I llama_init_from_model: n_ctx         = 2048
0.00.704.106 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.704.107 I llama_init_from_model: n_batch       = 2048
0.00.704.107 I llama_init_from_model: n_ubatch      = 512
0.00.704.107 I llama_init_from_model: flash_attn    = 0
0.00.704.110 I llama_init_from_model: freq_base     = 10000.0
0.00.704.110 I llama_init_from_model: freq_scale    = 1
0.00.704.113 I ggml_metal_init: allocating
0.00.704.193 I ggml_metal_init: found device: Apple M4
0.00.704.206 I ggml_metal_init: picking default device: Apple M4
0.00.706.075 I ggml_metal_init: using embedded metal library
0.00.712.764 I ggml_metal_init: GPU name:   Apple M4
0.00.712.770 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.712.770 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.712.771 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.712.772 I ggml_metal_init: simdgroup reduction   = true
0.00.712.772 I ggml_metal_init: simdgroup matrix mul. = true
0.00.712.772 I ggml_metal_init: has residency sets    = true
0.00.712.773 I ggml_metal_init: has bfloat            = true
0.00.712.773 I ggml_metal_init: use bfloat            = true
0.00.712.774 I ggml_metal_init: hasUnifiedMemory      = true
0.00.712.775 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.730.859 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.793.187 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.793.194 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.793.218 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.798.898 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.798.901 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.798.902 I llama_init_from_model: graph nodes  = 967
0.00.798.902 I llama_init_from_model: graph splits = 2
0.00.798.908 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.799.043 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.799.044 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.857.821 I main: llama threadpool init, n_threads = 4
0.00.857.864 I 
0.00.857.887 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.857.888 I 
0.00.858.043 I sampler seed: 1234
0.00.858.048 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.858.059 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.858.059 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.858.059 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.646.928 I llama_perf_sampler_print:    sampling time =       1.36 ms /    71 runs   (    0.02 ms per token, 52205.88 tokens per second)
0.01.646.929 I llama_perf_context_print:        load time =     846.78 ms
0.01.646.930 I llama_perf_context_print: prompt eval time =      52.86 ms /     7 tokens (    7.55 ms per token,   132.44 tokens per second)
0.01.646.930 I llama_perf_context_print:        eval time =     733.07 ms /    63 runs   (   11.64 ms per token,    85.94 tokens per second)
0.01.646.931 I llama_perf_context_print:       total time =     790.00 ms /    70 tokens
0.01.647.161 I ggml_metal_free: deallocating

real	0m1.666s
user	0m0.110s
sys	0m0.238s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4569 (2b8525d5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.077 I main: llama backend init
0.00.000.079 I main: load the model and apply lora adapter, if any
0.00.009.165 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.619 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.623 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.625 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.625 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.626 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.626 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.626 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.627 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.628 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.628 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.628 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.629 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.629 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.630 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.632 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.633 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.633 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.360 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.370 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.038 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.040 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.040 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.040 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.040 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.041 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.041 I llama_model_loader: - type  f32:  194 tensors
0.00.025.042 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.042 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.042 I print_info: file format = GGUF V3 (latest)
0.00.025.043 I print_info: file type   = Q5_1
0.00.025.044 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.032.761 I load: special tokens cache size = 25
0.00.038.697 I load: token to piece cache size = 0.2984 MB
0.00.038.700 I print_info: arch             = gptneox
0.00.038.700 I print_info: vocab_only       = 0
0.00.038.701 I print_info: n_ctx_train      = 2048
0.00.038.701 I print_info: n_embd           = 2048
0.00.038.701 I print_info: n_layer          = 24
0.00.038.704 I print_info: n_head           = 16
0.00.038.705 I print_info: n_head_kv        = 16
0.00.038.705 I print_info: n_rot            = 32
0.00.038.705 I print_info: n_swa            = 0
0.00.038.705 I print_info: n_embd_head_k    = 128
0.00.038.706 I print_info: n_embd_head_v    = 128
0.00.038.706 I print_info: n_gqa            = 1
0.00.038.707 I print_info: n_embd_k_gqa     = 2048
0.00.038.708 I print_info: n_embd_v_gqa     = 2048
0.00.038.708 I print_info: f_norm_eps       = 1.0e-05
0.00.038.709 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.710 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.710 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.711 I print_info: f_logit_scale    = 0.0e+00
0.00.038.711 I print_info: n_ff             = 8192
0.00.038.712 I print_info: n_expert         = 0
0.00.038.712 I print_info: n_expert_used    = 0
0.00.038.712 I print_info: causal attn      = 1
0.00.038.712 I print_info: pooling type     = 0
0.00.038.714 I print_info: rope type        = 2
0.00.038.716 I print_info: rope scaling     = linear
0.00.038.716 I print_info: freq_base_train  = 10000.0
0.00.038.717 I print_info: freq_scale_train = 1
0.00.038.717 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.717 I print_info: rope_finetuned   = unknown
0.00.038.717 I print_info: ssm_d_conv       = 0
0.00.038.717 I print_info: ssm_d_inner      = 0
0.00.038.717 I print_info: ssm_d_state      = 0
0.00.038.718 I print_info: ssm_dt_rank      = 0
0.00.038.718 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.718 I print_info: model type       = 1.4B
0.00.038.718 I print_info: model params     = 1.41 B
0.00.038.719 I print_info: general.name     = 1.4B
0.00.038.723 I print_info: vocab type       = BPE
0.00.038.723 I print_info: n_vocab          = 50304
0.00.038.723 I print_info: n_merges         = 50009
0.00.038.723 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.723 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.724 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.724 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.724 I print_info: LF token         = 128 'Ä'
0.00.038.728 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.731 I print_info: max token length = 1024
0.00.618.635 I load_tensors: offloading 24 repeating layers to GPU
0.00.618.639 I load_tensors: offloading output layer to GPU
0.00.618.640 I load_tensors: offloaded 25/25 layers to GPU
0.00.618.660 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.618.661 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.619.987 I llama_init_from_model: n_seq_max     = 1
0.00.619.989 I llama_init_from_model: n_ctx         = 2048
0.00.619.990 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.619.990 I llama_init_from_model: n_batch       = 2048
0.00.619.991 I llama_init_from_model: n_ubatch      = 512
0.00.619.991 I llama_init_from_model: flash_attn    = 0
0.00.619.992 I llama_init_from_model: freq_base     = 10000.0
0.00.619.992 I llama_init_from_model: freq_scale    = 1
0.00.619.998 I ggml_metal_init: allocating
0.00.620.018 I ggml_metal_init: found device: Apple M4
0.00.620.027 I ggml_metal_init: picking default device: Apple M4
0.00.621.551 I ggml_metal_init: using embedded metal library
0.00.627.597 I ggml_metal_init: GPU name:   Apple M4
0.00.627.601 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.627.601 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.627.602 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.627.603 I ggml_metal_init: simdgroup reduction   = true
0.00.627.603 I ggml_metal_init: simdgroup matrix mul. = true
0.00.627.603 I ggml_metal_init: has residency sets    = true
0.00.627.603 I ggml_metal_init: has bfloat            = true
0.00.627.604 I ggml_metal_init: use bfloat            = true
0.00.627.604 I ggml_metal_init: hasUnifiedMemory      = true
0.00.627.606 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.644.596 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.698.404 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.698.410 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.698.437 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.702.997 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.702.999 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.702.999 I llama_init_from_model: graph nodes  = 967
0.00.702.999 I llama_init_from_model: graph splits = 2
0.00.703.004 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.703.137 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.703.138 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.760.834 I main: llama threadpool init, n_threads = 4
0.00.760.871 I 
0.00.760.891 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.760.892 I 
0.00.761.072 I sampler seed: 1234
0.00.761.076 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.761.087 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.761.089 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.761.089 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.590.082 I llama_perf_sampler_print:    sampling time =       1.34 ms /    71 runs   (    0.02 ms per token, 52945.56 tokens per second)
0.01.590.083 I llama_perf_context_print:        load time =     750.74 ms
0.01.590.083 I llama_perf_context_print: prompt eval time =      41.95 ms /     7 tokens (    5.99 ms per token,   166.88 tokens per second)
0.01.590.084 I llama_perf_context_print:        eval time =     784.15 ms /    63 runs   (   12.45 ms per token,    80.34 tokens per second)
0.01.590.084 I llama_perf_context_print:       total time =     830.18 ms /    70 tokens
0.01.590.323 I ggml_metal_free: deallocating

real	0m1.607s
user	0m0.107s
sys	0m0.217s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.043 I build: 4569 (2b8525d5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.080 I main: llama backend init
0.00.000.082 I main: load the model and apply lora adapter, if any
0.00.009.936 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.481 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.486 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.488 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.488 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.489 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.489 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.490 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.490 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.491 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.491 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.492 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.492 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.492 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.493 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.494 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.495 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.495 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.386 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.387 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.180 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.181 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.181 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.182 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.182 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.182 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.183 I llama_model_loader: - type  f32:  194 tensors
0.00.025.183 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.183 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.183 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.184 I print_info: file format = GGUF V3 (latest)
0.00.025.184 I print_info: file type   = Q2_K - Medium
0.00.025.185 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.033.369 I load: special tokens cache size = 25
0.00.039.269 I load: token to piece cache size = 0.2984 MB
0.00.039.274 I print_info: arch             = gptneox
0.00.039.274 I print_info: vocab_only       = 0
0.00.039.274 I print_info: n_ctx_train      = 2048
0.00.039.274 I print_info: n_embd           = 2048
0.00.039.276 I print_info: n_layer          = 24
0.00.039.279 I print_info: n_head           = 16
0.00.039.280 I print_info: n_head_kv        = 16
0.00.039.280 I print_info: n_rot            = 32
0.00.039.280 I print_info: n_swa            = 0
0.00.039.280 I print_info: n_embd_head_k    = 128
0.00.039.280 I print_info: n_embd_head_v    = 128
0.00.039.281 I print_info: n_gqa            = 1
0.00.039.282 I print_info: n_embd_k_gqa     = 2048
0.00.039.283 I print_info: n_embd_v_gqa     = 2048
0.00.039.283 I print_info: f_norm_eps       = 1.0e-05
0.00.039.284 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.284 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.284 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.284 I print_info: f_logit_scale    = 0.0e+00
0.00.039.285 I print_info: n_ff             = 8192
0.00.039.285 I print_info: n_expert         = 0
0.00.039.285 I print_info: n_expert_used    = 0
0.00.039.285 I print_info: causal attn      = 1
0.00.039.287 I print_info: pooling type     = 0
0.00.039.287 I print_info: rope type        = 2
0.00.039.288 I print_info: rope scaling     = linear
0.00.039.288 I print_info: freq_base_train  = 10000.0
0.00.039.288 I print_info: freq_scale_train = 1
0.00.039.289 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.289 I print_info: rope_finetuned   = unknown
0.00.039.289 I print_info: ssm_d_conv       = 0
0.00.039.289 I print_info: ssm_d_inner      = 0
0.00.039.289 I print_info: ssm_d_state      = 0
0.00.039.289 I print_info: ssm_dt_rank      = 0
0.00.039.290 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.290 I print_info: model type       = 1.4B
0.00.039.290 I print_info: model params     = 1.41 B
0.00.039.290 I print_info: general.name     = 1.4B
0.00.039.291 I print_info: vocab type       = BPE
0.00.039.291 I print_info: n_vocab          = 50304
0.00.039.291 I print_info: n_merges         = 50009
0.00.039.291 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.292 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.292 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.292 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.292 I print_info: LF token         = 128 'Ä'
0.00.039.293 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.293 I print_info: max token length = 1024
0.00.359.337 I load_tensors: offloading 24 repeating layers to GPU
0.00.359.353 I load_tensors: offloading output layer to GPU
0.00.359.354 I load_tensors: offloaded 25/25 layers to GPU
0.00.359.387 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.359.388 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.360.723 I llama_init_from_model: n_seq_max     = 1
0.00.360.730 I llama_init_from_model: n_ctx         = 2048
0.00.360.730 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.360.731 I llama_init_from_model: n_batch       = 2048
0.00.360.731 I llama_init_from_model: n_ubatch      = 512
0.00.360.731 I llama_init_from_model: flash_attn    = 0
0.00.360.733 I llama_init_from_model: freq_base     = 10000.0
0.00.360.733 I llama_init_from_model: freq_scale    = 1
0.00.360.736 I ggml_metal_init: allocating
0.00.360.783 I ggml_metal_init: found device: Apple M4
0.00.360.797 I ggml_metal_init: picking default device: Apple M4
0.00.363.013 I ggml_metal_init: using embedded metal library
0.00.369.001 I ggml_metal_init: GPU name:   Apple M4
0.00.369.011 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.369.012 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.369.012 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.369.013 I ggml_metal_init: simdgroup reduction   = true
0.00.369.014 I ggml_metal_init: simdgroup matrix mul. = true
0.00.369.014 I ggml_metal_init: has residency sets    = true
0.00.369.014 I ggml_metal_init: has bfloat            = true
0.00.369.014 I ggml_metal_init: use bfloat            = true
0.00.369.017 I ggml_metal_init: hasUnifiedMemory      = true
0.00.369.021 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.391.367 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.451.992 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.452.001 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.452.023 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.456.471 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.456.473 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.456.473 I llama_init_from_model: graph nodes  = 967
0.00.456.473 I llama_init_from_model: graph splits = 2
0.00.456.480 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.456.612 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.456.613 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.516.972 I main: llama threadpool init, n_threads = 4
0.00.517.015 I 
0.00.517.040 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.517.041 I 
0.00.517.212 I sampler seed: 1234
0.00.517.216 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.517.234 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.517.235 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.517.235 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.200.660 I llama_perf_sampler_print:    sampling time =       1.34 ms /    71 runs   (    0.02 ms per token, 52985.07 tokens per second)
0.01.200.660 I llama_perf_context_print:        load time =     506.08 ms
0.01.200.662 I llama_perf_context_print: prompt eval time =      44.22 ms /     7 tokens (    6.32 ms per token,   158.32 tokens per second)
0.01.200.663 I llama_perf_context_print:        eval time =     636.34 ms /    63 runs   (   10.10 ms per token,    99.00 tokens per second)
0.01.200.663 I llama_perf_context_print:       total time =     684.64 ms /    70 tokens
0.01.200.898 I ggml_metal_free: deallocating

real	0m1.218s
user	0m0.113s
sys	0m0.184s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4569 (2b8525d5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.075 I main: llama backend init
0.00.000.078 I main: load the model and apply lora adapter, if any
0.00.008.768 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.275 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.280 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.282 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.282 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.283 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.283 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.283 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.284 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.284 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.285 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.285 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.286 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.288 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.289 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.291 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.291 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.292 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.163 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.193 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.931 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.932 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.933 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.933 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.933 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.934 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.934 I llama_model_loader: - type  f32:  194 tensors
0.00.024.934 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.935 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.935 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.935 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.936 I print_info: file format = GGUF V3 (latest)
0.00.024.936 I print_info: file type   = Q3_K - Medium
0.00.024.937 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.032.700 I load: special tokens cache size = 25
0.00.038.383 I load: token to piece cache size = 0.2984 MB
0.00.038.386 I print_info: arch             = gptneox
0.00.038.386 I print_info: vocab_only       = 0
0.00.038.386 I print_info: n_ctx_train      = 2048
0.00.038.386 I print_info: n_embd           = 2048
0.00.038.386 I print_info: n_layer          = 24
0.00.038.389 I print_info: n_head           = 16
0.00.038.390 I print_info: n_head_kv        = 16
0.00.038.390 I print_info: n_rot            = 32
0.00.038.390 I print_info: n_swa            = 0
0.00.038.390 I print_info: n_embd_head_k    = 128
0.00.038.391 I print_info: n_embd_head_v    = 128
0.00.038.391 I print_info: n_gqa            = 1
0.00.038.392 I print_info: n_embd_k_gqa     = 2048
0.00.038.393 I print_info: n_embd_v_gqa     = 2048
0.00.038.393 I print_info: f_norm_eps       = 1.0e-05
0.00.038.394 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.394 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.394 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.394 I print_info: f_logit_scale    = 0.0e+00
0.00.038.396 I print_info: n_ff             = 8192
0.00.038.396 I print_info: n_expert         = 0
0.00.038.396 I print_info: n_expert_used    = 0
0.00.038.398 I print_info: causal attn      = 1
0.00.038.399 I print_info: pooling type     = 0
0.00.038.400 I print_info: rope type        = 2
0.00.038.400 I print_info: rope scaling     = linear
0.00.038.400 I print_info: freq_base_train  = 10000.0
0.00.038.400 I print_info: freq_scale_train = 1
0.00.038.401 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.401 I print_info: rope_finetuned   = unknown
0.00.038.401 I print_info: ssm_d_conv       = 0
0.00.038.401 I print_info: ssm_d_inner      = 0
0.00.038.401 I print_info: ssm_d_state      = 0
0.00.038.401 I print_info: ssm_dt_rank      = 0
0.00.038.402 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.402 I print_info: model type       = 1.4B
0.00.038.402 I print_info: model params     = 1.41 B
0.00.038.402 I print_info: general.name     = 1.4B
0.00.038.403 I print_info: vocab type       = BPE
0.00.038.403 I print_info: n_vocab          = 50304
0.00.038.403 I print_info: n_merges         = 50009
0.00.038.404 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.404 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.404 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.404 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.405 I print_info: LF token         = 128 'Ä'
0.00.038.405 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.405 I print_info: max token length = 1024
0.00.457.899 I load_tensors: offloading 24 repeating layers to GPU
0.00.457.906 I load_tensors: offloading output layer to GPU
0.00.457.906 I load_tensors: offloaded 25/25 layers to GPU
0.00.457.939 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.457.940 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.459.331 I llama_init_from_model: n_seq_max     = 1
0.00.459.334 I llama_init_from_model: n_ctx         = 2048
0.00.459.335 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.459.335 I llama_init_from_model: n_batch       = 2048
0.00.459.335 I llama_init_from_model: n_ubatch      = 512
0.00.459.336 I llama_init_from_model: flash_attn    = 0
0.00.459.340 I llama_init_from_model: freq_base     = 10000.0
0.00.459.341 I llama_init_from_model: freq_scale    = 1
0.00.459.347 I ggml_metal_init: allocating
0.00.459.401 I ggml_metal_init: found device: Apple M4
0.00.459.416 I ggml_metal_init: picking default device: Apple M4
0.00.461.497 I ggml_metal_init: using embedded metal library
0.00.467.542 I ggml_metal_init: GPU name:   Apple M4
0.00.467.553 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.467.553 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.467.554 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.467.555 I ggml_metal_init: simdgroup reduction   = true
0.00.467.555 I ggml_metal_init: simdgroup matrix mul. = true
0.00.467.556 I ggml_metal_init: has residency sets    = true
0.00.467.556 I ggml_metal_init: has bfloat            = true
0.00.467.556 I ggml_metal_init: use bfloat            = true
0.00.467.559 I ggml_metal_init: hasUnifiedMemory      = true
0.00.467.563 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.487.915 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.540.653 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.540.661 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.540.691 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.545.040 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.545.042 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.545.042 I llama_init_from_model: graph nodes  = 967
0.00.545.042 I llama_init_from_model: graph splits = 2
0.00.545.048 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.545.165 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.545.165 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.600.727 I main: llama threadpool init, n_threads = 4
0.00.600.767 I 
0.00.600.791 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.600.791 I 
0.00.600.937 I sampler seed: 1234
0.00.600.942 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.600.979 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.600.983 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.600.983 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.346.287 I llama_perf_sampler_print:    sampling time =       1.35 ms /    71 runs   (    0.02 ms per token, 52592.59 tokens per second)
0.01.346.288 I llama_perf_context_print:        load time =     591.05 ms
0.01.346.288 I llama_perf_context_print: prompt eval time =      50.25 ms /     7 tokens (    7.18 ms per token,   139.31 tokens per second)
0.01.346.289 I llama_perf_context_print:        eval time =     692.13 ms /    63 runs   (   10.99 ms per token,    91.02 tokens per second)
0.01.346.290 I llama_perf_context_print:       total time =     746.47 ms /    70 tokens
0.01.346.512 I ggml_metal_free: deallocating

real	0m1.363s
user	0m0.110s
sys	0m0.195s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4569 (2b8525d5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.077 I main: llama backend init
0.00.000.079 I main: load the model and apply lora adapter, if any
0.00.008.832 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.581 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.587 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.588 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.589 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.589 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.590 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.590 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.591 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.591 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.592 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.592 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.592 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.594 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.594 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.598 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.598 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.598 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.311 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.297 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.955 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.956 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.956 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.956 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.957 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.957 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.958 I llama_model_loader: - type  f32:  194 tensors
0.00.024.958 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.958 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.958 I llama_model_loader: - type q6_K:   13 tensors
0.00.024.959 I print_info: file format = GGUF V3 (latest)
0.00.024.959 I print_info: file type   = Q4_K - Medium
0.00.024.960 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.032.702 I load: special tokens cache size = 25
0.00.038.628 I load: token to piece cache size = 0.2984 MB
0.00.038.631 I print_info: arch             = gptneox
0.00.038.631 I print_info: vocab_only       = 0
0.00.038.631 I print_info: n_ctx_train      = 2048
0.00.038.631 I print_info: n_embd           = 2048
0.00.038.632 I print_info: n_layer          = 24
0.00.038.634 I print_info: n_head           = 16
0.00.038.635 I print_info: n_head_kv        = 16
0.00.038.635 I print_info: n_rot            = 32
0.00.038.635 I print_info: n_swa            = 0
0.00.038.636 I print_info: n_embd_head_k    = 128
0.00.038.636 I print_info: n_embd_head_v    = 128
0.00.038.636 I print_info: n_gqa            = 1
0.00.038.637 I print_info: n_embd_k_gqa     = 2048
0.00.038.638 I print_info: n_embd_v_gqa     = 2048
0.00.038.638 I print_info: f_norm_eps       = 1.0e-05
0.00.038.639 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.639 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.639 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.639 I print_info: f_logit_scale    = 0.0e+00
0.00.038.640 I print_info: n_ff             = 8192
0.00.038.640 I print_info: n_expert         = 0
0.00.038.640 I print_info: n_expert_used    = 0
0.00.038.640 I print_info: causal attn      = 1
0.00.038.642 I print_info: pooling type     = 0
0.00.038.644 I print_info: rope type        = 2
0.00.038.644 I print_info: rope scaling     = linear
0.00.038.645 I print_info: freq_base_train  = 10000.0
0.00.038.645 I print_info: freq_scale_train = 1
0.00.038.645 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.645 I print_info: rope_finetuned   = unknown
0.00.038.646 I print_info: ssm_d_conv       = 0
0.00.038.646 I print_info: ssm_d_inner      = 0
0.00.038.646 I print_info: ssm_d_state      = 0
0.00.038.646 I print_info: ssm_dt_rank      = 0
0.00.038.646 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.646 I print_info: model type       = 1.4B
0.00.038.647 I print_info: model params     = 1.41 B
0.00.038.647 I print_info: general.name     = 1.4B
0.00.038.647 I print_info: vocab type       = BPE
0.00.038.648 I print_info: n_vocab          = 50304
0.00.038.648 I print_info: n_merges         = 50009
0.00.038.648 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.648 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.648 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.649 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.649 I print_info: LF token         = 128 'Ä'
0.00.038.649 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.650 I print_info: max token length = 1024
0.00.523.862 I load_tensors: offloading 24 repeating layers to GPU
0.00.523.875 I load_tensors: offloading output layer to GPU
0.00.523.876 I load_tensors: offloaded 25/25 layers to GPU
0.00.523.905 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.523.906 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.525.347 I llama_init_from_model: n_seq_max     = 1
0.00.525.354 I llama_init_from_model: n_ctx         = 2048
0.00.525.355 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.525.355 I llama_init_from_model: n_batch       = 2048
0.00.525.356 I llama_init_from_model: n_ubatch      = 512
0.00.525.356 I llama_init_from_model: flash_attn    = 0
0.00.525.357 I llama_init_from_model: freq_base     = 10000.0
0.00.525.361 I llama_init_from_model: freq_scale    = 1
0.00.525.364 I ggml_metal_init: allocating
0.00.525.414 I ggml_metal_init: found device: Apple M4
0.00.525.428 I ggml_metal_init: picking default device: Apple M4
0.00.527.264 I ggml_metal_init: using embedded metal library
0.00.533.316 I ggml_metal_init: GPU name:   Apple M4
0.00.533.322 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.533.323 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.533.324 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.533.325 I ggml_metal_init: simdgroup reduction   = true
0.00.533.325 I ggml_metal_init: simdgroup matrix mul. = true
0.00.533.325 I ggml_metal_init: has residency sets    = true
0.00.533.326 I ggml_metal_init: has bfloat            = true
0.00.533.326 I ggml_metal_init: use bfloat            = true
0.00.533.327 I ggml_metal_init: hasUnifiedMemory      = true
0.00.533.329 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.552.966 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.614.477 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.614.484 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.614.514 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.618.769 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.618.770 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.618.771 I llama_init_from_model: graph nodes  = 967
0.00.618.771 I llama_init_from_model: graph splits = 2
0.00.618.780 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.618.912 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.618.913 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.680.614 I main: llama threadpool init, n_threads = 4
0.00.680.654 I 
0.00.680.676 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.680.677 I 
0.00.680.843 I sampler seed: 1234
0.00.680.848 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.680.859 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.680.859 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.680.859 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.444.282 I llama_perf_sampler_print:    sampling time =       1.51 ms /    71 runs   (    0.02 ms per token, 47144.75 tokens per second)
0.01.444.283 I llama_perf_context_print:        load time =     670.91 ms
0.01.444.284 I llama_perf_context_print: prompt eval time =      57.47 ms /     7 tokens (    8.21 ms per token,   121.80 tokens per second)
0.01.444.285 I llama_perf_context_print:        eval time =     703.46 ms /    63 runs   (   11.17 ms per token,    89.56 tokens per second)
0.01.444.285 I llama_perf_context_print:       total time =     764.54 ms /    70 tokens
0.01.444.519 I ggml_metal_free: deallocating

real	0m1.460s
user	0m0.110s
sys	0m0.203s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4569 (2b8525d5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.092 I main: llama backend init
0.00.000.095 I main: load the model and apply lora adapter, if any
0.00.012.840 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.022.149 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.022.155 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.022.158 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.022.159 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.022.159 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.022.159 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.022.160 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.022.162 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.022.162 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.022.162 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.022.163 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.022.163 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.022.163 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.022.164 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.022.167 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.022.167 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.022.167 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.026.086 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.027.164 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.031.046 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.031.048 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.031.048 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.031.048 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.031.049 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.031.049 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.031.050 I llama_model_loader: - type  f32:  194 tensors
0.00.031.050 I llama_model_loader: - type q5_K:   61 tensors
0.00.031.050 I llama_model_loader: - type q6_K:   37 tensors
0.00.031.051 I print_info: file format = GGUF V3 (latest)
0.00.031.051 I print_info: file type   = Q5_K - Medium
0.00.031.053 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.039.651 I load: special tokens cache size = 25
0.00.045.800 I load: token to piece cache size = 0.2984 MB
0.00.045.805 I print_info: arch             = gptneox
0.00.045.805 I print_info: vocab_only       = 0
0.00.045.806 I print_info: n_ctx_train      = 2048
0.00.045.806 I print_info: n_embd           = 2048
0.00.045.806 I print_info: n_layer          = 24
0.00.045.810 I print_info: n_head           = 16
0.00.045.812 I print_info: n_head_kv        = 16
0.00.045.812 I print_info: n_rot            = 32
0.00.045.813 I print_info: n_swa            = 0
0.00.045.813 I print_info: n_embd_head_k    = 128
0.00.045.813 I print_info: n_embd_head_v    = 128
0.00.045.814 I print_info: n_gqa            = 1
0.00.045.815 I print_info: n_embd_k_gqa     = 2048
0.00.045.815 I print_info: n_embd_v_gqa     = 2048
0.00.045.816 I print_info: f_norm_eps       = 1.0e-05
0.00.045.816 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.045.816 I print_info: f_clamp_kqv      = 0.0e+00
0.00.045.818 I print_info: f_max_alibi_bias = 0.0e+00
0.00.045.819 I print_info: f_logit_scale    = 0.0e+00
0.00.045.820 I print_info: n_ff             = 8192
0.00.045.820 I print_info: n_expert         = 0
0.00.045.820 I print_info: n_expert_used    = 0
0.00.045.821 I print_info: causal attn      = 1
0.00.045.821 I print_info: pooling type     = 0
0.00.045.821 I print_info: rope type        = 2
0.00.045.822 I print_info: rope scaling     = linear
0.00.045.822 I print_info: freq_base_train  = 10000.0
0.00.045.822 I print_info: freq_scale_train = 1
0.00.045.823 I print_info: n_ctx_orig_yarn  = 2048
0.00.045.823 I print_info: rope_finetuned   = unknown
0.00.045.824 I print_info: ssm_d_conv       = 0
0.00.045.824 I print_info: ssm_d_inner      = 0
0.00.045.824 I print_info: ssm_d_state      = 0
0.00.045.824 I print_info: ssm_dt_rank      = 0
0.00.045.824 I print_info: ssm_dt_b_c_rms   = 0
0.00.045.824 I print_info: model type       = 1.4B
0.00.045.824 I print_info: model params     = 1.41 B
0.00.045.825 I print_info: general.name     = 1.4B
0.00.045.825 I print_info: vocab type       = BPE
0.00.045.825 I print_info: n_vocab          = 50304
0.00.045.825 I print_info: n_merges         = 50009
0.00.045.825 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.045.826 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.045.826 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.045.826 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.045.826 I print_info: LF token         = 128 'Ä'
0.00.045.826 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.045.827 I print_info: max token length = 1024
0.00.625.257 I load_tensors: offloading 24 repeating layers to GPU
0.00.625.265 I load_tensors: offloading output layer to GPU
0.00.625.266 I load_tensors: offloaded 25/25 layers to GPU
0.00.625.301 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.625.302 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.626.736 I llama_init_from_model: n_seq_max     = 1
0.00.626.746 I llama_init_from_model: n_ctx         = 2048
0.00.626.746 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.626.746 I llama_init_from_model: n_batch       = 2048
0.00.626.763 I llama_init_from_model: n_ubatch      = 512
0.00.626.764 I llama_init_from_model: flash_attn    = 0
0.00.626.765 I llama_init_from_model: freq_base     = 10000.0
0.00.626.766 I llama_init_from_model: freq_scale    = 1
0.00.626.768 I ggml_metal_init: allocating
0.00.626.831 I ggml_metal_init: found device: Apple M4
0.00.626.846 I ggml_metal_init: picking default device: Apple M4
0.00.628.920 I ggml_metal_init: using embedded metal library
0.00.635.787 I ggml_metal_init: GPU name:   Apple M4
0.00.635.796 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.635.796 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.635.797 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.635.798 I ggml_metal_init: simdgroup reduction   = true
0.00.635.798 I ggml_metal_init: simdgroup matrix mul. = true
0.00.635.798 I ggml_metal_init: has residency sets    = true
0.00.635.799 I ggml_metal_init: has bfloat            = true
0.00.635.799 I ggml_metal_init: use bfloat            = true
0.00.635.800 I ggml_metal_init: hasUnifiedMemory      = true
0.00.635.811 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.654.039 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.709.319 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.709.325 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.709.348 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.713.791 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.713.792 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.713.793 I llama_init_from_model: graph nodes  = 967
0.00.713.793 I llama_init_from_model: graph splits = 2
0.00.713.798 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.713.928 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.713.928 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.777.102 I main: llama threadpool init, n_threads = 4
0.00.777.145 I 
0.00.777.170 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.777.172 I 
0.00.777.350 I sampler seed: 1234
0.00.777.355 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.777.399 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.777.404 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.777.404 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.617.555 I llama_perf_sampler_print:    sampling time =       1.26 ms /    71 runs   (    0.02 ms per token, 56528.66 tokens per second)
0.01.617.556 I llama_perf_context_print:        load time =     763.31 ms
0.01.617.557 I llama_perf_context_print: prompt eval time =      51.57 ms /     7 tokens (    7.37 ms per token,   135.75 tokens per second)
0.01.617.558 I llama_perf_context_print:        eval time =     785.76 ms /    63 runs   (   12.47 ms per token,    80.18 tokens per second)
0.01.617.558 I llama_perf_context_print:       total time =     841.41 ms /    70 tokens
0.01.617.807 I ggml_metal_free: deallocating

real	0m1.637s
user	0m0.112s
sys	0m0.228s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.043 I build: 4569 (2b8525d5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.079 I main: llama backend init
0.00.000.082 I main: load the model and apply lora adapter, if any
0.00.008.780 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.548 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.553 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.554 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.555 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.555 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.555 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.555 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.556 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.557 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.557 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.557 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.558 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.558 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.559 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.560 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.561 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.561 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.442 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.442 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.301 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.303 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.303 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.304 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.304 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.304 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.305 I llama_model_loader: - type  f32:  194 tensors
0.00.025.305 I llama_model_loader: - type q6_K:   98 tensors
0.00.025.306 I print_info: file format = GGUF V3 (latest)
0.00.025.306 I print_info: file type   = Q6_K
0.00.025.307 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.033.374 I load: special tokens cache size = 25
0.00.039.301 I load: token to piece cache size = 0.2984 MB
0.00.039.304 I print_info: arch             = gptneox
0.00.039.305 I print_info: vocab_only       = 0
0.00.039.305 I print_info: n_ctx_train      = 2048
0.00.039.305 I print_info: n_embd           = 2048
0.00.039.305 I print_info: n_layer          = 24
0.00.039.309 I print_info: n_head           = 16
0.00.039.310 I print_info: n_head_kv        = 16
0.00.039.310 I print_info: n_rot            = 32
0.00.039.310 I print_info: n_swa            = 0
0.00.039.310 I print_info: n_embd_head_k    = 128
0.00.039.310 I print_info: n_embd_head_v    = 128
0.00.039.311 I print_info: n_gqa            = 1
0.00.039.312 I print_info: n_embd_k_gqa     = 2048
0.00.039.313 I print_info: n_embd_v_gqa     = 2048
0.00.039.313 I print_info: f_norm_eps       = 1.0e-05
0.00.039.313 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.314 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.314 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.314 I print_info: f_logit_scale    = 0.0e+00
0.00.039.315 I print_info: n_ff             = 8192
0.00.039.315 I print_info: n_expert         = 0
0.00.039.315 I print_info: n_expert_used    = 0
0.00.039.315 I print_info: causal attn      = 1
0.00.039.315 I print_info: pooling type     = 0
0.00.039.315 I print_info: rope type        = 2
0.00.039.316 I print_info: rope scaling     = linear
0.00.039.316 I print_info: freq_base_train  = 10000.0
0.00.039.316 I print_info: freq_scale_train = 1
0.00.039.316 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.317 I print_info: rope_finetuned   = unknown
0.00.039.317 I print_info: ssm_d_conv       = 0
0.00.039.318 I print_info: ssm_d_inner      = 0
0.00.039.318 I print_info: ssm_d_state      = 0
0.00.039.318 I print_info: ssm_dt_rank      = 0
0.00.039.320 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.320 I print_info: model type       = 1.4B
0.00.039.320 I print_info: model params     = 1.41 B
0.00.039.320 I print_info: general.name     = 1.4B
0.00.039.321 I print_info: vocab type       = BPE
0.00.039.321 I print_info: n_vocab          = 50304
0.00.039.321 I print_info: n_merges         = 50009
0.00.039.321 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.322 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.322 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.322 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.322 I print_info: LF token         = 128 'Ä'
0.00.039.327 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.327 I print_info: max token length = 1024
0.00.655.640 I load_tensors: offloading 24 repeating layers to GPU
0.00.655.644 I load_tensors: offloading output layer to GPU
0.00.655.645 I load_tensors: offloaded 25/25 layers to GPU
0.00.655.669 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.655.672 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.657.176 I llama_init_from_model: n_seq_max     = 1
0.00.657.178 I llama_init_from_model: n_ctx         = 2048
0.00.657.179 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.657.179 I llama_init_from_model: n_batch       = 2048
0.00.657.179 I llama_init_from_model: n_ubatch      = 512
0.00.657.180 I llama_init_from_model: flash_attn    = 0
0.00.657.181 I llama_init_from_model: freq_base     = 10000.0
0.00.657.181 I llama_init_from_model: freq_scale    = 1
0.00.657.182 I ggml_metal_init: allocating
0.00.657.221 I ggml_metal_init: found device: Apple M4
0.00.657.240 I ggml_metal_init: picking default device: Apple M4
0.00.658.680 I ggml_metal_init: using embedded metal library
0.00.664.622 I ggml_metal_init: GPU name:   Apple M4
0.00.664.624 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.664.625 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.664.626 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.664.627 I ggml_metal_init: simdgroup reduction   = true
0.00.664.627 I ggml_metal_init: simdgroup matrix mul. = true
0.00.664.627 I ggml_metal_init: has residency sets    = true
0.00.664.627 I ggml_metal_init: has bfloat            = true
0.00.664.628 I ggml_metal_init: use bfloat            = true
0.00.664.628 I ggml_metal_init: hasUnifiedMemory      = true
0.00.664.629 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.680.831 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.734.859 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.734.866 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.734.887 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.739.083 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.739.086 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.739.086 I llama_init_from_model: graph nodes  = 967
0.00.739.086 I llama_init_from_model: graph splits = 2
0.00.739.093 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.739.219 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.739.219 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.808.325 I main: llama threadpool init, n_threads = 4
0.00.808.372 I 
0.00.808.397 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.808.397 I 
0.00.808.570 I sampler seed: 1234
0.00.808.575 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.808.628 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.808.632 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.808.632 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.679.802 I llama_perf_sampler_print:    sampling time =       1.30 ms /    71 runs   (    0.02 ms per token, 54406.13 tokens per second)
0.01.679.803 I llama_perf_context_print:        load time =     798.60 ms
0.01.679.804 I llama_perf_context_print: prompt eval time =      54.46 ms /     7 tokens (    7.78 ms per token,   128.53 tokens per second)
0.01.679.804 I llama_perf_context_print:        eval time =     813.81 ms /    63 runs   (   12.92 ms per token,    77.41 tokens per second)
0.01.679.805 I llama_perf_context_print:       total time =     872.42 ms /    70 tokens
0.01.680.089 I ggml_metal_free: deallocating

real	0m1.697s
user	0m0.107s
sys	0m0.227s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.875 I build: 4569 (2b8525d5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.027.136 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.042.026 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.042.032 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.042.034 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.042.039 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.042.040 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.042.040 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.042.040 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.042.042 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.042.042 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.042.043 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.042.043 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.042.044 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.042.044 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.042.044 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.042.047 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.042.048 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.042.048 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.050.574 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.052.553 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.059.950 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.059.952 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.059.953 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.059.953 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.059.954 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.059.955 I llama_model_loader: - type  f32:  194 tensors
0.00.059.955 I llama_model_loader: - type  f16:   98 tensors
0.00.059.956 I print_info: file format = GGUF V3 (latest)
0.00.059.957 I print_info: file type   = all F32 (guessed)
0.00.059.959 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.072.946 I load: special tokens cache size = 25
0.00.081.124 I load: token to piece cache size = 0.2984 MB
0.00.081.128 I print_info: arch             = gptneox
0.00.081.128 I print_info: vocab_only       = 0
0.00.081.128 I print_info: n_ctx_train      = 2048
0.00.081.128 I print_info: n_embd           = 2048
0.00.081.129 I print_info: n_layer          = 24
0.00.081.132 I print_info: n_head           = 16
0.00.081.132 I print_info: n_head_kv        = 16
0.00.081.135 I print_info: n_rot            = 32
0.00.081.135 I print_info: n_swa            = 0
0.00.081.135 I print_info: n_embd_head_k    = 128
0.00.081.135 I print_info: n_embd_head_v    = 128
0.00.081.136 I print_info: n_gqa            = 1
0.00.081.137 I print_info: n_embd_k_gqa     = 2048
0.00.081.142 I print_info: n_embd_v_gqa     = 2048
0.00.081.143 I print_info: f_norm_eps       = 1.0e-05
0.00.081.143 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.081.143 I print_info: f_clamp_kqv      = 0.0e+00
0.00.081.144 I print_info: f_max_alibi_bias = 0.0e+00
0.00.081.145 I print_info: f_logit_scale    = 0.0e+00
0.00.081.145 I print_info: n_ff             = 8192
0.00.081.146 I print_info: n_expert         = 0
0.00.081.146 I print_info: n_expert_used    = 0
0.00.081.146 I print_info: causal attn      = 1
0.00.081.148 I print_info: pooling type     = 0
0.00.081.148 I print_info: rope type        = 2
0.00.081.148 I print_info: rope scaling     = linear
0.00.081.148 I print_info: freq_base_train  = 10000.0
0.00.081.149 I print_info: freq_scale_train = 1
0.00.081.149 I print_info: n_ctx_orig_yarn  = 2048
0.00.081.149 I print_info: rope_finetuned   = unknown
0.00.081.149 I print_info: ssm_d_conv       = 0
0.00.081.149 I print_info: ssm_d_inner      = 0
0.00.081.151 I print_info: ssm_d_state      = 0
0.00.081.151 I print_info: ssm_dt_rank      = 0
0.00.081.151 I print_info: ssm_dt_b_c_rms   = 0
0.00.081.151 I print_info: model type       = 1.4B
0.00.081.152 I print_info: model params     = 1.41 B
0.00.081.152 I print_info: general.name     = 1.4B
0.00.081.153 I print_info: vocab type       = BPE
0.00.081.153 I print_info: n_vocab          = 50304
0.00.081.153 I print_info: n_merges         = 50009
0.00.081.153 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.081.153 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.081.153 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.081.154 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.081.154 I print_info: LF token         = 128 'Ä'
0.00.081.155 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.081.155 I print_info: max token length = 1024
0.01.440.688 I load_tensors: offloading 24 repeating layers to GPU
0.01.440.694 I load_tensors: offloading output layer to GPU
0.01.440.695 I load_tensors: offloaded 25/25 layers to GPU
0.01.440.719 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.440.722 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.01.441.345 I llama_init_from_model: n_seq_max     = 1
0.01.441.346 I llama_init_from_model: n_ctx         = 128
0.01.441.347 I llama_init_from_model: n_ctx_per_seq = 128
0.01.441.347 I llama_init_from_model: n_batch       = 128
0.01.441.347 I llama_init_from_model: n_ubatch      = 128
0.01.441.347 I llama_init_from_model: flash_attn    = 0
0.01.441.348 I llama_init_from_model: freq_base     = 10000.0
0.01.441.348 I llama_init_from_model: freq_scale    = 1
0.01.441.348 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.441.350 I ggml_metal_init: allocating
0.01.441.389 I ggml_metal_init: found device: Apple M4
0.01.441.400 I ggml_metal_init: picking default device: Apple M4
0.01.442.413 I ggml_metal_init: using embedded metal library
0.01.446.211 I ggml_metal_init: GPU name:   Apple M4
0.01.446.214 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.446.214 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.446.215 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.446.215 I ggml_metal_init: simdgroup reduction   = true
0.01.446.215 I ggml_metal_init: simdgroup matrix mul. = true
0.01.446.215 I ggml_metal_init: has residency sets    = true
0.01.446.216 I ggml_metal_init: has bfloat            = true
0.01.446.216 I ggml_metal_init: use bfloat            = true
0.01.446.216 I ggml_metal_init: hasUnifiedMemory      = true
0.01.446.217 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.457.434 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.459.196 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.459.198 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.459.220 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.460.957 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.460.958 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.460.959 I llama_init_from_model: graph nodes  = 967
0.01.460.959 I llama_init_from_model: graph splits = 2
0.01.460.960 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.460.960 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.496.271 I 
0.01.496.313 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.496.333 I perplexity: tokenizing the input ..
0.01.501.894 I perplexity: tokenization took 5.559 ms
0.01.501.917 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.621.190 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.622.542 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.622.558 I llama_perf_context_print:        load time =    1469.12 ms
0.01.622.559 I llama_perf_context_print: prompt eval time =     118.97 ms /   128 tokens (    0.93 ms per token,  1075.94 tokens per second)
0.01.622.560 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.622.560 I llama_perf_context_print:       total time =     126.29 ms /   129 tokens
0.01.622.949 I ggml_metal_free: deallocating

real	0m1.841s
user	0m0.101s
sys	0m0.268s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.275 I build: 4569 (2b8525d5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.578 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.700 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.018.706 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.713 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.713 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.714 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.714 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.714 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.717 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.717 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.718 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.718 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.718 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.719 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.719 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.721 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.721 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.721 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.443 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.448 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.861 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.864 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.865 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.865 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.866 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.866 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.027.867 I llama_model_loader: - type  f32:  194 tensors
0.00.027.868 I llama_model_loader: - type q8_0:   98 tensors
0.00.027.869 I print_info: file format = GGUF V3 (latest)
0.00.027.870 I print_info: file type   = Q8_0
0.00.027.871 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.038.553 I load: special tokens cache size = 25
0.00.046.680 I load: token to piece cache size = 0.2984 MB
0.00.046.684 I print_info: arch             = gptneox
0.00.046.684 I print_info: vocab_only       = 0
0.00.046.684 I print_info: n_ctx_train      = 2048
0.00.046.684 I print_info: n_embd           = 2048
0.00.046.685 I print_info: n_layer          = 24
0.00.046.688 I print_info: n_head           = 16
0.00.046.689 I print_info: n_head_kv        = 16
0.00.046.689 I print_info: n_rot            = 32
0.00.046.690 I print_info: n_swa            = 0
0.00.046.690 I print_info: n_embd_head_k    = 128
0.00.046.690 I print_info: n_embd_head_v    = 128
0.00.046.691 I print_info: n_gqa            = 1
0.00.046.692 I print_info: n_embd_k_gqa     = 2048
0.00.046.695 I print_info: n_embd_v_gqa     = 2048
0.00.046.695 I print_info: f_norm_eps       = 1.0e-05
0.00.046.696 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.046.696 I print_info: f_clamp_kqv      = 0.0e+00
0.00.046.696 I print_info: f_max_alibi_bias = 0.0e+00
0.00.046.696 I print_info: f_logit_scale    = 0.0e+00
0.00.046.697 I print_info: n_ff             = 8192
0.00.046.699 I print_info: n_expert         = 0
0.00.046.701 I print_info: n_expert_used    = 0
0.00.046.701 I print_info: causal attn      = 1
0.00.046.701 I print_info: pooling type     = 0
0.00.046.701 I print_info: rope type        = 2
0.00.046.701 I print_info: rope scaling     = linear
0.00.046.702 I print_info: freq_base_train  = 10000.0
0.00.046.702 I print_info: freq_scale_train = 1
0.00.046.702 I print_info: n_ctx_orig_yarn  = 2048
0.00.046.703 I print_info: rope_finetuned   = unknown
0.00.046.703 I print_info: ssm_d_conv       = 0
0.00.046.703 I print_info: ssm_d_inner      = 0
0.00.046.703 I print_info: ssm_d_state      = 0
0.00.046.703 I print_info: ssm_dt_rank      = 0
0.00.046.703 I print_info: ssm_dt_b_c_rms   = 0
0.00.046.704 I print_info: model type       = 1.4B
0.00.046.709 I print_info: model params     = 1.41 B
0.00.046.709 I print_info: general.name     = 1.4B
0.00.046.710 I print_info: vocab type       = BPE
0.00.046.710 I print_info: n_vocab          = 50304
0.00.046.710 I print_info: n_merges         = 50009
0.00.046.711 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.046.711 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.046.711 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.046.711 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.046.711 I print_info: LF token         = 128 'Ä'
0.00.046.712 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.046.712 I print_info: max token length = 1024
0.00.886.039 I load_tensors: offloading 24 repeating layers to GPU
0.00.886.043 I load_tensors: offloading output layer to GPU
0.00.886.044 I load_tensors: offloaded 25/25 layers to GPU
0.00.886.058 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.886.059 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.00.887.068 I llama_init_from_model: n_seq_max     = 1
0.00.887.070 I llama_init_from_model: n_ctx         = 128
0.00.887.070 I llama_init_from_model: n_ctx_per_seq = 128
0.00.887.071 I llama_init_from_model: n_batch       = 128
0.00.887.073 I llama_init_from_model: n_ubatch      = 128
0.00.887.074 I llama_init_from_model: flash_attn    = 0
0.00.887.074 I llama_init_from_model: freq_base     = 10000.0
0.00.887.075 I llama_init_from_model: freq_scale    = 1
0.00.887.075 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.887.076 I ggml_metal_init: allocating
0.00.887.093 I ggml_metal_init: found device: Apple M4
0.00.887.100 I ggml_metal_init: picking default device: Apple M4
0.00.888.000 I ggml_metal_init: using embedded metal library
0.00.893.111 I ggml_metal_init: GPU name:   Apple M4
0.00.893.114 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.893.115 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.893.116 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.893.116 I ggml_metal_init: simdgroup reduction   = true
0.00.893.116 I ggml_metal_init: simdgroup matrix mul. = true
0.00.893.117 I ggml_metal_init: has residency sets    = true
0.00.893.117 I ggml_metal_init: has bfloat            = true
0.00.893.117 I ggml_metal_init: use bfloat            = true
0.00.893.118 I ggml_metal_init: hasUnifiedMemory      = true
0.00.893.119 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.908.221 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.911.490 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.911.498 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.911.534 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.914.545 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.914.547 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.914.547 I llama_init_from_model: graph nodes  = 967
0.00.914.547 I llama_init_from_model: graph splits = 2
0.00.914.550 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.914.550 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.942.494 I 
0.00.942.574 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.942.596 I perplexity: tokenizing the input ..
0.00.950.079 I perplexity: tokenization took 7.48 ms
0.00.950.101 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.086.478 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.087.818 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.087.832 I llama_perf_context_print:        load time =     930.91 ms
0.01.087.833 I llama_perf_context_print: prompt eval time =     135.50 ms /   128 tokens (    1.06 ms per token,   944.68 tokens per second)
0.01.087.834 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.087.834 I llama_perf_context_print:       total time =     145.34 ms /   129 tokens
0.01.088.204 I ggml_metal_free: deallocating

real	0m1.105s
user	0m0.081s
sys	0m0.172s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.268 I build: 4569 (2b8525d5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.246 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.683 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.017.688 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.690 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.691 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.691 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.691 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.691 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.692 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.693 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.693 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.694 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.694 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.694 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.695 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.697 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.697 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.697 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.580 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.576 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.449 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.450 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.450 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.450 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.451 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.451 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.026.452 I llama_model_loader: - type  f32:  194 tensors
0.00.026.452 I llama_model_loader: - type q4_0:   97 tensors
0.00.026.452 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.453 I print_info: file format = GGUF V3 (latest)
0.00.026.453 I print_info: file type   = Q4_0
0.00.026.454 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.034.643 I load: special tokens cache size = 25
0.00.040.571 I load: token to piece cache size = 0.2984 MB
0.00.040.574 I print_info: arch             = gptneox
0.00.040.574 I print_info: vocab_only       = 0
0.00.040.574 I print_info: n_ctx_train      = 2048
0.00.040.574 I print_info: n_embd           = 2048
0.00.040.574 I print_info: n_layer          = 24
0.00.040.578 I print_info: n_head           = 16
0.00.040.581 I print_info: n_head_kv        = 16
0.00.040.581 I print_info: n_rot            = 32
0.00.040.581 I print_info: n_swa            = 0
0.00.040.581 I print_info: n_embd_head_k    = 128
0.00.040.582 I print_info: n_embd_head_v    = 128
0.00.040.582 I print_info: n_gqa            = 1
0.00.040.583 I print_info: n_embd_k_gqa     = 2048
0.00.040.584 I print_info: n_embd_v_gqa     = 2048
0.00.040.585 I print_info: f_norm_eps       = 1.0e-05
0.00.040.585 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.585 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.585 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.586 I print_info: f_logit_scale    = 0.0e+00
0.00.040.586 I print_info: n_ff             = 8192
0.00.040.587 I print_info: n_expert         = 0
0.00.040.587 I print_info: n_expert_used    = 0
0.00.040.587 I print_info: causal attn      = 1
0.00.040.587 I print_info: pooling type     = 0
0.00.040.587 I print_info: rope type        = 2
0.00.040.587 I print_info: rope scaling     = linear
0.00.040.588 I print_info: freq_base_train  = 10000.0
0.00.040.588 I print_info: freq_scale_train = 1
0.00.040.588 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.588 I print_info: rope_finetuned   = unknown
0.00.040.592 I print_info: ssm_d_conv       = 0
0.00.040.593 I print_info: ssm_d_inner      = 0
0.00.040.593 I print_info: ssm_d_state      = 0
0.00.040.593 I print_info: ssm_dt_rank      = 0
0.00.040.593 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.594 I print_info: model type       = 1.4B
0.00.040.594 I print_info: model params     = 1.41 B
0.00.040.594 I print_info: general.name     = 1.4B
0.00.040.595 I print_info: vocab type       = BPE
0.00.040.595 I print_info: n_vocab          = 50304
0.00.040.595 I print_info: n_merges         = 50009
0.00.040.595 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.595 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.596 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.596 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.596 I print_info: LF token         = 128 'Ä'
0.00.040.596 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.596 I print_info: max token length = 1024
0.00.598.494 I load_tensors: offloading 24 repeating layers to GPU
0.00.598.511 I load_tensors: offloading output layer to GPU
0.00.598.512 I load_tensors: offloaded 25/25 layers to GPU
0.00.598.545 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.598.546 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.599.986 I llama_init_from_model: n_seq_max     = 1
0.00.599.990 I llama_init_from_model: n_ctx         = 128
0.00.599.991 I llama_init_from_model: n_ctx_per_seq = 128
0.00.599.992 I llama_init_from_model: n_batch       = 128
0.00.599.992 I llama_init_from_model: n_ubatch      = 128
0.00.599.993 I llama_init_from_model: flash_attn    = 0
0.00.599.994 I llama_init_from_model: freq_base     = 10000.0
0.00.599.995 I llama_init_from_model: freq_scale    = 1
0.00.599.996 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.599.999 I ggml_metal_init: allocating
0.00.600.088 I ggml_metal_init: found device: Apple M4
0.00.600.102 I ggml_metal_init: picking default device: Apple M4
0.00.601.895 I ggml_metal_init: using embedded metal library
0.00.608.383 I ggml_metal_init: GPU name:   Apple M4
0.00.608.389 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.608.390 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.608.391 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.608.392 I ggml_metal_init: simdgroup reduction   = true
0.00.608.392 I ggml_metal_init: simdgroup matrix mul. = true
0.00.608.393 I ggml_metal_init: has residency sets    = true
0.00.608.393 I ggml_metal_init: has bfloat            = true
0.00.608.393 I ggml_metal_init: use bfloat            = true
0.00.608.394 I ggml_metal_init: hasUnifiedMemory      = true
0.00.608.403 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.627.015 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.630.598 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.630.601 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.630.629 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.633.823 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.633.825 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.633.826 I llama_init_from_model: graph nodes  = 967
0.00.633.826 I llama_init_from_model: graph splits = 2
0.00.633.829 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.633.830 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.658.090 I 
0.00.658.176 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.658.196 I perplexity: tokenizing the input ..
0.00.665.154 I perplexity: tokenization took 6.955 ms
0.00.665.176 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.800.697 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.802.045 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.802.059 I llama_perf_context_print:        load time =     647.83 ms
0.00.802.060 I llama_perf_context_print: prompt eval time =     134.77 ms /   128 tokens (    1.05 ms per token,   949.79 tokens per second)
0.00.802.061 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.802.061 I llama_perf_context_print:       total time =     143.98 ms /   129 tokens
0.00.802.446 I ggml_metal_free: deallocating

real	0m0.818s
user	0m0.079s
sys	0m0.128s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.097 I build: 4569 (2b8525d5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.994 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.169 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.173 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.175 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.176 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.176 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.176 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.177 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.178 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.178 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.178 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.181 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.181 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.182 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.182 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.184 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.184 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.185 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.995 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.001 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.776 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.777 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.777 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.777 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.778 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.778 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.024.779 I llama_model_loader: - type  f32:  194 tensors
0.00.024.779 I llama_model_loader: - type q4_1:   97 tensors
0.00.024.779 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.780 I print_info: file format = GGUF V3 (latest)
0.00.024.780 I print_info: file type   = Q4_1
0.00.024.783 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.032.774 I load: special tokens cache size = 25
0.00.038.786 I load: token to piece cache size = 0.2984 MB
0.00.038.789 I print_info: arch             = gptneox
0.00.038.789 I print_info: vocab_only       = 0
0.00.038.789 I print_info: n_ctx_train      = 2048
0.00.038.789 I print_info: n_embd           = 2048
0.00.038.790 I print_info: n_layer          = 24
0.00.038.793 I print_info: n_head           = 16
0.00.038.795 I print_info: n_head_kv        = 16
0.00.038.796 I print_info: n_rot            = 32
0.00.038.796 I print_info: n_swa            = 0
0.00.038.796 I print_info: n_embd_head_k    = 128
0.00.038.796 I print_info: n_embd_head_v    = 128
0.00.038.797 I print_info: n_gqa            = 1
0.00.038.798 I print_info: n_embd_k_gqa     = 2048
0.00.038.798 I print_info: n_embd_v_gqa     = 2048
0.00.038.799 I print_info: f_norm_eps       = 1.0e-05
0.00.038.799 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.800 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.800 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.800 I print_info: f_logit_scale    = 0.0e+00
0.00.038.801 I print_info: n_ff             = 8192
0.00.038.801 I print_info: n_expert         = 0
0.00.038.801 I print_info: n_expert_used    = 0
0.00.038.801 I print_info: causal attn      = 1
0.00.038.801 I print_info: pooling type     = 0
0.00.038.801 I print_info: rope type        = 2
0.00.038.802 I print_info: rope scaling     = linear
0.00.038.802 I print_info: freq_base_train  = 10000.0
0.00.038.802 I print_info: freq_scale_train = 1
0.00.038.802 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.803 I print_info: rope_finetuned   = unknown
0.00.038.803 I print_info: ssm_d_conv       = 0
0.00.038.803 I print_info: ssm_d_inner      = 0
0.00.038.803 I print_info: ssm_d_state      = 0
0.00.038.803 I print_info: ssm_dt_rank      = 0
0.00.038.803 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.807 I print_info: model type       = 1.4B
0.00.038.808 I print_info: model params     = 1.41 B
0.00.038.808 I print_info: general.name     = 1.4B
0.00.038.809 I print_info: vocab type       = BPE
0.00.038.809 I print_info: n_vocab          = 50304
0.00.038.809 I print_info: n_merges         = 50009
0.00.038.809 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.809 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.810 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.810 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.811 I print_info: LF token         = 128 'Ä'
0.00.038.811 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.811 I print_info: max token length = 1024
0.00.621.204 I load_tensors: offloading 24 repeating layers to GPU
0.00.621.218 I load_tensors: offloading output layer to GPU
0.00.621.219 I load_tensors: offloaded 25/25 layers to GPU
0.00.621.253 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.621.255 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.622.828 I llama_init_from_model: n_seq_max     = 1
0.00.622.832 I llama_init_from_model: n_ctx         = 128
0.00.622.833 I llama_init_from_model: n_ctx_per_seq = 128
0.00.622.833 I llama_init_from_model: n_batch       = 128
0.00.622.834 I llama_init_from_model: n_ubatch      = 128
0.00.622.834 I llama_init_from_model: flash_attn    = 0
0.00.622.836 I llama_init_from_model: freq_base     = 10000.0
0.00.622.836 I llama_init_from_model: freq_scale    = 1
0.00.622.837 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.622.840 I ggml_metal_init: allocating
0.00.622.914 I ggml_metal_init: found device: Apple M4
0.00.622.929 I ggml_metal_init: picking default device: Apple M4
0.00.624.690 I ggml_metal_init: using embedded metal library
0.00.631.498 I ggml_metal_init: GPU name:   Apple M4
0.00.631.503 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.631.503 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.631.504 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.631.505 I ggml_metal_init: simdgroup reduction   = true
0.00.631.505 I ggml_metal_init: simdgroup matrix mul. = true
0.00.631.505 I ggml_metal_init: has residency sets    = true
0.00.631.506 I ggml_metal_init: has bfloat            = true
0.00.631.506 I ggml_metal_init: use bfloat            = true
0.00.631.507 I ggml_metal_init: hasUnifiedMemory      = true
0.00.631.508 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.651.206 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.654.694 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.654.698 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.654.731 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.658.069 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.658.071 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.658.072 I llama_init_from_model: graph nodes  = 967
0.00.658.072 I llama_init_from_model: graph splits = 2
0.00.658.075 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.658.076 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.685.650 I 
0.00.685.725 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.685.744 I perplexity: tokenizing the input ..
0.00.692.618 I perplexity: tokenization took 6.871 ms
0.00.692.635 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.828.822 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.830.250 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.830.270 I llama_perf_context_print:        load time =     676.65 ms
0.00.830.271 I llama_perf_context_print: prompt eval time =     135.23 ms /   128 tokens (    1.06 ms per token,   946.56 tokens per second)
0.00.830.272 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.830.273 I llama_perf_context_print:       total time =     144.62 ms /   129 tokens
0.00.830.682 I ggml_metal_free: deallocating

real	0m0.845s
user	0m0.080s
sys	0m0.121s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.101 I build: 4569 (2b8525d5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.129 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.470 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.017.475 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.481 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.482 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.482 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.483 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.483 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.484 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.484 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.485 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.485 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.485 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.485 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.486 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.487 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.488 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.488 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.340 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.357 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.139 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.140 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.141 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.141 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.141 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.142 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.026.142 I llama_model_loader: - type  f32:  194 tensors
0.00.026.143 I llama_model_loader: - type q5_0:   97 tensors
0.00.026.143 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.144 I print_info: file format = GGUF V3 (latest)
0.00.026.144 I print_info: file type   = Q5_0
0.00.026.145 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.034.300 I load: special tokens cache size = 25
0.00.040.304 I load: token to piece cache size = 0.2984 MB
0.00.040.307 I print_info: arch             = gptneox
0.00.040.307 I print_info: vocab_only       = 0
0.00.040.307 I print_info: n_ctx_train      = 2048
0.00.040.307 I print_info: n_embd           = 2048
0.00.040.307 I print_info: n_layer          = 24
0.00.040.311 I print_info: n_head           = 16
0.00.040.312 I print_info: n_head_kv        = 16
0.00.040.312 I print_info: n_rot            = 32
0.00.040.312 I print_info: n_swa            = 0
0.00.040.312 I print_info: n_embd_head_k    = 128
0.00.040.312 I print_info: n_embd_head_v    = 128
0.00.040.313 I print_info: n_gqa            = 1
0.00.040.314 I print_info: n_embd_k_gqa     = 2048
0.00.040.316 I print_info: n_embd_v_gqa     = 2048
0.00.040.317 I print_info: f_norm_eps       = 1.0e-05
0.00.040.317 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.317 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.318 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.318 I print_info: f_logit_scale    = 0.0e+00
0.00.040.325 I print_info: n_ff             = 8192
0.00.040.327 I print_info: n_expert         = 0
0.00.040.327 I print_info: n_expert_used    = 0
0.00.040.327 I print_info: causal attn      = 1
0.00.040.328 I print_info: pooling type     = 0
0.00.040.328 I print_info: rope type        = 2
0.00.040.328 I print_info: rope scaling     = linear
0.00.040.328 I print_info: freq_base_train  = 10000.0
0.00.040.328 I print_info: freq_scale_train = 1
0.00.040.329 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.329 I print_info: rope_finetuned   = unknown
0.00.040.329 I print_info: ssm_d_conv       = 0
0.00.040.329 I print_info: ssm_d_inner      = 0
0.00.040.329 I print_info: ssm_d_state      = 0
0.00.040.329 I print_info: ssm_dt_rank      = 0
0.00.040.329 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.330 I print_info: model type       = 1.4B
0.00.040.330 I print_info: model params     = 1.41 B
0.00.040.330 I print_info: general.name     = 1.4B
0.00.040.331 I print_info: vocab type       = BPE
0.00.040.331 I print_info: n_vocab          = 50304
0.00.040.331 I print_info: n_merges         = 50009
0.00.040.331 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.332 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.332 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.332 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.333 I print_info: LF token         = 128 'Ä'
0.00.040.334 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.335 I print_info: max token length = 1024
0.00.692.332 I load_tensors: offloading 24 repeating layers to GPU
0.00.692.349 I load_tensors: offloading output layer to GPU
0.00.692.350 I load_tensors: offloaded 25/25 layers to GPU
0.00.692.386 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.692.388 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.693.937 I llama_init_from_model: n_seq_max     = 1
0.00.693.941 I llama_init_from_model: n_ctx         = 128
0.00.693.942 I llama_init_from_model: n_ctx_per_seq = 128
0.00.693.942 I llama_init_from_model: n_batch       = 128
0.00.693.943 I llama_init_from_model: n_ubatch      = 128
0.00.693.943 I llama_init_from_model: flash_attn    = 0
0.00.693.945 I llama_init_from_model: freq_base     = 10000.0
0.00.693.946 I llama_init_from_model: freq_scale    = 1
0.00.693.946 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.693.948 I ggml_metal_init: allocating
0.00.694.028 I ggml_metal_init: found device: Apple M4
0.00.694.042 I ggml_metal_init: picking default device: Apple M4
0.00.695.744 I ggml_metal_init: using embedded metal library
0.00.702.547 I ggml_metal_init: GPU name:   Apple M4
0.00.702.552 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.702.552 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.702.553 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.702.554 I ggml_metal_init: simdgroup reduction   = true
0.00.702.554 I ggml_metal_init: simdgroup matrix mul. = true
0.00.702.554 I ggml_metal_init: has residency sets    = true
0.00.702.555 I ggml_metal_init: has bfloat            = true
0.00.702.555 I ggml_metal_init: use bfloat            = true
0.00.702.556 I ggml_metal_init: hasUnifiedMemory      = true
0.00.702.557 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.720.098 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.723.572 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.723.576 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.723.616 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.726.831 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.726.833 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.726.834 I llama_init_from_model: graph nodes  = 967
0.00.726.834 I llama_init_from_model: graph splits = 2
0.00.726.837 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.726.837 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.758.452 I 
0.00.758.535 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.758.555 I perplexity: tokenizing the input ..
0.00.766.195 I perplexity: tokenization took 7.637 ms
0.00.766.214 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.915.079 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.916.435 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.916.452 I llama_perf_context_print:        load time =     748.32 ms
0.00.916.453 I llama_perf_context_print: prompt eval time =     147.99 ms /   128 tokens (    1.16 ms per token,   864.92 tokens per second)
0.00.916.453 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.916.454 I llama_perf_context_print:       total time =     158.01 ms /   129 tokens
0.00.916.878 I ggml_metal_free: deallocating

real	0m0.933s
user	0m0.080s
sys	0m0.153s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.096 I build: 4569 (2b8525d5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.893 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.030 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.036 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.042 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.042 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.043 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.043 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.043 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.044 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.046 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.047 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.047 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.047 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.048 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.048 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.053 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.053 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.054 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.682 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.698 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.434 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.435 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.435 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.436 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.436 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.436 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.437 I llama_model_loader: - type  f32:  194 tensors
0.00.024.437 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.437 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.438 I print_info: file format = GGUF V3 (latest)
0.00.024.438 I print_info: file type   = Q5_1
0.00.024.444 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.032.450 I load: special tokens cache size = 25
0.00.038.408 I load: token to piece cache size = 0.2984 MB
0.00.038.410 I print_info: arch             = gptneox
0.00.038.410 I print_info: vocab_only       = 0
0.00.038.411 I print_info: n_ctx_train      = 2048
0.00.038.411 I print_info: n_embd           = 2048
0.00.038.411 I print_info: n_layer          = 24
0.00.038.414 I print_info: n_head           = 16
0.00.038.415 I print_info: n_head_kv        = 16
0.00.038.415 I print_info: n_rot            = 32
0.00.038.415 I print_info: n_swa            = 0
0.00.038.415 I print_info: n_embd_head_k    = 128
0.00.038.416 I print_info: n_embd_head_v    = 128
0.00.038.416 I print_info: n_gqa            = 1
0.00.038.417 I print_info: n_embd_k_gqa     = 2048
0.00.038.418 I print_info: n_embd_v_gqa     = 2048
0.00.038.418 I print_info: f_norm_eps       = 1.0e-05
0.00.038.419 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.419 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.419 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.419 I print_info: f_logit_scale    = 0.0e+00
0.00.038.420 I print_info: n_ff             = 8192
0.00.038.420 I print_info: n_expert         = 0
0.00.038.420 I print_info: n_expert_used    = 0
0.00.038.420 I print_info: causal attn      = 1
0.00.038.420 I print_info: pooling type     = 0
0.00.038.421 I print_info: rope type        = 2
0.00.038.421 I print_info: rope scaling     = linear
0.00.038.421 I print_info: freq_base_train  = 10000.0
0.00.038.422 I print_info: freq_scale_train = 1
0.00.038.422 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.424 I print_info: rope_finetuned   = unknown
0.00.038.424 I print_info: ssm_d_conv       = 0
0.00.038.424 I print_info: ssm_d_inner      = 0
0.00.038.425 I print_info: ssm_d_state      = 0
0.00.038.425 I print_info: ssm_dt_rank      = 0
0.00.038.425 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.425 I print_info: model type       = 1.4B
0.00.038.425 I print_info: model params     = 1.41 B
0.00.038.426 I print_info: general.name     = 1.4B
0.00.038.426 I print_info: vocab type       = BPE
0.00.038.426 I print_info: n_vocab          = 50304
0.00.038.426 I print_info: n_merges         = 50009
0.00.038.427 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.427 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.427 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.427 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.428 I print_info: LF token         = 128 'Ä'
0.00.038.431 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.431 I print_info: max token length = 1024
0.00.599.858 I load_tensors: offloading 24 repeating layers to GPU
0.00.599.874 I load_tensors: offloading output layer to GPU
0.00.599.874 I load_tensors: offloaded 25/25 layers to GPU
0.00.599.909 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.599.910 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.601.160 I llama_init_from_model: n_seq_max     = 1
0.00.601.163 I llama_init_from_model: n_ctx         = 128
0.00.601.163 I llama_init_from_model: n_ctx_per_seq = 128
0.00.601.163 I llama_init_from_model: n_batch       = 128
0.00.601.164 I llama_init_from_model: n_ubatch      = 128
0.00.601.164 I llama_init_from_model: flash_attn    = 0
0.00.601.165 I llama_init_from_model: freq_base     = 10000.0
0.00.601.166 I llama_init_from_model: freq_scale    = 1
0.00.601.167 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.601.168 I ggml_metal_init: allocating
0.00.601.203 I ggml_metal_init: found device: Apple M4
0.00.601.215 I ggml_metal_init: picking default device: Apple M4
0.00.602.626 I ggml_metal_init: using embedded metal library
0.00.608.833 I ggml_metal_init: GPU name:   Apple M4
0.00.608.837 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.608.838 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.608.839 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.608.839 I ggml_metal_init: simdgroup reduction   = true
0.00.608.840 I ggml_metal_init: simdgroup matrix mul. = true
0.00.608.840 I ggml_metal_init: has residency sets    = true
0.00.608.840 I ggml_metal_init: has bfloat            = true
0.00.608.840 I ggml_metal_init: use bfloat            = true
0.00.608.841 I ggml_metal_init: hasUnifiedMemory      = true
0.00.608.843 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.625.441 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.629.005 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.629.008 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.629.033 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.632.334 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.632.336 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.632.337 I llama_init_from_model: graph nodes  = 967
0.00.632.337 I llama_init_from_model: graph splits = 2
0.00.632.339 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.632.340 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.661.600 I 
0.00.661.688 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.661.707 I perplexity: tokenizing the input ..
0.00.669.296 I perplexity: tokenization took 7.586 ms
0.00.669.319 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.814.368 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.815.682 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.815.695 I llama_perf_context_print:        load time =     652.70 ms
0.00.815.696 I llama_perf_context_print: prompt eval time =     144.08 ms /   128 tokens (    1.13 ms per token,   888.40 tokens per second)
0.00.815.714 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.815.715 I llama_perf_context_print:       total time =     154.10 ms /   129 tokens
0.00.816.099 I ggml_metal_free: deallocating

real	0m0.830s
user	0m0.078s
sys	0m0.134s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.094 I build: 4569 (2b8525d5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.752 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.815 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.820 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.821 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.822 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.822 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.823 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.823 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.824 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.825 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.828 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.828 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.829 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.829 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.829 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.831 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.832 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.832 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.243 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.419 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.695 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.696 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.697 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.697 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.697 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.698 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.026.698 I llama_model_loader: - type  f32:  194 tensors
0.00.026.698 I llama_model_loader: - type q2_K:   49 tensors
0.00.026.699 I llama_model_loader: - type q3_K:   48 tensors
0.00.026.699 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.699 I print_info: file format = GGUF V3 (latest)
0.00.026.700 I print_info: file type   = Q2_K - Medium
0.00.026.701 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.035.722 I load: special tokens cache size = 25
0.00.043.487 I load: token to piece cache size = 0.2984 MB
0.00.043.491 I print_info: arch             = gptneox
0.00.043.491 I print_info: vocab_only       = 0
0.00.043.491 I print_info: n_ctx_train      = 2048
0.00.043.491 I print_info: n_embd           = 2048
0.00.043.492 I print_info: n_layer          = 24
0.00.043.495 I print_info: n_head           = 16
0.00.043.496 I print_info: n_head_kv        = 16
0.00.043.496 I print_info: n_rot            = 32
0.00.043.496 I print_info: n_swa            = 0
0.00.043.497 I print_info: n_embd_head_k    = 128
0.00.043.497 I print_info: n_embd_head_v    = 128
0.00.043.498 I print_info: n_gqa            = 1
0.00.043.498 I print_info: n_embd_k_gqa     = 2048
0.00.043.499 I print_info: n_embd_v_gqa     = 2048
0.00.043.500 I print_info: f_norm_eps       = 1.0e-05
0.00.043.500 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.043.500 I print_info: f_clamp_kqv      = 0.0e+00
0.00.043.501 I print_info: f_max_alibi_bias = 0.0e+00
0.00.043.501 I print_info: f_logit_scale    = 0.0e+00
0.00.043.501 I print_info: n_ff             = 8192
0.00.043.502 I print_info: n_expert         = 0
0.00.043.502 I print_info: n_expert_used    = 0
0.00.043.502 I print_info: causal attn      = 1
0.00.043.502 I print_info: pooling type     = 0
0.00.043.502 I print_info: rope type        = 2
0.00.043.503 I print_info: rope scaling     = linear
0.00.043.503 I print_info: freq_base_train  = 10000.0
0.00.043.503 I print_info: freq_scale_train = 1
0.00.043.504 I print_info: n_ctx_orig_yarn  = 2048
0.00.043.504 I print_info: rope_finetuned   = unknown
0.00.043.504 I print_info: ssm_d_conv       = 0
0.00.043.504 I print_info: ssm_d_inner      = 0
0.00.043.506 I print_info: ssm_d_state      = 0
0.00.043.507 I print_info: ssm_dt_rank      = 0
0.00.043.507 I print_info: ssm_dt_b_c_rms   = 0
0.00.043.507 I print_info: model type       = 1.4B
0.00.043.507 I print_info: model params     = 1.41 B
0.00.043.508 I print_info: general.name     = 1.4B
0.00.043.508 I print_info: vocab type       = BPE
0.00.043.508 I print_info: n_vocab          = 50304
0.00.043.508 I print_info: n_merges         = 50009
0.00.043.509 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.043.509 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.043.509 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.043.509 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.043.510 I print_info: LF token         = 128 'Ä'
0.00.043.510 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.043.510 I print_info: max token length = 1024
0.00.353.308 I load_tensors: offloading 24 repeating layers to GPU
0.00.353.321 I load_tensors: offloading output layer to GPU
0.00.353.322 I load_tensors: offloaded 25/25 layers to GPU
0.00.353.349 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.353.350 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.354.560 I llama_init_from_model: n_seq_max     = 1
0.00.354.566 I llama_init_from_model: n_ctx         = 128
0.00.354.567 I llama_init_from_model: n_ctx_per_seq = 128
0.00.354.567 I llama_init_from_model: n_batch       = 128
0.00.354.568 I llama_init_from_model: n_ubatch      = 128
0.00.354.568 I llama_init_from_model: flash_attn    = 0
0.00.354.569 I llama_init_from_model: freq_base     = 10000.0
0.00.354.570 I llama_init_from_model: freq_scale    = 1
0.00.354.570 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.354.573 I ggml_metal_init: allocating
0.00.354.624 I ggml_metal_init: found device: Apple M4
0.00.354.637 I ggml_metal_init: picking default device: Apple M4
0.00.356.229 I ggml_metal_init: using embedded metal library
0.00.362.035 I ggml_metal_init: GPU name:   Apple M4
0.00.362.048 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.362.049 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.362.050 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.362.050 I ggml_metal_init: simdgroup reduction   = true
0.00.362.050 I ggml_metal_init: simdgroup matrix mul. = true
0.00.362.051 I ggml_metal_init: has residency sets    = true
0.00.362.051 I ggml_metal_init: has bfloat            = true
0.00.362.051 I ggml_metal_init: use bfloat            = true
0.00.362.056 I ggml_metal_init: hasUnifiedMemory      = true
0.00.362.059 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.383.345 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.387.137 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.387.141 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.387.167 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.390.743 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.390.745 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.390.745 I llama_init_from_model: graph nodes  = 967
0.00.390.746 I llama_init_from_model: graph splits = 2
0.00.390.748 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.390.749 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.418.570 I 
0.00.418.644 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.418.663 I perplexity: tokenizing the input ..
0.00.425.697 I perplexity: tokenization took 7.033 ms
0.00.425.715 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.559.612 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.561.022 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.561.042 I llama_perf_context_print:        load time =     408.81 ms
0.00.561.044 I llama_perf_context_print: prompt eval time =     133.01 ms /   128 tokens (    1.04 ms per token,   962.32 tokens per second)
0.00.561.045 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.561.045 I llama_perf_context_print:       total time =     142.47 ms /   129 tokens
0.00.561.418 I ggml_metal_free: deallocating

real	0m0.591s
user	0m0.085s
sys	0m0.102s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.097 I build: 4569 (2b8525d5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.439 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.460 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.467 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.469 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.469 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.470 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.470 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.470 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.476 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.477 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.477 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.477 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.478 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.478 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.478 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.485 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.489 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.491 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.254 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.304 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.182 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.185 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.185 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.185 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.186 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.186 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.187 I llama_model_loader: - type  f32:  194 tensors
0.00.025.187 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.187 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.188 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.188 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.189 I print_info: file format = GGUF V3 (latest)
0.00.025.189 I print_info: file type   = Q3_K - Medium
0.00.025.190 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.033.402 I load: special tokens cache size = 25
0.00.039.427 I load: token to piece cache size = 0.2984 MB
0.00.039.431 I print_info: arch             = gptneox
0.00.039.431 I print_info: vocab_only       = 0
0.00.039.431 I print_info: n_ctx_train      = 2048
0.00.039.432 I print_info: n_embd           = 2048
0.00.039.432 I print_info: n_layer          = 24
0.00.039.436 I print_info: n_head           = 16
0.00.039.437 I print_info: n_head_kv        = 16
0.00.039.437 I print_info: n_rot            = 32
0.00.039.441 I print_info: n_swa            = 0
0.00.039.441 I print_info: n_embd_head_k    = 128
0.00.039.441 I print_info: n_embd_head_v    = 128
0.00.039.441 I print_info: n_gqa            = 1
0.00.039.442 I print_info: n_embd_k_gqa     = 2048
0.00.039.443 I print_info: n_embd_v_gqa     = 2048
0.00.039.443 I print_info: f_norm_eps       = 1.0e-05
0.00.039.444 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.444 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.444 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.444 I print_info: f_logit_scale    = 0.0e+00
0.00.039.445 I print_info: n_ff             = 8192
0.00.039.445 I print_info: n_expert         = 0
0.00.039.445 I print_info: n_expert_used    = 0
0.00.039.445 I print_info: causal attn      = 1
0.00.039.445 I print_info: pooling type     = 0
0.00.039.446 I print_info: rope type        = 2
0.00.039.448 I print_info: rope scaling     = linear
0.00.039.449 I print_info: freq_base_train  = 10000.0
0.00.039.449 I print_info: freq_scale_train = 1
0.00.039.450 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.450 I print_info: rope_finetuned   = unknown
0.00.039.450 I print_info: ssm_d_conv       = 0
0.00.039.450 I print_info: ssm_d_inner      = 0
0.00.039.450 I print_info: ssm_d_state      = 0
0.00.039.450 I print_info: ssm_dt_rank      = 0
0.00.039.450 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.450 I print_info: model type       = 1.4B
0.00.039.451 I print_info: model params     = 1.41 B
0.00.039.451 I print_info: general.name     = 1.4B
0.00.039.451 I print_info: vocab type       = BPE
0.00.039.452 I print_info: n_vocab          = 50304
0.00.039.452 I print_info: n_merges         = 50009
0.00.039.456 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.457 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.457 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.458 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.458 I print_info: LF token         = 128 'Ä'
0.00.039.458 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.458 I print_info: max token length = 1024
0.00.511.729 I load_tensors: offloading 24 repeating layers to GPU
0.00.511.734 I load_tensors: offloading output layer to GPU
0.00.511.735 I load_tensors: offloaded 25/25 layers to GPU
0.00.511.752 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.511.752 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.512.517 I llama_init_from_model: n_seq_max     = 1
0.00.512.522 I llama_init_from_model: n_ctx         = 128
0.00.512.522 I llama_init_from_model: n_ctx_per_seq = 128
0.00.512.522 I llama_init_from_model: n_batch       = 128
0.00.512.523 I llama_init_from_model: n_ubatch      = 128
0.00.512.523 I llama_init_from_model: flash_attn    = 0
0.00.512.524 I llama_init_from_model: freq_base     = 10000.0
0.00.512.525 I llama_init_from_model: freq_scale    = 1
0.00.512.525 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.512.529 I ggml_metal_init: allocating
0.00.512.577 I ggml_metal_init: found device: Apple M4
0.00.512.590 I ggml_metal_init: picking default device: Apple M4
0.00.513.591 I ggml_metal_init: using embedded metal library
0.00.517.651 I ggml_metal_init: GPU name:   Apple M4
0.00.517.658 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.517.658 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.517.659 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.517.659 I ggml_metal_init: simdgroup reduction   = true
0.00.517.660 I ggml_metal_init: simdgroup matrix mul. = true
0.00.517.660 I ggml_metal_init: has residency sets    = true
0.00.517.660 I ggml_metal_init: has bfloat            = true
0.00.517.661 I ggml_metal_init: use bfloat            = true
0.00.517.662 I ggml_metal_init: hasUnifiedMemory      = true
0.00.517.667 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.534.271 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.535.862 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.535.864 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.535.881 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.537.469 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.537.470 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.537.470 I llama_init_from_model: graph nodes  = 967
0.00.537.471 I llama_init_from_model: graph splits = 2
0.00.537.472 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.537.472 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.564.509 I 
0.00.564.581 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.564.608 I perplexity: tokenizing the input ..
0.00.570.200 I perplexity: tokenization took 5.59 ms
0.00.570.219 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.717.570 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.722.957 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.723.002 I llama_perf_context_print:        load time =     555.06 ms
0.00.723.003 I llama_perf_context_print: prompt eval time =     147.11 ms /   128 tokens (    1.15 ms per token,   870.10 tokens per second)
0.00.723.005 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.723.005 I llama_perf_context_print:       total time =     158.49 ms /   129 tokens
0.00.724.260 I ggml_metal_free: deallocating

real	0m0.747s
user	0m0.099s
sys	0m0.087s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.189 I build: 4569 (2b8525d5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.013.851 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.028.169 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.028.179 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.028.183 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.028.184 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.028.184 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.028.185 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.028.185 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.028.187 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.028.188 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.028.188 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.028.189 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.028.189 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.028.190 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.028.191 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.028.198 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.028.199 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.028.199 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.035.370 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.037.188 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.042.899 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.042.900 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.042.901 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.042.901 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.042.901 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.042.902 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.042.903 I llama_model_loader: - type  f32:  194 tensors
0.00.042.903 I llama_model_loader: - type q4_K:   61 tensors
0.00.042.903 I llama_model_loader: - type q5_K:   24 tensors
0.00.042.903 I llama_model_loader: - type q6_K:   13 tensors
0.00.042.904 I print_info: file format = GGUF V3 (latest)
0.00.042.905 I print_info: file type   = Q4_K - Medium
0.00.042.906 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.054.188 I load: special tokens cache size = 25
0.00.061.639 I load: token to piece cache size = 0.2984 MB
0.00.061.642 I print_info: arch             = gptneox
0.00.061.642 I print_info: vocab_only       = 0
0.00.061.643 I print_info: n_ctx_train      = 2048
0.00.061.643 I print_info: n_embd           = 2048
0.00.061.643 I print_info: n_layer          = 24
0.00.061.646 I print_info: n_head           = 16
0.00.061.647 I print_info: n_head_kv        = 16
0.00.061.647 I print_info: n_rot            = 32
0.00.061.647 I print_info: n_swa            = 0
0.00.061.647 I print_info: n_embd_head_k    = 128
0.00.061.648 I print_info: n_embd_head_v    = 128
0.00.061.648 I print_info: n_gqa            = 1
0.00.061.649 I print_info: n_embd_k_gqa     = 2048
0.00.061.650 I print_info: n_embd_v_gqa     = 2048
0.00.061.650 I print_info: f_norm_eps       = 1.0e-05
0.00.061.651 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.061.653 I print_info: f_clamp_kqv      = 0.0e+00
0.00.061.653 I print_info: f_max_alibi_bias = 0.0e+00
0.00.061.653 I print_info: f_logit_scale    = 0.0e+00
0.00.061.654 I print_info: n_ff             = 8192
0.00.061.654 I print_info: n_expert         = 0
0.00.061.654 I print_info: n_expert_used    = 0
0.00.061.656 I print_info: causal attn      = 1
0.00.061.656 I print_info: pooling type     = 0
0.00.061.656 I print_info: rope type        = 2
0.00.061.656 I print_info: rope scaling     = linear
0.00.061.656 I print_info: freq_base_train  = 10000.0
0.00.061.657 I print_info: freq_scale_train = 1
0.00.061.657 I print_info: n_ctx_orig_yarn  = 2048
0.00.061.657 I print_info: rope_finetuned   = unknown
0.00.061.657 I print_info: ssm_d_conv       = 0
0.00.061.657 I print_info: ssm_d_inner      = 0
0.00.061.657 I print_info: ssm_d_state      = 0
0.00.061.658 I print_info: ssm_dt_rank      = 0
0.00.061.658 I print_info: ssm_dt_b_c_rms   = 0
0.00.061.662 I print_info: model type       = 1.4B
0.00.061.662 I print_info: model params     = 1.41 B
0.00.061.663 I print_info: general.name     = 1.4B
0.00.061.663 I print_info: vocab type       = BPE
0.00.061.663 I print_info: n_vocab          = 50304
0.00.061.663 I print_info: n_merges         = 50009
0.00.061.664 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.061.664 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.061.664 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.061.664 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.061.665 I print_info: LF token         = 128 'Ä'
0.00.061.665 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.061.665 I print_info: max token length = 1024
0.00.561.430 I load_tensors: offloading 24 repeating layers to GPU
0.00.561.447 I load_tensors: offloading output layer to GPU
0.00.561.448 I load_tensors: offloaded 25/25 layers to GPU
0.00.561.485 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.561.486 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.562.942 I llama_init_from_model: n_seq_max     = 1
0.00.562.948 I llama_init_from_model: n_ctx         = 128
0.00.562.948 I llama_init_from_model: n_ctx_per_seq = 128
0.00.562.949 I llama_init_from_model: n_batch       = 128
0.00.562.949 I llama_init_from_model: n_ubatch      = 128
0.00.562.950 I llama_init_from_model: flash_attn    = 0
0.00.562.952 I llama_init_from_model: freq_base     = 10000.0
0.00.562.952 I llama_init_from_model: freq_scale    = 1
0.00.562.953 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.562.959 I ggml_metal_init: allocating
0.00.563.062 I ggml_metal_init: found device: Apple M4
0.00.563.077 I ggml_metal_init: picking default device: Apple M4
0.00.564.925 I ggml_metal_init: using embedded metal library
0.00.571.467 I ggml_metal_init: GPU name:   Apple M4
0.00.571.471 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.571.472 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.571.472 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.571.473 I ggml_metal_init: simdgroup reduction   = true
0.00.571.473 I ggml_metal_init: simdgroup matrix mul. = true
0.00.571.473 I ggml_metal_init: has residency sets    = true
0.00.571.474 I ggml_metal_init: has bfloat            = true
0.00.571.474 I ggml_metal_init: use bfloat            = true
0.00.571.475 I ggml_metal_init: hasUnifiedMemory      = true
0.00.571.477 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.588.560 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.591.996 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.592.000 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.592.037 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.595.207 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.595.208 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.595.209 I llama_init_from_model: graph nodes  = 967
0.00.595.209 I llama_init_from_model: graph splits = 2
0.00.595.211 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.595.212 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.627.577 I 
0.00.627.654 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.627.674 I perplexity: tokenizing the input ..
0.00.634.437 I perplexity: tokenization took 6.761 ms
0.00.634.450 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.775.003 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.776.344 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.776.365 I llama_perf_context_print:        load time =     613.72 ms
0.00.776.367 I llama_perf_context_print: prompt eval time =     140.26 ms /   128 tokens (    1.10 ms per token,   912.56 tokens per second)
0.00.776.368 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.776.368 I llama_perf_context_print:       total time =     148.79 ms /   129 tokens
0.00.776.780 I ggml_metal_free: deallocating

real	0m0.798s
user	0m0.100s
sys	0m0.136s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.097 I build: 4569 (2b8525d5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.848 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.597 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.603 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.609 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.610 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.610 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.610 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.612 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.613 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.613 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.614 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.614 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.614 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.615 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.618 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.620 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.621 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.621 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.352 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.367 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.113 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.114 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.114 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.115 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.115 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.115 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.116 I llama_model_loader: - type  f32:  194 tensors
0.00.025.116 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.116 I llama_model_loader: - type q6_K:   37 tensors
0.00.025.117 I print_info: file format = GGUF V3 (latest)
0.00.025.117 I print_info: file type   = Q5_K - Medium
0.00.025.118 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.032.864 I load: special tokens cache size = 25
0.00.038.751 I load: token to piece cache size = 0.2984 MB
0.00.038.753 I print_info: arch             = gptneox
0.00.038.753 I print_info: vocab_only       = 0
0.00.038.754 I print_info: n_ctx_train      = 2048
0.00.038.754 I print_info: n_embd           = 2048
0.00.038.754 I print_info: n_layer          = 24
0.00.038.757 I print_info: n_head           = 16
0.00.038.758 I print_info: n_head_kv        = 16
0.00.038.758 I print_info: n_rot            = 32
0.00.038.758 I print_info: n_swa            = 0
0.00.038.758 I print_info: n_embd_head_k    = 128
0.00.038.759 I print_info: n_embd_head_v    = 128
0.00.038.759 I print_info: n_gqa            = 1
0.00.038.760 I print_info: n_embd_k_gqa     = 2048
0.00.038.761 I print_info: n_embd_v_gqa     = 2048
0.00.038.761 I print_info: f_norm_eps       = 1.0e-05
0.00.038.762 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.762 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.762 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.762 I print_info: f_logit_scale    = 0.0e+00
0.00.038.763 I print_info: n_ff             = 8192
0.00.038.765 I print_info: n_expert         = 0
0.00.038.765 I print_info: n_expert_used    = 0
0.00.038.766 I print_info: causal attn      = 1
0.00.038.766 I print_info: pooling type     = 0
0.00.038.766 I print_info: rope type        = 2
0.00.038.766 I print_info: rope scaling     = linear
0.00.038.766 I print_info: freq_base_train  = 10000.0
0.00.038.767 I print_info: freq_scale_train = 1
0.00.038.767 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.767 I print_info: rope_finetuned   = unknown
0.00.038.767 I print_info: ssm_d_conv       = 0
0.00.038.768 I print_info: ssm_d_inner      = 0
0.00.038.768 I print_info: ssm_d_state      = 0
0.00.038.768 I print_info: ssm_dt_rank      = 0
0.00.038.768 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.768 I print_info: model type       = 1.4B
0.00.038.769 I print_info: model params     = 1.41 B
0.00.038.770 I print_info: general.name     = 1.4B
0.00.038.771 I print_info: vocab type       = BPE
0.00.038.771 I print_info: n_vocab          = 50304
0.00.038.771 I print_info: n_merges         = 50009
0.00.038.771 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.771 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.772 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.772 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.772 I print_info: LF token         = 128 'Ä'
0.00.038.772 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.773 I print_info: max token length = 1024
0.00.601.597 I load_tensors: offloading 24 repeating layers to GPU
0.00.601.600 I load_tensors: offloading output layer to GPU
0.00.601.601 I load_tensors: offloaded 25/25 layers to GPU
0.00.601.623 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.601.624 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.603.072 I llama_init_from_model: n_seq_max     = 1
0.00.603.075 I llama_init_from_model: n_ctx         = 128
0.00.603.075 I llama_init_from_model: n_ctx_per_seq = 128
0.00.603.079 I llama_init_from_model: n_batch       = 128
0.00.603.080 I llama_init_from_model: n_ubatch      = 128
0.00.603.080 I llama_init_from_model: flash_attn    = 0
0.00.603.081 I llama_init_from_model: freq_base     = 10000.0
0.00.603.082 I llama_init_from_model: freq_scale    = 1
0.00.603.083 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.603.084 I ggml_metal_init: allocating
0.00.603.140 I ggml_metal_init: found device: Apple M4
0.00.603.161 I ggml_metal_init: picking default device: Apple M4
0.00.604.664 I ggml_metal_init: using embedded metal library
0.00.610.701 I ggml_metal_init: GPU name:   Apple M4
0.00.610.705 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.610.706 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.610.707 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.610.708 I ggml_metal_init: simdgroup reduction   = true
0.00.610.708 I ggml_metal_init: simdgroup matrix mul. = true
0.00.610.708 I ggml_metal_init: has residency sets    = true
0.00.610.708 I ggml_metal_init: has bfloat            = true
0.00.610.709 I ggml_metal_init: use bfloat            = true
0.00.610.709 I ggml_metal_init: hasUnifiedMemory      = true
0.00.610.711 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.627.407 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.630.838 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.630.841 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.630.869 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.634.135 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.634.137 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.634.137 I llama_init_from_model: graph nodes  = 967
0.00.634.137 I llama_init_from_model: graph splits = 2
0.00.634.140 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.634.140 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.671.175 I 
0.00.671.258 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.671.279 I perplexity: tokenizing the input ..
0.00.677.375 I perplexity: tokenization took 6.094 ms
0.00.677.387 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.822.963 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.824.275 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.824.290 I llama_perf_context_print:        load time =     661.32 ms
0.00.824.291 I llama_perf_context_print: prompt eval time =     145.35 ms /   128 tokens (    1.14 ms per token,   880.66 tokens per second)
0.00.824.291 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.824.292 I llama_perf_context_print:       total time =     153.12 ms /   129 tokens
0.00.824.669 I ggml_metal_free: deallocating

real	0m0.840s
user	0m0.076s
sys	0m0.147s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.095 I build: 4569 (2b8525d5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.805 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.790 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.795 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.796 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.797 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.798 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.799 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.799 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.800 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.800 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.801 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.801 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.801 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.802 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.804 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.805 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.805 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.806 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.577 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.615 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.360 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.361 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.361 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.362 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.362 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.362 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.362 I llama_model_loader: - type  f32:  194 tensors
0.00.024.363 I llama_model_loader: - type q6_K:   98 tensors
0.00.024.363 I print_info: file format = GGUF V3 (latest)
0.00.024.364 I print_info: file type   = Q6_K
0.00.024.368 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.032.056 I load: special tokens cache size = 25
0.00.037.918 I load: token to piece cache size = 0.2984 MB
0.00.037.921 I print_info: arch             = gptneox
0.00.037.921 I print_info: vocab_only       = 0
0.00.037.921 I print_info: n_ctx_train      = 2048
0.00.037.922 I print_info: n_embd           = 2048
0.00.037.922 I print_info: n_layer          = 24
0.00.037.925 I print_info: n_head           = 16
0.00.037.925 I print_info: n_head_kv        = 16
0.00.037.926 I print_info: n_rot            = 32
0.00.037.927 I print_info: n_swa            = 0
0.00.037.928 I print_info: n_embd_head_k    = 128
0.00.037.928 I print_info: n_embd_head_v    = 128
0.00.037.929 I print_info: n_gqa            = 1
0.00.037.930 I print_info: n_embd_k_gqa     = 2048
0.00.037.930 I print_info: n_embd_v_gqa     = 2048
0.00.037.931 I print_info: f_norm_eps       = 1.0e-05
0.00.037.931 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.037.932 I print_info: f_clamp_kqv      = 0.0e+00
0.00.037.932 I print_info: f_max_alibi_bias = 0.0e+00
0.00.037.932 I print_info: f_logit_scale    = 0.0e+00
0.00.037.933 I print_info: n_ff             = 8192
0.00.037.933 I print_info: n_expert         = 0
0.00.037.933 I print_info: n_expert_used    = 0
0.00.037.933 I print_info: causal attn      = 1
0.00.037.933 I print_info: pooling type     = 0
0.00.037.933 I print_info: rope type        = 2
0.00.037.934 I print_info: rope scaling     = linear
0.00.037.934 I print_info: freq_base_train  = 10000.0
0.00.037.934 I print_info: freq_scale_train = 1
0.00.037.935 I print_info: n_ctx_orig_yarn  = 2048
0.00.037.935 I print_info: rope_finetuned   = unknown
0.00.037.935 I print_info: ssm_d_conv       = 0
0.00.037.935 I print_info: ssm_d_inner      = 0
0.00.037.935 I print_info: ssm_d_state      = 0
0.00.037.935 I print_info: ssm_dt_rank      = 0
0.00.037.935 I print_info: ssm_dt_b_c_rms   = 0
0.00.037.936 I print_info: model type       = 1.4B
0.00.037.936 I print_info: model params     = 1.41 B
0.00.037.936 I print_info: general.name     = 1.4B
0.00.037.937 I print_info: vocab type       = BPE
0.00.037.937 I print_info: n_vocab          = 50304
0.00.037.937 I print_info: n_merges         = 50009
0.00.037.937 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.037.937 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.037.938 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.037.938 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.037.938 I print_info: LF token         = 128 'Ä'
0.00.037.938 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.037.938 I print_info: max token length = 1024
0.00.599.096 I load_tensors: offloading 24 repeating layers to GPU
0.00.599.110 I load_tensors: offloading output layer to GPU
0.00.599.111 I load_tensors: offloaded 25/25 layers to GPU
0.00.599.143 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.599.144 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.600.583 I llama_init_from_model: n_seq_max     = 1
0.00.600.587 I llama_init_from_model: n_ctx         = 128
0.00.600.587 I llama_init_from_model: n_ctx_per_seq = 128
0.00.600.588 I llama_init_from_model: n_batch       = 128
0.00.600.588 I llama_init_from_model: n_ubatch      = 128
0.00.600.588 I llama_init_from_model: flash_attn    = 0
0.00.600.590 I llama_init_from_model: freq_base     = 10000.0
0.00.600.590 I llama_init_from_model: freq_scale    = 1
0.00.600.591 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.600.593 I ggml_metal_init: allocating
0.00.600.620 I ggml_metal_init: found device: Apple M4
0.00.600.634 I ggml_metal_init: picking default device: Apple M4
0.00.601.915 I ggml_metal_init: using embedded metal library
0.00.608.160 I ggml_metal_init: GPU name:   Apple M4
0.00.608.164 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.608.165 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.608.166 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.608.167 I ggml_metal_init: simdgroup reduction   = true
0.00.608.167 I ggml_metal_init: simdgroup matrix mul. = true
0.00.608.167 I ggml_metal_init: has residency sets    = true
0.00.608.167 I ggml_metal_init: has bfloat            = true
0.00.608.168 I ggml_metal_init: use bfloat            = true
0.00.608.168 I ggml_metal_init: hasUnifiedMemory      = true
0.00.608.172 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.624.802 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.628.079 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.628.086 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.628.122 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.631.348 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.631.350 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.631.350 I llama_init_from_model: graph nodes  = 967
0.00.631.350 I llama_init_from_model: graph splits = 2
0.00.631.354 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.631.354 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.670.088 I 
0.00.670.168 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.670.196 I perplexity: tokenizing the input ..
0.00.676.614 I perplexity: tokenization took 6.416 ms
0.00.676.629 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.817.001 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.818.403 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.818.420 I llama_perf_context_print:        load time =     661.27 ms
0.00.818.421 I llama_perf_context_print: prompt eval time =     139.91 ms /   128 tokens (    1.09 ms per token,   914.89 tokens per second)
0.00.818.422 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.818.422 I llama_perf_context_print:       total time =     148.34 ms /   129 tokens
0.00.818.819 I ggml_metal_free: deallocating

real	0m0.833s
user	0m0.076s
sys	0m0.133s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.243 I build: 4569 (2b8525d5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.024.478 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.038.718 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.038.724 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.038.726 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.038.727 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.038.727 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.038.733 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.038.734 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.038.736 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.038.737 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.038.741 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.038.742 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.038.743 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.038.743 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.038.744 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.038.750 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.038.751 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.038.752 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.046.989 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.048.818 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.056.040 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.056.042 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.056.042 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.056.043 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.056.043 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.056.044 I llama_model_loader: - type  f32:  194 tensors
0.00.056.044 I llama_model_loader: - type  f16:   98 tensors
0.00.056.045 I print_info: file format = GGUF V3 (latest)
0.00.056.046 I print_info: file type   = all F32 (guessed)
0.00.056.048 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.068.979 I load: special tokens cache size = 25
0.00.077.269 I load: token to piece cache size = 0.2984 MB
0.00.077.273 I print_info: arch             = gptneox
0.00.077.273 I print_info: vocab_only       = 0
0.00.077.273 I print_info: n_ctx_train      = 2048
0.00.077.273 I print_info: n_embd           = 2048
0.00.077.274 I print_info: n_layer          = 24
0.00.077.277 I print_info: n_head           = 16
0.00.077.278 I print_info: n_head_kv        = 16
0.00.077.278 I print_info: n_rot            = 32
0.00.077.278 I print_info: n_swa            = 0
0.00.077.279 I print_info: n_embd_head_k    = 128
0.00.077.281 I print_info: n_embd_head_v    = 128
0.00.077.281 I print_info: n_gqa            = 1
0.00.077.282 I print_info: n_embd_k_gqa     = 2048
0.00.077.284 I print_info: n_embd_v_gqa     = 2048
0.00.077.285 I print_info: f_norm_eps       = 1.0e-05
0.00.077.285 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.077.285 I print_info: f_clamp_kqv      = 0.0e+00
0.00.077.286 I print_info: f_max_alibi_bias = 0.0e+00
0.00.077.286 I print_info: f_logit_scale    = 0.0e+00
0.00.077.287 I print_info: n_ff             = 8192
0.00.077.287 I print_info: n_expert         = 0
0.00.077.287 I print_info: n_expert_used    = 0
0.00.077.287 I print_info: causal attn      = 1
0.00.077.287 I print_info: pooling type     = 0
0.00.077.287 I print_info: rope type        = 2
0.00.077.289 I print_info: rope scaling     = linear
0.00.077.289 I print_info: freq_base_train  = 10000.0
0.00.077.290 I print_info: freq_scale_train = 1
0.00.077.290 I print_info: n_ctx_orig_yarn  = 2048
0.00.077.290 I print_info: rope_finetuned   = unknown
0.00.077.290 I print_info: ssm_d_conv       = 0
0.00.077.290 I print_info: ssm_d_inner      = 0
0.00.077.290 I print_info: ssm_d_state      = 0
0.00.077.291 I print_info: ssm_dt_rank      = 0
0.00.077.291 I print_info: ssm_dt_b_c_rms   = 0
0.00.077.291 I print_info: model type       = 1.4B
0.00.077.291 I print_info: model params     = 1.41 B
0.00.077.292 I print_info: general.name     = 1.4B
0.00.077.292 I print_info: vocab type       = BPE
0.00.077.292 I print_info: n_vocab          = 50304
0.00.077.293 I print_info: n_merges         = 50009
0.00.077.293 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.077.293 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.077.293 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.077.293 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.077.294 I print_info: LF token         = 128 'Ä'
0.00.077.294 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.077.294 I print_info: max token length = 1024
0.01.304.690 I load_tensors: offloading 24 repeating layers to GPU
0.01.304.694 I load_tensors: offloading output layer to GPU
0.01.304.694 I load_tensors: offloaded 25/25 layers to GPU
0.01.304.722 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.304.723 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.01.305.322 I llama_init_from_model: n_seq_max     = 1
0.01.305.323 I llama_init_from_model: n_ctx         = 128
0.01.305.324 I llama_init_from_model: n_ctx_per_seq = 128
0.01.305.324 I llama_init_from_model: n_batch       = 128
0.01.305.324 I llama_init_from_model: n_ubatch      = 128
0.01.305.327 I llama_init_from_model: flash_attn    = 0
0.01.305.327 I llama_init_from_model: freq_base     = 10000.0
0.01.305.328 I llama_init_from_model: freq_scale    = 1
0.01.305.330 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.305.331 I ggml_metal_init: allocating
0.01.305.399 I ggml_metal_init: found device: Apple M4
0.01.305.406 I ggml_metal_init: picking default device: Apple M4
0.01.306.470 I ggml_metal_init: using embedded metal library
0.01.310.301 I ggml_metal_init: GPU name:   Apple M4
0.01.310.304 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.310.304 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.310.305 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.310.305 I ggml_metal_init: simdgroup reduction   = true
0.01.310.305 I ggml_metal_init: simdgroup matrix mul. = true
0.01.310.305 I ggml_metal_init: has residency sets    = true
0.01.310.305 I ggml_metal_init: has bfloat            = true
0.01.310.306 I ggml_metal_init: use bfloat            = true
0.01.310.306 I ggml_metal_init: hasUnifiedMemory      = true
0.01.310.307 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.321.093 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.322.821 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.322.823 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.322.853 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.324.513 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.324.515 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.324.515 I llama_init_from_model: graph nodes  = 967
0.01.324.515 I llama_init_from_model: graph splits = 2
0.01.324.516 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.324.517 I 
0.01.324.556 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.324.557 I compute_imatrix: tokenizing the input ..
0.01.328.753 I compute_imatrix: tokenization took 4.194 ms
0.01.328.755 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.594.563 I compute_imatrix: 0.27 seconds per pass - ETA 0.00 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.599.505 I llama_perf_context_print:        load time =    1570.08 ms
0.01.599.506 I llama_perf_context_print: prompt eval time =     264.06 ms /   128 tokens (    2.06 ms per token,   484.73 tokens per second)
0.01.599.506 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.599.507 I llama_perf_context_print:       total time =    1575.01 ms /   129 tokens
0.01.600.033 I ggml_metal_free: deallocating

real	0m1.797s
user	0m0.127s
sys	0m0.254s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4569 (2b8525d5)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 'Ä'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12ae07ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12ae085f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12ae08ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12ae09150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12ae09700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12ae09cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12ae0a260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12ae0a810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12ae0adc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12ae0b2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12ae0b7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12ae0bcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12ae0c7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12ae0cf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12ae0d7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12ae0dec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12ae0e5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12ae0ed00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12ae0f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12ae0fbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12ae10310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12ae10a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12ae11150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12ae119f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12ae12110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12ae123d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12ae129e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12ae13650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12ae13b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12ae13e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12ae142f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12ae145b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12ae14e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12ae15380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12ae15640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12ae15ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12ae15f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12ae16420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12ae168c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12ae16d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12ae17200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12ae176a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12ae17b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12ae17fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12ae182a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12ae188b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12ae18ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12ae197e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12ae19df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12ae1a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12ae1aa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12ae1b020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12ae1b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12ae1bc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12ae1c430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12ae1c8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12ae1cd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12ae1d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12ae1d640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12ae1de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12ae1e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12ae1e590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12ae1ea30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12ae1eed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12ae1f370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12ae1f810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12ae1fcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12ae20150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12ae205f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12ae20a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12ae20f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12ae213d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12ae21870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12ae21dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12ae22310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12ae22860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12ae22db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12ae23300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12ae23850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12ae23da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12ae242f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12ae24840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12ae24d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12ae252e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12ae25830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12ae25d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12ae262d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12ae26820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12ae26d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12ae272c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12ae27810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12ae27d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12ae282b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12ae28800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12ae28d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12ae292a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12ae297f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12ae194d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12ae29c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12ae2a410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12ae2a960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12ae2aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12ae2b400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12ae2b950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12ae2bea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12ae2c3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12ae2c940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12ae2ce90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12ae2d3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12ae2d930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12ae2de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12ae2e3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12ae2e920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12ae2edc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12ae2f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12ae2f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12ae2fba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12ae30040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12ae304e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12ae30980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12ae30e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12ae312c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12ae31760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12ae31c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12ae320a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12ae32540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12ae329e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12ae32e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12ae33320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12ae337c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12ae33c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12ae34100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12ae345a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12ae34a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12ae34ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12ae35380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12ae35820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12ae35cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12ae36160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12ae36600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12ae36aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12ae36f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12ae373e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12ae37880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12ae37d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12ae381c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12ae38660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12ae38b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12ae38fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12ae39440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12ae398e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12ae39d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12ae3a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12ae3a6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12ae3ab60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12ae3b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12ae3b4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12ae3b940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12ae3bde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12ae3c280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12ae3c720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12ae3cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12ae3d060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12ae3d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12ae3d9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12ae3de40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12ae3e2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12ae3e780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12ae3ec20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12ae3f0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12ae3f560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12ae3fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12ae3fea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12ae40340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12ae407e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12ae40c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12ae41120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12ae415c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12ae41a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12ae41f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12ae423a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12ae42840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12ae42ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12ae43180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12ae43620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12ae43ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12ae43f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12ae44400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12ae448a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12ae44d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12ae451e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12ae45680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12ae45b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12ae46070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12ae465c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12ae46b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12ae47060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12ae47320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12ae47930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12ae47f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12ae48550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12ae48d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12ae491e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12ae494a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12ae49ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12ae4a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12ae4a8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12ae4ad50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12ae4b1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12ae4b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12ae4be40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12ae4c390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12ae4c8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12ae4ce30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12ae4d380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12ae4d8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12ae4de20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12ae4e370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12ae4e8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12ae4ee10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12ae4f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12ae4f8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12ae4fe00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12ae50350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12ae508a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12ae50df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12ae51340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12ae51890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12ae51de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12ae52330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12ae52880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12ae52dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12ae53320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12ae53870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12ae53dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12ae54310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12ae54860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12ae54db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12ae55300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12ae55850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12ae55da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12ae562f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12ae56840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12ae56d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12ae572e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12ae57830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12ae57d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12ae582d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12ae58820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12ae58d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12ae592c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12ae59810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12ae59d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12ae5a2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12ae5a800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12ae5ad50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12ae5b2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12ae5b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12ae5bd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12ae5c290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12ae5c7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12ae5cd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12ae5d280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12ae5d7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12ae5dd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12ae5e270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12ae5e7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12ae5ec60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12ae5f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12ae5f5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12ae5fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12ae5fee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12ae60380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12ae60820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12ae60cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12ae61160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12ae61600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12ae61aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12ae61f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12ae623e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12ae62880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12ae62d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12ae63270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12ae63990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12ae640b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12ae647d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12ae64ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12ae651b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12ae659a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12ae65c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12ae66270 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.736.604 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.736.608 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13c204ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13c205150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13c2055c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13c205a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13c205ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13c206310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13c206780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13c206bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13c207060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13c2074d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13c207940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13c207fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13c208ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13c209280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13c209a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13c20a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13c20a8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13c20aff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13c20b710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13c20bee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13c20c600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13c20cd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13c20d440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13c20db60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13c20e280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13c20e540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13c20e800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13c20ec70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13c20f0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13c20f550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13c20f9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13c20fef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13c210360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13c210620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13c210a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13c210f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13c211370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13c2117e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13c211c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13c2120c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13c212530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13c2129a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13c212e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13c213280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13c2136f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13c213b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13c213fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13c214440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13c2148b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13c214d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13c215190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13c215600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13c215a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13c215ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13c216350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13c2167c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13c216d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13c217230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13c2176a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13c217b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13c217f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13c2183f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13c218860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13c218cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13c219140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13c2195b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13c219a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13c219e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13c21a300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13c21a770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13c21abe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13c21b050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13c21b4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13c21b930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13c21bda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13c21c210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13c21c680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13c21caf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13c21cf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13c21d3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13c21d840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13c21dcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13c21e120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13c21e590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13c21ea00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13c21ee70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13c21f2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13c21f750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13c21fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13c220030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13c2204a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13c220910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13c220d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13c2211f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13c221660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13c221ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13c221f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13c2223b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13c222820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13c222c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13c223100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13c223570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13c2239e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13c223e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13c2242c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13c224730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13c224ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13c225010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13c225480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13c2258f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13c225d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13c2261d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13c226640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13c226ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13c226f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13c227390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13c227800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13c227c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13c2280e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13c228550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13c2289c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13c228e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13c2292a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13c229710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13c229b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13c229ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13c22a460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13c22a8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13c22ad40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13c22b1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13c22b620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13c22ba90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13c22bf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13c22c370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13c22c7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13c22cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13c22d0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13c22d530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13c22d9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13c22de10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13c22e280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13c22e6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13c22eb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13c22efd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13c22f440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13c22f8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13c22fd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13c230190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13c230600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13c230a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13c230ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13c231350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13c2317c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13c231c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13c2320a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13c232510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13c232980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13c232df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13c233260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13c2336d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13c233b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13c233fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13c234420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13c234890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13c234d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13c235170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13c235da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13c236060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13c236320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13c236790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13c236c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13c237070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13c2374e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13c237950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13c237dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13c238230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13c2386a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13c238b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13c238f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13c2393f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13c239860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13c239cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13c23a140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13c23a5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13c23aa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13c23ae90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13c23b300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13c23b770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13c23bbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13c23c050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13c23c4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13c23c930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13c23cda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13c23d210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13c23d680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13c23daf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13c23df60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13c23e3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13c23e840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13c23ecb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13c23f120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13c23f590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13c23faf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13c240000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13c240470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13c2408e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13c240d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13c2411c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13c2416e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13c241bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13c242760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13c242a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13c242fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13c2435a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13c243b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13c244120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13c2446e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13c244ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13c245260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13c245820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13c245de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13c2463a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13c246960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13c246f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13c2474e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13c247aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13c248060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13c248620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13c248be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13c2491a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13c249760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13c249d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13c24a2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13c24a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13c24ae60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13c24b420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13c24b9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13c24bfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13c24c560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13c24cb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13c24d0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13c24d6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13c24dc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13c24e220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13c24e7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13c24eda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13c24f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13c24f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13c24fee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13c2504a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13c250a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13c251020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13c2515e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13c251ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13c252160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13c252720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13c252ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13c2532a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13c253860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13c253e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13c2543e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13c2549a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13c254f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13c255520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13c255ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13c2560a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13c256660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13c256c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13c257120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13c257620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13c257b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13c258020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13c258520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13c258a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13c258f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13c259420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13c259920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13c259e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13c25a320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13c25a820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13c25ad20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13c25b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13c25b720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13c25c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13c25c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13c25cf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13c25d690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13c25d950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13c25e140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13c25e400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13c25ea10 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x11cb044e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x11cb04950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x11cb04dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x11cb05230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x11cb056a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x11cb05b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x11cb05f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x11cb063f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x11cb06860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x11cb06cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x11cb07140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x11cb077c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x11cb082e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x11cb08a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x11cb092a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x11cb099c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x11cb0a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x11cb0a800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x11cb0af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x11cb0b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x11cb0be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x11cb0c530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x11cb0cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x11cb0d370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x11cb0da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x11cb0dd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x11cb0e010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x11cb0e480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x11cb0e8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x11cb0ed60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x11cb0f1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x11cb0f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x11cb0fb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x11cb0fe30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x11cb102a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x11cb10710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x11cb10b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x11cb10ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x11cb11460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x11cb118d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x11cb11d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x11cb121b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x11cb12620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x11cb12a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x11cb12f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x11cb13370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x11cb137e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x11cb13c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x11cb140c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x11cb14530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x11cb149a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x11cb14e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x11cb15280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x11cb156f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x11cb15b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x11cb15fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x11cb16540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x11cb16a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x11cb16eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x11cb17320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x11cb17790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x11cb17c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x11cb18070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x11cb184e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x11cb18950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x11cb18dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x11cb19230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x11cb196a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x11cb19b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x11cb19f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x11cb1a3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x11cb1a860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x11cb1acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x11cb1b140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x11cb1b5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x11cb1ba20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x11cb1be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x11cb1c300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x11cb1c770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x11cb1cbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x11cb1d050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x11cb1d4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x11cb1d930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x11cb1dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x11cb1e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x11cb1e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x11cb1eaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x11cb1ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x11cb1f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x11cb1f840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x11cb1fcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x11cb20120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x11cb20590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x11cb20a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x11cb20e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x11cb212e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x11cb21750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x11cb21bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x11cb22030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x11cb224a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x11cb22910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x11cb22d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x11cb231f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x11cb23a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x11cb23d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x11cb241b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x11cb24620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x11cb24a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x11cb24f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x11cb25370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x11cb257e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x11cb25c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x11cb260c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x11cb26530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x11cb269a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x11cb26e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x11cb27280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x11cb276f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x11cb27b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x11cb27fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x11cb28440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x11cb288b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x11cb28d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x11cb29190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x11cb29600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x11cb29a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x11cb29ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x11cb2a350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x11cb2a7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x11cb2ac30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x11cb2b0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x11cb2b510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x11cb2b980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x11cb2bdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x11cb2c260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x11cb2c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x11cb2cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x11cb2cfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x11cb2d420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x11cb2d890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x11cb2dd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x11cb2e170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x11cb2e5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x11cb2ea50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x11cb2eec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x11cb2f330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x11cb2f7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x11cb2fc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x11cb30080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x11cb304f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x11cb30960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x11cb30dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x11cb31240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x11cb316b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x11cb31b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x11cb31f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x11cb32400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x11cb32870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x11cb32ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x11cb33150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x11cb335c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x11cb33a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x11cb33ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x11cb34310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x11cb34780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x11cb34bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x11cb35060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x11cb354d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x11cb35940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x11cb35db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x11cb36220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x11cb36690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x11cb36b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x11cb36f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x11cb373e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x11cb37850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x11cb37cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x11cb38130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x11cb385a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x11cb38a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x11cb38e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x11cb392f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x11cb39760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x11cb39bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x11cb3a040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x11cb3a4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x11cb3a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x11cb3ad90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x11cb3b200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x11cb3b670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x11cb3bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x11cb3bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x11cb3c3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x11cb3c830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x11cb3cca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x11cb3d110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x11cb3d580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x11cb3d9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x11cb3de60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x11cb3e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x11cb3e740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x11cb3ebb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x11cb3f020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x11cb3f490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x11cb3f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x11cb3fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x11cb401e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x11cb40650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x11cb40ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x11cb40f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x11cb41ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x11cb41d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x11cb42030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x11cb424a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x11cb42910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x11cb42d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x11cb431f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x11cb43660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x11cb43ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x11cb43f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x11cb443b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x11cb44820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x11cb44c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x11cb45100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x11cb45570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x11cb459e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x11cb45e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x11cb462c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x11cb46730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x11cb46ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x11cb47010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x11cb47480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x11cb478f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x11cb47d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x11cb481d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x11cb48640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x11cb48ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x11cb48f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x11cb49390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x11cb49800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x11cb49c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x11cb4a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x11cb4a550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x11cb4a9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x11cb4ae30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x11cb4b2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x11cb4b710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x11cb4bb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x11cb4bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x11cb4c460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x11cb4c8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x11cb4cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x11cb4d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x11cb4d620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x11cb4da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x11cb4df00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x11cb4e370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x11cb4e7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x11cb4ec50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x11cb4f0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x11cb4f530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x11cb4f9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x11cb4fe10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x11cb50280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x11cb506f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x11cb50b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x11cb50fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x11cb51440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x11cb518b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x11cb51d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x11cb52190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x11cb52600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x11cb52a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x11cb52ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x11cb53350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x11cb537c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x11cb53c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x11cb540a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x11cb54510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x11cb54980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x11cb54df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x11cb55260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x11cb556d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x11cb56140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x11cb56860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x11cb56f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x11cb576a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x11cb57960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x11cb57dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x11cb583d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x11cb589e0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.794s
user	0m0.282s
sys	0m0.318s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4569 (2b8525d5)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 'Ä'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13ee0e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13ee0ed90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13ee0f340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13ee0f8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13ee0fea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13ee10450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13ee10a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13ee10fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13ee11560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13ee11a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13ee11f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13ee12460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13ee12f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13ee13730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13ee13f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13ee14660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13ee14d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13ee154a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13ee15bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13ee16390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13ee16ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13ee171d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13ee178f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13ee18190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13ee188b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13ee18b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13ee19180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13ee19df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13ee1a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13ee1a5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13ee1aa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13ee1ad50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13ee1b5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13ee1bb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13ee1bde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13ee1c280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13ee1c720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13ee1cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13ee1d060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13ee1d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13ee1d9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13ee1de40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13ee1e2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13ee1e780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13ee1ea40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13ee1f050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13ee1f660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13ee1ff80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13ee20590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13ee20ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13ee211b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13ee217c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13ee21dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13ee223e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13ee22bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13ee23070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13ee23510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13ee237d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13ee23de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13ee245d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13ee24890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13ee24d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13ee251d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13ee25670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13ee25b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13ee25fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13ee26450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13ee268f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13ee26d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13ee27230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13ee276d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13ee27b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13ee28010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13ee28560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13ee28ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13ee29000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13ee29550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13ee29aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13ee29ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13ee2a540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13ee2aa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13ee2afe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13ee2b530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13ee2ba80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13ee2bfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13ee2c520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13ee2ca70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13ee2cfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13ee2d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13ee2da60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13ee2dfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13ee2e500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13ee2ea50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13ee2efa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13ee2f4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13ee2fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13ee2ff90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13ee1fc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13ee30400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13ee30bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13ee31100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13ee31650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13ee31ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13ee320f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13ee32640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13ee32b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13ee330e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13ee33630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13ee33b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13ee340d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13ee34620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13ee34b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13ee350c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13ee35560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13ee35a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13ee35ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13ee36340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13ee367e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13ee36c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13ee37120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13ee375c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13ee37a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13ee37f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13ee383a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13ee38840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13ee38ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13ee39180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13ee39620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13ee39ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13ee39f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13ee3a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13ee3a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13ee3ad40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13ee3b1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13ee3b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13ee3bb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13ee3bfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13ee3c460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13ee3c900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13ee3cda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13ee3d240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13ee3d6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13ee3db80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13ee3e020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13ee3e4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13ee3e960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13ee3ee00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13ee3f2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13ee3f740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13ee3fbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13ee40080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13ee40520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13ee409c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13ee40e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13ee41300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13ee417a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13ee41c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13ee420e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13ee42580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13ee42a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13ee42ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13ee43360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13ee43800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13ee43ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13ee44140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13ee445e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13ee44a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13ee44f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13ee453c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13ee45860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13ee45d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13ee461a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13ee46640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13ee46ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13ee46f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13ee47420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13ee478c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13ee47d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13ee48200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13ee486a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13ee48b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13ee48fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13ee49480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13ee49920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13ee49dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13ee4a260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13ee4a700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13ee4aba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13ee4b040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13ee4b4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13ee4b980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13ee4be20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13ee4c2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13ee4c810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13ee4cd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13ee4d2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13ee4d800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13ee4dac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13ee4e0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13ee4e6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13ee4ecf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13ee4f4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13ee4f980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13ee4fc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13ee50250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13ee50860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13ee51050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13ee514f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13ee51990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13ee51e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13ee525e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13ee52b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13ee53080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13ee535d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13ee53b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13ee54070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13ee545c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13ee54b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13ee55060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13ee555b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13ee55b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13ee56050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13ee565a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13ee56af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13ee57040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13ee57590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13ee57ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13ee58030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13ee58580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13ee58ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13ee59020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13ee59570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13ee59ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13ee5a010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13ee5a560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13ee5aab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13ee5b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13ee5b550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13ee5baa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13ee5bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13ee5c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13ee5ca90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13ee5cfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13ee5d530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13ee5da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13ee5dfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13ee5e520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13ee5ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13ee5efc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13ee5f510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13ee5fa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13ee5ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13ee60500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13ee60a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13ee60fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13ee614f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13ee61a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13ee61f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13ee624e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13ee62a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13ee62f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13ee634d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13ee63a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13ee63f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13ee644c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13ee64a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13ee64f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13ee65400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13ee658a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13ee65d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13ee661e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13ee66680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13ee66b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13ee66fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13ee67460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13ee67900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13ee67da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13ee68240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13ee686e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13ee68b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13ee69020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13ee694c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13ee69a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13ee6a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13ee6a850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13ee6af70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13ee6b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13ee6b950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13ee6c140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13ee6c400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13ee6ca10 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.103.473 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.103.478 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13ef079e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13ef07e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13ef082c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13ef08730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13ef08ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13ef09010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13ef09480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13ef098f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13ef09d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13ef0a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13ef0a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13ef0ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13ef0b820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13ef0bfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13ef0c7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13ef0cf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13ef0d620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13ef0dd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13ef0e460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13ef0ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13ef0f350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13ef0fa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13ef10190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13ef108b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13ef10fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13ef11290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13ef11550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13ef119c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13ef11e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13ef122a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13ef12710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13ef12c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13ef130b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13ef13370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13ef137e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13ef13c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13ef140c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13ef14530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13ef149a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13ef14e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13ef15280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13ef156f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13ef15b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13ef15fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13ef16440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13ef168b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13ef16d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13ef17190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13ef17600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13ef17a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13ef17ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13ef18350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13ef187c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13ef18c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13ef190a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13ef19510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13ef19a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13ef19f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13ef1a3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13ef1a860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13ef1acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13ef1b140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13ef1b5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13ef1ba20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13ef1be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13ef1c300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13ef1c770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13ef1cbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13ef1d050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13ef1d4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13ef1d930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13ef1dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13ef1e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13ef1e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13ef1eaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13ef1ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13ef1f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13ef1f840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13ef1fcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13ef20120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13ef20590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13ef20a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13ef20e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13ef212e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13ef21750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13ef21bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13ef22030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13ef224a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13ef22910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13ef22d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13ef231f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13ef23660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13ef23ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13ef23f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13ef243b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13ef24820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13ef24c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13ef25100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13ef25570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13ef259e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13ef25e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13ef262c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13ef26730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13ef26ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13ef27010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13ef27480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13ef278f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13ef27d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13ef281d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13ef28640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13ef28ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13ef28f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13ef29390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13ef29800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13ef29c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13ef2a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13ef2a550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13ef2a9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13ef2ae30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13ef2b2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13ef2b710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13ef2bb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13ef2bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13ef2c460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13ef2c8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13ef2cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13ef2d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13ef2d620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13ef2da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13ef2df00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13ef2e370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13ef2e7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13ef2ec50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13ef2f0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13ef2f530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13ef2f9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13ef2fe10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13ef30280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13ef306f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13ef30b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13ef30fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13ef31440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13ef318b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13ef31d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13ef32190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13ef32600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13ef32a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13ef32ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13ef33350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13ef337c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13ef33c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13ef340a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13ef34510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13ef34980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13ef34df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13ef35260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13ef356d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13ef35b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13ef35fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13ef36420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13ef36890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13ef36d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13ef37170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13ef375e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13ef37a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13ef37ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13ef38af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13ef38db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13ef39070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13ef394e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13ef39950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13ef39dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13ef3a230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13ef3a6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13ef3ab10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13ef3af80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13ef3b3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13ef3b860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13ef3bcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13ef3c140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13ef3c5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13ef3ca20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13ef3ce90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13ef3d300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13ef3d770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13ef3dbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13ef3e050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13ef3e4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13ef3e930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13ef3eda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13ef3f210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13ef3f680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13ef3faf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13ef3ff60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13ef403d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13ef40840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13ef40cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13ef41120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13ef41590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13ef41a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13ef41e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13ef422e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13ef42840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13ef42d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13ef431c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13ef43630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13ef43aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13ef43f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13ef44430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13ef44940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13ef454b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13ef45770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13ef45d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13ef462f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13ef468b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13ef46e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13ef47430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13ef479f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13ef47fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13ef48570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13ef48b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13ef490f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13ef496b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13ef49c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13ef4a230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13ef4a7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13ef4adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13ef4b370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13ef4b930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13ef4bef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13ef4c4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13ef4ca70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13ef4d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13ef4d5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13ef4dbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13ef4e170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13ef4e730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13ef4ecf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13ef4f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13ef4f870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13ef4fe30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13ef503f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13ef509b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13ef50f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13ef51530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13ef51af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13ef520b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13ef52670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13ef52c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13ef531f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13ef537b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13ef53d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13ef54330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13ef548f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13ef54eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13ef55470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13ef55a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13ef55ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13ef565b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13ef56b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13ef57130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13ef576f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13ef57cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13ef58270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13ef58830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13ef58df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13ef593b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13ef59970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13ef59e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13ef5a370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13ef5a870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13ef5ad70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13ef5b270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13ef5b770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13ef5bc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13ef5c170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13ef5c670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13ef5cb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13ef5d070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13ef5d570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13ef5da70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13ef5df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13ef5e470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13ef5ee80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13ef5f5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13ef5fcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13ef603e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13ef606a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13ef60e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13ef61150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13ef61760 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12ef046e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12ef04b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12ef04fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12ef05430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12ef058a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12ef05d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12ef06180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12ef065f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12ef06a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12ef06ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12ef07340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12ef07a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12ef08580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12ef08d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12ef09540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12ef09c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12ef0a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12ef0aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12ef0b1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12ef0b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12ef0c010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12ef0c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12ef0ce50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12ef0d570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12ef0dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12ef0df50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12ef0e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12ef0e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12ef0eaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12ef0ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12ef0f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12ef0f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12ef0fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12ef10030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12ef104a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12ef10910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12ef10d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12ef111f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12ef11660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12ef11ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12ef11f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12ef123b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12ef12820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12ef12c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12ef13100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12ef13570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12ef139e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12ef13e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12ef142c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12ef14730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12ef14ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12ef15010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12ef15480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12ef158f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12ef15d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12ef161d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12ef16740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12ef16c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12ef170b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12ef17520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12ef17990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12ef17e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12ef18270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12ef186e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12ef18b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12ef18fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12ef19430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12ef198a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12ef19d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12ef1a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12ef1a5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12ef1aa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12ef1aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12ef1b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12ef1b7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12ef1bc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12ef1c090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12ef1c500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12ef1c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12ef1cde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12ef1d250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12ef1d6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12ef1db30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12ef1dfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12ef1e410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12ef1e880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12ef1ecf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12ef1f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12ef1f5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12ef1fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12ef1feb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12ef20320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12ef20790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12ef20c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12ef21070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12ef214e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12ef21950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12ef21dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12ef22230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12ef226a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12ef22b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12ef22f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12ef233f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12ef23c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12ef23f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12ef243b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12ef24820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12ef24c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12ef25100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12ef25570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12ef259e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12ef25e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12ef262c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12ef26730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12ef26ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12ef27010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12ef27480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12ef278f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12ef27d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12ef281d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12ef28640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12ef28ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12ef28f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12ef29390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12ef29800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12ef29c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12ef2a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12ef2a550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12ef2a9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12ef2ae30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12ef2b2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12ef2b710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12ef2bb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12ef2bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12ef2c460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12ef2c8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12ef2cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12ef2d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12ef2d620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12ef2da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12ef2df00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12ef2e370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12ef2e7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12ef2ec50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12ef2f0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12ef2f530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12ef2f9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12ef2fe10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12ef30280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12ef306f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12ef30b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12ef30fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12ef31440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12ef318b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12ef31d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12ef32190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12ef32600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12ef32a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12ef32ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12ef33350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12ef337c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12ef33c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12ef340a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12ef34510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12ef34980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12ef34df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12ef35260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12ef356d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12ef35b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12ef35fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12ef36420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12ef36890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12ef36d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12ef37170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12ef375e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12ef37a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12ef37ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12ef38330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12ef387a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12ef38c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12ef39080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12ef394f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12ef39960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12ef39dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12ef3a240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12ef3a6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12ef3ab20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12ef3af90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12ef3b400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12ef3b870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12ef3bce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12ef3c150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12ef3c5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12ef3ca30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12ef3cea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12ef3d310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12ef3d780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12ef3dbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12ef3e060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12ef3e4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12ef3e940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12ef3edb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12ef3f220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12ef3f690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12ef3fb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12ef3ff70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12ef403e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12ef40850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12ef40cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12ef41130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12ef41cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12ef41f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12ef42230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12ef426a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12ef42b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12ef42f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12ef433f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12ef43860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12ef43cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12ef44140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12ef445b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12ef44a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12ef44e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12ef45300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12ef45770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12ef45be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12ef46050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12ef464c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12ef46930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12ef46da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12ef47210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12ef47680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12ef47af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12ef47f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12ef483d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12ef48840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12ef48cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12ef49120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12ef49590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12ef49a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12ef49e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12ef4a2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12ef4a750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12ef4abc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12ef4b030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12ef4b4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12ef4b910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12ef4bd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12ef4c1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12ef4c660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12ef4cad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12ef4cf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12ef4d3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12ef4d820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12ef4dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12ef4e100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12ef4e570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12ef4e9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12ef4ee50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12ef4f2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12ef4f730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12ef4fba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12ef50010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12ef50480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12ef508f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12ef50d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12ef511d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12ef51640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12ef51ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12ef51f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12ef52390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12ef52800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12ef52c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12ef530e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12ef53550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12ef539c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12ef53e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12ef542a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12ef54710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12ef54b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12ef54ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12ef55460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12ef558d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12ef56340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12ef56a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12ef57180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12ef578a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12ef57b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12ef57fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12ef585d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12ef58be0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.964s
user	0m0.235s
sys	0m0.185s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
