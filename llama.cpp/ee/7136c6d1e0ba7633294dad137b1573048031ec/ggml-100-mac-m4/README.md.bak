### ctest_debug

Runs ctest in debug mode
- status: 0
```
+ ctest --output-on-failure -L main -E test-opt
Test project /Users/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-tokenizer-0-bert-bge
 1/28 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.21 sec
      Start  2: test-tokenizer-0-command-r
 2/28 Test  #2: test-tokenizer-0-command-r ........   Passed    1.83 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/28 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.23 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/28 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.70 sec
      Start  5: test-tokenizer-0-falcon
 5/28 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.43 sec
      Start  6: test-tokenizer-0-gpt-2
 6/28 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.34 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/28 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    1.47 sec
      Start  8: test-tokenizer-0-llama-spm
 8/28 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.08 sec
      Start  9: test-tokenizer-0-mpt
 9/28 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.34 sec
      Start 10: test-tokenizer-0-phi-3
10/28 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.08 sec
      Start 11: test-tokenizer-0-qwen2
11/28 Test #11: test-tokenizer-0-qwen2 ............   Passed    1.02 sec
      Start 12: test-tokenizer-0-refact
12/28 Test #12: test-tokenizer-0-refact ...........   Passed    0.33 sec
      Start 13: test-tokenizer-0-starcoder
13/28 Test #13: test-tokenizer-0-starcoder ........   Passed    0.34 sec
      Start 14: test-sampling
14/28 Test #14: test-sampling .....................   Passed    2.13 sec
      Start 15: test-grammar-parser
15/28 Test #15: test-grammar-parser ...............   Passed    0.26 sec
      Start 16: test-grammar-integration
16/28 Test #16: test-grammar-integration ..........   Passed    0.23 sec
      Start 17: test-llama-grammar
17/28 Test #17: test-llama-grammar ................   Passed    0.18 sec
      Start 18: test-json-schema-to-grammar
18/28 Test #18: test-json-schema-to-grammar .......   Passed    2.22 sec
      Start 19: test-tokenizer-1-llama-spm
19/28 Test #19: test-tokenizer-1-llama-spm ........   Passed    1.06 sec
      Start 20: test-log
20/28 Test #20: test-log ..........................   Passed    0.21 sec
      Start 21: test-arg-parser
21/28 Test #21: test-arg-parser ...................   Passed    0.29 sec
      Start 22: test-chat-template
22/28 Test #22: test-chat-template ................   Passed    0.18 sec
      Start 23: test-gguf
23/28 Test #23: test-gguf .........................   Passed    0.82 sec
      Start 24: test-backend-ops
24/28 Test #24: test-backend-ops ..................   Passed  180.81 sec
      Start 27: test-barrier
25/28 Test #27: test-barrier ......................   Passed    0.91 sec
      Start 28: test-quantize-fns
26/28 Test #28: test-quantize-fns .................   Passed   25.97 sec
      Start 29: test-quantize-perf
27/28 Test #29: test-quantize-perf ................   Passed    0.33 sec
      Start 30: test-rope
28/28 Test #30: test-rope .........................   Passed    0.21 sec

100% tests passed, 0 tests failed out of 28

Label Time Summary:
main    = 224.22 sec*proc (28 tests)

Total Test time (real) = 224.23 sec

real	3m44.263s
user	7m51.151s
sys	0m6.341s
```

### ctest_release

Runs ctest in release mode
- status: 0
```
+ ctest --output-on-failure -L main
Test project /Users/ggml/work/llama.cpp/build-ci-release
      Start  1: test-tokenizer-0-bert-bge
 1/28 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.12 sec
      Start  2: test-tokenizer-0-command-r
 2/28 Test  #2: test-tokenizer-0-command-r ........   Passed    0.31 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/28 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.04 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/28 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.11 sec
      Start  5: test-tokenizer-0-falcon
 5/28 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.07 sec
      Start  6: test-tokenizer-0-gpt-2
 6/28 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.06 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/28 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.22 sec
      Start  8: test-tokenizer-0-llama-spm
 8/28 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.03 sec
      Start  9: test-tokenizer-0-mpt
 9/28 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.06 sec
      Start 10: test-tokenizer-0-phi-3
10/28 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.03 sec
      Start 11: test-tokenizer-0-qwen2
11/28 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.16 sec
      Start 12: test-tokenizer-0-refact
12/28 Test #12: test-tokenizer-0-refact ...........   Passed    0.06 sec
      Start 13: test-tokenizer-0-starcoder
13/28 Test #13: test-tokenizer-0-starcoder ........   Passed    0.06 sec
      Start 14: test-sampling
14/28 Test #14: test-sampling .....................   Passed    0.91 sec
      Start 15: test-grammar-parser
15/28 Test #15: test-grammar-parser ...............   Passed    0.16 sec
      Start 16: test-grammar-integration
16/28 Test #16: test-grammar-integration ..........   Passed    0.17 sec
      Start 17: test-llama-grammar
17/28 Test #17: test-llama-grammar ................   Passed    0.17 sec
      Start 18: test-json-schema-to-grammar
18/28 Test #18: test-json-schema-to-grammar .......   Passed    2.16 sec
      Start 19: test-tokenizer-1-llama-spm
19/28 Test #19: test-tokenizer-1-llama-spm ........   Passed    0.31 sec
      Start 20: test-log
20/28 Test #20: test-log ..........................   Passed    0.17 sec
      Start 21: test-arg-parser
21/28 Test #21: test-arg-parser ...................   Passed    0.23 sec
      Start 22: test-chat-template
22/28 Test #22: test-chat-template ................   Passed    0.16 sec
      Start 23: test-gguf
23/28 Test #23: test-gguf .........................   Passed    0.52 sec
      Start 24: test-backend-ops
24/28 Test #24: test-backend-ops ..................   Passed   29.25 sec
      Start 27: test-barrier
25/28 Test #27: test-barrier ......................   Passed    0.40 sec
      Start 28: test-quantize-fns
26/28 Test #28: test-quantize-fns .................   Passed   14.13 sec
      Start 29: test-quantize-perf
27/28 Test #29: test-quantize-perf ................   Passed    0.20 sec
      Start 30: test-rope
28/28 Test #30: test-rope .........................   Passed    0.20 sec

100% tests passed, 0 tests failed out of 28

Label Time Summary:
main    =  51.50 sec*proc (28 tests)

Total Test time (real) =  51.51 sec

real	0m51.524s
user	1m12.110s
sys	0m5.562s
```
### embd_bge_small

BGE Small (BERT):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.073 I build: 4457 (ee7136c6) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.015.328 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.020.473 I llama_model_loader: loaded meta data with 25 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.020.481 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.020.484 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.020.485 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.020.486 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.020.486 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.020.487 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.020.492 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.020.493 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.020.494 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.020.497 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.020.497 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.020.500 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.020.501 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.020.502 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.020.502 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.020.503 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.020.504 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.020.504 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.025.103 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.026.288 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.290 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.026.291 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.026.291 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.026.292 I llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
0.00.026.292 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
0.00.026.293 I llama_model_loader: - kv  24:               general.quantization_version u32              = 2
0.00.026.294 I llama_model_loader: - type  f32:  124 tensors
0.00.026.294 I llama_model_loader: - type  f16:   73 tensors
0.00.030.916 I llm_load_vocab: special tokens cache size = 5
0.00.033.162 I llm_load_vocab: token to piece cache size = 0.2032 MB
0.00.033.166 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.033.167 I llm_load_print_meta: arch             = bert
0.00.033.167 I llm_load_print_meta: vocab type       = WPM
0.00.033.167 I llm_load_print_meta: n_vocab          = 30522
0.00.033.168 I llm_load_print_meta: n_merges         = 0
0.00.033.168 I llm_load_print_meta: vocab_only       = 0
0.00.033.168 I llm_load_print_meta: n_ctx_train      = 512
0.00.033.169 I llm_load_print_meta: n_embd           = 384
0.00.033.169 I llm_load_print_meta: n_layer          = 12
0.00.033.172 I llm_load_print_meta: n_head           = 12
0.00.033.173 I llm_load_print_meta: n_head_kv        = 12
0.00.033.174 I llm_load_print_meta: n_rot            = 32
0.00.033.174 I llm_load_print_meta: n_swa            = 0
0.00.033.174 I llm_load_print_meta: n_embd_head_k    = 32
0.00.033.174 I llm_load_print_meta: n_embd_head_v    = 32
0.00.033.175 I llm_load_print_meta: n_gqa            = 1
0.00.033.176 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.033.177 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.033.178 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.033.178 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.033.179 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.033.179 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.033.179 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.033.180 I llm_load_print_meta: n_ff             = 1536
0.00.033.180 I llm_load_print_meta: n_expert         = 0
0.00.033.180 I llm_load_print_meta: n_expert_used    = 0
0.00.033.181 I llm_load_print_meta: causal attn      = 0
0.00.033.181 I llm_load_print_meta: pooling type     = 2
0.00.033.181 I llm_load_print_meta: rope type        = 2
0.00.033.183 I llm_load_print_meta: rope scaling     = linear
0.00.033.184 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.033.184 I llm_load_print_meta: freq_scale_train = 1
0.00.033.185 I llm_load_print_meta: n_ctx_orig_yarn  = 512
0.00.033.185 I llm_load_print_meta: rope_finetuned   = unknown
0.00.033.185 I llm_load_print_meta: ssm_d_conv       = 0
0.00.033.186 I llm_load_print_meta: ssm_d_inner      = 0
0.00.033.186 I llm_load_print_meta: ssm_d_state      = 0
0.00.033.186 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.033.186 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.033.187 I llm_load_print_meta: model type       = 33M
0.00.033.212 I llm_load_print_meta: model ftype      = F16
0.00.033.212 I llm_load_print_meta: model params     = 33.21 M
0.00.033.213 I llm_load_print_meta: model size       = 63.84 MiB (16.12 BPW) 
0.00.033.213 I llm_load_print_meta: general.name     = Bge Small
0.00.033.214 I llm_load_print_meta: UNK token        = 100 '[UNK]'
0.00.033.214 I llm_load_print_meta: SEP token        = 102 '[SEP]'
0.00.033.214 I llm_load_print_meta: PAD token        = 0 '[PAD]'
0.00.033.214 I llm_load_print_meta: CLS token        = 101 '[CLS]'
0.00.033.215 I llm_load_print_meta: MASK token       = 103 '[MASK]'
0.00.033.215 I llm_load_print_meta: LF token         = 0 '[PAD]'
0.00.033.215 I llm_load_print_meta: max token length = 21
0.00.035.265 I llm_load_tensors: offloading 12 repeating layers to GPU
0.00.035.267 I llm_load_tensors: offloading output layer to GPU
0.00.035.267 I llm_load_tensors: offloaded 13/13 layers to GPU
0.00.035.294 I llm_load_tensors: Metal_Mapped model buffer size =    40.73 MiB
0.00.035.296 I llm_load_tensors:   CPU_Mapped model buffer size =    23.11 MiB
................................................
0.00.035.576 I llama_new_context_with_model: n_seq_max     = 1
0.00.035.577 I llama_new_context_with_model: n_ctx         = 512
0.00.035.577 I llama_new_context_with_model: n_ctx_per_seq = 512
0.00.035.578 I llama_new_context_with_model: n_batch       = 2048
0.00.035.578 I llama_new_context_with_model: n_ubatch      = 2048
0.00.035.578 I llama_new_context_with_model: flash_attn    = 0
0.00.035.579 I llama_new_context_with_model: freq_base     = 10000.0
0.00.035.579 I llama_new_context_with_model: freq_scale    = 1
0.00.035.580 I ggml_metal_init: allocating
0.00.035.585 I ggml_metal_init: found device: Apple M4
0.00.035.587 I ggml_metal_init: picking default device: Apple M4
0.00.036.450 I ggml_metal_init: using embedded metal library
0.00.040.612 I ggml_metal_init: GPU name:   Apple M4
0.00.040.615 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.040.615 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.040.616 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.040.616 I ggml_metal_init: simdgroup reduction   = true
0.00.040.617 I ggml_metal_init: simdgroup matrix mul. = true
0.00.040.617 I ggml_metal_init: has bfloat            = true
0.00.040.617 I ggml_metal_init: use bfloat            = true
0.00.040.617 I ggml_metal_init: hasUnifiedMemory      = true
0.00.040.618 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.052.966 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.053.537 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.053.539 I llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.053.541 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.054.364 I llama_new_context_with_model:      Metal compute buffer size =    16.00 MiB
0.00.054.365 I llama_new_context_with_model:        CPU compute buffer size =     2.51 MiB
0.00.054.366 I llama_new_context_with_model: graph nodes  = 429
0.00.054.366 I llama_new_context_with_model: graph splits = 2
0.00.054.367 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.054.367 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.060.217 I 
0.00.060.232 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.060.884 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.043984 -0.019895  0.007670 -0.000838  0.001372 -0.037036  0.109436  0.042589  0.092046 -0.015918  0.006773 -0.035692 -0.017889  0.015040  0.018126  0.015860 -0.011308  0.010431 -0.085223 -0.008454  0.091368 -0.017067 -0.060328 -0.024484  0.027524  0.076059  0.027984 -0.014564  0.017645 -0.033268 -0.037874 -0.019012  0.068670 -0.009845 -0.025030  0.072346 -0.046570  0.011026 -0.050252  0.047693  0.032387 -0.011761  0.022055  0.049645  0.010465  0.005795 -0.028860  0.008933 -0.018507 -0.051466 -0.046036  0.030497 -0.035418  0.054204 -0.069650  0.044238  0.029786  0.046305  0.073410 -0.042580  0.076109  0.038851 -0.181182  0.082503  0.042250 -0.064549 -0.060097 -0.017853  0.006472  0.005873  0.017196 -0.026615  0.064578  0.112612  0.035159 -0.067416  0.027073 -0.067295 -0.033493 -0.033219  0.033252  0.013523 -0.003346 -0.037473 -0.052055  0.055155 -0.001982 -0.038276  0.064447  0.028824 -0.043334 -0.029236 -0.039462  0.036315  0.008379 -0.015481 -0.036573  0.018130  0.028600  0.342831 -0.044484  0.056114  0.017624 -0.020867 -0.066808  0.000146 -0.037885 -0.030063 -0.008541 -0.021595  0.000548 -0.003219  0.004020  0.018908 -0.008548  0.025835  0.049433  0.000091  0.050920 -0.042479 -0.031884  0.023606  0.030691 -0.023148 -0.046271 -0.079278  0.115198  0.046757  0.027829 -0.040710  0.067777 -0.022955  0.010332 -0.032937 -0.018305  0.043844  0.024268  0.052413  0.007469  0.008905  0.011244 -0.074655 -0.065561 -0.026750 -0.041192 -0.023884  0.026710  0.006912  0.027741  0.052873 -0.036673  0.057707 -0.000170  0.031764 -0.019760 -0.022083  0.041053 -0.058903  0.019619  0.043144  0.043582  0.041593 -0.022527  0.027037 -0.021833  0.005430 -0.041317 -0.001239  0.024452  0.002088  0.044333 -0.022745  0.043662  0.064763  0.055430  0.037060 -0.000919  0.046111  0.045790 -0.008487  0.063057 -0.073235 -0.011935  0.032107  0.023957  0.014725 -0.033688  0.001083 -0.015827 -0.019002  0.047872  0.110842  0.028428  0.031366 -0.013288 -0.057499  0.006644  0.005140 -0.012262 -0.051443 -0.000965 -0.017642 -0.019440 -0.040937  0.009209 -0.057957  0.050961  0.052340 -0.009617 -0.040259 -0.014075 -0.024877 -0.017251  0.006302  0.006588 -0.026919  0.015600  0.030760  0.002578  0.023206 -0.022199 -0.098557 -0.051105 -0.278031 -0.014994 -0.061553 -0.027218  0.017666 -0.010939 -0.017077  0.035055  0.046994 -0.015432  0.015232 -0.025477  0.047844 -0.005949 -0.000749 -0.061008 -0.068946 -0.060377 -0.035942  0.043319 -0.055047  0.015075  0.000537 -0.058175 -0.010439  0.012643  0.151495  0.127111 -0.013622  0.041993 -0.025672  0.014026 -0.001037 -0.150469  0.044857  0.005320 -0.036274 -0.029801 -0.020192 -0.034891  0.010235  0.033548 -0.048178 -0.051795 -0.017463 -0.023492  0.047349  0.052076 -0.016783 -0.055452  0.025836 -0.005707  0.010710  0.038705  0.008191 -0.009764 -0.105779 -0.027430 -0.096101  0.025068 -0.011260  0.092355  0.056101  0.003758  0.027791  0.002092 -0.051085 -0.039890 -0.013542 -0.044956 -0.015335  0.002910 -0.043498 -0.077951  0.065217 -0.006840 -0.001630 -0.014642  0.071560  0.023714 -0.037180  0.009175  0.001562 -0.032268  0.015468  0.037880  0.000337 -0.053207  0.021321 -0.039829  0.000031  0.013400  0.019798 -0.057879  0.006473 -0.049531 -0.267858  0.039156 -0.067972  0.038248 -0.012333  0.041485 -0.016117  0.052383 -0.071363  0.011369  0.024719 -0.007232  0.082087  0.028535 -0.021504  0.040494 -0.004570 -0.074594 -0.014749  0.020037  0.002304  0.023148  0.197212 -0.043222 -0.025983 -0.004959 -0.019278  0.074258  0.001716 -0.031979 -0.036599 -0.045082  0.000547 -0.011560  0.018126 -0.029461 -0.008463  0.006419  0.050799 -0.014959  0.006176  0.026096 -0.030809  0.048055  0.114093 -0.040826 -0.011457  0.005419 -0.003599  0.025154 -0.059142  0.013753 -0.010399  0.038709  0.051449  0.035413  0.035031 -0.017027  0.026360 -0.014490 -0.050032  0.003219  0.054121  0.039723 -0.039127 

0.00.064.405 I llama_perf_context_print:        load time =      44.88 ms
0.00.064.406 I llama_perf_context_print: prompt eval time =       3.38 ms /     9 tokens (    0.38 ms per token,  2665.09 tokens per second)
0.00.064.407 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.064.407 I llama_perf_context_print:       total time =       4.19 ms /    10 tokens
0.00.064.537 I ggml_metal_free: deallocating

real	0m0.236s
user	0m0.049s
sys	0m0.027s
```
- q8_0:
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.037 I build: 4457 (ee7136c6) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.189 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.011.862 I llama_model_loader: loaded meta data with 25 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.011.866 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.011.867 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.011.868 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.011.868 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.011.869 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.011.869 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.011.870 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.011.870 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.011.871 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.011.871 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.011.871 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.011.873 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.011.874 I llama_model_loader: - kv  11:                      bert.attention.causal bool             = false
0.00.011.874 I llama_model_loader: - kv  12:                          bert.pooling_type u32              = 2
0.00.011.875 I llama_model_loader: - kv  13:            tokenizer.ggml.token_type_count u32              = 2
0.00.011.875 I llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = bert
0.00.011.875 I llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.014.260 I llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.014.931 I llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.014.932 I llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.014.932 I llama_model_loader: - kv  19:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.014.932 I llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 0
0.00.014.933 I llama_model_loader: - kv  21:                tokenizer.ggml.cls_token_id u32              = 101
0.00.014.933 I llama_model_loader: - kv  22:               tokenizer.ggml.mask_token_id u32              = 103
0.00.014.933 I llama_model_loader: - kv  23:               general.quantization_version u32              = 2
0.00.014.933 I llama_model_loader: - kv  24:                          general.file_type u32              = 7
0.00.014.934 I llama_model_loader: - type  f32:  124 tensors
0.00.014.934 I llama_model_loader: - type q8_0:   73 tensors
0.00.017.381 I llm_load_vocab: special tokens cache size = 5
0.00.018.680 I llm_load_vocab: token to piece cache size = 0.2032 MB
0.00.018.683 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.018.684 I llm_load_print_meta: arch             = bert
0.00.018.684 I llm_load_print_meta: vocab type       = WPM
0.00.018.685 I llm_load_print_meta: n_vocab          = 30522
0.00.018.685 I llm_load_print_meta: n_merges         = 0
0.00.018.685 I llm_load_print_meta: vocab_only       = 0
0.00.018.685 I llm_load_print_meta: n_ctx_train      = 512
0.00.018.685 I llm_load_print_meta: n_embd           = 384
0.00.018.686 I llm_load_print_meta: n_layer          = 12
0.00.018.689 I llm_load_print_meta: n_head           = 12
0.00.018.689 I llm_load_print_meta: n_head_kv        = 12
0.00.018.691 I llm_load_print_meta: n_rot            = 32
0.00.018.691 I llm_load_print_meta: n_swa            = 0
0.00.018.691 I llm_load_print_meta: n_embd_head_k    = 32
0.00.018.691 I llm_load_print_meta: n_embd_head_v    = 32
0.00.018.692 I llm_load_print_meta: n_gqa            = 1
0.00.018.692 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.018.693 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.018.694 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.018.694 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.018.694 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.018.694 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.018.695 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.018.697 I llm_load_print_meta: n_ff             = 1536
0.00.018.697 I llm_load_print_meta: n_expert         = 0
0.00.018.697 I llm_load_print_meta: n_expert_used    = 0
0.00.018.698 I llm_load_print_meta: causal attn      = 0
0.00.018.698 I llm_load_print_meta: pooling type     = 2
0.00.018.698 I llm_load_print_meta: rope type        = 2
0.00.018.698 I llm_load_print_meta: rope scaling     = linear
0.00.018.699 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.018.699 I llm_load_print_meta: freq_scale_train = 1
0.00.018.699 I llm_load_print_meta: n_ctx_orig_yarn  = 512
0.00.018.702 I llm_load_print_meta: rope_finetuned   = unknown
0.00.018.702 I llm_load_print_meta: ssm_d_conv       = 0
0.00.018.702 I llm_load_print_meta: ssm_d_inner      = 0
0.00.018.702 I llm_load_print_meta: ssm_d_state      = 0
0.00.018.702 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.018.702 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.018.703 I llm_load_print_meta: model type       = 33M
0.00.018.709 I llm_load_print_meta: model ftype      = Q8_0
0.00.018.710 I llm_load_print_meta: model params     = 33.21 M
0.00.018.710 I llm_load_print_meta: model size       = 34.38 MiB (8.68 BPW) 
0.00.018.711 I llm_load_print_meta: general.name     = Bge Small
0.00.018.711 I llm_load_print_meta: UNK token        = 100 '[UNK]'
0.00.018.711 I llm_load_print_meta: SEP token        = 102 '[SEP]'
0.00.018.711 I llm_load_print_meta: PAD token        = 0 '[PAD]'
0.00.018.711 I llm_load_print_meta: CLS token        = 101 '[CLS]'
0.00.018.711 I llm_load_print_meta: MASK token       = 103 '[MASK]'
0.00.018.712 I llm_load_print_meta: LF token         = 0 '[PAD]'
0.00.018.713 I llm_load_print_meta: max token length = 21
0.00.020.013 I llm_load_tensors: offloading 12 repeating layers to GPU
0.00.020.014 I llm_load_tensors: offloading output layer to GPU
0.00.020.014 I llm_load_tensors: offloaded 13/13 layers to GPU
0.00.020.022 I llm_load_tensors: Metal_Mapped model buffer size =    21.75 MiB
0.00.020.023 I llm_load_tensors:   CPU_Mapped model buffer size =    12.63 MiB
................................................
0.00.020.201 I llama_new_context_with_model: n_seq_max     = 1
0.00.020.202 I llama_new_context_with_model: n_ctx         = 512
0.00.020.202 I llama_new_context_with_model: n_ctx_per_seq = 512
0.00.020.203 I llama_new_context_with_model: n_batch       = 2048
0.00.020.203 I llama_new_context_with_model: n_ubatch      = 2048
0.00.020.203 I llama_new_context_with_model: flash_attn    = 0
0.00.020.203 I llama_new_context_with_model: freq_base     = 10000.0
0.00.020.204 I llama_new_context_with_model: freq_scale    = 1
0.00.020.204 I ggml_metal_init: allocating
0.00.020.213 I ggml_metal_init: found device: Apple M4
0.00.020.216 I ggml_metal_init: picking default device: Apple M4
0.00.020.849 I ggml_metal_init: using embedded metal library
0.00.023.175 I ggml_metal_init: GPU name:   Apple M4
0.00.023.176 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.023.177 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.023.177 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.023.177 I ggml_metal_init: simdgroup reduction   = true
0.00.023.178 I ggml_metal_init: simdgroup matrix mul. = true
0.00.023.178 I ggml_metal_init: has bfloat            = true
0.00.023.178 I ggml_metal_init: use bfloat            = true
0.00.023.179 I ggml_metal_init: hasUnifiedMemory      = true
0.00.023.180 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.033.544 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.034.049 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.034.052 I llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.034.053 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.034.647 I llama_new_context_with_model:      Metal compute buffer size =    16.00 MiB
0.00.034.648 I llama_new_context_with_model:        CPU compute buffer size =     2.51 MiB
0.00.034.648 I llama_new_context_with_model: graph nodes  = 429
0.00.034.648 I llama_new_context_with_model: graph splits = 2
0.00.034.650 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.034.650 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.038.354 I 
0.00.038.371 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.038.911 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.044994 -0.019858  0.007986 -0.001410  0.001531 -0.036276  0.109404  0.043235  0.092836 -0.015201  0.005971 -0.036330 -0.018159  0.015015  0.017667  0.014510 -0.011792  0.011812 -0.084764 -0.008081  0.091650 -0.017638 -0.061084 -0.025066  0.026879  0.076180  0.028256 -0.014273  0.017420 -0.033553 -0.037520 -0.018280  0.068843 -0.010129 -0.024896  0.071968 -0.046406  0.011471 -0.050333  0.048931  0.032317 -0.012107  0.021812  0.049796  0.010366  0.005264 -0.028665  0.008581 -0.018508 -0.052358 -0.046410  0.029426 -0.034989  0.053289 -0.069849  0.043767  0.029291  0.046759  0.073664 -0.043090  0.075213  0.038641 -0.180496  0.081648  0.043389 -0.065615 -0.059878 -0.017387  0.006913  0.005155  0.017165 -0.026679  0.064347  0.112564  0.034871 -0.067783  0.027124 -0.066989 -0.034296 -0.033344  0.033189  0.014325 -0.004253 -0.036945 -0.052008  0.054151 -0.001715 -0.038112  0.063016  0.028668 -0.041665 -0.028879 -0.039263  0.036879  0.007718 -0.015657 -0.036270  0.018699  0.029792  0.344948 -0.043880  0.056487  0.018022 -0.021448 -0.065281  0.000253 -0.037910 -0.029663 -0.008828 -0.020643  0.001162 -0.003510  0.003091  0.018576 -0.009637  0.025316  0.049298 -0.000937  0.051568 -0.042363 -0.031550  0.023315  0.030044 -0.023267 -0.045483 -0.079644  0.114489  0.047503  0.027208 -0.041477  0.067717 -0.022845  0.010232 -0.033486 -0.017199  0.044319  0.022956  0.052194  0.008122  0.008112  0.009951 -0.074643 -0.065035 -0.025938 -0.040806 -0.024533  0.027409  0.006078  0.027693  0.052243 -0.036587  0.058682  0.001467  0.032191 -0.019964 -0.021840  0.041805 -0.058801  0.019348  0.042626  0.043999  0.040911 -0.021909  0.028025 -0.022801  0.005520 -0.041266 -0.000085  0.024325  0.001532  0.044802 -0.022888  0.043056  0.065030  0.056292  0.037666 -0.001004  0.047521  0.045624 -0.008614  0.062270 -0.073313 -0.011718  0.032708  0.023348  0.014650 -0.033924  0.001158 -0.015872 -0.018974  0.047982  0.110740  0.029177  0.030881 -0.012325 -0.057836  0.006939  0.004378 -0.011864 -0.051754 -0.003018 -0.017766 -0.020051 -0.041018  0.009118 -0.058568  0.050632  0.051795 -0.008968 -0.040668 -0.014765 -0.024320 -0.015980  0.005823  0.007147 -0.027335  0.016275  0.030734  0.002342  0.023167 -0.022169 -0.098059 -0.050901 -0.277240 -0.015100 -0.061566 -0.026219  0.017141 -0.010285 -0.017522  0.034725  0.047897 -0.016594  0.015489 -0.024132  0.048613 -0.004993 -0.000001 -0.060860 -0.068346 -0.059622 -0.036157  0.043605 -0.056353  0.014713  0.000292 -0.058627 -0.010995  0.012274  0.151597  0.127025 -0.012582  0.042765 -0.025227  0.013629 -0.000381 -0.150686  0.043974  0.004714 -0.036955 -0.029586 -0.019853 -0.034063  0.009522  0.034004 -0.048909 -0.051765 -0.017243 -0.025080  0.047588  0.050981 -0.017553 -0.056746  0.024054 -0.006285  0.011003  0.038719  0.008083 -0.008595 -0.106361 -0.027365 -0.096966  0.025294 -0.011096  0.091579  0.056180  0.004494  0.028066  0.001727 -0.051388 -0.039497 -0.013817 -0.045907 -0.015238  0.002920 -0.043938 -0.077716  0.066102 -0.006195 -0.000885 -0.013768  0.071562  0.023826 -0.036363  0.007848  0.001487 -0.032916  0.016336  0.037378  0.000625 -0.052389  0.021549 -0.039695 -0.000026  0.013496  0.020436 -0.057513  0.005900 -0.049906 -0.267546  0.038686 -0.067509  0.037450 -0.011471  0.041547 -0.015904  0.051347 -0.072313  0.012571  0.024808 -0.007420  0.082069  0.028175 -0.021956  0.040736 -0.003945 -0.074192 -0.014923  0.020417  0.002644  0.023207  0.197144 -0.043068 -0.025876 -0.004886 -0.018582  0.073936  0.001341 -0.031663 -0.037140 -0.044262  0.000049 -0.010741  0.018225 -0.028045 -0.008828  0.006047  0.050183 -0.015017  0.007037  0.025894 -0.030628  0.048075  0.112795 -0.039982 -0.011695  0.005039 -0.002800  0.025072 -0.059720  0.014182 -0.010019  0.038100  0.051088  0.034726  0.035947 -0.017035  0.027243 -0.015391 -0.051195  0.003744  0.053928  0.040014 -0.039140 

0.00.042.106 I llama_perf_context_print:        load time =      29.16 ms
0.00.042.107 I llama_perf_context_print: prompt eval time =       3.06 ms /     9 tokens (    0.34 ms per token,  2940.22 tokens per second)
0.00.042.108 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.042.108 I llama_perf_context_print:       total time =       3.75 ms /    10 tokens
0.00.042.292 I ggml_metal_free: deallocating

real	0m0.054s
user	0m0.030s
sys	0m0.015s
```
### rerank_tiny

Rerank Tiny (Jina):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/rerank-tiny/ggml-model-f16.gguf -p 'what is panda?</s></s>hi\nwhat is panda?</s></s>it'\''s a bear\nwhat is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.' -ngl 99 -c 0 --pooling rank --embd-normalize -1 --verbose-prompt
0.00.000.120 I build: 4457 (ee7136c6) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.015.458 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.025.240 I llama_model_loader: loaded meta data with 29 key-value pairs and 70 tensors from ../models-mnt/rerank-tiny/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.025.244 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.025.246 I llama_model_loader: - kv   0:                       general.architecture str              = jina-bert-v2
0.00.025.247 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.025.247 I llama_model_loader: - kv   2:                               general.name str              = Jina Bert Implementation
0.00.025.248 I llama_model_loader: - kv   3:                       general.organization str              = Jinaai
0.00.025.251 I llama_model_loader: - kv   4:                         general.size_label str              = 33M
0.00.025.252 I llama_model_loader: - kv   5:                   jina-bert-v2.block_count u32              = 4
0.00.025.256 I llama_model_loader: - kv   6:                jina-bert-v2.context_length u32              = 8192
0.00.025.256 I llama_model_loader: - kv   7:              jina-bert-v2.embedding_length u32              = 384
0.00.025.257 I llama_model_loader: - kv   8:           jina-bert-v2.feed_forward_length u32              = 1536
0.00.025.257 I llama_model_loader: - kv   9:          jina-bert-v2.attention.head_count u32              = 12
0.00.025.260 I llama_model_loader: - kv  10:  jina-bert-v2.attention.layer_norm_epsilon f32              = 0.000000
0.00.025.262 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.025.262 I llama_model_loader: - kv  12:              jina-bert-v2.attention.causal bool             = false
0.00.025.263 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.025.263 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = jina-v1-en
0.00.028.893 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,61056]   = ["<s>", "<pad>", "</s>", "<unk>", "<m...
0.00.030.005 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,61056]   = [3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, ...
0.00.032.664 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,39382]   = ["t h", "i n", "a n", "e r", "th e", ...
0.00.032.666 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.032.667 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
0.00.032.667 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 3
0.00.032.668 I llama_model_loader: - kv  21:          tokenizer.ggml.seperator_token_id u32              = 2
0.00.032.668 I llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 1
0.00.032.668 I llama_model_loader: - kv  23:                tokenizer.ggml.cls_token_id u32              = 0
0.00.032.668 I llama_model_loader: - kv  24:               tokenizer.ggml.mask_token_id u32              = 4
0.00.032.669 I llama_model_loader: - kv  25:            tokenizer.ggml.token_type_count u32              = 2
0.00.032.669 I llama_model_loader: - kv  26:               tokenizer.ggml.add_bos_token bool             = true
0.00.032.669 I llama_model_loader: - kv  27:               tokenizer.ggml.add_eos_token bool             = true
0.00.032.670 I llama_model_loader: - kv  28:               general.quantization_version u32              = 2
0.00.032.670 I llama_model_loader: - type  f32:   40 tensors
0.00.032.674 I llama_model_loader: - type  f16:   30 tensors
0.00.044.442 W llm_load_vocab: empty token at index 5
0.00.047.940 W llm_load_vocab: model vocab missing newline token, using special_pad_id instead
0.00.048.999 W llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
0.00.049.032 I llm_load_vocab: special tokens cache size = 5
0.00.317.209 I llm_load_vocab: token to piece cache size = 1.5060 MB
0.00.317.227 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.317.227 I llm_load_print_meta: arch             = jina-bert-v2
0.00.317.228 I llm_load_print_meta: vocab type       = BPE
0.00.317.228 I llm_load_print_meta: n_vocab          = 61056
0.00.317.228 I llm_load_print_meta: n_merges         = 39382
0.00.317.228 I llm_load_print_meta: vocab_only       = 0
0.00.317.229 I llm_load_print_meta: n_ctx_train      = 8192
0.00.317.229 I llm_load_print_meta: n_embd           = 384
0.00.317.229 I llm_load_print_meta: n_layer          = 4
0.00.317.235 I llm_load_print_meta: n_head           = 12
0.00.317.235 I llm_load_print_meta: n_head_kv        = 12
0.00.317.236 I llm_load_print_meta: n_rot            = 32
0.00.317.236 I llm_load_print_meta: n_swa            = 0
0.00.317.236 I llm_load_print_meta: n_embd_head_k    = 32
0.00.317.236 I llm_load_print_meta: n_embd_head_v    = 32
0.00.317.237 I llm_load_print_meta: n_gqa            = 1
0.00.317.237 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.317.237 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.317.239 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.317.239 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.317.239 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.317.239 I llm_load_print_meta: f_max_alibi_bias = 8.0e+00
0.00.317.240 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.317.240 I llm_load_print_meta: n_ff             = 1536
0.00.317.240 I llm_load_print_meta: n_expert         = 0
0.00.317.240 I llm_load_print_meta: n_expert_used    = 0
0.00.317.241 I llm_load_print_meta: causal attn      = 0
0.00.317.241 I llm_load_print_meta: pooling type     = -1
0.00.317.243 I llm_load_print_meta: rope type        = -1
0.00.317.243 I llm_load_print_meta: rope scaling     = linear
0.00.317.243 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.317.243 I llm_load_print_meta: freq_scale_train = 1
0.00.317.243 I llm_load_print_meta: n_ctx_orig_yarn  = 8192
0.00.317.244 I llm_load_print_meta: rope_finetuned   = unknown
0.00.317.244 I llm_load_print_meta: ssm_d_conv       = 0
0.00.317.244 I llm_load_print_meta: ssm_d_inner      = 0
0.00.317.244 I llm_load_print_meta: ssm_d_state      = 0
0.00.317.244 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.317.244 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.317.245 I llm_load_print_meta: model type       = 33M
0.00.317.268 I llm_load_print_meta: model ftype      = F16
0.00.317.269 I llm_load_print_meta: model params     = 32.90 M
0.00.317.269 I llm_load_print_meta: model size       = 62.78 MiB (16.01 BPW) 
0.00.317.270 I llm_load_print_meta: general.name     = Jina Bert Implementation
0.00.317.270 I llm_load_print_meta: BOS token        = 0 '<s>'
0.00.317.270 I llm_load_print_meta: EOS token        = 2 '</s>'
0.00.317.271 I llm_load_print_meta: UNK token        = 3 '<unk>'
0.00.317.271 I llm_load_print_meta: SEP token        = 2 '</s>'
0.00.317.271 I llm_load_print_meta: PAD token        = 1 '<pad>'
0.00.317.271 I llm_load_print_meta: CLS token        = 0 '<s>'
0.00.317.271 I llm_load_print_meta: MASK token       = 4 '<mask>'
0.00.317.272 I llm_load_print_meta: EOG token        = 2 '</s>'
0.00.317.272 I llm_load_print_meta: max token length = 45
0.00.318.312 I llm_load_tensors: offloading 4 repeating layers to GPU
0.00.318.312 I llm_load_tensors: offloading output layer to GPU
0.00.318.313 I llm_load_tensors: offloaded 5/5 layers to GPU
0.00.318.331 I llm_load_tensors: Metal_Mapped model buffer size =    18.06 MiB
0.00.318.333 I llm_load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
....................
0.00.318.660 I llama_new_context_with_model: n_seq_max     = 1
0.00.318.662 I llama_new_context_with_model: n_ctx         = 8192
0.00.318.662 I llama_new_context_with_model: n_ctx_per_seq = 8192
0.00.318.662 I llama_new_context_with_model: n_batch       = 2048
0.00.318.662 I llama_new_context_with_model: n_ubatch      = 2048
0.00.318.662 I llama_new_context_with_model: flash_attn    = 0
0.00.318.663 I llama_new_context_with_model: freq_base     = 10000.0
0.00.318.663 I llama_new_context_with_model: freq_scale    = 1
0.00.318.663 I ggml_metal_init: allocating
0.00.318.666 I ggml_metal_init: found device: Apple M4
0.00.318.669 I ggml_metal_init: picking default device: Apple M4
0.00.319.457 I ggml_metal_init: using embedded metal library
0.00.322.250 I ggml_metal_init: GPU name:   Apple M4
0.00.322.251 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.322.252 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.322.252 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.322.252 I ggml_metal_init: simdgroup reduction   = true
0.00.322.253 I ggml_metal_init: simdgroup matrix mul. = true
0.00.322.253 I ggml_metal_init: has bfloat            = true
0.00.322.253 I ggml_metal_init: use bfloat            = true
0.00.322.253 I ggml_metal_init: hasUnifiedMemory      = true
0.00.322.254 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.332.792 I llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 4, can_shift = 1
0.00.335.385 I llama_kv_cache_init:      Metal KV buffer size =    48.00 MiB
0.00.335.388 I llama_new_context_with_model: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB
0.00.335.402 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.335.952 I llama_new_context_with_model:      Metal compute buffer size =   220.01 MiB
0.00.335.953 I llama_new_context_with_model:        CPU compute buffer size =    22.02 MiB
0.00.335.953 I llama_new_context_with_model: graph nodes  = 154
0.00.335.953 I llama_new_context_with_model: graph splits = 2
0.00.335.955 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 8192
0.00.335.955 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.349.544 I 
0.00.349.571 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.349.730 I main: prompt 0: 'what is panda?</s></s>hi'
0.00.349.731 I main: number of tokens in prompt = 9
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 23233 -> 'hi'
     2 -> '</s>'


0.00.349.734 I main: prompt 1: 'what is panda?</s></s>it's a bear'
0.00.349.734 I main: number of tokens in prompt = 13
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21153 -> 'it'
    37 -> '''
    87 -> 's'
    69 -> 'a'
 25706 -> 'bear'
     2 -> '</s>'


0.00.349.737 I main: prompt 2: 'what is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'
0.00.349.738 I main: number of tokens in prompt = 40
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21215 -> 'he'
 28390 -> 'giant'
 49990 -> 'panda'
    38 -> '('
 21163 -> 'il'
 26237 -> 'uro'
 21223 -> 'po'
 23179 -> 'da'
 36906 -> 'melan'
 26791 -> 'ole'
    89 -> 'u'
 21402 -> 'ca'
 21686 -> '),'
 23314 -> 'sometimes'
 22517 -> 'called'
    69 -> 'a'
 49990 -> 'panda'
 25706 -> 'bear'
 21142 -> 'or'
 22810 -> 'simply'
 49990 -> 'panda'
    42 -> ','
 21152 -> 'is'
    69 -> 'a'
 25706 -> 'bear'
 25677 -> 'species'
 28930 -> 'ende'
 22024 -> 'mic'
 21148 -> 'to'
    76 -> 'h'
 22344 -> 'ina'
    44 -> '.'
     2 -> '</s>'


0.00.350.330 I batch_decode: n_tokens = 62, n_seq = 3

rerank score 0:    0.023
rerank score 1:    0.024
rerank score 2:    0.199

0.00.353.878 I llama_perf_context_print:        load time =     334.08 ms
0.00.353.879 I llama_perf_context_print: prompt eval time =       3.54 ms /    62 tokens (    0.06 ms per token, 17524.02 tokens per second)
0.00.353.879 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.353.880 I llama_perf_context_print:       total time =       4.33 ms /    63 tokens
0.00.354.091 I ggml_metal_free: deallocating

real	0m1.085s
user	0m0.329s
sys	0m0.039s
  - rerank score 0 @ 0.023 OK
  - rerank score 1 @ 0.024 OK
  - rerank score 2 @ 0.199 OK
```
### pythia_1_4b

Pythia 1.4B:
- status: 0
- perplexity:
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
- imatrix:
```
Final estimate: PPL = 10.1498 +/- 3.22650
```
- f16: 
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.196 I build: 4457 (ee7136c6) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.330 I main: llama backend init
0.00.000.339 I main: load the model and apply lora adapter, if any
0.00.058.618 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.071.567 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.071.595 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.071.599 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.071.600 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.071.601 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.071.601 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.071.602 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.071.604 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.071.605 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.071.605 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.071.606 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.071.607 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.071.608 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.071.609 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.071.614 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.071.615 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.071.616 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.078.486 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.080.654 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.087.456 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.087.467 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.087.468 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.087.469 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.087.469 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.087.471 I llama_model_loader: - type  f32:  194 tensors
0.00.087.471 I llama_model_loader: - type  f16:   98 tensors
0.00.125.761 I llm_load_vocab: special tokens cache size = 25
0.00.133.457 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.133.462 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.133.462 I llm_load_print_meta: arch             = gptneox
0.00.133.462 I llm_load_print_meta: vocab type       = BPE
0.00.133.464 I llm_load_print_meta: n_vocab          = 50304
0.00.133.468 I llm_load_print_meta: n_merges         = 50009
0.00.133.468 I llm_load_print_meta: vocab_only       = 0
0.00.133.469 I llm_load_print_meta: n_ctx_train      = 2048
0.00.133.469 I llm_load_print_meta: n_embd           = 2048
0.00.133.470 I llm_load_print_meta: n_layer          = 24
0.00.133.475 I llm_load_print_meta: n_head           = 16
0.00.133.475 I llm_load_print_meta: n_head_kv        = 16
0.00.133.476 I llm_load_print_meta: n_rot            = 32
0.00.133.476 I llm_load_print_meta: n_swa            = 0
0.00.133.476 I llm_load_print_meta: n_embd_head_k    = 128
0.00.133.476 I llm_load_print_meta: n_embd_head_v    = 128
0.00.133.477 I llm_load_print_meta: n_gqa            = 1
0.00.133.478 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.133.479 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.133.480 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.133.480 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.133.480 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.133.480 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.133.480 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.133.481 I llm_load_print_meta: n_ff             = 8192
0.00.133.481 I llm_load_print_meta: n_expert         = 0
0.00.133.481 I llm_load_print_meta: n_expert_used    = 0
0.00.133.482 I llm_load_print_meta: causal attn      = 1
0.00.133.482 I llm_load_print_meta: pooling type     = 0
0.00.133.482 I llm_load_print_meta: rope type        = 2
0.00.133.482 I llm_load_print_meta: rope scaling     = linear
0.00.133.482 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.133.483 I llm_load_print_meta: freq_scale_train = 1
0.00.133.483 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.133.483 I llm_load_print_meta: rope_finetuned   = unknown
0.00.133.483 I llm_load_print_meta: ssm_d_conv       = 0
0.00.133.484 I llm_load_print_meta: ssm_d_inner      = 0
0.00.133.484 I llm_load_print_meta: ssm_d_state      = 0
0.00.133.484 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.133.484 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.133.484 I llm_load_print_meta: model type       = 1.4B
0.00.133.505 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.133.505 I llm_load_print_meta: model params     = 1.41 B
0.00.133.506 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.133.506 I llm_load_print_meta: general.name     = 1.4B
0.00.133.506 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.133.508 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.133.508 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.133.508 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.133.508 I llm_load_print_meta: LF token         = 128 ''
0.00.133.509 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.133.509 I llm_load_print_meta: max token length = 1024
0.00.135.567 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.135.567 I llm_load_tensors: offloading output layer to GPU
0.00.135.567 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.135.586 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.135.587 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.136.004 I llama_new_context_with_model: n_seq_max     = 1
0.00.136.005 I llama_new_context_with_model: n_ctx         = 2048
0.00.136.005 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.136.006 I llama_new_context_with_model: n_batch       = 2048
0.00.136.006 I llama_new_context_with_model: n_ubatch      = 512
0.00.136.006 I llama_new_context_with_model: flash_attn    = 0
0.00.136.007 I llama_new_context_with_model: freq_base     = 10000.0
0.00.136.007 I llama_new_context_with_model: freq_scale    = 1
0.00.136.008 I ggml_metal_init: allocating
0.00.136.018 I ggml_metal_init: found device: Apple M4
0.00.136.021 I ggml_metal_init: picking default device: Apple M4
0.00.136.742 I ggml_metal_init: using embedded metal library
0.00.146.491 I ggml_metal_init: GPU name:   Apple M4
0.00.146.493 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.146.494 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.146.494 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.146.494 I ggml_metal_init: simdgroup reduction   = true
0.00.146.495 I ggml_metal_init: simdgroup matrix mul. = true
0.00.146.495 I ggml_metal_init: has bfloat            = true
0.00.146.495 I ggml_metal_init: use bfloat            = true
0.00.146.495 I ggml_metal_init: hasUnifiedMemory      = true
0.00.146.496 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.171.325 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.193.610 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.193.620 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.193.643 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.194.722 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.194.724 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.194.724 I llama_new_context_with_model: graph nodes  = 967
0.00.194.725 I llama_new_context_with_model: graph splits = 2
0.00.194.728 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.194.869 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.194.870 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.279.769 I main: llama threadpool init, n_threads = 4
0.00.279.814 I 
0.00.279.840 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.279.840 I 
0.00.279.910 I sampler seed: 1234
0.00.279.914 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.279.943 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.279.945 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.279.945 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.121.705 I llama_perf_sampler_print:    sampling time =       1.18 ms /    71 runs   (    0.02 ms per token, 60374.15 tokens per second)
0.02.121.706 I llama_perf_context_print:        load time =     221.13 ms
0.02.121.707 I llama_perf_context_print: prompt eval time =      43.45 ms /     7 tokens (    6.21 ms per token,   161.10 tokens per second)
0.02.121.708 I llama_perf_context_print:        eval time =    1795.48 ms /    63 runs   (   28.50 ms per token,    35.09 tokens per second)
0.02.121.708 I llama_perf_context_print:       total time =    1841.94 ms /    70 tokens
0.02.121.924 I ggml_metal_free: deallocating

real	0m2.445s
user	0m0.154s
sys	0m0.107s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.602 I build: 4457 (ee7136c6) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.030.393 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.043.953 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.043.963 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.043.966 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.043.967 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.043.968 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.043.969 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.043.969 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.043.971 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.043.972 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.043.973 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.043.973 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.043.974 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.043.975 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.043.976 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.043.979 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.043.980 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.043.980 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.053.440 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.055.632 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.063.010 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.063.012 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.063.013 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.063.013 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.063.014 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.063.014 I llama_model_loader: - type  f32:  194 tensors
0.00.063.015 I llama_model_loader: - type  f16:   98 tensors
0.00.093.380 I llm_load_vocab: special tokens cache size = 25
0.00.100.404 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.100.407 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.100.407 I llm_load_print_meta: arch             = gptneox
0.00.100.407 I llm_load_print_meta: vocab type       = BPE
0.00.100.408 I llm_load_print_meta: n_vocab          = 50304
0.00.100.408 I llm_load_print_meta: n_merges         = 50009
0.00.100.408 I llm_load_print_meta: vocab_only       = 0
0.00.100.408 I llm_load_print_meta: n_ctx_train      = 2048
0.00.100.408 I llm_load_print_meta: n_embd           = 2048
0.00.100.408 I llm_load_print_meta: n_layer          = 24
0.00.100.411 I llm_load_print_meta: n_head           = 16
0.00.100.412 I llm_load_print_meta: n_head_kv        = 16
0.00.100.412 I llm_load_print_meta: n_rot            = 32
0.00.100.415 I llm_load_print_meta: n_swa            = 0
0.00.100.415 I llm_load_print_meta: n_embd_head_k    = 128
0.00.100.415 I llm_load_print_meta: n_embd_head_v    = 128
0.00.100.416 I llm_load_print_meta: n_gqa            = 1
0.00.100.417 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.100.417 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.100.418 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.100.418 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.100.418 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.100.419 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.100.424 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.100.432 I llm_load_print_meta: n_ff             = 8192
0.00.100.432 I llm_load_print_meta: n_expert         = 0
0.00.100.432 I llm_load_print_meta: n_expert_used    = 0
0.00.100.433 I llm_load_print_meta: causal attn      = 1
0.00.100.433 I llm_load_print_meta: pooling type     = 0
0.00.100.433 I llm_load_print_meta: rope type        = 2
0.00.100.433 I llm_load_print_meta: rope scaling     = linear
0.00.100.434 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.100.434 I llm_load_print_meta: freq_scale_train = 1
0.00.100.434 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.100.434 I llm_load_print_meta: rope_finetuned   = unknown
0.00.100.434 I llm_load_print_meta: ssm_d_conv       = 0
0.00.100.436 I llm_load_print_meta: ssm_d_inner      = 0
0.00.100.436 I llm_load_print_meta: ssm_d_state      = 0
0.00.100.436 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.100.436 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.100.437 I llm_load_print_meta: model type       = 1.4B
0.00.100.449 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.100.449 I llm_load_print_meta: model params     = 1.41 B
0.00.100.450 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.100.450 I llm_load_print_meta: general.name     = 1.4B
0.00.100.451 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.100.451 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.100.452 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.100.452 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.100.452 I llm_load_print_meta: LF token         = 128 ''
0.00.100.454 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.100.454 I llm_load_print_meta: max token length = 1024
0.00.103.075 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.103.076 I llm_load_tensors: offloading output layer to GPU
0.00.103.076 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.103.087 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.103.088 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.103.491 I llama_new_context_with_model: n_seq_max     = 1
0.00.103.491 I llama_new_context_with_model: n_ctx         = 128
0.00.103.492 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.103.492 I llama_new_context_with_model: n_batch       = 128
0.00.103.492 I llama_new_context_with_model: n_ubatch      = 128
0.00.103.492 I llama_new_context_with_model: flash_attn    = 0
0.00.103.493 I llama_new_context_with_model: freq_base     = 10000.0
0.00.103.493 I llama_new_context_with_model: freq_scale    = 1
0.00.103.493 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.103.494 I ggml_metal_init: allocating
0.00.103.500 I ggml_metal_init: found device: Apple M4
0.00.103.504 I ggml_metal_init: picking default device: Apple M4
0.00.104.123 I ggml_metal_init: using embedded metal library
0.00.107.635 I ggml_metal_init: GPU name:   Apple M4
0.00.107.637 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.107.637 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.107.638 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.107.638 I ggml_metal_init: simdgroup reduction   = true
0.00.107.638 I ggml_metal_init: simdgroup matrix mul. = true
0.00.107.638 I ggml_metal_init: has bfloat            = true
0.00.107.638 I ggml_metal_init: use bfloat            = true
0.00.107.639 I ggml_metal_init: hasUnifiedMemory      = true
0.00.107.639 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.117.077 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.118.381 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.118.385 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.118.402 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.119.339 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.119.340 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.119.340 I llama_new_context_with_model: graph nodes  = 967
0.00.119.341 I llama_new_context_with_model: graph splits = 2
0.00.119.342 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.119.342 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.250.823 I 
0.01.250.857 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.250.883 I perplexity: tokenizing the input ..
0.01.263.871 I perplexity: tokenization took 12.987 ms
0.01.263.877 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.386.112 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.387.774 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.387.823 I llama_perf_context_print:        load time =    1220.42 ms
0.01.387.825 I llama_perf_context_print: prompt eval time =     121.35 ms /   128 tokens (    0.95 ms per token,  1054.77 tokens per second)
0.01.387.826 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.387.827 I llama_perf_context_print:       total time =     137.00 ms /   129 tokens
0.01.388.626 I ggml_metal_free: deallocating

real	0m1.577s
user	0m0.128s
sys	0m0.213s
```
- q8_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.050 I build: 4457 (ee7136c6) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.083 I main: llama backend init
0.00.000.085 I main: load the model and apply lora adapter, if any
0.00.009.763 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.027.536 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.027.543 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.027.545 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.027.546 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.027.546 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.027.546 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.027.547 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.027.548 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.027.548 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.027.548 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.027.549 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.027.549 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.027.549 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.027.550 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.027.552 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.027.552 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.027.553 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.031.539 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.032.560 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.036.482 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.036.484 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.036.484 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.036.484 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.036.485 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.036.485 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.036.486 I llama_model_loader: - type  f32:  194 tensors
0.00.036.486 I llama_model_loader: - type q8_0:   98 tensors
0.00.059.768 I llm_load_vocab: special tokens cache size = 25
0.00.065.849 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.065.853 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.065.853 I llm_load_print_meta: arch             = gptneox
0.00.065.854 I llm_load_print_meta: vocab type       = BPE
0.00.065.856 I llm_load_print_meta: n_vocab          = 50304
0.00.065.856 I llm_load_print_meta: n_merges         = 50009
0.00.065.858 I llm_load_print_meta: vocab_only       = 0
0.00.065.858 I llm_load_print_meta: n_ctx_train      = 2048
0.00.065.858 I llm_load_print_meta: n_embd           = 2048
0.00.065.858 I llm_load_print_meta: n_layer          = 24
0.00.065.863 I llm_load_print_meta: n_head           = 16
0.00.065.864 I llm_load_print_meta: n_head_kv        = 16
0.00.065.864 I llm_load_print_meta: n_rot            = 32
0.00.065.864 I llm_load_print_meta: n_swa            = 0
0.00.065.865 I llm_load_print_meta: n_embd_head_k    = 128
0.00.065.865 I llm_load_print_meta: n_embd_head_v    = 128
0.00.065.865 I llm_load_print_meta: n_gqa            = 1
0.00.065.866 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.065.867 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.065.867 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.065.868 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.065.868 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.065.868 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.065.868 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.065.869 I llm_load_print_meta: n_ff             = 8192
0.00.065.869 I llm_load_print_meta: n_expert         = 0
0.00.065.869 I llm_load_print_meta: n_expert_used    = 0
0.00.065.869 I llm_load_print_meta: causal attn      = 1
0.00.065.869 I llm_load_print_meta: pooling type     = 0
0.00.065.870 I llm_load_print_meta: rope type        = 2
0.00.065.870 I llm_load_print_meta: rope scaling     = linear
0.00.065.870 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.065.870 I llm_load_print_meta: freq_scale_train = 1
0.00.065.870 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.065.872 I llm_load_print_meta: rope_finetuned   = unknown
0.00.065.872 I llm_load_print_meta: ssm_d_conv       = 0
0.00.065.872 I llm_load_print_meta: ssm_d_inner      = 0
0.00.065.872 I llm_load_print_meta: ssm_d_state      = 0
0.00.065.872 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.065.872 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.065.873 I llm_load_print_meta: model type       = 1.4B
0.00.065.886 I llm_load_print_meta: model ftype      = Q8_0
0.00.065.887 I llm_load_print_meta: model params     = 1.41 B
0.00.065.887 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.065.887 I llm_load_print_meta: general.name     = 1.4B
0.00.065.888 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.065.888 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.065.889 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.065.889 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.065.889 I llm_load_print_meta: LF token         = 128 ''
0.00.065.889 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.065.890 I llm_load_print_meta: max token length = 1024
0.00.068.412 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.068.412 I llm_load_tensors: offloading output layer to GPU
0.00.068.412 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.068.424 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.068.425 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.068.837 I llama_new_context_with_model: n_seq_max     = 1
0.00.068.838 I llama_new_context_with_model: n_ctx         = 2048
0.00.068.838 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.068.838 I llama_new_context_with_model: n_batch       = 2048
0.00.068.838 I llama_new_context_with_model: n_ubatch      = 512
0.00.068.838 I llama_new_context_with_model: flash_attn    = 0
0.00.068.839 I llama_new_context_with_model: freq_base     = 10000.0
0.00.068.839 I llama_new_context_with_model: freq_scale    = 1
0.00.068.840 I ggml_metal_init: allocating
0.00.068.844 I ggml_metal_init: found device: Apple M4
0.00.068.846 I ggml_metal_init: picking default device: Apple M4
0.00.069.609 I ggml_metal_init: using embedded metal library
0.00.072.236 I ggml_metal_init: GPU name:   Apple M4
0.00.072.238 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.072.238 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.072.239 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.072.239 I ggml_metal_init: simdgroup reduction   = true
0.00.072.239 I ggml_metal_init: simdgroup matrix mul. = true
0.00.072.239 I ggml_metal_init: has bfloat            = true
0.00.072.240 I ggml_metal_init: use bfloat            = true
0.00.072.240 I ggml_metal_init: hasUnifiedMemory      = true
0.00.072.241 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.082.718 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.108.659 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.108.670 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.108.692 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.109.826 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.109.829 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.109.830 I llama_new_context_with_model: graph nodes  = 967
0.00.109.830 I llama_new_context_with_model: graph splits = 2
0.00.109.833 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.109.962 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.109.962 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.311.652 I main: llama threadpool init, n_threads = 4
0.01.311.689 I 
0.01.311.710 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.311.710 I 
0.01.311.941 I sampler seed: 1234
0.01.311.947 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.311.987 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.311.991 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.311.991 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.400.025 I llama_perf_sampler_print:    sampling time =       1.18 ms /    71 runs   (    0.02 ms per token, 60271.65 tokens per second)
0.02.400.026 I llama_perf_context_print:        load time =    1301.88 ms
0.02.400.027 I llama_perf_context_print: prompt eval time =      43.74 ms /     7 tokens (    6.25 ms per token,   160.02 tokens per second)
0.02.400.028 I llama_perf_context_print:        eval time =    1041.41 ms /    63 runs   (   16.53 ms per token,    60.49 tokens per second)
0.02.400.028 I llama_perf_context_print:       total time =    1088.38 ms /    70 tokens
0.02.400.286 I ggml_metal_free: deallocating

real	0m2.419s
user	0m0.116s
sys	0m0.221s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.132 I build: 4457 (ee7136c6) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.959 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.022.719 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.022.726 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.022.728 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.022.729 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.022.729 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.022.729 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.022.730 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.022.731 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.022.731 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.022.732 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.022.735 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.022.735 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.022.736 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.022.736 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.022.738 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.022.740 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.022.741 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.028.660 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.030.195 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.036.069 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.036.071 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.036.071 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.036.072 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.036.072 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.036.073 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.036.073 I llama_model_loader: - type  f32:  194 tensors
0.00.036.074 I llama_model_loader: - type q8_0:   98 tensors
0.00.064.751 I llm_load_vocab: special tokens cache size = 25
0.00.071.746 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.071.749 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.071.749 I llm_load_print_meta: arch             = gptneox
0.00.071.750 I llm_load_print_meta: vocab type       = BPE
0.00.071.750 I llm_load_print_meta: n_vocab          = 50304
0.00.071.750 I llm_load_print_meta: n_merges         = 50009
0.00.071.750 I llm_load_print_meta: vocab_only       = 0
0.00.071.751 I llm_load_print_meta: n_ctx_train      = 2048
0.00.071.751 I llm_load_print_meta: n_embd           = 2048
0.00.071.751 I llm_load_print_meta: n_layer          = 24
0.00.071.753 I llm_load_print_meta: n_head           = 16
0.00.071.756 I llm_load_print_meta: n_head_kv        = 16
0.00.071.756 I llm_load_print_meta: n_rot            = 32
0.00.071.756 I llm_load_print_meta: n_swa            = 0
0.00.071.757 I llm_load_print_meta: n_embd_head_k    = 128
0.00.071.757 I llm_load_print_meta: n_embd_head_v    = 128
0.00.071.757 I llm_load_print_meta: n_gqa            = 1
0.00.071.758 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.071.759 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.071.759 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.071.759 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.071.760 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.071.760 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.071.760 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.071.761 I llm_load_print_meta: n_ff             = 8192
0.00.071.761 I llm_load_print_meta: n_expert         = 0
0.00.071.761 I llm_load_print_meta: n_expert_used    = 0
0.00.071.761 I llm_load_print_meta: causal attn      = 1
0.00.071.761 I llm_load_print_meta: pooling type     = 0
0.00.071.761 I llm_load_print_meta: rope type        = 2
0.00.071.761 I llm_load_print_meta: rope scaling     = linear
0.00.071.762 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.071.762 I llm_load_print_meta: freq_scale_train = 1
0.00.071.762 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.071.763 I llm_load_print_meta: rope_finetuned   = unknown
0.00.071.763 I llm_load_print_meta: ssm_d_conv       = 0
0.00.071.764 I llm_load_print_meta: ssm_d_inner      = 0
0.00.071.765 I llm_load_print_meta: ssm_d_state      = 0
0.00.071.765 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.071.765 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.071.765 I llm_load_print_meta: model type       = 1.4B
0.00.071.778 I llm_load_print_meta: model ftype      = Q8_0
0.00.071.778 I llm_load_print_meta: model params     = 1.41 B
0.00.071.779 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.071.779 I llm_load_print_meta: general.name     = 1.4B
0.00.071.779 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.071.779 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.071.780 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.071.781 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.071.781 I llm_load_print_meta: LF token         = 128 ''
0.00.071.782 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.071.782 I llm_load_print_meta: max token length = 1024
0.00.073.909 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.073.909 I llm_load_tensors: offloading output layer to GPU
0.00.073.909 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.073.915 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.073.915 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.074.270 I llama_new_context_with_model: n_seq_max     = 1
0.00.074.271 I llama_new_context_with_model: n_ctx         = 128
0.00.074.271 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.074.271 I llama_new_context_with_model: n_batch       = 128
0.00.074.271 I llama_new_context_with_model: n_ubatch      = 128
0.00.074.271 I llama_new_context_with_model: flash_attn    = 0
0.00.074.272 I llama_new_context_with_model: freq_base     = 10000.0
0.00.074.272 I llama_new_context_with_model: freq_scale    = 1
0.00.074.272 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.074.273 I ggml_metal_init: allocating
0.00.074.276 I ggml_metal_init: found device: Apple M4
0.00.074.278 I ggml_metal_init: picking default device: Apple M4
0.00.074.902 I ggml_metal_init: using embedded metal library
0.00.077.549 I ggml_metal_init: GPU name:   Apple M4
0.00.077.550 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.077.551 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.077.551 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.077.552 I ggml_metal_init: simdgroup reduction   = true
0.00.077.552 I ggml_metal_init: simdgroup matrix mul. = true
0.00.077.552 I ggml_metal_init: has bfloat            = true
0.00.077.552 I ggml_metal_init: use bfloat            = true
0.00.077.552 I ggml_metal_init: hasUnifiedMemory      = true
0.00.077.553 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.087.088 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.088.366 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.088.372 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.088.390 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.089.250 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.089.251 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.089.252 I llama_new_context_with_model: graph nodes  = 967
0.00.089.252 I llama_new_context_with_model: graph splits = 2
0.00.089.253 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.089.253 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.953.900 I 
0.00.953.922 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.953.931 I perplexity: tokenizing the input ..
0.00.962.091 I perplexity: tokenization took 8.159 ms
0.00.962.095 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.086.581 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.087.743 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.087.766 I llama_perf_context_print:        load time =     941.94 ms
0.01.087.767 I llama_perf_context_print: prompt eval time =     124.26 ms /   128 tokens (    0.97 ms per token,  1030.07 tokens per second)
0.01.087.768 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.087.769 I llama_perf_context_print:       total time =     133.87 ms /   129 tokens
0.01.088.143 I ggml_metal_free: deallocating

real	0m1.108s
user	0m0.101s
sys	0m0.142s
```
- q4_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.049 I build: 4457 (ee7136c6) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.080 I main: llama backend init
0.00.000.082 I main: load the model and apply lora adapter, if any
0.00.015.789 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.032.038 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.032.044 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.032.047 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.032.047 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.032.048 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.032.048 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.032.049 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.032.050 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.032.050 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.032.050 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.032.051 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.032.051 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.032.051 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.032.052 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.032.055 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.032.055 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.032.055 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.036.043 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.037.267 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.041.774 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.041.776 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.041.776 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.041.776 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.041.777 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.041.777 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.041.778 I llama_model_loader: - type  f32:  194 tensors
0.00.041.778 I llama_model_loader: - type q4_0:   97 tensors
0.00.041.778 I llama_model_loader: - type q6_K:    1 tensors
0.00.067.606 I llm_load_vocab: special tokens cache size = 25
0.00.074.074 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.074.077 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.074.078 I llm_load_print_meta: arch             = gptneox
0.00.074.078 I llm_load_print_meta: vocab type       = BPE
0.00.074.079 I llm_load_print_meta: n_vocab          = 50304
0.00.074.079 I llm_load_print_meta: n_merges         = 50009
0.00.074.079 I llm_load_print_meta: vocab_only       = 0
0.00.074.079 I llm_load_print_meta: n_ctx_train      = 2048
0.00.074.079 I llm_load_print_meta: n_embd           = 2048
0.00.074.080 I llm_load_print_meta: n_layer          = 24
0.00.074.088 I llm_load_print_meta: n_head           = 16
0.00.074.090 I llm_load_print_meta: n_head_kv        = 16
0.00.074.090 I llm_load_print_meta: n_rot            = 32
0.00.074.090 I llm_load_print_meta: n_swa            = 0
0.00.074.092 I llm_load_print_meta: n_embd_head_k    = 128
0.00.074.092 I llm_load_print_meta: n_embd_head_v    = 128
0.00.074.093 I llm_load_print_meta: n_gqa            = 1
0.00.074.094 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.074.094 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.074.095 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.074.096 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.074.096 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.074.096 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.074.096 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.074.097 I llm_load_print_meta: n_ff             = 8192
0.00.074.097 I llm_load_print_meta: n_expert         = 0
0.00.074.097 I llm_load_print_meta: n_expert_used    = 0
0.00.074.097 I llm_load_print_meta: causal attn      = 1
0.00.074.098 I llm_load_print_meta: pooling type     = 0
0.00.074.098 I llm_load_print_meta: rope type        = 2
0.00.074.099 I llm_load_print_meta: rope scaling     = linear
0.00.074.100 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.074.101 I llm_load_print_meta: freq_scale_train = 1
0.00.074.102 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.074.102 I llm_load_print_meta: rope_finetuned   = unknown
0.00.074.102 I llm_load_print_meta: ssm_d_conv       = 0
0.00.074.102 I llm_load_print_meta: ssm_d_inner      = 0
0.00.074.103 I llm_load_print_meta: ssm_d_state      = 0
0.00.074.103 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.074.103 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.074.103 I llm_load_print_meta: model type       = 1.4B
0.00.074.116 I llm_load_print_meta: model ftype      = Q4_0
0.00.074.116 I llm_load_print_meta: model params     = 1.41 B
0.00.074.116 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.074.117 I llm_load_print_meta: general.name     = 1.4B
0.00.074.117 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.074.118 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.074.118 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.074.118 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.074.119 I llm_load_print_meta: LF token         = 128 ''
0.00.074.119 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.074.119 I llm_load_print_meta: max token length = 1024
0.00.076.470 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.076.470 I llm_load_tensors: offloading output layer to GPU
0.00.076.470 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.076.482 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.076.483 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.076.828 I llama_new_context_with_model: n_seq_max     = 1
0.00.076.829 I llama_new_context_with_model: n_ctx         = 2048
0.00.076.829 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.076.829 I llama_new_context_with_model: n_batch       = 2048
0.00.076.830 I llama_new_context_with_model: n_ubatch      = 512
0.00.076.830 I llama_new_context_with_model: flash_attn    = 0
0.00.076.830 I llama_new_context_with_model: freq_base     = 10000.0
0.00.076.831 I llama_new_context_with_model: freq_scale    = 1
0.00.076.831 I ggml_metal_init: allocating
0.00.076.834 I ggml_metal_init: found device: Apple M4
0.00.076.836 I ggml_metal_init: picking default device: Apple M4
0.00.077.610 I ggml_metal_init: using embedded metal library
0.00.080.462 I ggml_metal_init: GPU name:   Apple M4
0.00.080.464 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.080.464 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.080.465 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.080.465 I ggml_metal_init: simdgroup reduction   = true
0.00.080.465 I ggml_metal_init: simdgroup matrix mul. = true
0.00.080.465 I ggml_metal_init: has bfloat            = true
0.00.080.465 I ggml_metal_init: use bfloat            = true
0.00.080.466 I ggml_metal_init: hasUnifiedMemory      = true
0.00.080.466 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.096.366 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.121.963 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.121.981 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.122.011 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.123.193 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.123.196 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.123.196 I llama_new_context_with_model: graph nodes  = 967
0.00.123.196 I llama_new_context_with_model: graph splits = 2
0.00.123.200 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.123.341 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.123.341 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.713.715 I main: llama threadpool init, n_threads = 4
0.00.713.762 I 
0.00.713.797 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.713.799 I 
0.00.714.031 I sampler seed: 1234
0.00.714.038 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.714.081 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.714.081 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.714.081 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.400.502 I llama_perf_sampler_print:    sampling time =       1.30 ms /    71 runs   (    0.02 ms per token, 54615.38 tokens per second)
0.01.400.502 I llama_perf_context_print:        load time =     697.92 ms
0.01.400.503 I llama_perf_context_print: prompt eval time =      43.72 ms /     7 tokens (    6.25 ms per token,   160.11 tokens per second)
0.01.400.504 I llama_perf_context_print:        eval time =     639.57 ms /    63 runs   (   10.15 ms per token,    98.50 tokens per second)
0.01.400.504 I llama_perf_context_print:       total time =     686.79 ms /    70 tokens
0.01.400.750 I ggml_metal_free: deallocating

real	0m1.419s
user	0m0.124s
sys	0m0.157s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.096 I build: 4457 (ee7136c6) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.201 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.443 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.017.448 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.450 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.450 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.450 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.451 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.451 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.452 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.453 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.454 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.454 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.454 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.455 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.455 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.457 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.457 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.457 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.222 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.234 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.995 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.996 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.997 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.997 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.997 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.998 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.025.998 I llama_model_loader: - type  f32:  194 tensors
0.00.025.999 I llama_model_loader: - type q4_0:   97 tensors
0.00.025.999 I llama_model_loader: - type q6_K:    1 tensors
0.00.046.910 I llm_load_vocab: special tokens cache size = 25
0.00.052.935 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.937 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.938 I llm_load_print_meta: arch             = gptneox
0.00.052.938 I llm_load_print_meta: vocab type       = BPE
0.00.052.938 I llm_load_print_meta: n_vocab          = 50304
0.00.052.939 I llm_load_print_meta: n_merges         = 50009
0.00.052.939 I llm_load_print_meta: vocab_only       = 0
0.00.052.939 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.939 I llm_load_print_meta: n_embd           = 2048
0.00.052.939 I llm_load_print_meta: n_layer          = 24
0.00.052.942 I llm_load_print_meta: n_head           = 16
0.00.052.943 I llm_load_print_meta: n_head_kv        = 16
0.00.052.943 I llm_load_print_meta: n_rot            = 32
0.00.052.943 I llm_load_print_meta: n_swa            = 0
0.00.052.943 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.944 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.944 I llm_load_print_meta: n_gqa            = 1
0.00.052.945 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.946 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.946 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.947 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.947 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.947 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.948 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.948 I llm_load_print_meta: n_ff             = 8192
0.00.052.949 I llm_load_print_meta: n_expert         = 0
0.00.052.949 I llm_load_print_meta: n_expert_used    = 0
0.00.052.949 I llm_load_print_meta: causal attn      = 1
0.00.052.949 I llm_load_print_meta: pooling type     = 0
0.00.052.949 I llm_load_print_meta: rope type        = 2
0.00.052.950 I llm_load_print_meta: rope scaling     = linear
0.00.052.951 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.951 I llm_load_print_meta: freq_scale_train = 1
0.00.052.951 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.952 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.952 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.952 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.954 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.954 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.954 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.955 I llm_load_print_meta: model type       = 1.4B
0.00.052.966 I llm_load_print_meta: model ftype      = Q4_0
0.00.052.967 I llm_load_print_meta: model params     = 1.41 B
0.00.052.967 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.052.967 I llm_load_print_meta: general.name     = 1.4B
0.00.052.967 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.968 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.968 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.968 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.968 I llm_load_print_meta: LF token         = 128 ''
0.00.052.968 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.970 I llm_load_print_meta: max token length = 1024
0.00.054.954 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.955 I llm_load_tensors: offloading output layer to GPU
0.00.054.955 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.965 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.054.967 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.055.302 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.302 I llama_new_context_with_model: n_ctx         = 128
0.00.055.303 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.055.303 I llama_new_context_with_model: n_batch       = 128
0.00.055.303 I llama_new_context_with_model: n_ubatch      = 128
0.00.055.303 I llama_new_context_with_model: flash_attn    = 0
0.00.055.303 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.304 I llama_new_context_with_model: freq_scale    = 1
0.00.055.304 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.055.304 I ggml_metal_init: allocating
0.00.055.308 I ggml_metal_init: found device: Apple M4
0.00.055.310 I ggml_metal_init: picking default device: Apple M4
0.00.055.878 I ggml_metal_init: using embedded metal library
0.00.058.237 I ggml_metal_init: GPU name:   Apple M4
0.00.058.239 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.239 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.239 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.240 I ggml_metal_init: simdgroup reduction   = true
0.00.058.240 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.240 I ggml_metal_init: has bfloat            = true
0.00.058.240 I ggml_metal_init: use bfloat            = true
0.00.058.241 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.241 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.070.252 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.071.548 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.071.552 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.071.568 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.072.445 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.072.446 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.072.446 I llama_new_context_with_model: graph nodes  = 967
0.00.072.446 I llama_new_context_with_model: graph splits = 2
0.00.072.447 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.072.447 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.751.398 I 
0.00.751.547 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.751.593 I perplexity: tokenizing the input ..
0.00.768.867 I perplexity: tokenization took 17.267 ms
0.00.768.889 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.911.168 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.916.800 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.916.861 I llama_perf_context_print:        load time =     741.18 ms
0.00.916.863 I llama_perf_context_print: prompt eval time =     141.29 ms /   128 tokens (    1.10 ms per token,   905.92 tokens per second)
0.00.916.864 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.916.865 I llama_perf_context_print:       total time =     165.47 ms /   129 tokens
0.00.918.297 I ggml_metal_free: deallocating

real	0m0.951s
user	0m0.124s
sys	0m0.128s
```
- q4_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.047 I build: 4457 (ee7136c6) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.076 I main: llama backend init
0.00.000.078 I main: load the model and apply lora adapter, if any
0.00.008.695 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.023.856 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.023.861 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.023.864 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.023.864 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.023.864 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.023.865 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.023.865 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.023.866 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.023.866 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.023.867 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.023.867 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.023.868 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.023.868 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.023.869 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.023.870 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.023.871 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.023.871 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.027.676 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.028.725 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.032.649 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.032.650 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.032.651 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.032.651 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.032.651 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.032.652 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.032.652 I llama_model_loader: - type  f32:  194 tensors
0.00.032.652 I llama_model_loader: - type q4_1:   97 tensors
0.00.032.653 I llama_model_loader: - type q6_K:    1 tensors
0.00.053.062 I llm_load_vocab: special tokens cache size = 25
0.00.059.052 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.059.055 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.059.055 I llm_load_print_meta: arch             = gptneox
0.00.059.055 I llm_load_print_meta: vocab type       = BPE
0.00.059.056 I llm_load_print_meta: n_vocab          = 50304
0.00.059.056 I llm_load_print_meta: n_merges         = 50009
0.00.059.056 I llm_load_print_meta: vocab_only       = 0
0.00.059.056 I llm_load_print_meta: n_ctx_train      = 2048
0.00.059.056 I llm_load_print_meta: n_embd           = 2048
0.00.059.056 I llm_load_print_meta: n_layer          = 24
0.00.059.059 I llm_load_print_meta: n_head           = 16
0.00.059.060 I llm_load_print_meta: n_head_kv        = 16
0.00.059.060 I llm_load_print_meta: n_rot            = 32
0.00.059.063 I llm_load_print_meta: n_swa            = 0
0.00.059.063 I llm_load_print_meta: n_embd_head_k    = 128
0.00.059.063 I llm_load_print_meta: n_embd_head_v    = 128
0.00.059.064 I llm_load_print_meta: n_gqa            = 1
0.00.059.065 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.059.065 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.059.066 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.059.068 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.059.068 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.059.068 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.059.069 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.059.069 I llm_load_print_meta: n_ff             = 8192
0.00.059.069 I llm_load_print_meta: n_expert         = 0
0.00.059.069 I llm_load_print_meta: n_expert_used    = 0
0.00.059.070 I llm_load_print_meta: causal attn      = 1
0.00.059.070 I llm_load_print_meta: pooling type     = 0
0.00.059.070 I llm_load_print_meta: rope type        = 2
0.00.059.070 I llm_load_print_meta: rope scaling     = linear
0.00.059.070 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.059.071 I llm_load_print_meta: freq_scale_train = 1
0.00.059.071 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.059.071 I llm_load_print_meta: rope_finetuned   = unknown
0.00.059.071 I llm_load_print_meta: ssm_d_conv       = 0
0.00.059.071 I llm_load_print_meta: ssm_d_inner      = 0
0.00.059.071 I llm_load_print_meta: ssm_d_state      = 0
0.00.059.072 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.059.072 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.059.072 I llm_load_print_meta: model type       = 1.4B
0.00.059.083 I llm_load_print_meta: model ftype      = Q4_1
0.00.059.084 I llm_load_print_meta: model params     = 1.41 B
0.00.059.084 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.059.084 I llm_load_print_meta: general.name     = 1.4B
0.00.059.085 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.059.085 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.059.085 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.059.085 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.059.085 I llm_load_print_meta: LF token         = 128 ''
0.00.059.086 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.059.086 I llm_load_print_meta: max token length = 1024
0.00.061.008 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.061.008 I llm_load_tensors: offloading output layer to GPU
0.00.061.009 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.061.019 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.061.020 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.061.379 I llama_new_context_with_model: n_seq_max     = 1
0.00.061.380 I llama_new_context_with_model: n_ctx         = 2048
0.00.061.380 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.061.380 I llama_new_context_with_model: n_batch       = 2048
0.00.061.381 I llama_new_context_with_model: n_ubatch      = 512
0.00.061.381 I llama_new_context_with_model: flash_attn    = 0
0.00.061.381 I llama_new_context_with_model: freq_base     = 10000.0
0.00.061.381 I llama_new_context_with_model: freq_scale    = 1
0.00.061.382 I ggml_metal_init: allocating
0.00.061.388 I ggml_metal_init: found device: Apple M4
0.00.061.390 I ggml_metal_init: picking default device: Apple M4
0.00.061.978 I ggml_metal_init: using embedded metal library
0.00.064.329 I ggml_metal_init: GPU name:   Apple M4
0.00.064.331 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.064.331 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.064.331 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.064.331 I ggml_metal_init: simdgroup reduction   = true
0.00.064.332 I ggml_metal_init: simdgroup matrix mul. = true
0.00.064.332 I ggml_metal_init: has bfloat            = true
0.00.064.332 I ggml_metal_init: use bfloat            = true
0.00.064.332 I ggml_metal_init: hasUnifiedMemory      = true
0.00.064.333 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.073.870 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.094.615 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.094.621 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.094.639 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.095.655 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.095.657 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.095.657 I llama_new_context_with_model: graph nodes  = 967
0.00.095.658 I llama_new_context_with_model: graph splits = 2
0.00.095.660 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.095.795 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.095.795 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.828.783 I main: llama threadpool init, n_threads = 4
0.00.828.831 I 
0.00.828.856 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.828.856 I 
0.00.829.099 I sampler seed: 1234
0.00.829.105 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.829.117 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.829.117 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.829.117 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.560.730 I llama_perf_sampler_print:    sampling time =       1.14 ms /    71 runs   (    0.02 ms per token, 62117.24 tokens per second)
0.01.560.732 I llama_perf_context_print:        load time =     820.08 ms
0.01.560.732 I llama_perf_context_print: prompt eval time =      39.57 ms /     7 tokens (    5.65 ms per token,   176.88 tokens per second)
0.01.560.733 I llama_perf_context_print:        eval time =     689.32 ms /    63 runs   (   10.94 ms per token,    91.39 tokens per second)
0.01.560.733 I llama_perf_context_print:       total time =     731.95 ms /    70 tokens
0.01.560.979 I ggml_metal_free: deallocating

real	0m1.579s
user	0m0.111s
sys	0m0.145s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.180 I build: 4457 (ee7136c6) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.014.097 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.025.905 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.025.911 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.025.914 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.025.915 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.025.915 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.025.916 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.025.916 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.025.918 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.025.918 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.025.919 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.025.923 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.025.923 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.025.923 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.025.924 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.025.926 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.025.927 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.025.927 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.031.623 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.033.010 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.038.250 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.038.252 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.038.252 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.038.253 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.038.253 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.038.253 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.038.254 I llama_model_loader: - type  f32:  194 tensors
0.00.038.254 I llama_model_loader: - type q4_1:   97 tensors
0.00.038.255 I llama_model_loader: - type q6_K:    1 tensors
0.00.063.233 I llm_load_vocab: special tokens cache size = 25
0.00.069.388 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.069.390 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.069.391 I llm_load_print_meta: arch             = gptneox
0.00.069.391 I llm_load_print_meta: vocab type       = BPE
0.00.069.391 I llm_load_print_meta: n_vocab          = 50304
0.00.069.391 I llm_load_print_meta: n_merges         = 50009
0.00.069.391 I llm_load_print_meta: vocab_only       = 0
0.00.069.392 I llm_load_print_meta: n_ctx_train      = 2048
0.00.069.392 I llm_load_print_meta: n_embd           = 2048
0.00.069.392 I llm_load_print_meta: n_layer          = 24
0.00.069.394 I llm_load_print_meta: n_head           = 16
0.00.069.397 I llm_load_print_meta: n_head_kv        = 16
0.00.069.397 I llm_load_print_meta: n_rot            = 32
0.00.069.397 I llm_load_print_meta: n_swa            = 0
0.00.069.397 I llm_load_print_meta: n_embd_head_k    = 128
0.00.069.398 I llm_load_print_meta: n_embd_head_v    = 128
0.00.069.398 I llm_load_print_meta: n_gqa            = 1
0.00.069.399 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.069.400 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.069.400 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.069.401 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.069.401 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.069.401 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.069.401 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.069.402 I llm_load_print_meta: n_ff             = 8192
0.00.069.402 I llm_load_print_meta: n_expert         = 0
0.00.069.403 I llm_load_print_meta: n_expert_used    = 0
0.00.069.403 I llm_load_print_meta: causal attn      = 1
0.00.069.403 I llm_load_print_meta: pooling type     = 0
0.00.069.403 I llm_load_print_meta: rope type        = 2
0.00.069.403 I llm_load_print_meta: rope scaling     = linear
0.00.069.404 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.069.404 I llm_load_print_meta: freq_scale_train = 1
0.00.069.405 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.069.405 I llm_load_print_meta: rope_finetuned   = unknown
0.00.069.405 I llm_load_print_meta: ssm_d_conv       = 0
0.00.069.405 I llm_load_print_meta: ssm_d_inner      = 0
0.00.069.407 I llm_load_print_meta: ssm_d_state      = 0
0.00.069.407 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.069.407 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.069.408 I llm_load_print_meta: model type       = 1.4B
0.00.069.419 I llm_load_print_meta: model ftype      = Q4_1
0.00.069.420 I llm_load_print_meta: model params     = 1.41 B
0.00.069.421 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.069.421 I llm_load_print_meta: general.name     = 1.4B
0.00.069.421 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.069.421 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.069.421 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.069.422 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.069.423 I llm_load_print_meta: LF token         = 128 ''
0.00.069.423 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.069.423 I llm_load_print_meta: max token length = 1024
0.00.071.354 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.071.354 I llm_load_tensors: offloading output layer to GPU
0.00.071.354 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.071.365 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.071.366 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.071.729 I llama_new_context_with_model: n_seq_max     = 1
0.00.071.730 I llama_new_context_with_model: n_ctx         = 128
0.00.071.730 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.071.731 I llama_new_context_with_model: n_batch       = 128
0.00.071.731 I llama_new_context_with_model: n_ubatch      = 128
0.00.071.731 I llama_new_context_with_model: flash_attn    = 0
0.00.071.731 I llama_new_context_with_model: freq_base     = 10000.0
0.00.071.732 I llama_new_context_with_model: freq_scale    = 1
0.00.071.732 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.071.732 I ggml_metal_init: allocating
0.00.071.738 I ggml_metal_init: found device: Apple M4
0.00.071.742 I ggml_metal_init: picking default device: Apple M4
0.00.072.318 I ggml_metal_init: using embedded metal library
0.00.074.655 I ggml_metal_init: GPU name:   Apple M4
0.00.074.657 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.074.657 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.074.657 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.074.658 I ggml_metal_init: simdgroup reduction   = true
0.00.074.658 I ggml_metal_init: simdgroup matrix mul. = true
0.00.074.658 I ggml_metal_init: has bfloat            = true
0.00.074.658 I ggml_metal_init: use bfloat            = true
0.00.074.659 I ggml_metal_init: hasUnifiedMemory      = true
0.00.074.659 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.084.294 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.085.616 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.085.620 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.085.633 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.537 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.086.538 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.086.538 I llama_new_context_with_model: graph nodes  = 967
0.00.086.539 I llama_new_context_with_model: graph splits = 2
0.00.086.540 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.086.540 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.705.879 I 
0.00.705.956 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.705.984 I perplexity: tokenizing the input ..
0.00.719.193 I perplexity: tokenization took 13.202 ms
0.00.719.199 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.854.523 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.855.758 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.855.811 I llama_perf_context_print:        load time =     691.76 ms
0.00.855.812 I llama_perf_context_print: prompt eval time =     134.40 ms /   128 tokens (    1.05 ms per token,   952.37 tokens per second)
0.00.855.813 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.855.814 I llama_perf_context_print:       total time =     149.94 ms /   129 tokens
0.00.856.587 I ggml_metal_free: deallocating

real	0m0.878s
user	0m0.103s
sys	0m0.103s
```
- q5_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.053 I build: 4457 (ee7136c6) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.086 I main: llama backend init
0.00.000.088 I main: load the model and apply lora adapter, if any
0.00.015.947 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.034.029 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.034.035 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.034.037 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.034.038 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.034.038 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.034.038 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.034.038 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.034.039 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.034.040 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.034.040 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.034.040 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.034.040 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.034.041 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.034.041 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.034.044 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.034.045 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.034.045 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.038.029 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.039.049 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.042.972 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.042.974 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.042.974 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.042.975 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.042.975 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.042.975 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.042.976 I llama_model_loader: - type  f32:  194 tensors
0.00.042.976 I llama_model_loader: - type q5_0:   97 tensors
0.00.042.976 I llama_model_loader: - type q6_K:    1 tensors
0.00.064.700 I llm_load_vocab: special tokens cache size = 25
0.00.070.942 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.070.947 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.070.948 I llm_load_print_meta: arch             = gptneox
0.00.070.948 I llm_load_print_meta: vocab type       = BPE
0.00.070.948 I llm_load_print_meta: n_vocab          = 50304
0.00.070.949 I llm_load_print_meta: n_merges         = 50009
0.00.070.949 I llm_load_print_meta: vocab_only       = 0
0.00.070.951 I llm_load_print_meta: n_ctx_train      = 2048
0.00.070.951 I llm_load_print_meta: n_embd           = 2048
0.00.070.951 I llm_load_print_meta: n_layer          = 24
0.00.070.956 I llm_load_print_meta: n_head           = 16
0.00.070.958 I llm_load_print_meta: n_head_kv        = 16
0.00.070.958 I llm_load_print_meta: n_rot            = 32
0.00.070.959 I llm_load_print_meta: n_swa            = 0
0.00.070.959 I llm_load_print_meta: n_embd_head_k    = 128
0.00.070.959 I llm_load_print_meta: n_embd_head_v    = 128
0.00.070.959 I llm_load_print_meta: n_gqa            = 1
0.00.070.960 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.070.962 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.070.962 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.070.965 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.070.967 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.070.967 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.070.967 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.070.968 I llm_load_print_meta: n_ff             = 8192
0.00.070.968 I llm_load_print_meta: n_expert         = 0
0.00.070.968 I llm_load_print_meta: n_expert_used    = 0
0.00.070.968 I llm_load_print_meta: causal attn      = 1
0.00.070.968 I llm_load_print_meta: pooling type     = 0
0.00.070.968 I llm_load_print_meta: rope type        = 2
0.00.070.968 I llm_load_print_meta: rope scaling     = linear
0.00.070.969 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.070.969 I llm_load_print_meta: freq_scale_train = 1
0.00.070.969 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.070.969 I llm_load_print_meta: rope_finetuned   = unknown
0.00.070.969 I llm_load_print_meta: ssm_d_conv       = 0
0.00.070.970 I llm_load_print_meta: ssm_d_inner      = 0
0.00.070.970 I llm_load_print_meta: ssm_d_state      = 0
0.00.070.970 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.070.970 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.070.970 I llm_load_print_meta: model type       = 1.4B
0.00.070.984 I llm_load_print_meta: model ftype      = Q5_0
0.00.070.985 I llm_load_print_meta: model params     = 1.41 B
0.00.070.986 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.070.986 I llm_load_print_meta: general.name     = 1.4B
0.00.070.986 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.070.987 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.070.987 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.070.988 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.070.988 I llm_load_print_meta: LF token         = 128 ''
0.00.070.988 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.070.988 I llm_load_print_meta: max token length = 1024
0.00.073.098 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.073.099 I llm_load_tensors: offloading output layer to GPU
0.00.073.099 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.073.110 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.073.111 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.073.413 I llama_new_context_with_model: n_seq_max     = 1
0.00.073.414 I llama_new_context_with_model: n_ctx         = 2048
0.00.073.414 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.073.414 I llama_new_context_with_model: n_batch       = 2048
0.00.073.414 I llama_new_context_with_model: n_ubatch      = 512
0.00.073.414 I llama_new_context_with_model: flash_attn    = 0
0.00.073.415 I llama_new_context_with_model: freq_base     = 10000.0
0.00.073.415 I llama_new_context_with_model: freq_scale    = 1
0.00.073.416 I ggml_metal_init: allocating
0.00.073.419 I ggml_metal_init: found device: Apple M4
0.00.073.421 I ggml_metal_init: picking default device: Apple M4
0.00.074.104 I ggml_metal_init: using embedded metal library
0.00.076.504 I ggml_metal_init: GPU name:   Apple M4
0.00.076.506 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.076.508 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.076.508 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.076.508 I ggml_metal_init: simdgroup reduction   = true
0.00.076.509 I ggml_metal_init: simdgroup matrix mul. = true
0.00.076.509 I ggml_metal_init: has bfloat            = true
0.00.076.509 I ggml_metal_init: use bfloat            = true
0.00.076.509 I ggml_metal_init: hasUnifiedMemory      = true
0.00.076.510 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.086.850 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.106.550 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.106.559 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.106.580 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.107.482 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.107.483 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.107.484 I llama_new_context_with_model: graph nodes  = 967
0.00.107.484 I llama_new_context_with_model: graph splits = 2
0.00.107.489 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.107.630 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.107.631 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.909.453 I main: llama threadpool init, n_threads = 4
0.00.909.511 I 
0.00.909.533 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.909.533 I 
0.00.909.792 I sampler seed: 1234
0.00.909.796 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.909.838 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.909.838 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.909.838 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.704.603 I llama_perf_sampler_print:    sampling time =       1.22 ms /    71 runs   (    0.02 ms per token, 58436.21 tokens per second)
0.01.704.603 I llama_perf_context_print:        load time =     893.50 ms
0.01.704.604 I llama_perf_context_print: prompt eval time =      51.04 ms /     7 tokens (    7.29 ms per token,   137.16 tokens per second)
0.01.704.605 I llama_perf_context_print:        eval time =     740.72 ms /    63 runs   (   11.76 ms per token,    85.05 tokens per second)
0.01.704.605 I llama_perf_context_print:       total time =     795.15 ms /    70 tokens
0.01.704.841 I ggml_metal_free: deallocating

real	0m1.731s
user	0m0.118s
sys	0m0.184s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.098 I build: 4457 (ee7136c6) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.295 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.043 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.017.047 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.048 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.051 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.051 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.051 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.051 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.056 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.056 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.057 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.057 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.057 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.058 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.058 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.060 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.060 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.060 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.761 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.754 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.482 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.484 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.484 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.484 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.484 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.485 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.025.485 I llama_model_loader: - type  f32:  194 tensors
0.00.025.486 I llama_model_loader: - type q5_0:   97 tensors
0.00.025.486 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.450 I llm_load_vocab: special tokens cache size = 25
0.00.051.637 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.640 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.640 I llm_load_print_meta: arch             = gptneox
0.00.051.641 I llm_load_print_meta: vocab type       = BPE
0.00.051.641 I llm_load_print_meta: n_vocab          = 50304
0.00.051.641 I llm_load_print_meta: n_merges         = 50009
0.00.051.641 I llm_load_print_meta: vocab_only       = 0
0.00.051.642 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.642 I llm_load_print_meta: n_embd           = 2048
0.00.051.642 I llm_load_print_meta: n_layer          = 24
0.00.051.645 I llm_load_print_meta: n_head           = 16
0.00.051.646 I llm_load_print_meta: n_head_kv        = 16
0.00.051.646 I llm_load_print_meta: n_rot            = 32
0.00.051.649 I llm_load_print_meta: n_swa            = 0
0.00.051.649 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.649 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.650 I llm_load_print_meta: n_gqa            = 1
0.00.051.650 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.651 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.652 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.652 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.652 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.652 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.658 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.662 I llm_load_print_meta: n_ff             = 8192
0.00.051.662 I llm_load_print_meta: n_expert         = 0
0.00.051.663 I llm_load_print_meta: n_expert_used    = 0
0.00.051.664 I llm_load_print_meta: causal attn      = 1
0.00.051.664 I llm_load_print_meta: pooling type     = 0
0.00.051.664 I llm_load_print_meta: rope type        = 2
0.00.051.664 I llm_load_print_meta: rope scaling     = linear
0.00.051.665 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.665 I llm_load_print_meta: freq_scale_train = 1
0.00.051.665 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.665 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.665 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.666 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.667 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.667 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.667 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.667 I llm_load_print_meta: model type       = 1.4B
0.00.051.680 I llm_load_print_meta: model ftype      = Q5_0
0.00.051.680 I llm_load_print_meta: model params     = 1.41 B
0.00.051.681 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.051.681 I llm_load_print_meta: general.name     = 1.4B
0.00.051.681 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.681 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.681 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.681 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.682 I llm_load_print_meta: LF token         = 128 ''
0.00.051.683 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.683 I llm_load_print_meta: max token length = 1024
0.00.053.619 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.619 I llm_load_tensors: offloading output layer to GPU
0.00.053.619 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.630 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.053.631 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.054.034 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.035 I llama_new_context_with_model: n_ctx         = 128
0.00.054.035 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.035 I llama_new_context_with_model: n_batch       = 128
0.00.054.035 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.035 I llama_new_context_with_model: flash_attn    = 0
0.00.054.036 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.036 I llama_new_context_with_model: freq_scale    = 1
0.00.054.036 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.037 I ggml_metal_init: allocating
0.00.054.040 I ggml_metal_init: found device: Apple M4
0.00.054.041 I ggml_metal_init: picking default device: Apple M4
0.00.054.633 I ggml_metal_init: using embedded metal library
0.00.056.937 I ggml_metal_init: GPU name:   Apple M4
0.00.056.939 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.939 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.940 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.940 I ggml_metal_init: simdgroup reduction   = true
0.00.056.940 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.940 I ggml_metal_init: has bfloat            = true
0.00.056.940 I ggml_metal_init: use bfloat            = true
0.00.056.941 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.941 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.517 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.067.810 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.814 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.839 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.734 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.735 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.735 I llama_new_context_with_model: graph nodes  = 967
0.00.068.736 I llama_new_context_with_model: graph splits = 2
0.00.068.737 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.737 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.755.235 I 
0.00.755.267 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.755.277 I perplexity: tokenizing the input ..
0.00.763.559 I perplexity: tokenization took 8.28 ms
0.00.763.562 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.897.924 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.899.855 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.899.877 I llama_perf_context_print:        load time =     744.93 ms
0.00.899.878 I llama_perf_context_print: prompt eval time =     134.10 ms /   128 tokens (    1.05 ms per token,   954.48 tokens per second)
0.00.899.878 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.899.879 I llama_perf_context_print:       total time =     144.65 ms /   129 tokens
0.00.900.227 I ggml_metal_free: deallocating

real	0m0.915s
user	0m0.080s
sys	0m0.121s
```
- q5_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.044 I build: 4457 (ee7136c6) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.072 I main: llama backend init
0.00.000.074 I main: load the model and apply lora adapter, if any
0.00.008.699 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.112 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.117 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.119 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.119 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.119 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.120 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.120 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.124 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.124 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.125 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.125 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.125 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.126 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.126 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.130 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.130 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.132 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.883 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.896 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.633 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.634 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.634 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.634 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.635 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.635 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.636 I llama_model_loader: - type  f32:  194 tensors
0.00.024.636 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.636 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.694 I llm_load_vocab: special tokens cache size = 25
0.00.050.680 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.683 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.683 I llm_load_print_meta: arch             = gptneox
0.00.050.683 I llm_load_print_meta: vocab type       = BPE
0.00.050.684 I llm_load_print_meta: n_vocab          = 50304
0.00.050.684 I llm_load_print_meta: n_merges         = 50009
0.00.050.684 I llm_load_print_meta: vocab_only       = 0
0.00.050.684 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.684 I llm_load_print_meta: n_embd           = 2048
0.00.050.685 I llm_load_print_meta: n_layer          = 24
0.00.050.688 I llm_load_print_meta: n_head           = 16
0.00.050.688 I llm_load_print_meta: n_head_kv        = 16
0.00.050.688 I llm_load_print_meta: n_rot            = 32
0.00.050.689 I llm_load_print_meta: n_swa            = 0
0.00.050.690 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.691 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.691 I llm_load_print_meta: n_gqa            = 1
0.00.050.692 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.693 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.698 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.698 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.698 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.699 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.699 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.700 I llm_load_print_meta: n_ff             = 8192
0.00.050.700 I llm_load_print_meta: n_expert         = 0
0.00.050.700 I llm_load_print_meta: n_expert_used    = 0
0.00.050.702 I llm_load_print_meta: causal attn      = 1
0.00.050.704 I llm_load_print_meta: pooling type     = 0
0.00.050.704 I llm_load_print_meta: rope type        = 2
0.00.050.704 I llm_load_print_meta: rope scaling     = linear
0.00.050.704 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.705 I llm_load_print_meta: freq_scale_train = 1
0.00.050.705 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.706 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.706 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.706 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.706 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.706 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.706 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.707 I llm_load_print_meta: model type       = 1.4B
0.00.050.719 I llm_load_print_meta: model ftype      = Q5_1
0.00.050.719 I llm_load_print_meta: model params     = 1.41 B
0.00.050.719 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.050.720 I llm_load_print_meta: general.name     = 1.4B
0.00.050.721 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.721 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.721 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.721 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.721 I llm_load_print_meta: LF token         = 128 ''
0.00.050.722 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.722 I llm_load_print_meta: max token length = 1024
0.00.052.716 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.716 I llm_load_tensors: offloading output layer to GPU
0.00.052.716 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.727 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.052.728 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.053.060 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.061 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.061 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.061 I llama_new_context_with_model: n_batch       = 2048
0.00.053.061 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.062 I llama_new_context_with_model: flash_attn    = 0
0.00.053.062 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.062 I llama_new_context_with_model: freq_scale    = 1
0.00.053.063 I ggml_metal_init: allocating
0.00.053.065 I ggml_metal_init: found device: Apple M4
0.00.053.067 I ggml_metal_init: picking default device: Apple M4
0.00.053.652 I ggml_metal_init: using embedded metal library
0.00.055.984 I ggml_metal_init: GPU name:   Apple M4
0.00.055.985 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.985 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.986 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.986 I ggml_metal_init: simdgroup reduction   = true
0.00.055.987 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.988 I ggml_metal_init: has bfloat            = true
0.00.055.988 I ggml_metal_init: use bfloat            = true
0.00.055.988 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.989 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.604 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.084.956 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.963 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.983 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.085.938 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.085.940 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.085.940 I llama_new_context_with_model: graph nodes  = 967
0.00.085.941 I llama_new_context_with_model: graph splits = 2
0.00.085.943 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.086.090 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.086.091 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.726.236 I main: llama threadpool init, n_threads = 4
0.00.726.281 I 
0.00.726.332 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.726.333 I 
0.00.726.571 I sampler seed: 1234
0.00.726.575 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.726.612 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.726.628 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.726.628 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.558.282 I llama_perf_sampler_print:    sampling time =       1.23 ms /    71 runs   (    0.02 ms per token, 57723.58 tokens per second)
0.01.558.282 I llama_perf_context_print:        load time =     717.53 ms
0.01.558.283 I llama_perf_context_print: prompt eval time =      46.08 ms /     7 tokens (    6.58 ms per token,   151.92 tokens per second)
0.01.558.284 I llama_perf_context_print:        eval time =     782.50 ms /    63 runs   (   12.42 ms per token,    80.51 tokens per second)
0.01.558.284 I llama_perf_context_print:       total time =     832.05 ms /    70 tokens
0.01.558.542 I ggml_metal_free: deallocating

real	0m1.573s
user	0m0.108s
sys	0m0.164s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.102 I build: 4457 (ee7136c6) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.189 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.900 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.015.906 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.907 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.908 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.908 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.908 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.908 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.909 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.910 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.910 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.910 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.911 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.911 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.911 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.914 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.914 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.915 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.651 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.745 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.622 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.627 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.627 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.628 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.634 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.634 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.634 I llama_model_loader: - type  f32:  194 tensors
0.00.024.635 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.635 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.625 I llm_load_vocab: special tokens cache size = 25
0.00.051.869 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.877 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.877 I llm_load_print_meta: arch             = gptneox
0.00.051.877 I llm_load_print_meta: vocab type       = BPE
0.00.051.878 I llm_load_print_meta: n_vocab          = 50304
0.00.051.878 I llm_load_print_meta: n_merges         = 50009
0.00.051.878 I llm_load_print_meta: vocab_only       = 0
0.00.051.878 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.878 I llm_load_print_meta: n_embd           = 2048
0.00.051.878 I llm_load_print_meta: n_layer          = 24
0.00.051.882 I llm_load_print_meta: n_head           = 16
0.00.051.883 I llm_load_print_meta: n_head_kv        = 16
0.00.051.883 I llm_load_print_meta: n_rot            = 32
0.00.051.883 I llm_load_print_meta: n_swa            = 0
0.00.051.883 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.884 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.884 I llm_load_print_meta: n_gqa            = 1
0.00.051.885 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.885 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.886 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.886 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.886 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.886 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.887 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.887 I llm_load_print_meta: n_ff             = 8192
0.00.051.888 I llm_load_print_meta: n_expert         = 0
0.00.051.888 I llm_load_print_meta: n_expert_used    = 0
0.00.051.888 I llm_load_print_meta: causal attn      = 1
0.00.051.888 I llm_load_print_meta: pooling type     = 0
0.00.051.888 I llm_load_print_meta: rope type        = 2
0.00.051.889 I llm_load_print_meta: rope scaling     = linear
0.00.051.889 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.889 I llm_load_print_meta: freq_scale_train = 1
0.00.051.889 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.890 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.890 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.890 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.890 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.890 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.890 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.891 I llm_load_print_meta: model type       = 1.4B
0.00.051.904 I llm_load_print_meta: model ftype      = Q5_1
0.00.051.904 I llm_load_print_meta: model params     = 1.41 B
0.00.051.904 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.051.905 I llm_load_print_meta: general.name     = 1.4B
0.00.051.905 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.905 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.905 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.905 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.906 I llm_load_print_meta: LF token         = 128 ''
0.00.051.906 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.906 I llm_load_print_meta: max token length = 1024
0.00.053.898 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.898 I llm_load_tensors: offloading output layer to GPU
0.00.053.898 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.909 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.053.910 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.054.287 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.288 I llama_new_context_with_model: n_ctx         = 128
0.00.054.288 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.288 I llama_new_context_with_model: n_batch       = 128
0.00.054.289 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.289 I llama_new_context_with_model: flash_attn    = 0
0.00.054.289 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.289 I llama_new_context_with_model: freq_scale    = 1
0.00.054.290 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.290 I ggml_metal_init: allocating
0.00.054.295 I ggml_metal_init: found device: Apple M4
0.00.054.296 I ggml_metal_init: picking default device: Apple M4
0.00.054.909 I ggml_metal_init: using embedded metal library
0.00.057.305 I ggml_metal_init: GPU name:   Apple M4
0.00.057.307 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.307 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.308 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.308 I ggml_metal_init: simdgroup reduction   = true
0.00.057.308 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.308 I ggml_metal_init: has bfloat            = true
0.00.057.308 I ggml_metal_init: use bfloat            = true
0.00.057.309 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.309 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.017 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.068.326 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.330 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.347 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.069.217 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.069.218 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.069.219 I llama_new_context_with_model: graph nodes  = 967
0.00.069.219 I llama_new_context_with_model: graph splits = 2
0.00.069.220 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.069.220 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.654.455 I 
0.00.654.493 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.654.505 I perplexity: tokenizing the input ..
0.00.661.981 I perplexity: tokenization took 7.473 ms
0.00.661.988 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.796.178 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.797.606 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.797.629 I llama_perf_context_print:        load time =     645.26 ms
0.00.797.633 I llama_perf_context_print: prompt eval time =     133.96 ms /   128 tokens (    1.05 ms per token,   955.53 tokens per second)
0.00.797.634 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.797.634 I llama_perf_context_print:       total time =     143.18 ms /   129 tokens
0.00.797.998 I ggml_metal_free: deallocating

real	0m0.813s
user	0m0.080s
sys	0m0.093s
```
- q2_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.046 I build: 4457 (ee7136c6) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.074 I main: llama backend init
0.00.000.076 I main: load the model and apply lora adapter, if any
0.00.009.847 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.751 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.756 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.762 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.762 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.763 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.763 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.763 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.764 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.764 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.765 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.767 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.767 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.767 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.768 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.769 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.770 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.770 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.505 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.551 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.267 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.268 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.269 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.269 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.269 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.269 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.270 I llama_model_loader: - type  f32:  194 tensors
0.00.025.270 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.270 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.271 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.520 I llm_load_vocab: special tokens cache size = 25
0.00.051.678 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.681 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.681 I llm_load_print_meta: arch             = gptneox
0.00.051.682 I llm_load_print_meta: vocab type       = BPE
0.00.051.682 I llm_load_print_meta: n_vocab          = 50304
0.00.051.682 I llm_load_print_meta: n_merges         = 50009
0.00.051.682 I llm_load_print_meta: vocab_only       = 0
0.00.051.682 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.683 I llm_load_print_meta: n_embd           = 2048
0.00.051.683 I llm_load_print_meta: n_layer          = 24
0.00.051.686 I llm_load_print_meta: n_head           = 16
0.00.051.687 I llm_load_print_meta: n_head_kv        = 16
0.00.051.687 I llm_load_print_meta: n_rot            = 32
0.00.051.687 I llm_load_print_meta: n_swa            = 0
0.00.051.687 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.688 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.689 I llm_load_print_meta: n_gqa            = 1
0.00.051.690 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.693 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.693 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.693 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.694 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.694 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.694 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.695 I llm_load_print_meta: n_ff             = 8192
0.00.051.695 I llm_load_print_meta: n_expert         = 0
0.00.051.695 I llm_load_print_meta: n_expert_used    = 0
0.00.051.697 I llm_load_print_meta: causal attn      = 1
0.00.051.697 I llm_load_print_meta: pooling type     = 0
0.00.051.697 I llm_load_print_meta: rope type        = 2
0.00.051.697 I llm_load_print_meta: rope scaling     = linear
0.00.051.697 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.698 I llm_load_print_meta: freq_scale_train = 1
0.00.051.698 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.698 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.698 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.698 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.699 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.699 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.699 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.699 I llm_load_print_meta: model type       = 1.4B
0.00.051.711 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.051.713 I llm_load_print_meta: model params     = 1.41 B
0.00.051.714 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.051.714 I llm_load_print_meta: general.name     = 1.4B
0.00.051.714 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.714 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.715 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.715 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.715 I llm_load_print_meta: LF token         = 128 ''
0.00.051.715 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.715 I llm_load_print_meta: max token length = 1024
0.00.053.654 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.654 I llm_load_tensors: offloading output layer to GPU
0.00.053.654 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.665 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.053.666 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.054.074 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.075 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.075 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.075 I llama_new_context_with_model: n_batch       = 2048
0.00.054.075 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.076 I llama_new_context_with_model: flash_attn    = 0
0.00.054.076 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.076 I llama_new_context_with_model: freq_scale    = 1
0.00.054.077 I ggml_metal_init: allocating
0.00.054.079 I ggml_metal_init: found device: Apple M4
0.00.054.081 I ggml_metal_init: picking default device: Apple M4
0.00.054.676 I ggml_metal_init: using embedded metal library
0.00.057.070 I ggml_metal_init: GPU name:   Apple M4
0.00.057.072 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.072 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.072 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.072 I ggml_metal_init: simdgroup reduction   = true
0.00.057.073 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.073 I ggml_metal_init: has bfloat            = true
0.00.057.073 I ggml_metal_init: use bfloat            = true
0.00.057.074 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.074 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.734 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.086.169 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.178 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.200 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.294 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.295 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.296 I llama_new_context_with_model: graph nodes  = 967
0.00.087.296 I llama_new_context_with_model: graph splits = 2
0.00.087.298 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.087.446 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.447 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.437.267 I main: llama threadpool init, n_threads = 4
0.00.437.302 I 
0.00.437.325 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.437.325 I 
0.00.437.547 I sampler seed: 1234
0.00.437.552 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.437.563 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.437.564 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.437.564 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.114.912 I llama_perf_sampler_print:    sampling time =       1.17 ms /    71 runs   (    0.02 ms per token, 60683.76 tokens per second)
0.01.114.914 I llama_perf_context_print:        load time =     427.42 ms
0.01.114.914 I llama_perf_context_print: prompt eval time =      35.74 ms /     7 tokens (    5.11 ms per token,   195.83 tokens per second)
0.01.114.915 I llama_perf_context_print:        eval time =     638.71 ms /    63 runs   (   10.14 ms per token,    98.64 tokens per second)
0.01.114.915 I llama_perf_context_print:       total time =     677.65 ms /    70 tokens
0.01.115.116 I ggml_metal_free: deallocating

real	0m1.133s
user	0m0.109s
sys	0m0.107s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.105 I build: 4457 (ee7136c6) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.578 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.515 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.017.520 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.522 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.523 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.523 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.523 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.524 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.524 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.525 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.525 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.525 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.526 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.526 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.526 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.528 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.528 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.530 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.392 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.483 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.331 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.333 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.333 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.333 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.334 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.334 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.026.334 I llama_model_loader: - type  f32:  194 tensors
0.00.026.335 I llama_model_loader: - type q2_K:   49 tensors
0.00.026.335 I llama_model_loader: - type q3_K:   48 tensors
0.00.026.335 I llama_model_loader: - type q6_K:    1 tensors
0.00.047.712 I llm_load_vocab: special tokens cache size = 25
0.00.053.883 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.053.888 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.053.889 I llm_load_print_meta: arch             = gptneox
0.00.053.889 I llm_load_print_meta: vocab type       = BPE
0.00.053.891 I llm_load_print_meta: n_vocab          = 50304
0.00.053.891 I llm_load_print_meta: n_merges         = 50009
0.00.053.891 I llm_load_print_meta: vocab_only       = 0
0.00.053.892 I llm_load_print_meta: n_ctx_train      = 2048
0.00.053.892 I llm_load_print_meta: n_embd           = 2048
0.00.053.892 I llm_load_print_meta: n_layer          = 24
0.00.053.895 I llm_load_print_meta: n_head           = 16
0.00.053.896 I llm_load_print_meta: n_head_kv        = 16
0.00.053.896 I llm_load_print_meta: n_rot            = 32
0.00.053.897 I llm_load_print_meta: n_swa            = 0
0.00.053.897 I llm_load_print_meta: n_embd_head_k    = 128
0.00.053.897 I llm_load_print_meta: n_embd_head_v    = 128
0.00.053.898 I llm_load_print_meta: n_gqa            = 1
0.00.053.898 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.053.899 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.053.899 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.053.900 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.053.900 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.053.900 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.053.902 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.053.902 I llm_load_print_meta: n_ff             = 8192
0.00.053.903 I llm_load_print_meta: n_expert         = 0
0.00.053.903 I llm_load_print_meta: n_expert_used    = 0
0.00.053.903 I llm_load_print_meta: causal attn      = 1
0.00.053.903 I llm_load_print_meta: pooling type     = 0
0.00.053.903 I llm_load_print_meta: rope type        = 2
0.00.053.903 I llm_load_print_meta: rope scaling     = linear
0.00.053.904 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.053.904 I llm_load_print_meta: freq_scale_train = 1
0.00.053.904 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.053.904 I llm_load_print_meta: rope_finetuned   = unknown
0.00.053.904 I llm_load_print_meta: ssm_d_conv       = 0
0.00.053.909 I llm_load_print_meta: ssm_d_inner      = 0
0.00.053.909 I llm_load_print_meta: ssm_d_state      = 0
0.00.053.909 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.053.909 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.053.911 I llm_load_print_meta: model type       = 1.4B
0.00.053.923 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.053.923 I llm_load_print_meta: model params     = 1.41 B
0.00.053.924 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.053.924 I llm_load_print_meta: general.name     = 1.4B
0.00.053.924 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.053.924 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.053.924 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.053.924 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.053.925 I llm_load_print_meta: LF token         = 128 ''
0.00.053.925 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.053.925 I llm_load_print_meta: max token length = 1024
0.00.055.752 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.055.752 I llm_load_tensors: offloading output layer to GPU
0.00.055.752 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.055.763 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.055.764 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.056.168 I llama_new_context_with_model: n_seq_max     = 1
0.00.056.169 I llama_new_context_with_model: n_ctx         = 128
0.00.056.169 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.056.169 I llama_new_context_with_model: n_batch       = 128
0.00.056.169 I llama_new_context_with_model: n_ubatch      = 128
0.00.056.170 I llama_new_context_with_model: flash_attn    = 0
0.00.056.170 I llama_new_context_with_model: freq_base     = 10000.0
0.00.056.170 I llama_new_context_with_model: freq_scale    = 1
0.00.056.171 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.056.171 I ggml_metal_init: allocating
0.00.056.174 I ggml_metal_init: found device: Apple M4
0.00.056.176 I ggml_metal_init: picking default device: Apple M4
0.00.056.767 I ggml_metal_init: using embedded metal library
0.00.059.124 I ggml_metal_init: GPU name:   Apple M4
0.00.059.126 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.059.126 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.059.126 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.059.127 I ggml_metal_init: simdgroup reduction   = true
0.00.059.127 I ggml_metal_init: simdgroup matrix mul. = true
0.00.059.127 I ggml_metal_init: has bfloat            = true
0.00.059.127 I ggml_metal_init: use bfloat            = true
0.00.059.128 I ggml_metal_init: hasUnifiedMemory      = true
0.00.059.129 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.069.519 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.070.890 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.070.894 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.070.909 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.071.771 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.071.772 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.071.772 I llama_new_context_with_model: graph nodes  = 967
0.00.071.773 I llama_new_context_with_model: graph splits = 2
0.00.071.774 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.071.774 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.386.851 I 
0.00.386.879 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.386.892 I perplexity: tokenizing the input ..
0.00.394.830 I perplexity: tokenization took 7.937 ms
0.00.394.834 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.527.270 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.528.449 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.528.476 I llama_perf_context_print:        load time =     376.27 ms
0.00.528.477 I llama_perf_context_print: prompt eval time =     132.21 ms /   128 tokens (    1.03 ms per token,   968.15 tokens per second)
0.00.528.482 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.528.483 I llama_perf_context_print:       total time =     141.62 ms /   129 tokens
0.00.529.001 I ggml_metal_free: deallocating

real	0m0.546s
user	0m0.082s
sys	0m0.070s
```
- q3_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4457 (ee7136c6) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.074 I main: llama backend init
0.00.000.076 I main: load the model and apply lora adapter, if any
0.00.010.953 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.018.638 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.018.642 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.643 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.644 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.644 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.644 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.644 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.648 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.648 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.648 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.649 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.649 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.650 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.651 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.653 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.653 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.654 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.462 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.448 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.186 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.027.187 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.187 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.187 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.187 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.188 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.027.188 I llama_model_loader: - type  f32:  194 tensors
0.00.027.189 I llama_model_loader: - type q3_K:   25 tensors
0.00.027.189 I llama_model_loader: - type q4_K:   71 tensors
0.00.027.189 I llama_model_loader: - type q5_K:    1 tensors
0.00.027.189 I llama_model_loader: - type q6_K:    1 tensors
0.00.047.289 I llm_load_vocab: special tokens cache size = 25
0.00.053.384 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.053.386 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.053.387 I llm_load_print_meta: arch             = gptneox
0.00.053.387 I llm_load_print_meta: vocab type       = BPE
0.00.053.387 I llm_load_print_meta: n_vocab          = 50304
0.00.053.387 I llm_load_print_meta: n_merges         = 50009
0.00.053.387 I llm_load_print_meta: vocab_only       = 0
0.00.053.388 I llm_load_print_meta: n_ctx_train      = 2048
0.00.053.388 I llm_load_print_meta: n_embd           = 2048
0.00.053.388 I llm_load_print_meta: n_layer          = 24
0.00.053.391 I llm_load_print_meta: n_head           = 16
0.00.053.391 I llm_load_print_meta: n_head_kv        = 16
0.00.053.392 I llm_load_print_meta: n_rot            = 32
0.00.053.392 I llm_load_print_meta: n_swa            = 0
0.00.053.392 I llm_load_print_meta: n_embd_head_k    = 128
0.00.053.392 I llm_load_print_meta: n_embd_head_v    = 128
0.00.053.393 I llm_load_print_meta: n_gqa            = 1
0.00.053.394 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.053.394 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.053.395 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.053.395 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.053.396 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.053.396 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.053.396 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.053.397 I llm_load_print_meta: n_ff             = 8192
0.00.053.398 I llm_load_print_meta: n_expert         = 0
0.00.053.399 I llm_load_print_meta: n_expert_used    = 0
0.00.053.400 I llm_load_print_meta: causal attn      = 1
0.00.053.400 I llm_load_print_meta: pooling type     = 0
0.00.053.400 I llm_load_print_meta: rope type        = 2
0.00.053.400 I llm_load_print_meta: rope scaling     = linear
0.00.053.401 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.053.401 I llm_load_print_meta: freq_scale_train = 1
0.00.053.401 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.053.401 I llm_load_print_meta: rope_finetuned   = unknown
0.00.053.401 I llm_load_print_meta: ssm_d_conv       = 0
0.00.053.401 I llm_load_print_meta: ssm_d_inner      = 0
0.00.053.402 I llm_load_print_meta: ssm_d_state      = 0
0.00.053.402 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.053.402 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.053.402 I llm_load_print_meta: model type       = 1.4B
0.00.053.414 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.053.415 I llm_load_print_meta: model params     = 1.41 B
0.00.053.416 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.053.416 I llm_load_print_meta: general.name     = 1.4B
0.00.053.416 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.053.416 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.053.416 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.053.417 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.053.417 I llm_load_print_meta: LF token         = 128 ''
0.00.053.417 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.053.417 I llm_load_print_meta: max token length = 1024
0.00.055.373 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.055.373 I llm_load_tensors: offloading output layer to GPU
0.00.055.374 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.055.384 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.055.385 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.055.703 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.703 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.704 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.704 I llama_new_context_with_model: n_batch       = 2048
0.00.055.704 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.704 I llama_new_context_with_model: flash_attn    = 0
0.00.055.704 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.705 I llama_new_context_with_model: freq_scale    = 1
0.00.055.705 I ggml_metal_init: allocating
0.00.055.708 I ggml_metal_init: found device: Apple M4
0.00.055.710 I ggml_metal_init: picking default device: Apple M4
0.00.056.312 I ggml_metal_init: using embedded metal library
0.00.058.653 I ggml_metal_init: GPU name:   Apple M4
0.00.058.656 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.656 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.656 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.657 I ggml_metal_init: simdgroup reduction   = true
0.00.058.657 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.657 I ggml_metal_init: has bfloat            = true
0.00.058.657 I ggml_metal_init: use bfloat            = true
0.00.058.658 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.662 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.294 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.087.682 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.087.692 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.087.721 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.088.710 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.088.712 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.088.712 I llama_new_context_with_model: graph nodes  = 967
0.00.088.713 I llama_new_context_with_model: graph splits = 2
0.00.088.715 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.088.858 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.088.858 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.533.657 I main: llama threadpool init, n_threads = 4
0.00.533.699 I 
0.00.533.722 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.533.722 I 
0.00.533.952 I sampler seed: 1234
0.00.533.956 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.534.002 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.534.003 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.534.004 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.281.818 I llama_perf_sampler_print:    sampling time =       1.22 ms /    71 runs   (    0.02 ms per token, 58436.21 tokens per second)
0.01.281.819 I llama_perf_context_print:        load time =     522.70 ms
0.01.281.819 I llama_perf_context_print: prompt eval time =      43.81 ms /     7 tokens (    6.26 ms per token,   159.79 tokens per second)
0.01.281.820 I llama_perf_context_print:        eval time =     700.99 ms /    63 runs   (   11.13 ms per token,    89.87 tokens per second)
0.01.281.821 I llama_perf_context_print:       total time =     748.16 ms /    70 tokens
0.01.282.017 I ggml_metal_free: deallocating

real	0m1.298s
user	0m0.110s
sys	0m0.120s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.089 I build: 4457 (ee7136c6) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.224 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.214 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.219 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.221 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.222 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.222 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.222 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.224 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.225 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.226 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.226 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.226 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.227 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.227 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.227 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.229 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.230 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.230 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.011 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.000 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.756 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.757 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.757 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.757 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.758 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.758 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.759 I llama_model_loader: - type  f32:  194 tensors
0.00.024.759 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.759 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.759 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.760 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.654 I llm_load_vocab: special tokens cache size = 25
0.00.050.765 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.768 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.768 I llm_load_print_meta: arch             = gptneox
0.00.050.769 I llm_load_print_meta: vocab type       = BPE
0.00.050.769 I llm_load_print_meta: n_vocab          = 50304
0.00.050.769 I llm_load_print_meta: n_merges         = 50009
0.00.050.769 I llm_load_print_meta: vocab_only       = 0
0.00.050.769 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.770 I llm_load_print_meta: n_embd           = 2048
0.00.050.770 I llm_load_print_meta: n_layer          = 24
0.00.050.773 I llm_load_print_meta: n_head           = 16
0.00.050.773 I llm_load_print_meta: n_head_kv        = 16
0.00.050.774 I llm_load_print_meta: n_rot            = 32
0.00.050.774 I llm_load_print_meta: n_swa            = 0
0.00.050.774 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.774 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.775 I llm_load_print_meta: n_gqa            = 1
0.00.050.776 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.777 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.777 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.778 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.780 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.780 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.780 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.781 I llm_load_print_meta: n_ff             = 8192
0.00.050.781 I llm_load_print_meta: n_expert         = 0
0.00.050.781 I llm_load_print_meta: n_expert_used    = 0
0.00.050.781 I llm_load_print_meta: causal attn      = 1
0.00.050.783 I llm_load_print_meta: pooling type     = 0
0.00.050.783 I llm_load_print_meta: rope type        = 2
0.00.050.783 I llm_load_print_meta: rope scaling     = linear
0.00.050.784 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.784 I llm_load_print_meta: freq_scale_train = 1
0.00.050.784 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.785 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.785 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.785 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.785 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.785 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.786 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.786 I llm_load_print_meta: model type       = 1.4B
0.00.050.799 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.050.799 I llm_load_print_meta: model params     = 1.41 B
0.00.050.800 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.050.800 I llm_load_print_meta: general.name     = 1.4B
0.00.050.800 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.800 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.801 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.801 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.801 I llm_load_print_meta: LF token         = 128 ''
0.00.050.801 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.801 I llm_load_print_meta: max token length = 1024
0.00.052.753 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.754 I llm_load_tensors: offloading output layer to GPU
0.00.052.754 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.764 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.052.766 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.053.139 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.140 I llama_new_context_with_model: n_ctx         = 128
0.00.053.140 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.140 I llama_new_context_with_model: n_batch       = 128
0.00.053.140 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.140 I llama_new_context_with_model: flash_attn    = 0
0.00.053.141 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.141 I llama_new_context_with_model: freq_scale    = 1
0.00.053.141 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.142 I ggml_metal_init: allocating
0.00.053.145 I ggml_metal_init: found device: Apple M4
0.00.053.146 I ggml_metal_init: picking default device: Apple M4
0.00.053.686 I ggml_metal_init: using embedded metal library
0.00.056.009 I ggml_metal_init: GPU name:   Apple M4
0.00.056.010 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.011 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.011 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.011 I ggml_metal_init: simdgroup reduction   = true
0.00.056.012 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.012 I ggml_metal_init: has bfloat            = true
0.00.056.012 I ggml_metal_init: use bfloat            = true
0.00.056.012 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.013 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.747 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.067.054 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.060 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.077 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.013 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.014 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.015 I llama_new_context_with_model: graph nodes  = 967
0.00.068.015 I llama_new_context_with_model: graph splits = 2
0.00.068.016 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.016 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.465.624 I 
0.00.465.657 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.465.668 I perplexity: tokenizing the input ..
0.00.473.593 I perplexity: tokenization took 7.923 ms
0.00.473.596 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.606.074 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.607.313 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.607.335 I llama_perf_context_print:        load time =     456.39 ms
0.00.607.336 I llama_perf_context_print: prompt eval time =     132.24 ms /   128 tokens (    1.03 ms per token,   967.91 tokens per second)
0.00.607.337 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.607.337 I llama_perf_context_print:       total time =     141.71 ms /   129 tokens
0.00.607.698 I ggml_metal_free: deallocating

real	0m0.620s
user	0m0.078s
sys	0m0.078s
```
- q4_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4457 (ee7136c6) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.078 I main: llama backend init
0.00.000.080 I main: load the model and apply lora adapter, if any
0.00.009.062 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.511 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.516 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.517 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.518 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.518 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.519 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.519 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.523 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.523 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.523 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.524 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.524 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.524 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.529 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.532 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.533 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.533 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.479 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.542 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.404 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.405 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.405 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.406 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.406 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.406 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.407 I llama_model_loader: - type  f32:  194 tensors
0.00.025.407 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.407 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.407 I llama_model_loader: - type q6_K:   13 tensors
0.00.045.625 I llm_load_vocab: special tokens cache size = 25
0.00.051.583 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.586 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.586 I llm_load_print_meta: arch             = gptneox
0.00.051.587 I llm_load_print_meta: vocab type       = BPE
0.00.051.587 I llm_load_print_meta: n_vocab          = 50304
0.00.051.587 I llm_load_print_meta: n_merges         = 50009
0.00.051.587 I llm_load_print_meta: vocab_only       = 0
0.00.051.587 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.587 I llm_load_print_meta: n_embd           = 2048
0.00.051.588 I llm_load_print_meta: n_layer          = 24
0.00.051.590 I llm_load_print_meta: n_head           = 16
0.00.051.591 I llm_load_print_meta: n_head_kv        = 16
0.00.051.591 I llm_load_print_meta: n_rot            = 32
0.00.051.591 I llm_load_print_meta: n_swa            = 0
0.00.051.591 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.592 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.592 I llm_load_print_meta: n_gqa            = 1
0.00.051.593 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.594 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.595 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.595 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.595 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.595 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.595 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.596 I llm_load_print_meta: n_ff             = 8192
0.00.051.596 I llm_load_print_meta: n_expert         = 0
0.00.051.598 I llm_load_print_meta: n_expert_used    = 0
0.00.051.599 I llm_load_print_meta: causal attn      = 1
0.00.051.599 I llm_load_print_meta: pooling type     = 0
0.00.051.599 I llm_load_print_meta: rope type        = 2
0.00.051.599 I llm_load_print_meta: rope scaling     = linear
0.00.051.600 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.600 I llm_load_print_meta: freq_scale_train = 1
0.00.051.600 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.601 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.601 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.601 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.601 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.601 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.601 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.602 I llm_load_print_meta: model type       = 1.4B
0.00.051.608 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.051.609 I llm_load_print_meta: model params     = 1.41 B
0.00.051.609 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.051.609 I llm_load_print_meta: general.name     = 1.4B
0.00.051.609 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.610 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.610 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.610 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.610 I llm_load_print_meta: LF token         = 128 ''
0.00.051.611 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.611 I llm_load_print_meta: max token length = 1024
0.00.053.380 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.380 I llm_load_tensors: offloading output layer to GPU
0.00.053.380 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.386 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.053.386 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.053.822 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.823 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.823 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.823 I llama_new_context_with_model: n_batch       = 2048
0.00.053.823 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.824 I llama_new_context_with_model: flash_attn    = 0
0.00.053.824 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.824 I llama_new_context_with_model: freq_scale    = 1
0.00.053.825 I ggml_metal_init: allocating
0.00.053.828 I ggml_metal_init: found device: Apple M4
0.00.053.830 I ggml_metal_init: picking default device: Apple M4
0.00.054.434 I ggml_metal_init: using embedded metal library
0.00.056.796 I ggml_metal_init: GPU name:   Apple M4
0.00.056.798 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.798 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.799 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.800 I ggml_metal_init: simdgroup reduction   = true
0.00.056.801 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.801 I ggml_metal_init: has bfloat            = true
0.00.056.801 I ggml_metal_init: use bfloat            = true
0.00.056.801 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.802 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.480 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.085.471 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.482 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.499 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.565 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.567 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.567 I llama_new_context_with_model: graph nodes  = 967
0.00.086.567 I llama_new_context_with_model: graph splits = 2
0.00.086.570 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.086.691 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.086.692 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.614.132 I main: llama threadpool init, n_threads = 4
0.00.614.174 I 
0.00.614.223 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.614.224 I 
0.00.614.462 I sampler seed: 1234
0.00.614.466 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.614.508 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.614.509 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.614.509 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.375.141 I llama_perf_sampler_print:    sampling time =       1.25 ms /    71 runs   (    0.02 ms per token, 56982.34 tokens per second)
0.01.375.142 I llama_perf_context_print:        load time =     605.07 ms
0.01.375.143 I llama_perf_context_print: prompt eval time =      47.13 ms /     7 tokens (    6.73 ms per token,   148.53 tokens per second)
0.01.375.144 I llama_perf_context_print:        eval time =     710.48 ms /    63 runs   (   11.28 ms per token,    88.67 tokens per second)
0.01.375.144 I llama_perf_context_print:       total time =     761.01 ms /    70 tokens
0.01.375.415 I ggml_metal_free: deallocating

real	0m1.393s
user	0m0.110s
sys	0m0.142s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.095 I build: 4457 (ee7136c6) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.891 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.971 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.015.976 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.978 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.978 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.979 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.979 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.979 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.981 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.981 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.982 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.982 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.982 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.983 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.983 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.986 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.987 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.987 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.778 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.753 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.466 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.467 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.467 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.468 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.468 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.468 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.469 I llama_model_loader: - type  f32:  194 tensors
0.00.024.469 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.470 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.470 I llama_model_loader: - type q6_K:   13 tensors
0.00.045.215 I llm_load_vocab: special tokens cache size = 25
0.00.051.289 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.292 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.292 I llm_load_print_meta: arch             = gptneox
0.00.051.293 I llm_load_print_meta: vocab type       = BPE
0.00.051.293 I llm_load_print_meta: n_vocab          = 50304
0.00.051.293 I llm_load_print_meta: n_merges         = 50009
0.00.051.293 I llm_load_print_meta: vocab_only       = 0
0.00.051.294 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.294 I llm_load_print_meta: n_embd           = 2048
0.00.051.294 I llm_load_print_meta: n_layer          = 24
0.00.051.296 I llm_load_print_meta: n_head           = 16
0.00.051.297 I llm_load_print_meta: n_head_kv        = 16
0.00.051.298 I llm_load_print_meta: n_rot            = 32
0.00.051.298 I llm_load_print_meta: n_swa            = 0
0.00.051.300 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.301 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.301 I llm_load_print_meta: n_gqa            = 1
0.00.051.302 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.303 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.303 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.304 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.304 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.304 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.304 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.305 I llm_load_print_meta: n_ff             = 8192
0.00.051.305 I llm_load_print_meta: n_expert         = 0
0.00.051.305 I llm_load_print_meta: n_expert_used    = 0
0.00.051.306 I llm_load_print_meta: causal attn      = 1
0.00.051.306 I llm_load_print_meta: pooling type     = 0
0.00.051.306 I llm_load_print_meta: rope type        = 2
0.00.051.308 I llm_load_print_meta: rope scaling     = linear
0.00.051.308 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.308 I llm_load_print_meta: freq_scale_train = 1
0.00.051.308 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.309 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.309 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.309 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.309 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.309 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.310 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.310 I llm_load_print_meta: model type       = 1.4B
0.00.051.327 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.051.327 I llm_load_print_meta: model params     = 1.41 B
0.00.051.328 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.051.328 I llm_load_print_meta: general.name     = 1.4B
0.00.051.328 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.329 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.329 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.329 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.329 I llm_load_print_meta: LF token         = 128 ''
0.00.051.330 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.330 I llm_load_print_meta: max token length = 1024
0.00.053.255 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.255 I llm_load_tensors: offloading output layer to GPU
0.00.053.256 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.266 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.053.268 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.053.641 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.642 I llama_new_context_with_model: n_ctx         = 128
0.00.053.642 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.642 I llama_new_context_with_model: n_batch       = 128
0.00.053.643 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.643 I llama_new_context_with_model: flash_attn    = 0
0.00.053.643 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.643 I llama_new_context_with_model: freq_scale    = 1
0.00.053.644 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.644 I ggml_metal_init: allocating
0.00.053.648 I ggml_metal_init: found device: Apple M4
0.00.053.649 I ggml_metal_init: picking default device: Apple M4
0.00.054.218 I ggml_metal_init: using embedded metal library
0.00.056.584 I ggml_metal_init: GPU name:   Apple M4
0.00.056.586 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.586 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.587 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.587 I ggml_metal_init: simdgroup reduction   = true
0.00.056.587 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.587 I ggml_metal_init: has bfloat            = true
0.00.056.587 I ggml_metal_init: use bfloat            = true
0.00.056.588 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.588 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.402 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.067.641 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.644 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.660 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.550 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.551 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.552 I llama_new_context_with_model: graph nodes  = 967
0.00.068.552 I llama_new_context_with_model: graph splits = 2
0.00.068.553 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.553 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.550.707 I 
0.00.550.743 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.550.757 I perplexity: tokenizing the input ..
0.00.558.519 I perplexity: tokenization took 7.761 ms
0.00.558.524 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.693.021 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.694.184 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.694.211 I llama_perf_context_print:        load time =     541.81 ms
0.00.694.212 I llama_perf_context_print: prompt eval time =     134.27 ms /   128 tokens (    1.05 ms per token,   953.30 tokens per second)
0.00.694.213 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.694.213 I llama_perf_context_print:       total time =     143.51 ms /   129 tokens
0.00.694.713 I ggml_metal_free: deallocating

real	0m0.710s
user	0m0.079s
sys	0m0.096s
```
- q5_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.046 I build: 4457 (ee7136c6) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.075 I main: llama backend init
0.00.000.077 I main: load the model and apply lora adapter, if any
0.00.010.548 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.018.332 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.018.341 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.342 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.343 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.343 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.344 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.346 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.346 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.347 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.347 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.348 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.348 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.348 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.350 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.351 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.352 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.352 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.274 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.272 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.144 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.027.145 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.146 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.146 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.146 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.147 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.027.147 I llama_model_loader: - type  f32:  194 tensors
0.00.027.147 I llama_model_loader: - type q5_K:   61 tensors
0.00.027.148 I llama_model_loader: - type q6_K:   37 tensors
0.00.048.056 I llm_load_vocab: special tokens cache size = 25
0.00.053.816 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.053.819 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.053.820 I llm_load_print_meta: arch             = gptneox
0.00.053.820 I llm_load_print_meta: vocab type       = BPE
0.00.053.820 I llm_load_print_meta: n_vocab          = 50304
0.00.053.820 I llm_load_print_meta: n_merges         = 50009
0.00.053.821 I llm_load_print_meta: vocab_only       = 0
0.00.053.821 I llm_load_print_meta: n_ctx_train      = 2048
0.00.053.821 I llm_load_print_meta: n_embd           = 2048
0.00.053.821 I llm_load_print_meta: n_layer          = 24
0.00.053.824 I llm_load_print_meta: n_head           = 16
0.00.053.825 I llm_load_print_meta: n_head_kv        = 16
0.00.053.825 I llm_load_print_meta: n_rot            = 32
0.00.053.825 I llm_load_print_meta: n_swa            = 0
0.00.053.825 I llm_load_print_meta: n_embd_head_k    = 128
0.00.053.825 I llm_load_print_meta: n_embd_head_v    = 128
0.00.053.826 I llm_load_print_meta: n_gqa            = 1
0.00.053.827 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.053.827 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.053.830 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.053.830 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.053.830 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.053.830 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.053.830 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.053.831 I llm_load_print_meta: n_ff             = 8192
0.00.053.831 I llm_load_print_meta: n_expert         = 0
0.00.053.831 I llm_load_print_meta: n_expert_used    = 0
0.00.053.832 I llm_load_print_meta: causal attn      = 1
0.00.053.832 I llm_load_print_meta: pooling type     = 0
0.00.053.832 I llm_load_print_meta: rope type        = 2
0.00.053.832 I llm_load_print_meta: rope scaling     = linear
0.00.053.833 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.053.833 I llm_load_print_meta: freq_scale_train = 1
0.00.053.833 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.053.833 I llm_load_print_meta: rope_finetuned   = unknown
0.00.053.834 I llm_load_print_meta: ssm_d_conv       = 0
0.00.053.834 I llm_load_print_meta: ssm_d_inner      = 0
0.00.053.834 I llm_load_print_meta: ssm_d_state      = 0
0.00.053.834 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.053.834 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.053.834 I llm_load_print_meta: model type       = 1.4B
0.00.053.846 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.053.847 I llm_load_print_meta: model params     = 1.41 B
0.00.053.847 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.053.847 I llm_load_print_meta: general.name     = 1.4B
0.00.053.848 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.053.848 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.053.848 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.053.849 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.053.849 I llm_load_print_meta: LF token         = 128 ''
0.00.053.849 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.053.849 I llm_load_print_meta: max token length = 1024
0.00.055.917 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.055.917 I llm_load_tensors: offloading output layer to GPU
0.00.055.917 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.055.928 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.055.929 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.056.254 I llama_new_context_with_model: n_seq_max     = 1
0.00.056.254 I llama_new_context_with_model: n_ctx         = 2048
0.00.056.254 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.056.255 I llama_new_context_with_model: n_batch       = 2048
0.00.056.255 I llama_new_context_with_model: n_ubatch      = 512
0.00.056.255 I llama_new_context_with_model: flash_attn    = 0
0.00.056.255 I llama_new_context_with_model: freq_base     = 10000.0
0.00.056.256 I llama_new_context_with_model: freq_scale    = 1
0.00.056.256 I ggml_metal_init: allocating
0.00.056.259 I ggml_metal_init: found device: Apple M4
0.00.056.261 I ggml_metal_init: picking default device: Apple M4
0.00.056.863 I ggml_metal_init: using embedded metal library
0.00.059.228 I ggml_metal_init: GPU name:   Apple M4
0.00.059.229 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.059.230 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.059.230 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.059.230 I ggml_metal_init: simdgroup reduction   = true
0.00.059.230 I ggml_metal_init: simdgroup matrix mul. = true
0.00.059.230 I ggml_metal_init: has bfloat            = true
0.00.059.231 I ggml_metal_init: use bfloat            = true
0.00.059.231 I ggml_metal_init: hasUnifiedMemory      = true
0.00.059.232 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.069.178 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.088.145 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.088.151 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.088.169 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.089.164 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.089.166 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.089.166 I llama_new_context_with_model: graph nodes  = 967
0.00.089.166 I llama_new_context_with_model: graph splits = 2
0.00.089.169 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.089.317 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.089.318 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.710.144 I main: llama threadpool init, n_threads = 4
0.00.710.188 I 
0.00.710.207 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.710.208 I 
0.00.710.432 I sampler seed: 1234
0.00.710.436 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.710.448 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.710.448 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.710.448 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.564.304 I llama_perf_sampler_print:    sampling time =       1.16 ms /    71 runs   (    0.02 ms per token, 61206.90 tokens per second)
0.01.564.305 I llama_perf_context_print:        load time =     699.59 ms
0.01.564.306 I llama_perf_context_print: prompt eval time =      55.50 ms /     7 tokens (    7.93 ms per token,   126.12 tokens per second)
0.01.564.306 I llama_perf_context_print:        eval time =     795.36 ms /    63 runs   (   12.62 ms per token,    79.21 tokens per second)
0.01.564.307 I llama_perf_context_print:       total time =     854.16 ms /    70 tokens
0.01.564.574 I ggml_metal_free: deallocating

real	0m1.583s
user	0m0.111s
sys	0m0.151s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.090 I build: 4457 (ee7136c6) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.760 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.677 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.682 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.684 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.684 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.685 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.685 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.685 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.686 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.686 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.687 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.687 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.688 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.688 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.689 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.690 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.690 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.690 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.463 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.446 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.227 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.228 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.228 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.228 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.229 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.229 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.230 I llama_model_loader: - type  f32:  194 tensors
0.00.025.230 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.230 I llama_model_loader: - type q6_K:   37 tensors
0.00.045.080 I llm_load_vocab: special tokens cache size = 25
0.00.051.253 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.256 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.257 I llm_load_print_meta: arch             = gptneox
0.00.051.257 I llm_load_print_meta: vocab type       = BPE
0.00.051.257 I llm_load_print_meta: n_vocab          = 50304
0.00.051.257 I llm_load_print_meta: n_merges         = 50009
0.00.051.258 I llm_load_print_meta: vocab_only       = 0
0.00.051.258 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.258 I llm_load_print_meta: n_embd           = 2048
0.00.051.258 I llm_load_print_meta: n_layer          = 24
0.00.051.261 I llm_load_print_meta: n_head           = 16
0.00.051.262 I llm_load_print_meta: n_head_kv        = 16
0.00.051.262 I llm_load_print_meta: n_rot            = 32
0.00.051.262 I llm_load_print_meta: n_swa            = 0
0.00.051.263 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.263 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.264 I llm_load_print_meta: n_gqa            = 1
0.00.051.264 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.266 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.267 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.267 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.273 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.274 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.275 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.277 I llm_load_print_meta: n_ff             = 8192
0.00.051.277 I llm_load_print_meta: n_expert         = 0
0.00.051.278 I llm_load_print_meta: n_expert_used    = 0
0.00.051.278 I llm_load_print_meta: causal attn      = 1
0.00.051.278 I llm_load_print_meta: pooling type     = 0
0.00.051.278 I llm_load_print_meta: rope type        = 2
0.00.051.278 I llm_load_print_meta: rope scaling     = linear
0.00.051.279 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.279 I llm_load_print_meta: freq_scale_train = 1
0.00.051.280 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.280 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.280 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.280 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.281 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.281 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.281 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.281 I llm_load_print_meta: model type       = 1.4B
0.00.051.294 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.051.295 I llm_load_print_meta: model params     = 1.41 B
0.00.051.296 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.051.296 I llm_load_print_meta: general.name     = 1.4B
0.00.051.296 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.296 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.297 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.297 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.297 I llm_load_print_meta: LF token         = 128 ''
0.00.051.298 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.298 I llm_load_print_meta: max token length = 1024
0.00.053.277 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.277 I llm_load_tensors: offloading output layer to GPU
0.00.053.277 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.288 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.053.289 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.053.645 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.646 I llama_new_context_with_model: n_ctx         = 128
0.00.053.646 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.646 I llama_new_context_with_model: n_batch       = 128
0.00.053.646 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.647 I llama_new_context_with_model: flash_attn    = 0
0.00.053.647 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.647 I llama_new_context_with_model: freq_scale    = 1
0.00.053.648 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.648 I ggml_metal_init: allocating
0.00.053.654 I ggml_metal_init: found device: Apple M4
0.00.053.656 I ggml_metal_init: picking default device: Apple M4
0.00.054.186 I ggml_metal_init: using embedded metal library
0.00.056.531 I ggml_metal_init: GPU name:   Apple M4
0.00.056.533 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.533 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.534 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.534 I ggml_metal_init: simdgroup reduction   = true
0.00.056.534 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.534 I ggml_metal_init: has bfloat            = true
0.00.056.535 I ggml_metal_init: use bfloat            = true
0.00.056.535 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.537 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.022 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.067.268 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.270 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.292 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.142 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.144 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.144 I llama_new_context_with_model: graph nodes  = 967
0.00.068.144 I llama_new_context_with_model: graph splits = 2
0.00.068.145 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.146 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.678.094 I 
0.00.678.132 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.678.210 I perplexity: tokenizing the input ..
0.00.686.384 I perplexity: tokenization took 8.173 ms
0.00.686.387 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.827.567 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.828.714 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.828.740 I llama_perf_context_print:        load time =     668.33 ms
0.00.828.741 I llama_perf_context_print: prompt eval time =     140.93 ms /   128 tokens (    1.10 ms per token,   908.25 tokens per second)
0.00.828.742 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.828.743 I llama_perf_context_print:       total time =     150.65 ms /   129 tokens
0.00.829.348 I ggml_metal_free: deallocating

real	0m0.844s
user	0m0.079s
sys	0m0.121s
```
- q6_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.044 I build: 4457 (ee7136c6) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.073 I main: llama backend init
0.00.000.075 I main: load the model and apply lora adapter, if any
0.00.009.137 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.201 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.017.206 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.207 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.208 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.208 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.209 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.209 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.211 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.211 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.212 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.212 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.212 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.213 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.213 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.216 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.216 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.216 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.996 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.984 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.744 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.746 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.746 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.746 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.747 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.747 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.747 I llama_model_loader: - type  f32:  194 tensors
0.00.025.748 I llama_model_loader: - type q6_K:   98 tensors
0.00.045.834 I llm_load_vocab: special tokens cache size = 25
0.00.051.933 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.935 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.936 I llm_load_print_meta: arch             = gptneox
0.00.051.936 I llm_load_print_meta: vocab type       = BPE
0.00.051.937 I llm_load_print_meta: n_vocab          = 50304
0.00.051.937 I llm_load_print_meta: n_merges         = 50009
0.00.051.937 I llm_load_print_meta: vocab_only       = 0
0.00.051.937 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.937 I llm_load_print_meta: n_embd           = 2048
0.00.051.937 I llm_load_print_meta: n_layer          = 24
0.00.051.941 I llm_load_print_meta: n_head           = 16
0.00.051.943 I llm_load_print_meta: n_head_kv        = 16
0.00.051.944 I llm_load_print_meta: n_rot            = 32
0.00.051.944 I llm_load_print_meta: n_swa            = 0
0.00.051.944 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.944 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.945 I llm_load_print_meta: n_gqa            = 1
0.00.051.950 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.951 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.952 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.952 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.952 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.952 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.953 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.953 I llm_load_print_meta: n_ff             = 8192
0.00.051.953 I llm_load_print_meta: n_expert         = 0
0.00.051.953 I llm_load_print_meta: n_expert_used    = 0
0.00.051.954 I llm_load_print_meta: causal attn      = 1
0.00.051.957 I llm_load_print_meta: pooling type     = 0
0.00.051.958 I llm_load_print_meta: rope type        = 2
0.00.051.958 I llm_load_print_meta: rope scaling     = linear
0.00.051.959 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.959 I llm_load_print_meta: freq_scale_train = 1
0.00.051.959 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.959 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.959 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.960 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.960 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.960 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.960 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.960 I llm_load_print_meta: model type       = 1.4B
0.00.051.972 I llm_load_print_meta: model ftype      = Q6_K
0.00.051.972 I llm_load_print_meta: model params     = 1.41 B
0.00.051.972 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.051.973 I llm_load_print_meta: general.name     = 1.4B
0.00.051.973 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.973 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.973 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.973 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.974 I llm_load_print_meta: LF token         = 128 ''
0.00.051.974 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.974 I llm_load_print_meta: max token length = 1024
0.00.054.017 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.017 I llm_load_tensors: offloading output layer to GPU
0.00.054.017 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.027 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.054.029 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.054.375 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.376 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.376 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.376 I llama_new_context_with_model: n_batch       = 2048
0.00.054.377 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.377 I llama_new_context_with_model: flash_attn    = 0
0.00.054.377 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.377 I llama_new_context_with_model: freq_scale    = 1
0.00.054.378 I ggml_metal_init: allocating
0.00.054.381 I ggml_metal_init: found device: Apple M4
0.00.054.382 I ggml_metal_init: picking default device: Apple M4
0.00.054.964 I ggml_metal_init: using embedded metal library
0.00.057.274 I ggml_metal_init: GPU name:   Apple M4
0.00.057.275 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.276 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.276 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.276 I ggml_metal_init: simdgroup reduction   = true
0.00.057.278 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.278 I ggml_metal_init: has bfloat            = true
0.00.057.278 I ggml_metal_init: use bfloat            = true
0.00.057.279 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.279 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.882 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.086.224 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.229 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.248 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.316 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.318 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.318 I llama_new_context_with_model: graph nodes  = 967
0.00.087.318 I llama_new_context_with_model: graph splits = 2
0.00.087.321 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.087.438 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.439 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.771.705 I main: llama threadpool init, n_threads = 4
0.00.771.744 I 
0.00.771.785 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.771.786 I 
0.00.772.021 I sampler seed: 1234
0.00.772.026 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.772.067 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.772.088 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.772.088 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.

He added: Ive

0.01.643.478 I llama_perf_sampler_print:    sampling time =       1.13 ms /    71 runs   (    0.02 ms per token, 62720.85 tokens per second)
0.01.643.479 I llama_perf_context_print:        load time =     762.56 ms
0.01.643.480 I llama_perf_context_print: prompt eval time =      54.52 ms /     7 tokens (    7.79 ms per token,   128.39 tokens per second)
0.01.643.484 I llama_perf_context_print:        eval time =     814.02 ms /    63 runs   (   12.92 ms per token,    77.39 tokens per second)
0.01.643.484 I llama_perf_context_print:       total time =     871.78 ms /    70 tokens
0.01.643.710 I ggml_metal_free: deallocating

real	0m1.662s
user	0m0.110s
sys	0m0.168s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.089 I build: 4457 (ee7136c6) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.653 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.517 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.521 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.523 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.524 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.524 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.524 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.525 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.526 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.527 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.529 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.529 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.529 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.530 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.530 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.535 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.536 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.536 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.360 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.367 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.196 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.197 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.197 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.198 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.198 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.198 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.199 I llama_model_loader: - type  f32:  194 tensors
0.00.024.199 I llama_model_loader: - type q6_K:   98 tensors
0.00.044.901 I llm_load_vocab: special tokens cache size = 25
0.00.050.960 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.963 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.963 I llm_load_print_meta: arch             = gptneox
0.00.050.964 I llm_load_print_meta: vocab type       = BPE
0.00.050.964 I llm_load_print_meta: n_vocab          = 50304
0.00.050.964 I llm_load_print_meta: n_merges         = 50009
0.00.050.964 I llm_load_print_meta: vocab_only       = 0
0.00.050.964 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.965 I llm_load_print_meta: n_embd           = 2048
0.00.050.965 I llm_load_print_meta: n_layer          = 24
0.00.050.968 I llm_load_print_meta: n_head           = 16
0.00.050.968 I llm_load_print_meta: n_head_kv        = 16
0.00.050.969 I llm_load_print_meta: n_rot            = 32
0.00.050.969 I llm_load_print_meta: n_swa            = 0
0.00.050.969 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.969 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.970 I llm_load_print_meta: n_gqa            = 1
0.00.050.971 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.971 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.972 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.972 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.973 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.973 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.973 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.974 I llm_load_print_meta: n_ff             = 8192
0.00.050.974 I llm_load_print_meta: n_expert         = 0
0.00.050.974 I llm_load_print_meta: n_expert_used    = 0
0.00.050.974 I llm_load_print_meta: causal attn      = 1
0.00.050.975 I llm_load_print_meta: pooling type     = 0
0.00.050.975 I llm_load_print_meta: rope type        = 2
0.00.050.975 I llm_load_print_meta: rope scaling     = linear
0.00.050.975 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.976 I llm_load_print_meta: freq_scale_train = 1
0.00.050.976 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.976 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.976 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.977 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.977 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.977 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.977 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.977 I llm_load_print_meta: model type       = 1.4B
0.00.050.991 I llm_load_print_meta: model ftype      = Q6_K
0.00.050.991 I llm_load_print_meta: model params     = 1.41 B
0.00.050.992 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.050.992 I llm_load_print_meta: general.name     = 1.4B
0.00.050.992 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.992 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.992 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.993 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.993 I llm_load_print_meta: LF token         = 128 ''
0.00.050.993 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.993 I llm_load_print_meta: max token length = 1024
0.00.053.063 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.063 I llm_load_tensors: offloading output layer to GPU
0.00.053.064 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.074 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.053.076 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.053.461 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.462 I llama_new_context_with_model: n_ctx         = 128
0.00.053.462 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.462 I llama_new_context_with_model: n_batch       = 128
0.00.053.463 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.463 I llama_new_context_with_model: flash_attn    = 0
0.00.053.463 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.463 I llama_new_context_with_model: freq_scale    = 1
0.00.053.464 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.464 I ggml_metal_init: allocating
0.00.053.470 I ggml_metal_init: found device: Apple M4
0.00.053.472 I ggml_metal_init: picking default device: Apple M4
0.00.054.051 I ggml_metal_init: using embedded metal library
0.00.056.377 I ggml_metal_init: GPU name:   Apple M4
0.00.056.379 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.379 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.379 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.380 I ggml_metal_init: simdgroup reduction   = true
0.00.056.380 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.380 I ggml_metal_init: has bfloat            = true
0.00.056.380 I ggml_metal_init: use bfloat            = true
0.00.056.380 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.381 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.034 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.067.296 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.300 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.315 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.160 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.161 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.161 I llama_new_context_with_model: graph nodes  = 967
0.00.068.162 I llama_new_context_with_model: graph splits = 2
0.00.068.163 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.163 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.481.400 I 
0.00.481.433 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.481.446 I perplexity: tokenizing the input ..
0.00.489.475 I perplexity: tokenization took 8.029 ms
0.00.489.479 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.629.351 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.630.564 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.630.593 I llama_perf_context_print:        load time =     472.74 ms
0.00.630.594 I llama_perf_context_print: prompt eval time =     139.65 ms /   128 tokens (    1.09 ms per token,   916.60 tokens per second)
0.00.630.595 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.630.595 I llama_perf_context_print:       total time =     149.20 ms /   129 tokens
0.00.631.036 I ggml_metal_free: deallocating

real	0m0.647s
user	0m0.080s
sys	0m0.088s
```
- save-load-state: 
```
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4457 (ee7136c6)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x143e0a7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x143e0aef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x143e0b4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x143e0ba50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x143e0c000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x143e0c5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x143e0cb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x143e0d110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x143e0d6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x143e0dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x143e0e0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x143e0e5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x143e0f0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x143e0f890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x143e100a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x143e107c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x143e10ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x143e11600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x143e11d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x143e124f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x143e12c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x143e13330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x143e13a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x143e142f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x143e14a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x143e14cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x143e152e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x143e15f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x143e16490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x143e16750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x143e16bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x143e16eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x143e17740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x143e17c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x143e17f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x143e183e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x143e18880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x143e18d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x143e191c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x143e19660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x143e19b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x143e19fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x143e1a440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x143e1a8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x143e1aba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x143e1b1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x143e1b7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x143e1c0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x143e1c6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x143e1cd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x143e1d310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x143e1d920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x143e1df30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x143e1e540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x143e1ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x143e1f1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x143e1f670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x143e1f930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x143e1ff40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x143e20730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x143e209f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x143e20e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x143e21330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x143e217d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x143e21c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x143e22110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x143e225b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x143e22a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x143e22ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x143e23390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x143e23830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x143e23cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x143e24170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x143e246c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x143e24c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x143e25160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x143e256b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x143e25c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x143e26150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x143e266a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x143e26bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x143e27140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x143e27690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x143e27be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x143e28130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x143e28680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x143e28bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x143e29120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x143e29670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x143e29bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x143e2a110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x143e2a660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x143e2abb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x143e2b100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x143e2b650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x143e2bba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x143e2c0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x143e1bdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x143e2c560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x143e2cd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x143e2d260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x143e2d7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x143e2dd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x143e2e250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x143e2e7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x143e2ecf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x143e2f240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x143e2f790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x143e2fce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x143e30230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x143e30780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x143e30cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x143e31220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x143e316c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x143e31b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x143e32000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x143e324a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x143e32940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x143e32de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x143e33280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x143e33720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x143e33bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x143e34060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x143e34500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x143e349a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x143e34e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x143e352e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x143e35780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x143e35c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x143e360c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x143e36560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x143e36a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x143e36ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x143e37340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x143e377e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x143e37c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x143e38120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x143e385c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x143e38a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x143e38f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x143e393a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x143e39840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x143e39ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x143e3a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x143e3a620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x143e3aac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x143e3af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x143e3b400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x143e3b8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x143e3bd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x143e3c1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x143e3c680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x143e3cb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x143e3cfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x143e3d460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x143e3d900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x143e3dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x143e3e240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x143e3e6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x143e3eb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x143e3f020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x143e3f4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x143e3f960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x143e3fe00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x143e402a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x143e40740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x143e40be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x143e41080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x143e41520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x143e419c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x143e41e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x143e42300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x143e427a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x143e42c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x143e430e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x143e43580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x143e43a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x143e43ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x143e44360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x143e44800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x143e44ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x143e45140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x143e455e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x143e45a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x143e45f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x143e463c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x143e46860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x143e46d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x143e471a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x143e47640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x143e47ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x143e47f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x143e48420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x143e48970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x143e48ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x143e49410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x143e49960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x143e49c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x143e4a230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x143e4a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x143e4ae50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x143e4b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x143e4bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x143e4bda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x143e4c3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x143e4c9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x143e4d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x143e4d650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x143e4daf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x143e4df90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x143e4e740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x143e4ec90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x143e4f1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x143e4f730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x143e4fc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x143e501d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x143e50720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x143e50c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x143e511c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x143e51710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x143e51c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x143e521b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x143e52700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x143e52c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x143e531a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x143e536f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x143e53c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x143e54190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x143e546e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x143e54c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x143e55180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x143e556d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x143e55c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x143e56170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x143e566c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x143e56c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x143e57160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x143e576b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x143e57c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x143e58150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x143e586a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x143e58bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x143e59140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x143e59690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x143e59be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x143e5a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x143e5a680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x143e5abd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x143e5b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x143e5b670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x143e5bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x143e5c110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x143e5c660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x143e5cbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x143e5d100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x143e5d650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x143e5dba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x143e5e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x143e5e640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x143e5eb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x143e5f0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x143e5f630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x143e5fb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x143e600d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x143e60620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x143e60b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x143e610c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x143e61560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x143e61a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x143e61ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x143e62340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x143e627e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x143e62c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x143e63120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x143e635c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x143e63a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x143e63f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x143e643a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x143e64840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x143e64ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x143e65180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x143e65620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x143e65b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x143e66290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x143e669b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x143e670d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x143e677f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x143e67ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x143e682a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x143e68560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x143e68b70 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
0.00.138.604 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.138.608 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x126304b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x126304f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x126305400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x126305870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x126305ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x126306150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x1263065c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x126306a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x126306ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x126307310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x126307780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x126307e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x126308990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x126309140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x126309950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12630a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12630a790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12630aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12630b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12630bd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12630c420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12630cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12630d260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12630d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12630e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12630e360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12630e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12630ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12630ef00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12630f370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12630f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12630fd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x126310180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x126310440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1263108b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x126310d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x126311190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x126311600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x126311a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x126311ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x126312350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x1263127c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x126312c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x1263130a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x126313510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x126313980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x126313df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x126314260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x1263146d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x126314b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x126314fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x126315420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x126315890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x126315d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x126316170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x1263165e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x126316b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x126317050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x1263174c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x126317930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x126317da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x126318210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x126318680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x126318af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x126318f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x1263193d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x126319840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x126319cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12631a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12631a590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12631aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12631ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12631b2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12631b750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12631bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12631c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12631c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12631c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12631cd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12631d1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12631d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12631dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12631df40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12631e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12631e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12631ec90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12631f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12631f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12631f9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12631fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x1263202c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x126320730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x126320ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x126321010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x126321480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x1263218f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x126321d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x1263221d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x126322640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x126322ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x126322f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x126323390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x126323800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x126323c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x1263240e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x126324550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x1263249c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x126324e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x1263252a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x126325710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x126325b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x126325ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x126326460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x1263268d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x126326d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x1263271b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x126327620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x126327a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x126327f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x126328370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x1263287e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x126328c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x1263290c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x126329530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x1263299a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x126329e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12632a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12632a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12632ab60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12632afd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12632b440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12632b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12632bd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12632c190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12632c600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12632ca70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12632cee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12632d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12632d7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12632dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12632e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12632e510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12632e980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12632edf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12632f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12632f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12632fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12632ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x126330420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x126330890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x126330d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x126331170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x1263315e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x126331a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x126331ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x126332330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x1263327a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x126332c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x126333080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x1263334f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x126333960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x126333dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x126334240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x1263346b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x126334b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x126334f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x126335bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x126335e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x126336140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x1263365b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x126336a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x126336e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x126337300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x126337770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x126337be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x126338050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x1263384c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x126338930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x126338da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x126339210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x126339680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x126339af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x126339f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12633a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12633a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12633acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12633b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12633b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12633ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12633be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12633c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12633c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12633cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12633d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12633d4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12633d910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12633dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12633e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12633e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12633ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12633ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12633f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12633f910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12633fe20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x126340290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x126340700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x126340b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x126340fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x126341500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x126341a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x126342580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x126342840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x126342e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x1263433c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x126343980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x126343f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x126344500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x126344ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x126345080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x126345640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x126345c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x1263461c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x126346780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x126346d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x126347300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x1263478c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x126347e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x126348440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x126348a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x126348fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x126349580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x126349b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12634a100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12634a6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12634ac80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12634b240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12634b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12634bdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12634c380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12634c940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12634cf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12634d4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12634da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12634e040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12634e600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12634ebc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12634f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12634f740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12634fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x1263502c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x126350880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x126350e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x126351400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x1263519c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x126351f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x126352540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x126352b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x1263530c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x126353680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x126353c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x126354200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x1263547c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x126354d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x126355340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x126355900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x126355ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x126356480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x126356a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x126356f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x126357440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x126357940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x126357e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x126358340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x126358840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x126358d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x126359240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x126359740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x126359c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12635a140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12635a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12635ab40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12635b040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12635b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12635bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12635c670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12635cd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12635d4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12635d770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12635df60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12635e220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12635e830 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x143d08700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x143d067b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x143d08d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x143d09190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x143d09600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x143d09a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x143d09ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x143d0a350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x143d0a7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x143d0ac30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x143d0b0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x143d0b720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x143d0c240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x143d0c9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x143d0d200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x143d0d920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x143d0e040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x143d0e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x143d0ee80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x143d0f650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x143d0fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x143d10490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x143d10bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x143d112d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x143d119f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x143d11cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x143d11f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x143d123e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x143d12850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x143d12cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x143d131c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x143d136d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x143d13b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x143d13e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x143d14270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x143d146e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x143d14c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x143d15140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x143d15640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x143d15b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x143d16040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x143d16540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x143d16a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x143d16f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x143d17440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x143d178b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x143d17d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x143d18190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x143d18600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x143d18a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x143d18ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x143d19350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x143d197c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x143d19c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x143d1a0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x143d1a870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x143d1ad10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x143d1afd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x143d1b5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x143d1bdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x143d1c270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x143d1c710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x143d1cbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x143d1d050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x143d1d4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x143d1d990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x143d1de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x143d1e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x143d1e770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x143d1ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x143d1f0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x143d1f550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x143d1f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x143d1ff40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x143d20490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x143d209e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x143d20f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x143d21480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x143d219d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x143d21f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x143d22470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x143d229c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x143d22f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x143d23460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x143d239b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x143d23f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x143d24450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x143d249a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x143d24ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x143d25440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x143d25990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x143d25ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x143d26430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x143d26980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x143d26ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x143d27420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x143d27970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x143d27ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x143d28410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x143d28960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x143d28eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x143d29400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x143d29950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x143d29ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x143d2a3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x143d2a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x143d2ae90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x143d2b3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x143d2b930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x143d2be80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x143d2c3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x143d2c920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x143d2ce70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x143d2d310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x143d2d7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x143d2dc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x143d2e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x143d2e590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x143d2ea30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x143d2eed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x143d2f370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x143d2f810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x143d2fcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x143d30150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x143d305f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x143d30a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x143d30f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x143d313d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x143d31870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x143d31d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x143d321b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x143d32650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x143d32af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x143d32f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x143d33430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x143d338d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x143d33d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x143d34210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x143d346b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x143d34b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x143d34ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x143d35490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x143d35930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x143d35dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x143d36270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x143d36710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x143d36bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x143d37050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x143d374f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x143d37990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x143d37e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x143d382d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x143d38770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x143d38c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x143d390b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x143d39550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x143d399f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x143d39e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x143d3a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x143d3a7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x143d3ac70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x143d3b110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x143d3b5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x143d3ba50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x143d3bef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x143d3c390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x143d3c830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x143d3ccd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x143d3d170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x143d3d610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x143d3dab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x143d3df50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x143d3e3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x143d3e890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x143d3ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x143d3f1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x143d3f670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x143d3fb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x143d3ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x143d40450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x143d408f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x143d40d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x143d41230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x143d416d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x143d41b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x143d42010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x143d424b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x143d42950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x143d42df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x143d43290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x143d43730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x143d43bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x143d44070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x143d445c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x143d44b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x143d45060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x143d455b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x143d45870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x143d45e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x143d46490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x143d46aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x143d47290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x143d47730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x143d479f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x143d48000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x143d48610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x143d48e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x143d492a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x143d49740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x143d49be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x143d4a390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x143d4a8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x143d4ae30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x143d4b380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x143d4b8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x143d4be20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x143d4c370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x143d4c8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x143d4ce10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x143d4d360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x143d4d8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x143d4de00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x143d4e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x143d4e8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x143d4edf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x143d4f340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x143d4f890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x143d4fde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x143d50330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x143d50880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x143d50dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x143d51320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x143d51870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x143d51dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x143d52310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x143d52860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x143d52db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x143d53300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x143d53850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x143d53da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x143d542f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x143d54840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x143d54d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x143d552e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x143d55830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x143d55d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x143d562d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x143d56820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x143d56d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x143d572c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x143d57810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x143d57d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x143d582b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x143d58800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x143d58d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x143d592a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x143d597f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x143d59d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x143d5a290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x143d5a7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x143d5ad30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x143d5b280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x143d5b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x143d5bd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x143d5c270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x143d5c7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x143d5cd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x143d5d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x143d5d650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x143d5daf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x143d5df90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x143d5e430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x143d5e8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x143d5ed70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x143d5f210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x143d5f6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x143d5fb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x143d5fff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x143d60490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x143d60930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x143d60dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x143d61270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x143d617c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x143d61ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x143d62600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x143d62d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x143d63440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x143d63700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x143d63ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x143d641b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x143d647c0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.789s
user	0m0.294s
sys	0m0.298s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4457 (ee7136c6)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x11c70fe30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x11c710540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x11c710af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x11c7110a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x11c711650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x11c711c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x11c7121b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x11c712760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x11c712d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x11c713210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x11c713710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x11c713c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x11c714730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x11c714ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x11c7156f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x11c715e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x11c716530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x11c716c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x11c717370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x11c717b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x11c718260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x11c718980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x11c7190a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x11c719940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x11c71a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x11c71a320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x11c71a930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x11c71b5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x11c71bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x11c71bda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x11c71c240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x11c71c500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x11c71cd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x11c71d2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x11c71d590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x11c71da30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x11c71ded0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x11c71e370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x11c71e810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x11c71ecb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x11c71f150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x11c71f5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x11c71fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x11c71ff30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x11c7201f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x11c720800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x11c720e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x11c721730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x11c721d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x11c722350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x11c722960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x11c722f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x11c723580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x11c723b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x11c724380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x11c724820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x11c724cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x11c724f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x11c725590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x11c725d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x11c726040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x11c7264e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x11c726980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x11c726e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x11c7272c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x11c727760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x11c727c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x11c7280a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x11c728540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x11c7289e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x11c728e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x11c729320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x11c7297c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x11c729d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x11c72a260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x11c72a7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x11c72ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x11c72b250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x11c72b7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x11c72bcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x11c72c240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x11c72c790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x11c72cce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x11c72d230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x11c72d780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x11c72dcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x11c72e220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x11c72e770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x11c72ecc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x11c72f210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x11c72f760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x11c72fcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x11c730200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x11c730750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x11c730ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x11c7311f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x11c731740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x11c721420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x11c731bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x11c732360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x11c7328b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x11c732e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x11c733350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x11c7338a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x11c733df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x11c734340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x11c734890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x11c734de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x11c735330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x11c735880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x11c735dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x11c736320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x11c736870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x11c736d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x11c7371b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x11c737650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x11c737af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x11c737f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x11c738430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x11c7388d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x11c738d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x11c739210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x11c7396b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x11c739b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x11c739ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x11c73a490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x11c73a930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x11c73add0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x11c73b270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x11c73b710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x11c73bbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x11c73c050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x11c73c4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x11c73c990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x11c73ce30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x11c73d2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x11c73d770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x11c73dc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x11c73e0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x11c73e550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x11c73e9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x11c73ee90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x11c73f330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x11c73f7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x11c73fc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x11c740110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x11c7405b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x11c740a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x11c740ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x11c741390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x11c741830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x11c741cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x11c742170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x11c742610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x11c742ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x11c742f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x11c7433f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x11c743890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x11c743d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x11c7441d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x11c744670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x11c744b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x11c744fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x11c745450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x11c7458f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x11c745d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x11c746230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x11c7466d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x11c746b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x11c747010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x11c7474b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x11c747950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x11c747df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x11c748290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x11c748730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x11c748bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x11c749070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x11c749510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x11c7499b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x11c749e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x11c74a2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x11c74a790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x11c74ac30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x11c74b0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x11c74b570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x11c74ba10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x11c74beb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x11c74c350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x11c74c7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x11c74cc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x11c74d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x11c74d5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x11c74da70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x11c74dfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x11c74e510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x11c74ea60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x11c74efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x11c74f270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x11c74f880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x11c74fe90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x11c7504a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x11c750c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x11c751130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x11c7513f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x11c751a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x11c752010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x11c752800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x11c752ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x11c753140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x11c7535e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x11c753d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x11c7542e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x11c754830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x11c754d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x11c7552d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x11c755820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x11c755d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x11c7562c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x11c756810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x11c756d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x11c7572b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x11c757800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x11c757d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x11c7582a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x11c7587f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x11c758d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x11c759290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x11c7597e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x11c759d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x11c75a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x11c75a7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x11c75ad20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x11c75b270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x11c75b7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x11c75bd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x11c75c260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x11c75c7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x11c75cd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x11c75d250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x11c75d7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x11c75dcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x11c75e240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x11c75e790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x11c75ece0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x11c75f230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x11c75f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x11c75fcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x11c760220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x11c760770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x11c760cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x11c761210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x11c761760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x11c761cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x11c762200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x11c762750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x11c762ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x11c7631f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x11c763740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x11c763c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x11c7641e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x11c764730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x11c764c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x11c7651d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x11c765720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x11c765c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x11c7661c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x11c766710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x11c766bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x11c767050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x11c7674f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x11c767990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x11c767e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x11c7682d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x11c768770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x11c768c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x11c7690b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x11c769550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x11c7699f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x11c769e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x11c76a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x11c76a7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x11c76ac70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x11c76b1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x11c76b8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x11c76c000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x11c76c720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x11c76ce40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x11c76d100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x11c76d8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x11c76dbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x11c76e1c0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
0.00.087.418 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.422 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x118c04d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x118c051c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x118c05630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x118c05aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x118c05f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x118c06380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x118c067f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x118c06c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x118c070d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x118c07540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x118c079b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x118c080a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x118c08bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x118c09370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x118c09b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x118c0a2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x118c0a9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x118c0b0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x118c0b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x118c0bf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x118c0c650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x118c0cd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x118c0d490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x118c0dbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x118c0e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x118c0e590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x118c0e850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x118c0ecc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x118c0f130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x118c0f5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x118c0fa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x118c0ff40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x118c103b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x118c10670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x118c10ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x118c10f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x118c113c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x118c11830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x118c11ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x118c12110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x118c12580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x118c129f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x118c12e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x118c132d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x118c13740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x118c13bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x118c14020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x118c14490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x118c14900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x118c14d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x118c151e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x118c15650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x118c15ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x118c15f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x118c163a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x118c16810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x118c16d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x118c17280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x118c176f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x118c17b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x118c17fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x118c18440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x118c188b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x118c18d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x118c19190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x118c19600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x118c19a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x118c19ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x118c1a350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x118c1a7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x118c1ac30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x118c1b0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x118c1b510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x118c1b980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x118c1bdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x118c1c260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x118c1c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x118c1cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x118c1cfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x118c1d420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x118c1d890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x118c1dd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x118c1e170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x118c1e5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x118c1ea50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x118c1eec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x118c1f330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x118c1f7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x118c1fc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x118c20080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x118c204f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x118c20960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x118c20dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x118c21240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x118c216b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x118c21b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x118c21f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x118c22400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x118c22870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x118c22ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x118c23150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x118c235c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x118c23a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x118c23ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x118c24310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x118c24780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x118c24bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x118c25060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x118c254d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x118c25940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x118c25db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x118c26220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x118c26690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x118c26b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x118c26f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x118c273e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x118c27850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x118c27cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x118c28130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x118c285a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x118c28a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x118c28e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x118c292f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x118c29760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x118c29bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x118c2a040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x118c2a4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x118c2a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x118c2ad90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x118c2b200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x118c2b670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x118c2bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x118c2bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x118c2c3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x118c2c830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x118c2cca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x118c2d110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x118c2d580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x118c2d9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x118c2de60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x118c2e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x118c2e740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x118c2ebb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x118c2f020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x118c2f490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x118c2f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x118c2fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x118c301e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x118c30650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x118c30ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x118c30f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x118c313a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x118c31810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x118c31c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x118c320f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x118c32560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x118c329d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x118c32e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x118c332b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x118c33720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x118c33b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x118c34000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x118c34470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x118c348e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x118c34d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x118c351c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x118c35df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x118c360b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x118c36370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x118c367e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x118c36c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x118c370c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x118c37530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x118c379a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x118c37e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x118c38280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x118c386f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x118c38b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x118c38fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x118c39440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x118c398b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x118c39d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x118c3a190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x118c3a600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x118c3aa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x118c3aee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x118c3b350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x118c3b7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x118c3bc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x118c3c0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x118c3c510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x118c3c980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x118c3cdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x118c3d260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x118c3d6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x118c3db40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x118c3dfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x118c3e420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x118c3e890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x118c3ed00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x118c3f170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x118c3f5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x118c3fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x118c40050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x118c404c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x118c40930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x118c40da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x118c41210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x118c41730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x118c41c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x118c427b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x118c42a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x118c43030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x118c435f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x118c43bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x118c44170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x118c44730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x118c44cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x118c452b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x118c45870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x118c45e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x118c463f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x118c469b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x118c46f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x118c47530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x118c47af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x118c480b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x118c48670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x118c48c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x118c491f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x118c497b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x118c49d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x118c4a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x118c4a8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x118c4aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x118c4b470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x118c4ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x118c4bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x118c4c5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x118c4cb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x118c4d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x118c4d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x118c4dcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x118c4e270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x118c4e830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x118c4edf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x118c4f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x118c4f970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x118c4ff30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x118c504f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x118c50ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x118c51070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x118c51630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x118c51bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x118c521b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x118c52770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x118c52d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x118c532f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x118c538b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x118c53e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x118c54430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x118c549f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x118c54fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x118c55570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x118c55b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x118c560f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x118c566b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x118c56c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x118c57170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x118c57670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x118c57b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x118c58070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x118c58570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x118c58a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x118c58f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x118c59470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x118c59970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x118c59e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x118c5a370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x118c5a870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x118c5ad70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x118c5b270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x118c5b770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x118c5c180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x118c5c8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x118c5cfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x118c5d6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x118c5d9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x118c5e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x118c5e450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x118c5ea60 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x11c6057b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x11c605c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x11c606090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x11c606500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x11c606970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x11c606de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x11c607250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x11c6076c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x11c607b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x11c607fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x11c608410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x11c608af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x11c609610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x11c609dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x11c60a5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x11c60acf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x11c60b410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x11c60bb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x11c60c250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x11c60ca20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x11c60d140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x11c60d860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x11c60df80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x11c60e6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x11c60edc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x11c60f080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x11c60f340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x11c60f7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x11c60fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x11c610090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x11c610500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x11c610a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x11c610ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x11c611160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x11c6115d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x11c611a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x11c611eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x11c612320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x11c612790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x11c612c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x11c613070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x11c6134e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x11c613950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x11c613dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x11c614230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x11c6146a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x11c614b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x11c614f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x11c6153f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x11c615860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x11c615cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x11c616140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x11c6165b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x11c616a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x11c616e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x11c617300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x11c617870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x11c617d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x11c6181e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x11c618650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x11c618ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x11c618f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x11c6193a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x11c619810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x11c619c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x11c61a0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x11c61a560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x11c61a9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x11c61ae40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x11c61b2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x11c61b720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x11c61bb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x11c61c000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x11c61c470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x11c61c8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x11c61cd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x11c61d1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x11c61d630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x11c61daa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x11c61df10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x11c61e380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x11c61e7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x11c61ec60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x11c61f0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x11c61f540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x11c61f9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x11c61fe20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x11c620290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x11c620700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x11c620b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x11c620fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x11c621450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x11c6218c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x11c621d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x11c6221a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x11c622610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x11c622a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x11c622ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x11c623360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x11c6237d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x11c623c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x11c6240b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x11c624520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x11c624db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x11c625070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x11c6254e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x11c625950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x11c625dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x11c626230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x11c6266a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x11c626b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x11c626f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x11c6273f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x11c627860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x11c627cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x11c628140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x11c6285b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x11c628a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x11c628e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x11c629300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x11c629770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x11c629be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x11c62a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x11c62a4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x11c62a930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x11c62ada0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x11c62b210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x11c62b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x11c62baf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x11c62bf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x11c62c3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x11c62c840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x11c62ccb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x11c62d120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x11c62d590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x11c62da00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x11c62de70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x11c62e2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x11c62e750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x11c62ebc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x11c62f030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x11c62f4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x11c62f910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x11c62fd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x11c6301f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x11c630660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x11c630ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x11c630f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x11c6313b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x11c631820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x11c631c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x11c632100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x11c632570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x11c6329e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x11c632e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x11c6332c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x11c633730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x11c633ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x11c634010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x11c634480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x11c6348f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x11c634d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x11c6351d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x11c635640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x11c635ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x11c635f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x11c636390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x11c636800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x11c636c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x11c6370e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x11c637550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x11c6379c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x11c637e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x11c6382a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x11c638710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x11c638b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x11c638ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x11c639460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x11c6398d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x11c639d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x11c63a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x11c63a620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x11c63aa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x11c63af00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x11c63b370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x11c63b7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x11c63bc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x11c63c0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x11c63c530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x11c63c9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x11c63ce10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x11c63d280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x11c63d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x11c63db60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x11c63dfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x11c63e440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x11c63e8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x11c63ed20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x11c63f190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x11c63f600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x11c63fa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x11c63fee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x11c640350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x11c6407c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x11c640c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x11c6410a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x11c641510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x11c641980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x11c641df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x11c642260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x11c642de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x11c6430a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x11c643360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x11c6437d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x11c643c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x11c6440b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x11c644520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x11c644990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x11c644e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x11c645270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x11c6456e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x11c645b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x11c645fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x11c646430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x11c6468a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x11c646d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x11c647180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x11c6475f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x11c647a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x11c647ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x11c648340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x11c6487b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x11c648c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x11c649090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x11c649500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x11c649970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x11c649de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x11c64a250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x11c64a6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x11c64ab30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x11c64afa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x11c64b410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x11c64b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x11c64bcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x11c64c160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x11c64c5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x11c64ca40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x11c64ceb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x11c64d320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x11c64d790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x11c64dc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x11c64e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x11c64e4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x11c64e950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x11c64edc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x11c64f230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x11c64f6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x11c64fb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x11c64ff80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x11c6503f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x11c650860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x11c650cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x11c651140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x11c6515b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x11c651a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x11c651e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x11c652300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x11c652770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x11c652be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x11c653050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x11c6534c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x11c653930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x11c653da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x11c654210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x11c654680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x11c654af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x11c654f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x11c6553d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x11c655840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x11c655cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x11c656120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x11c656590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x11c656a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x11c657470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x11c657b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x11c6582b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x11c6589d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x11c658c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x11c659100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x11c659700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x11c659d10 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.916s
user	0m0.243s
sys	0m0.136s
```
### ctest_with_model_debug

Runs ctest with model files in debug mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-debug
    Start 25: test-model-load-cancel
1/2 Test #25: test-model-load-cancel ...........   Passed    0.55 sec
    Start 26: test-autorelease
2/2 Test #26: test-autorelease .................   Passed    0.59 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   1.14 sec*proc (2 tests)

Total Test time (real) =   1.15 sec
        1.17 real         0.75 user         0.05 sys
```
### ctest_with_model_release

Runs ctest with model files in release mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-release
    Start 25: test-model-load-cancel
1/2 Test #25: test-model-load-cancel ...........   Passed    0.24 sec
    Start 26: test-autorelease
2/2 Test #26: test-autorelease .................   Passed    0.26 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   0.50 sec*proc (2 tests)

Total Test time (real) =   0.51 sec
        0.51 real         0.15 user         0.04 sys
```
