Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:44 (message):
  OpenMP not found


-- Using llamafile
-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORT_DOTPROD
-- Performing Test GGML_COMPILER_SUPPORT_DOTPROD - Success
-- ARM feature DOTPROD enabled
-- Performing Test GGML_COMPILER_SUPPORT_MATMUL_INT8
-- Performing Test GGML_COMPILER_SUPPORT_MATMUL_INT8 - Success
-- ARM feature MATMUL_INT8 enabled
-- Using runtime weight conversion of Q4_0 to Q4_0_x_x to enable optimized GEMM/GEMV kernels
-- Including CPU backend
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (1.4s)
-- Generating done (0.2s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m1.613s
user	0m0.730s
sys	0m0.959s
++ nproc
+ make -j10
[  0%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  1%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  3%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  3%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  4%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  6%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  6%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  7%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  7%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-aarch64.c.o
[  7%] Built target sha1
[  7%] Built target sha256
[  7%] Built target build_info
[  7%] Built target xxhash
[  8%] Linking CXX shared library libggml-base.dylib
[  8%] Built target ggml-base
[  8%] Generate assembly for embedded Metal library
Embedding Metal library
[  8%] Building C object ggml/src/ggml-cpu/CMakeFiles/ggml-cpu.dir/ggml-cpu.c.o
[  9%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[  9%] Building CXX object ggml/src/ggml-cpu/CMakeFiles/ggml-cpu.dir/ggml-cpu.cpp.o
[  9%] Building C object ggml/src/ggml-cpu/CMakeFiles/ggml-cpu.dir/ggml-cpu-aarch64.c.o
[ 10%] Building C object ggml/src/ggml-cpu/CMakeFiles/ggml-cpu.dir/ggml-cpu-quants.c.o
[ 10%] Building CXX object ggml/src/ggml-cpu/CMakeFiles/ggml-cpu.dir/amx/amx.cpp.o
[ 10%] Building CXX object ggml/src/ggml-cpu/CMakeFiles/ggml-cpu.dir/llamafile/sgemm.cpp.o
[ 11%] Building CXX object ggml/src/ggml-cpu/CMakeFiles/ggml-cpu.dir/amx/mmq.cpp.o
[ 12%] Building CXX object ggml/src/ggml-cpu/CMakeFiles/ggml-cpu.dir/cpu-feats-x86.cpp.o
[ 14%] Linking CXX shared library libggml-blas.dylib
[ 14%] Linking CXX shared library libggml-cpu.dylib
[ 14%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 15%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 15%] Built target ggml-blas
[ 15%] Built target ggml-cpu
[ 16%] Linking C shared library libggml-metal.dylib
[ 16%] Built target ggml-metal
[ 17%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 17%] Linking CXX shared library libggml.dylib
[ 17%] Built target ggml
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 20%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 22%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 22%] Linking CXX executable ../../bin/llama-gguf
[ 22%] Linking CXX shared library libllama.dylib
[ 22%] Linking CXX executable ../../bin/llama-gguf-hash
[ 22%] Built target llama-gguf-hash
[ 22%] Built target llama-gguf
[ 22%] Built target llama
[ 23%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 23%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 23%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 25%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 25%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 26%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 29%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 28%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 29%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 29%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 29%] Linking C executable ../bin/test-c
[ 30%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 30%] Linking CXX executable ../../bin/llama-run
[ 30%] Linking CXX executable ../../bin/llama-quantize-stats
[ 30%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 30%] Linking CXX executable ../../bin/llama-simple
[ 30%] Linking CXX executable ../../bin/llama-simple-chat
[ 31%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 31%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 31%] Built target llava
[ 32%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 33%] Linking CXX static library libllava_static.a
[ 33%] Linking CXX shared library libllava_shared.dylib
[ 33%] Built target test-c
[ 33%] Linking CXX static library libcommon.a
[ 33%] Built target llama-quantize-stats
[ 33%] Built target llama-run
[ 33%] Built target llama-simple
[ 33%] Built target llama-simple-chat
[ 33%] Built target llava_static
[ 33%] Built target common
[ 33%] Built target llava_shared
[ 34%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 34%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 34%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 34%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 34%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 34%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 36%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 36%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 39%] Linking CXX executable ../bin/test-tokenizer-0
[ 39%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 41%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 41%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 42%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 44%] Linking CXX executable ../bin/test-log
[ 45%] Linking CXX executable ../bin/test-grammar-parser
[ 45%] Linking CXX executable ../bin/test-llama-grammar
[ 45%] Linking CXX executable ../bin/test-grammar-integration
[ 46%] Linking CXX executable ../bin/test-chat-template
[ 46%] Linking CXX executable ../bin/test-arg-parser
[ 47%] Linking CXX executable ../bin/test-sampling
[ 47%] Built target test-tokenizer-0
[ 47%] Built target test-tokenizer-1-bpe
[ 47%] Built target test-tokenizer-1-spm
[ 47%] Built target test-log
[ 47%] Built target test-chat-template
[ 47%] Built target test-llama-grammar
[ 47%] Built target test-grammar-integration
[ 48%] Built target test-sampling
[ 48%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 49%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 48%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 49%] Built target test-arg-parser
[ 49%] Built target test-grammar-parser
[ 49%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 53%] Linking CXX executable ../bin/test-model-load-cancel
[ 54%] Linking CXX executable ../bin/test-backend-ops
[ 55%] Linking CXX executable ../bin/test-autorelease
[ 55%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 57%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 57%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 57%] Linking CXX executable ../bin/test-barrier
[ 57%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 58%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 59%] Linking CXX executable ../bin/test-quantize-perf
[ 60%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 60%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 61%] Linking CXX executable ../../bin/llama-batched-bench
[ 62%] Linking CXX executable ../bin/test-quantize-fns
[ 62%] Built target test-model-load-cancel
[ 62%] Built target test-backend-ops
[ 62%] Built target test-autorelease
[ 62%] Linking CXX executable ../bin/test-rope
[ 62%] Linking CXX executable ../../bin/llama-batched
[ 63%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 63%] Built target test-barrier
[ 63%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 63%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 63%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 63%] Built target test-quantize-fns
[ 64%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 64%] Built target test-quantize-perf
[ 64%] Built target llama-batched-bench
[ 65%] Linking CXX executable ../../bin/llama-embedding
[ 65%] Built target test-rope
[ 66%] Linking CXX executable ../../bin/llama-eval-callback
[ 67%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 67%] Built target test-json-schema-to-grammar
[ 67%] Built target llama-batched
[ 68%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 68%] Linking CXX executable ../../bin/llama-gguf-split
[ 69%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 70%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 70%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 71%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 71%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 71%] Linking CXX executable ../../bin/llama-gritlm
[ 71%] Linking CXX executable ../../bin/llama-infill
[ 71%] Linking CXX executable ../../bin/llama-imatrix
[ 71%] Built target llama-gbnf-validator
[ 72%] Built target llama-embedding
[ 72%] Built target llama-eval-callback
[ 72%] Linking CXX executable ../../bin/llama-bench
[ 73%] Linking CXX executable ../../bin/llama-lookahead
[ 73%] Built target llama-gguf-split
[ 74%] Linking CXX executable ../../bin/llama-lookup
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 74%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 74%] Built target llama-infill
[ 74%] Built target llama-gritlm
[ 74%] Built target llama-lookup
[ 75%] Built target llama-lookahead
[ 75%] Built target llama-imatrix
[ 75%] Linking CXX executable ../../bin/llama-lookup-merge
[ 76%] Linking CXX executable ../../bin/llama-lookup-create
[ 78%] Linking CXX executable ../../bin/llama-cli
[ 78%] Linking CXX executable ../../bin/llama-lookup-stats
[ 78%] Built target llama-bench
[ 79%] Generating loading.html.hpp
[ 79%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 80%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 81%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 82%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 82%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 82%] Generating completion.js.hpp
[ 83%] Linking CXX executable ../../bin/llama-parallel
[ 83%] Linking CXX executable ../../bin/llama-retrieval
[ 83%] Built target llama-lookup-merge
[ 83%] Built target llama-cli
[ 83%] Built target llama-lookup-create
[ 83%] Linking CXX executable ../../bin/llama-perplexity
[ 83%] Linking CXX executable ../../bin/llama-quantize
[ 84%] Generating deps_daisyui.min.css.hpp
[ 84%] Built target llama-lookup-stats
[ 85%] Linking CXX executable ../../bin/llama-passkey
[ 86%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 87%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 88%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 89%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 89%] Built target llama-parallel
[ 89%] Built target llama-retrieval
[ 89%] Linking CXX executable ../../bin/llama-speculative
[ 89%] Built target llama-perplexity
[ 89%] Built target llama-quantize
[ 89%] Linking CXX executable ../../bin/llama-speculative-simple
[ 89%] Linking CXX executable ../../bin/llama-save-load-state
[ 89%] Built target llama-passkey
[ 89%] Generating deps_markdown-it.js.hpp
[ 90%] Generating deps_tailwindcss.js.hpp
[ 91%] Linking CXX executable ../../bin/llama-tokenize
[ 91%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 91%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 91%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 91%] Built target llama-save-load-state
[ 91%] Built target llama-speculative-simple
[ 91%] Built target llama-tokenize
[ 92%] Linking CXX executable ../../bin/llama-cvector-generator
[ 93%] Linking CXX executable ../../bin/llama-export-lora
[ 94%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 94%] Built target llama-speculative
[ 94%] Generating deps_vue.esm-browser.js.hpp
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 95%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 95%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 96%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 96%] Linking CXX executable ../../bin/llama-llava-cli
[ 97%] Linking CXX executable ../../bin/llama-vdot
[ 98%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 98%] Built target llama-export-lora
[ 98%] Built target llama-convert-llama2c-to-ggml
[ 98%] Built target llama-cvector-generator
[ 99%] Generating index.html.hpp
[ 99%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Built target llama-minicpmv-cli
[ 99%] Built target llama-vdot
[ 99%] Built target llama-llava-cli
[ 99%] Built target llama-q8dot
[100%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m2.646s
user	0m5.901s
sys	0m9.693s

main: quantize time =  5220.81 ms
main:    total time =  5220.81 ms

main: quantize time =  1729.01 ms
main:    total time =  1729.01 ms

main: quantize time =  1423.24 ms
main:    total time =  1423.24 ms

main: quantize time =  2526.45 ms
main:    total time =  2526.45 ms

main: quantize time =  2546.99 ms
main:    total time =  2546.99 ms

main: quantize time =  5622.41 ms
main:    total time =  5622.41 ms

main: quantize time =  5908.20 ms
main:    total time =  5908.20 ms

main: quantize time =  6871.59 ms
main:    total time =  6871.59 ms

main: quantize time =  6001.79 ms
main:    total time =  6001.79 ms

main: quantize time =  4581.13 ms
main:    total time =  4581.13 ms
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.149 I build: 4227 (0ca0cc3c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.253 I main: llama backend init
0.00.000.266 I main: load the model and apply lora adapter, if any
0.00.044.746 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.056.286 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.056.310 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.056.314 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.056.315 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.056.316 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.056.316 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.056.317 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.056.319 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.056.319 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.056.320 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.056.321 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.056.322 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.056.322 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.056.323 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.056.327 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.056.328 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.056.328 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.063.712 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.066.695 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.075.367 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.075.370 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.075.371 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.075.372 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.075.372 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.075.373 I llama_model_loader: - type  f32:  194 tensors
0.00.075.374 I llama_model_loader: - type  f16:   98 tensors
0.00.108.232 I llm_load_vocab: special tokens cache size = 25
0.00.115.560 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.115.563 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.115.563 I llm_load_print_meta: arch             = gptneox
0.00.115.564 I llm_load_print_meta: vocab type       = BPE
0.00.115.564 I llm_load_print_meta: n_vocab          = 50304
0.00.115.564 I llm_load_print_meta: n_merges         = 50009
0.00.115.564 I llm_load_print_meta: vocab_only       = 0
0.00.115.564 I llm_load_print_meta: n_ctx_train      = 2048
0.00.115.564 I llm_load_print_meta: n_embd           = 2048
0.00.115.565 I llm_load_print_meta: n_layer          = 24
0.00.115.568 I llm_load_print_meta: n_head           = 16
0.00.115.570 I llm_load_print_meta: n_head_kv        = 16
0.00.115.570 I llm_load_print_meta: n_rot            = 32
0.00.115.570 I llm_load_print_meta: n_swa            = 0
0.00.115.571 I llm_load_print_meta: n_embd_head_k    = 128
0.00.115.571 I llm_load_print_meta: n_embd_head_v    = 128
0.00.115.571 I llm_load_print_meta: n_gqa            = 1
0.00.115.572 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.115.573 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.115.573 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.115.573 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.115.573 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.115.574 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.115.574 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.115.574 I llm_load_print_meta: n_ff             = 8192
0.00.115.574 I llm_load_print_meta: n_expert         = 0
0.00.115.575 I llm_load_print_meta: n_expert_used    = 0
0.00.115.575 I llm_load_print_meta: causal attn      = 1
0.00.115.575 I llm_load_print_meta: pooling type     = 0
0.00.115.575 I llm_load_print_meta: rope type        = 2
0.00.115.575 I llm_load_print_meta: rope scaling     = linear
0.00.115.581 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.115.581 I llm_load_print_meta: freq_scale_train = 1
0.00.115.582 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.115.582 I llm_load_print_meta: rope_finetuned   = unknown
0.00.115.582 I llm_load_print_meta: ssm_d_conv       = 0
0.00.115.582 I llm_load_print_meta: ssm_d_inner      = 0
0.00.115.583 I llm_load_print_meta: ssm_d_state      = 0
0.00.115.583 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.115.583 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.115.596 I llm_load_print_meta: model type       = 1.4B
0.00.115.596 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.115.597 I llm_load_print_meta: model params     = 1.41 B
0.00.115.597 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.115.599 I llm_load_print_meta: general.name     = 1.4B
0.00.115.599 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.115.599 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.115.599 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.115.599 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.115.600 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.115.600 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.115.600 I llm_load_print_meta: max token length = 1024
0.00.118.250 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.118.250 I llm_load_tensors: offloading output layer to GPU
0.00.118.250 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.118.268 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.118.269 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.119.253 I llama_new_context_with_model: n_seq_max     = 1
0.00.119.254 I llama_new_context_with_model: n_ctx         = 2048
0.00.119.254 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.119.254 I llama_new_context_with_model: n_batch       = 2048
0.00.119.255 I llama_new_context_with_model: n_ubatch      = 512
0.00.119.255 I llama_new_context_with_model: flash_attn    = 0
0.00.119.255 I llama_new_context_with_model: freq_base     = 10000.0
0.00.119.256 I llama_new_context_with_model: freq_scale    = 1
0.00.119.256 I ggml_metal_init: allocating
0.00.119.266 I ggml_metal_init: found device: Apple M4
0.00.119.268 I ggml_metal_init: picking default device: Apple M4
0.00.119.915 I ggml_metal_init: using embedded metal library
0.00.127.954 I ggml_metal_init: GPU name:   Apple M4
0.00.127.956 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.127.956 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.127.957 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.127.957 I ggml_metal_init: simdgroup reduction   = true
0.00.127.957 I ggml_metal_init: simdgroup matrix mul. = true
0.00.127.957 I ggml_metal_init: has bfloat            = true
0.00.127.957 I ggml_metal_init: use bfloat            = true
0.00.127.958 I ggml_metal_init: hasUnifiedMemory      = true
0.00.127.958 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.171.061 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.171.067 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.171.086 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.171.987 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.171.988 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.171.989 I llama_new_context_with_model: graph nodes  = 967
0.00.171.989 I llama_new_context_with_model: graph splits = 2
0.00.172.005 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.243.748 I main: llama threadpool init, n_threads = 4
0.00.243.781 I 
0.00.243.816 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.243.818 I 
0.00.243.887 I sampler seed: 1234
0.00.243.891 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.243.925 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.243.927 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.243.927 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling depressed, and I've had a hard time getting out of bed. I think the only thing I

0.02.093.565 I llama_perf_sampler_print:    sampling time =       1.28 ms /    71 runs   (    0.02 ms per token, 55295.95 tokens per second)
0.02.093.566 I llama_perf_context_print:        load time =     198.99 ms
0.02.093.567 I llama_perf_context_print: prompt eval time =      37.30 ms /     7 tokens (    5.33 ms per token,   187.65 tokens per second)
0.02.093.568 I llama_perf_context_print:        eval time =    1809.34 ms /    63 runs   (   28.72 ms per token,    34.82 tokens per second)
0.02.093.568 I llama_perf_context_print:       total time =    1849.82 ms /    70 tokens
0.02.093.743 I ggml_metal_free: deallocating

real	0m2.415s
user	0m0.148s
sys	0m0.097s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.042 I build: 4227 (0ca0cc3c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.074 I main: llama backend init
0.00.000.076 I main: load the model and apply lora adapter, if any
0.00.009.761 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.025.762 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.025.767 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.025.771 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.025.772 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.025.772 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.025.772 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.025.773 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.025.774 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.025.774 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.025.774 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.025.775 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.025.775 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.025.778 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.025.778 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.025.780 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.025.780 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.025.780 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.029.889 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.030.983 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.035.095 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.035.097 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.035.098 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.035.098 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.035.098 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.035.099 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.035.099 I llama_model_loader: - type  f32:  194 tensors
0.00.035.100 I llama_model_loader: - type q8_0:   98 tensors
0.00.055.916 I llm_load_vocab: special tokens cache size = 25
0.00.061.778 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.061.782 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.061.782 I llm_load_print_meta: arch             = gptneox
0.00.061.783 I llm_load_print_meta: vocab type       = BPE
0.00.061.783 I llm_load_print_meta: n_vocab          = 50304
0.00.061.783 I llm_load_print_meta: n_merges         = 50009
0.00.061.784 I llm_load_print_meta: vocab_only       = 0
0.00.061.784 I llm_load_print_meta: n_ctx_train      = 2048
0.00.061.784 I llm_load_print_meta: n_embd           = 2048
0.00.061.784 I llm_load_print_meta: n_layer          = 24
0.00.061.791 I llm_load_print_meta: n_head           = 16
0.00.061.792 I llm_load_print_meta: n_head_kv        = 16
0.00.061.792 I llm_load_print_meta: n_rot            = 32
0.00.061.794 I llm_load_print_meta: n_swa            = 0
0.00.061.794 I llm_load_print_meta: n_embd_head_k    = 128
0.00.061.795 I llm_load_print_meta: n_embd_head_v    = 128
0.00.061.795 I llm_load_print_meta: n_gqa            = 1
0.00.061.797 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.061.799 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.061.800 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.061.801 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.061.801 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.061.801 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.061.801 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.061.802 I llm_load_print_meta: n_ff             = 8192
0.00.061.802 I llm_load_print_meta: n_expert         = 0
0.00.061.803 I llm_load_print_meta: n_expert_used    = 0
0.00.061.803 I llm_load_print_meta: causal attn      = 1
0.00.061.803 I llm_load_print_meta: pooling type     = 0
0.00.061.803 I llm_load_print_meta: rope type        = 2
0.00.061.803 I llm_load_print_meta: rope scaling     = linear
0.00.061.804 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.061.804 I llm_load_print_meta: freq_scale_train = 1
0.00.061.804 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.061.804 I llm_load_print_meta: rope_finetuned   = unknown
0.00.061.804 I llm_load_print_meta: ssm_d_conv       = 0
0.00.061.805 I llm_load_print_meta: ssm_d_inner      = 0
0.00.061.805 I llm_load_print_meta: ssm_d_state      = 0
0.00.061.805 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.061.805 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.061.818 I llm_load_print_meta: model type       = 1.4B
0.00.061.818 I llm_load_print_meta: model ftype      = Q8_0
0.00.061.819 I llm_load_print_meta: model params     = 1.41 B
0.00.061.819 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.061.819 I llm_load_print_meta: general.name     = 1.4B
0.00.061.819 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.061.820 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.061.820 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.061.820 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.061.820 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.061.821 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.061.821 I llm_load_print_meta: max token length = 1024
0.00.064.218 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.064.219 I llm_load_tensors: offloading output layer to GPU
0.00.064.219 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.064.229 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.064.231 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.065.183 I llama_new_context_with_model: n_seq_max     = 1
0.00.065.184 I llama_new_context_with_model: n_ctx         = 2048
0.00.065.184 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.065.184 I llama_new_context_with_model: n_batch       = 2048
0.00.065.185 I llama_new_context_with_model: n_ubatch      = 512
0.00.065.185 I llama_new_context_with_model: flash_attn    = 0
0.00.065.185 I llama_new_context_with_model: freq_base     = 10000.0
0.00.065.185 I llama_new_context_with_model: freq_scale    = 1
0.00.065.186 I ggml_metal_init: allocating
0.00.065.189 I ggml_metal_init: found device: Apple M4
0.00.065.190 I ggml_metal_init: picking default device: Apple M4
0.00.065.830 I ggml_metal_init: using embedded metal library
0.00.068.028 I ggml_metal_init: GPU name:   Apple M4
0.00.068.029 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.068.030 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.068.030 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.068.030 I ggml_metal_init: simdgroup reduction   = true
0.00.068.030 I ggml_metal_init: simdgroup matrix mul. = true
0.00.068.030 I ggml_metal_init: has bfloat            = true
0.00.068.031 I ggml_metal_init: use bfloat            = true
0.00.068.031 I ggml_metal_init: hasUnifiedMemory      = true
0.00.068.032 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.102.253 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.102.261 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.102.284 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.103.482 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.103.484 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.103.485 I llama_new_context_with_model: graph nodes  = 967
0.00.103.485 I llama_new_context_with_model: graph splits = 2
0.00.103.503 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.322.342 I main: llama threadpool init, n_threads = 4
0.01.322.375 I 
0.01.322.403 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.01.322.405 I 
0.01.322.627 I sampler seed: 1234
0.01.322.631 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.322.670 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.322.674 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.322.674 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.408.694 I llama_perf_sampler_print:    sampling time =       1.22 ms /    71 runs   (    0.02 ms per token, 58340.18 tokens per second)
0.02.408.695 I llama_perf_context_print:        load time =    1312.58 ms
0.02.408.695 I llama_perf_context_print: prompt eval time =      37.21 ms /     7 tokens (    5.32 ms per token,   188.12 tokens per second)
0.02.408.696 I llama_perf_context_print:        eval time =    1045.79 ms /    63 runs   (   16.60 ms per token,    60.24 tokens per second)
0.02.408.696 I llama_perf_context_print:       total time =    1086.35 ms /    70 tokens
0.02.408.877 I ggml_metal_free: deallocating

real	0m2.425s
user	0m0.112s
sys	0m0.225s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.039 I build: 4227 (0ca0cc3c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.070 I main: llama backend init
0.00.000.072 I main: load the model and apply lora adapter, if any
0.00.011.591 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.018.216 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.018.222 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.228 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.229 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.229 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.229 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.230 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.231 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.231 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.231 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.232 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.232 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.232 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.232 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.235 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.235 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.236 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.366 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.466 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.543 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.544 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.544 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.545 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.545 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.545 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.027.546 I llama_model_loader: - type  f32:  194 tensors
0.00.027.547 I llama_model_loader: - type q4_0:   97 tensors
0.00.027.547 I llama_model_loader: - type q6_K:    1 tensors
0.00.048.873 I llm_load_vocab: special tokens cache size = 25
0.00.054.896 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.054.899 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.054.899 I llm_load_print_meta: arch             = gptneox
0.00.054.899 I llm_load_print_meta: vocab type       = BPE
0.00.054.900 I llm_load_print_meta: n_vocab          = 50304
0.00.054.900 I llm_load_print_meta: n_merges         = 50009
0.00.054.900 I llm_load_print_meta: vocab_only       = 0
0.00.054.900 I llm_load_print_meta: n_ctx_train      = 2048
0.00.054.900 I llm_load_print_meta: n_embd           = 2048
0.00.054.901 I llm_load_print_meta: n_layer          = 24
0.00.054.903 I llm_load_print_meta: n_head           = 16
0.00.054.904 I llm_load_print_meta: n_head_kv        = 16
0.00.054.905 I llm_load_print_meta: n_rot            = 32
0.00.054.905 I llm_load_print_meta: n_swa            = 0
0.00.054.905 I llm_load_print_meta: n_embd_head_k    = 128
0.00.054.905 I llm_load_print_meta: n_embd_head_v    = 128
0.00.054.907 I llm_load_print_meta: n_gqa            = 1
0.00.054.909 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.054.910 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.054.911 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.054.911 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.054.911 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.054.912 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.054.912 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.054.913 I llm_load_print_meta: n_ff             = 8192
0.00.054.913 I llm_load_print_meta: n_expert         = 0
0.00.054.913 I llm_load_print_meta: n_expert_used    = 0
0.00.054.913 I llm_load_print_meta: causal attn      = 1
0.00.054.913 I llm_load_print_meta: pooling type     = 0
0.00.054.914 I llm_load_print_meta: rope type        = 2
0.00.054.914 I llm_load_print_meta: rope scaling     = linear
0.00.054.914 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.054.915 I llm_load_print_meta: freq_scale_train = 1
0.00.054.915 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.054.915 I llm_load_print_meta: rope_finetuned   = unknown
0.00.054.915 I llm_load_print_meta: ssm_d_conv       = 0
0.00.054.915 I llm_load_print_meta: ssm_d_inner      = 0
0.00.054.916 I llm_load_print_meta: ssm_d_state      = 0
0.00.054.917 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.054.917 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.054.931 I llm_load_print_meta: model type       = 1.4B
0.00.054.931 I llm_load_print_meta: model ftype      = Q4_0
0.00.054.932 I llm_load_print_meta: model params     = 1.41 B
0.00.054.932 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.054.932 I llm_load_print_meta: general.name     = 1.4B
0.00.054.933 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.054.933 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.054.933 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.054.933 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.054.934 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.054.934 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.054.934 I llm_load_print_meta: max token length = 1024
0.00.057.104 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.057.104 I llm_load_tensors: offloading output layer to GPU
0.00.057.104 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.057.110 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.057.111 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.058.055 I llama_new_context_with_model: n_seq_max     = 1
0.00.058.056 I llama_new_context_with_model: n_ctx         = 2048
0.00.058.056 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.058.056 I llama_new_context_with_model: n_batch       = 2048
0.00.058.056 I llama_new_context_with_model: n_ubatch      = 512
0.00.058.057 I llama_new_context_with_model: flash_attn    = 0
0.00.058.057 I llama_new_context_with_model: freq_base     = 10000.0
0.00.058.057 I llama_new_context_with_model: freq_scale    = 1
0.00.058.058 I ggml_metal_init: allocating
0.00.058.061 I ggml_metal_init: found device: Apple M4
0.00.058.063 I ggml_metal_init: picking default device: Apple M4
0.00.058.753 I ggml_metal_init: using embedded metal library
0.00.060.874 I ggml_metal_init: GPU name:   Apple M4
0.00.060.875 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.060.876 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.060.876 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.060.876 I ggml_metal_init: simdgroup reduction   = true
0.00.060.876 I ggml_metal_init: simdgroup matrix mul. = true
0.00.060.877 I ggml_metal_init: has bfloat            = true
0.00.060.877 I ggml_metal_init: use bfloat            = true
0.00.060.877 I ggml_metal_init: hasUnifiedMemory      = true
0.00.060.878 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.096.657 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.096.668 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.096.692 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.097.885 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.097.886 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.097.887 I llama_new_context_with_model: graph nodes  = 967
0.00.097.887 I llama_new_context_with_model: graph splits = 2
0.00.097.901 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.638.540 I main: llama threadpool init, n_threads = 4
0.00.638.581 I 
0.00.638.610 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.638.610 I 
0.00.638.830 I sampler seed: 1234
0.00.638.834 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.638.889 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.638.902 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.638.902 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.320.526 I llama_perf_sampler_print:    sampling time =       1.29 ms /    71 runs   (    0.02 ms per token, 54953.56 tokens per second)
0.01.320.527 I llama_perf_context_print:        load time =     626.94 ms
0.01.320.527 I llama_perf_context_print: prompt eval time =      39.88 ms /     7 tokens (    5.70 ms per token,   175.54 tokens per second)
0.01.320.528 I llama_perf_context_print:        eval time =     638.63 ms /    63 runs   (   10.14 ms per token,    98.65 tokens per second)
0.01.320.531 I llama_perf_context_print:       total time =     681.99 ms /    70 tokens
0.01.320.718 I ggml_metal_free: deallocating

real	0m1.342s
user	0m0.111s
sys	0m0.151s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.044 I build: 4227 (0ca0cc3c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.072 I main: llama backend init
0.00.000.074 I main: load the model and apply lora adapter, if any
0.00.008.776 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.103 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.015.107 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.113 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.114 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.114 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.115 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.116 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.117 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.117 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.117 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.118 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.118 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.118 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.119 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.120 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.121 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.121 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.219 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.335 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.294 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.295 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.296 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.296 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.296 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.296 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.024.297 I llama_model_loader: - type  f32:  194 tensors
0.00.024.297 I llama_model_loader: - type q4_1:   97 tensors
0.00.024.298 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.171 I llm_load_vocab: special tokens cache size = 25
0.00.051.090 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.093 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.093 I llm_load_print_meta: arch             = gptneox
0.00.051.094 I llm_load_print_meta: vocab type       = BPE
0.00.051.094 I llm_load_print_meta: n_vocab          = 50304
0.00.051.094 I llm_load_print_meta: n_merges         = 50009
0.00.051.094 I llm_load_print_meta: vocab_only       = 0
0.00.051.094 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.095 I llm_load_print_meta: n_embd           = 2048
0.00.051.095 I llm_load_print_meta: n_layer          = 24
0.00.051.098 I llm_load_print_meta: n_head           = 16
0.00.051.099 I llm_load_print_meta: n_head_kv        = 16
0.00.051.099 I llm_load_print_meta: n_rot            = 32
0.00.051.099 I llm_load_print_meta: n_swa            = 0
0.00.051.099 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.099 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.100 I llm_load_print_meta: n_gqa            = 1
0.00.051.101 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.102 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.102 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.103 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.103 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.103 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.103 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.104 I llm_load_print_meta: n_ff             = 8192
0.00.051.104 I llm_load_print_meta: n_expert         = 0
0.00.051.104 I llm_load_print_meta: n_expert_used    = 0
0.00.051.104 I llm_load_print_meta: causal attn      = 1
0.00.051.107 I llm_load_print_meta: pooling type     = 0
0.00.051.107 I llm_load_print_meta: rope type        = 2
0.00.051.107 I llm_load_print_meta: rope scaling     = linear
0.00.051.108 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.108 I llm_load_print_meta: freq_scale_train = 1
0.00.051.108 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.108 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.109 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.109 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.109 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.109 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.109 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.122 I llm_load_print_meta: model type       = 1.4B
0.00.051.123 I llm_load_print_meta: model ftype      = Q4_1
0.00.051.123 I llm_load_print_meta: model params     = 1.41 B
0.00.051.124 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.051.125 I llm_load_print_meta: general.name     = 1.4B
0.00.051.125 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.125 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.125 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.125 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.126 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.126 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.126 I llm_load_print_meta: max token length = 1024
0.00.053.165 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.165 I llm_load_tensors: offloading output layer to GPU
0.00.053.165 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.175 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.053.176 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.054.144 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.145 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.145 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.146 I llama_new_context_with_model: n_batch       = 2048
0.00.054.146 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.146 I llama_new_context_with_model: flash_attn    = 0
0.00.054.147 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.147 I llama_new_context_with_model: freq_scale    = 1
0.00.054.147 I ggml_metal_init: allocating
0.00.054.153 I ggml_metal_init: found device: Apple M4
0.00.054.155 I ggml_metal_init: picking default device: Apple M4
0.00.054.694 I ggml_metal_init: using embedded metal library
0.00.056.610 I ggml_metal_init: GPU name:   Apple M4
0.00.056.611 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.612 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.612 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.612 I ggml_metal_init: simdgroup reduction   = true
0.00.056.613 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.613 I ggml_metal_init: has bfloat            = true
0.00.056.613 I ggml_metal_init: use bfloat            = true
0.00.056.613 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.614 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.083.159 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.083.169 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.083.197 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.084.135 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.084.136 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.084.137 I llama_new_context_with_model: graph nodes  = 967
0.00.084.137 I llama_new_context_with_model: graph splits = 2
0.00.084.151 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.711.366 I main: llama threadpool init, n_threads = 4
0.00.711.405 I 
0.00.711.433 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.711.436 I 
0.00.711.660 I sampler seed: 1234
0.00.711.664 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.711.697 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.711.699 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.711.699 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.440.980 I llama_perf_sampler_print:    sampling time =       1.16 ms /    71 runs   (    0.02 ms per token, 61471.86 tokens per second)
0.01.440.981 I llama_perf_context_print:        load time =     702.58 ms
0.01.440.982 I llama_perf_context_print: prompt eval time =      36.69 ms /     7 tokens (    5.24 ms per token,   190.77 tokens per second)
0.01.440.982 I llama_perf_context_print:        eval time =     689.62 ms /    63 runs   (   10.95 ms per token,    91.35 tokens per second)
0.01.440.983 I llama_perf_context_print:       total time =     729.62 ms /    70 tokens
0.01.441.177 I ggml_metal_free: deallocating

real	0m1.458s
user	0m0.109s
sys	0m0.141s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.037 I build: 4227 (0ca0cc3c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.067 I main: llama backend init
0.00.000.069 I main: load the model and apply lora adapter, if any
0.00.012.386 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.018.788 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.018.792 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.794 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.795 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.795 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.795 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.797 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.798 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.798 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.799 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.799 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.799 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.800 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.800 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.802 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.803 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.803 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.848 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.905 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.932 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.933 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.933 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.934 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.934 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.934 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.027.935 I llama_model_loader: - type  f32:  194 tensors
0.00.027.935 I llama_model_loader: - type q5_0:   97 tensors
0.00.027.936 I llama_model_loader: - type q6_K:    1 tensors
0.00.048.088 I llm_load_vocab: special tokens cache size = 25
0.00.053.881 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.053.883 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.053.884 I llm_load_print_meta: arch             = gptneox
0.00.053.884 I llm_load_print_meta: vocab type       = BPE
0.00.053.884 I llm_load_print_meta: n_vocab          = 50304
0.00.053.884 I llm_load_print_meta: n_merges         = 50009
0.00.053.885 I llm_load_print_meta: vocab_only       = 0
0.00.053.885 I llm_load_print_meta: n_ctx_train      = 2048
0.00.053.885 I llm_load_print_meta: n_embd           = 2048
0.00.053.885 I llm_load_print_meta: n_layer          = 24
0.00.053.888 I llm_load_print_meta: n_head           = 16
0.00.053.889 I llm_load_print_meta: n_head_kv        = 16
0.00.053.889 I llm_load_print_meta: n_rot            = 32
0.00.053.889 I llm_load_print_meta: n_swa            = 0
0.00.053.889 I llm_load_print_meta: n_embd_head_k    = 128
0.00.053.889 I llm_load_print_meta: n_embd_head_v    = 128
0.00.053.890 I llm_load_print_meta: n_gqa            = 1
0.00.053.891 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.053.892 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.053.892 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.053.893 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.053.893 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.053.893 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.053.893 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.053.894 I llm_load_print_meta: n_ff             = 8192
0.00.053.894 I llm_load_print_meta: n_expert         = 0
0.00.053.894 I llm_load_print_meta: n_expert_used    = 0
0.00.053.898 I llm_load_print_meta: causal attn      = 1
0.00.053.898 I llm_load_print_meta: pooling type     = 0
0.00.053.898 I llm_load_print_meta: rope type        = 2
0.00.053.899 I llm_load_print_meta: rope scaling     = linear
0.00.053.899 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.053.899 I llm_load_print_meta: freq_scale_train = 1
0.00.053.900 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.053.900 I llm_load_print_meta: rope_finetuned   = unknown
0.00.053.900 I llm_load_print_meta: ssm_d_conv       = 0
0.00.053.900 I llm_load_print_meta: ssm_d_inner      = 0
0.00.053.900 I llm_load_print_meta: ssm_d_state      = 0
0.00.053.900 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.053.900 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.053.907 I llm_load_print_meta: model type       = 1.4B
0.00.053.908 I llm_load_print_meta: model ftype      = Q5_0
0.00.053.908 I llm_load_print_meta: model params     = 1.41 B
0.00.053.909 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.053.909 I llm_load_print_meta: general.name     = 1.4B
0.00.053.909 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.053.909 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.053.909 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.053.909 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.053.911 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.053.911 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.053.911 I llm_load_print_meta: max token length = 1024
0.00.055.663 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.055.663 I llm_load_tensors: offloading output layer to GPU
0.00.055.663 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.055.668 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.055.668 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.056.569 I llama_new_context_with_model: n_seq_max     = 1
0.00.056.570 I llama_new_context_with_model: n_ctx         = 2048
0.00.056.571 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.056.571 I llama_new_context_with_model: n_batch       = 2048
0.00.056.571 I llama_new_context_with_model: n_ubatch      = 512
0.00.056.571 I llama_new_context_with_model: flash_attn    = 0
0.00.056.572 I llama_new_context_with_model: freq_base     = 10000.0
0.00.056.572 I llama_new_context_with_model: freq_scale    = 1
0.00.056.572 I ggml_metal_init: allocating
0.00.056.576 I ggml_metal_init: found device: Apple M4
0.00.056.578 I ggml_metal_init: picking default device: Apple M4
0.00.057.138 I ggml_metal_init: using embedded metal library
0.00.059.068 I ggml_metal_init: GPU name:   Apple M4
0.00.059.069 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.059.070 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.059.070 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.059.070 I ggml_metal_init: simdgroup reduction   = true
0.00.059.070 I ggml_metal_init: simdgroup matrix mul. = true
0.00.059.070 I ggml_metal_init: has bfloat            = true
0.00.059.070 I ggml_metal_init: use bfloat            = true
0.00.059.071 I ggml_metal_init: hasUnifiedMemory      = true
0.00.059.074 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.086.000 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.007 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.026 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.088 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.089 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.089 I llama_new_context_with_model: graph nodes  = 967
0.00.087.090 I llama_new_context_with_model: graph splits = 2
0.00.087.103 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.761.556 I main: llama threadpool init, n_threads = 4
0.00.761.593 I 
0.00.761.622 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.761.622 I 
0.00.761.849 I sampler seed: 1234
0.00.761.854 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.761.897 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.761.901 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.761.902 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.545.910 I llama_perf_sampler_print:    sampling time =       1.23 ms /    71 runs   (    0.02 ms per token, 57676.69 tokens per second)
0.01.545.911 I llama_perf_context_print:        load time =     749.17 ms
0.01.545.912 I llama_perf_context_print: prompt eval time =      36.63 ms /     7 tokens (    5.23 ms per token,   191.09 tokens per second)
0.01.545.912 I llama_perf_context_print:        eval time =     744.33 ms /    63 runs   (   11.81 ms per token,    84.64 tokens per second)
0.01.545.913 I llama_perf_context_print:       total time =     784.36 ms /    70 tokens
0.01.546.095 I ggml_metal_free: deallocating

real	0m1.567s
user	0m0.108s
sys	0m0.168s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.037 I build: 4227 (0ca0cc3c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.066 I main: llama backend init
0.00.000.068 I main: load the model and apply lora adapter, if any
0.00.008.666 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.224 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.228 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.230 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.230 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.231 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.231 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.231 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.232 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.232 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.233 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.233 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.233 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.234 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.234 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.237 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.238 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.240 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.258 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.299 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.265 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.266 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.267 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.267 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.267 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.268 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.268 I llama_model_loader: - type  f32:  194 tensors
0.00.025.269 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.269 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.585 I llm_load_vocab: special tokens cache size = 25
0.00.051.438 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.441 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.441 I llm_load_print_meta: arch             = gptneox
0.00.051.442 I llm_load_print_meta: vocab type       = BPE
0.00.051.442 I llm_load_print_meta: n_vocab          = 50304
0.00.051.442 I llm_load_print_meta: n_merges         = 50009
0.00.051.443 I llm_load_print_meta: vocab_only       = 0
0.00.051.443 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.443 I llm_load_print_meta: n_embd           = 2048
0.00.051.443 I llm_load_print_meta: n_layer          = 24
0.00.051.446 I llm_load_print_meta: n_head           = 16
0.00.051.448 I llm_load_print_meta: n_head_kv        = 16
0.00.051.448 I llm_load_print_meta: n_rot            = 32
0.00.051.448 I llm_load_print_meta: n_swa            = 0
0.00.051.448 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.448 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.451 I llm_load_print_meta: n_gqa            = 1
0.00.051.452 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.452 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.453 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.453 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.454 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.454 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.454 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.461 I llm_load_print_meta: n_ff             = 8192
0.00.051.463 I llm_load_print_meta: n_expert         = 0
0.00.051.463 I llm_load_print_meta: n_expert_used    = 0
0.00.051.465 I llm_load_print_meta: causal attn      = 1
0.00.051.466 I llm_load_print_meta: pooling type     = 0
0.00.051.466 I llm_load_print_meta: rope type        = 2
0.00.051.466 I llm_load_print_meta: rope scaling     = linear
0.00.051.467 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.467 I llm_load_print_meta: freq_scale_train = 1
0.00.051.467 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.467 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.467 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.469 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.469 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.469 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.469 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.481 I llm_load_print_meta: model type       = 1.4B
0.00.051.481 I llm_load_print_meta: model ftype      = Q5_1
0.00.051.482 I llm_load_print_meta: model params     = 1.41 B
0.00.051.482 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.051.482 I llm_load_print_meta: general.name     = 1.4B
0.00.051.483 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.484 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.484 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.484 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.484 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.484 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.485 I llm_load_print_meta: max token length = 1024
0.00.053.529 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.530 I llm_load_tensors: offloading output layer to GPU
0.00.053.530 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.540 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.053.541 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.054.496 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.497 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.497 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.497 I llama_new_context_with_model: n_batch       = 2048
0.00.054.497 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.497 I llama_new_context_with_model: flash_attn    = 0
0.00.054.498 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.498 I llama_new_context_with_model: freq_scale    = 1
0.00.054.498 I ggml_metal_init: allocating
0.00.054.502 I ggml_metal_init: found device: Apple M4
0.00.054.504 I ggml_metal_init: picking default device: Apple M4
0.00.055.062 I ggml_metal_init: using embedded metal library
0.00.057.004 I ggml_metal_init: GPU name:   Apple M4
0.00.057.005 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.006 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.006 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.006 I ggml_metal_init: simdgroup reduction   = true
0.00.057.008 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.008 I ggml_metal_init: has bfloat            = true
0.00.057.008 I ggml_metal_init: use bfloat            = true
0.00.057.008 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.010 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.083.626 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.083.631 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.083.650 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.084.600 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.084.601 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.084.602 I llama_new_context_with_model: graph nodes  = 967
0.00.084.602 I llama_new_context_with_model: graph splits = 2
0.00.084.615 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.773.334 I main: llama threadpool init, n_threads = 4
0.00.773.370 I 
0.00.773.413 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.773.415 I 
0.00.773.646 I sampler seed: 1234
0.00.773.650 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.773.673 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.773.674 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.773.674 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, literature and art.

If one's life is not happy, one can have no other way but

0.01.612.823 I llama_perf_sampler_print:    sampling time =       1.23 ms /    71 runs   (    0.02 ms per token, 57817.59 tokens per second)
0.01.612.824 I llama_perf_context_print:        load time =     764.66 ms
0.01.612.825 I llama_perf_context_print: prompt eval time =      39.83 ms /     7 tokens (    5.69 ms per token,   175.74 tokens per second)
0.01.612.825 I llama_perf_context_print:        eval time =     796.30 ms /    63 runs   (   12.64 ms per token,    79.12 tokens per second)
0.01.612.826 I llama_perf_context_print:       total time =     839.49 ms /    70 tokens
0.01.612.993 I ggml_metal_free: deallocating

real	0m1.629s
user	0m0.108s
sys	0m0.159s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.038 I build: 4227 (0ca0cc3c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.069 I main: llama backend init
0.00.000.071 I main: load the model and apply lora adapter, if any
0.00.012.046 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.441 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.017.445 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.446 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.447 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.447 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.447 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.447 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.448 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.449 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.449 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.449 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.450 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.451 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.451 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.453 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.453 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.454 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.582 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.662 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.860 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.861 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.861 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.862 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.862 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.862 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.026.863 I llama_model_loader: - type  f32:  194 tensors
0.00.026.863 I llama_model_loader: - type q2_K:   49 tensors
0.00.026.863 I llama_model_loader: - type q3_K:   48 tensors
0.00.026.864 I llama_model_loader: - type q6_K:    1 tensors
0.00.047.823 I llm_load_vocab: special tokens cache size = 25
0.00.053.792 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.053.796 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.053.797 I llm_load_print_meta: arch             = gptneox
0.00.053.797 I llm_load_print_meta: vocab type       = BPE
0.00.053.797 I llm_load_print_meta: n_vocab          = 50304
0.00.053.797 I llm_load_print_meta: n_merges         = 50009
0.00.053.798 I llm_load_print_meta: vocab_only       = 0
0.00.053.798 I llm_load_print_meta: n_ctx_train      = 2048
0.00.053.798 I llm_load_print_meta: n_embd           = 2048
0.00.053.798 I llm_load_print_meta: n_layer          = 24
0.00.053.801 I llm_load_print_meta: n_head           = 16
0.00.053.802 I llm_load_print_meta: n_head_kv        = 16
0.00.053.802 I llm_load_print_meta: n_rot            = 32
0.00.053.803 I llm_load_print_meta: n_swa            = 0
0.00.053.803 I llm_load_print_meta: n_embd_head_k    = 128
0.00.053.803 I llm_load_print_meta: n_embd_head_v    = 128
0.00.053.804 I llm_load_print_meta: n_gqa            = 1
0.00.053.804 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.053.805 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.053.806 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.053.806 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.053.806 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.053.806 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.053.806 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.053.807 I llm_load_print_meta: n_ff             = 8192
0.00.053.807 I llm_load_print_meta: n_expert         = 0
0.00.053.807 I llm_load_print_meta: n_expert_used    = 0
0.00.053.807 I llm_load_print_meta: causal attn      = 1
0.00.053.807 I llm_load_print_meta: pooling type     = 0
0.00.053.809 I llm_load_print_meta: rope type        = 2
0.00.053.810 I llm_load_print_meta: rope scaling     = linear
0.00.053.810 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.053.810 I llm_load_print_meta: freq_scale_train = 1
0.00.053.811 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.053.811 I llm_load_print_meta: rope_finetuned   = unknown
0.00.053.811 I llm_load_print_meta: ssm_d_conv       = 0
0.00.053.811 I llm_load_print_meta: ssm_d_inner      = 0
0.00.053.811 I llm_load_print_meta: ssm_d_state      = 0
0.00.053.811 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.053.811 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.053.818 I llm_load_print_meta: model type       = 1.4B
0.00.053.820 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.053.820 I llm_load_print_meta: model params     = 1.41 B
0.00.053.820 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.053.821 I llm_load_print_meta: general.name     = 1.4B
0.00.053.821 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.053.821 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.053.821 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.053.821 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.053.822 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.053.822 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.053.822 I llm_load_print_meta: max token length = 1024
0.00.055.651 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.055.651 I llm_load_tensors: offloading output layer to GPU
0.00.055.652 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.055.657 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.055.657 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.056.762 I llama_new_context_with_model: n_seq_max     = 1
0.00.056.763 I llama_new_context_with_model: n_ctx         = 2048
0.00.056.763 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.056.763 I llama_new_context_with_model: n_batch       = 2048
0.00.056.763 I llama_new_context_with_model: n_ubatch      = 512
0.00.056.763 I llama_new_context_with_model: flash_attn    = 0
0.00.056.764 I llama_new_context_with_model: freq_base     = 10000.0
0.00.056.764 I llama_new_context_with_model: freq_scale    = 1
0.00.056.765 I ggml_metal_init: allocating
0.00.056.770 I ggml_metal_init: found device: Apple M4
0.00.056.773 I ggml_metal_init: picking default device: Apple M4
0.00.057.331 I ggml_metal_init: using embedded metal library
0.00.059.250 I ggml_metal_init: GPU name:   Apple M4
0.00.059.252 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.059.252 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.059.252 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.059.253 I ggml_metal_init: simdgroup reduction   = true
0.00.059.253 I ggml_metal_init: simdgroup matrix mul. = true
0.00.059.253 I ggml_metal_init: has bfloat            = true
0.00.059.253 I ggml_metal_init: use bfloat            = true
0.00.059.253 I ggml_metal_init: hasUnifiedMemory      = true
0.00.059.254 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.086.362 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.373 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.392 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.493 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.494 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.494 I llama_new_context_with_model: graph nodes  = 967
0.00.087.495 I llama_new_context_with_model: graph splits = 2
0.00.087.508 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.510.235 I main: llama threadpool init, n_threads = 4
0.00.510.269 I 
0.00.510.298 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.510.298 I 
0.00.510.508 I sampler seed: 1234
0.00.510.513 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.510.528 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.510.528 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.510.528 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.191.381 I llama_perf_sampler_print:    sampling time =       1.11 ms /    71 runs   (    0.02 ms per token, 64137.31 tokens per second)
0.01.191.382 I llama_perf_context_print:        load time =     498.19 ms
0.01.191.383 I llama_perf_context_print: prompt eval time =      35.81 ms /     7 tokens (    5.12 ms per token,   195.48 tokens per second)
0.01.191.385 I llama_perf_context_print:        eval time =     642.25 ms /    63 runs   (   10.19 ms per token,    98.09 tokens per second)
0.01.191.385 I llama_perf_context_print:       total time =     681.15 ms /    70 tokens
0.01.191.568 I ggml_metal_free: deallocating

real	0m1.211s
user	0m0.110s
sys	0m0.117s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4227 (0ca0cc3c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.072 I main: llama backend init
0.00.000.073 I main: load the model and apply lora adapter, if any
0.00.008.875 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.351 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.015.356 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.358 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.358 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.359 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.359 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.359 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.360 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.360 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.361 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.361 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.361 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.362 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.362 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.365 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.366 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.367 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.558 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.631 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.808 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.809 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.809 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.810 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.810 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.810 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.811 I llama_model_loader: - type  f32:  194 tensors
0.00.024.811 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.812 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.812 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.812 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.837 I llm_load_vocab: special tokens cache size = 25
0.00.051.676 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.679 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.679 I llm_load_print_meta: arch             = gptneox
0.00.051.680 I llm_load_print_meta: vocab type       = BPE
0.00.051.680 I llm_load_print_meta: n_vocab          = 50304
0.00.051.680 I llm_load_print_meta: n_merges         = 50009
0.00.051.680 I llm_load_print_meta: vocab_only       = 0
0.00.051.680 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.681 I llm_load_print_meta: n_embd           = 2048
0.00.051.681 I llm_load_print_meta: n_layer          = 24
0.00.051.684 I llm_load_print_meta: n_head           = 16
0.00.051.684 I llm_load_print_meta: n_head_kv        = 16
0.00.051.685 I llm_load_print_meta: n_rot            = 32
0.00.051.685 I llm_load_print_meta: n_swa            = 0
0.00.051.685 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.685 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.686 I llm_load_print_meta: n_gqa            = 1
0.00.051.687 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.688 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.688 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.688 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.689 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.689 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.689 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.690 I llm_load_print_meta: n_ff             = 8192
0.00.051.691 I llm_load_print_meta: n_expert         = 0
0.00.051.693 I llm_load_print_meta: n_expert_used    = 0
0.00.051.693 I llm_load_print_meta: causal attn      = 1
0.00.051.693 I llm_load_print_meta: pooling type     = 0
0.00.051.694 I llm_load_print_meta: rope type        = 2
0.00.051.694 I llm_load_print_meta: rope scaling     = linear
0.00.051.694 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.695 I llm_load_print_meta: freq_scale_train = 1
0.00.051.695 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.695 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.695 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.695 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.695 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.695 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.695 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.707 I llm_load_print_meta: model type       = 1.4B
0.00.051.707 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.051.708 I llm_load_print_meta: model params     = 1.41 B
0.00.051.708 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.051.708 I llm_load_print_meta: general.name     = 1.4B
0.00.051.709 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.710 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.710 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.710 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.711 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.711 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.711 I llm_load_print_meta: max token length = 1024
0.00.053.712 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.712 I llm_load_tensors: offloading output layer to GPU
0.00.053.712 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.722 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.053.723 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.054.677 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.677 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.678 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.678 I llama_new_context_with_model: n_batch       = 2048
0.00.054.678 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.678 I llama_new_context_with_model: flash_attn    = 0
0.00.054.678 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.679 I llama_new_context_with_model: freq_scale    = 1
0.00.054.679 I ggml_metal_init: allocating
0.00.054.685 I ggml_metal_init: found device: Apple M4
0.00.054.688 I ggml_metal_init: picking default device: Apple M4
0.00.055.235 I ggml_metal_init: using embedded metal library
0.00.057.490 I ggml_metal_init: GPU name:   Apple M4
0.00.057.493 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.493 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.493 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.494 I ggml_metal_init: simdgroup reduction   = true
0.00.057.494 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.494 I ggml_metal_init: has bfloat            = true
0.00.057.494 I ggml_metal_init: use bfloat            = true
0.00.057.495 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.495 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.084.297 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.304 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.323 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.085.305 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.085.306 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.085.306 I llama_new_context_with_model: graph nodes  = 967
0.00.085.306 I llama_new_context_with_model: graph splits = 2
0.00.085.320 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.593.425 I main: llama threadpool init, n_threads = 4
0.00.593.464 I 
0.00.593.501 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.593.502 I 
0.00.593.727 I sampler seed: 1234
0.00.593.732 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.593.764 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.593.765 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.593.765 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do that is most useful to my fellow men?'"

-Albert Einstein

"The way

0.01.342.332 I llama_perf_sampler_print:    sampling time =       1.18 ms /    71 runs   (    0.02 ms per token, 60322.85 tokens per second)
0.01.342.333 I llama_perf_context_print:        load time =     584.55 ms
0.01.342.333 I llama_perf_context_print: prompt eval time =      38.79 ms /     7 tokens (    5.54 ms per token,   180.48 tokens per second)
0.01.342.334 I llama_perf_context_print:        eval time =     706.81 ms /    63 runs   (   11.22 ms per token,    89.13 tokens per second)
0.01.342.334 I llama_perf_context_print:       total time =     748.91 ms /    70 tokens
0.01.342.517 I ggml_metal_free: deallocating

real	0m1.359s
user	0m0.109s
sys	0m0.142s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.035 I build: 4227 (0ca0cc3c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.065 I main: llama backend init
0.00.000.067 I main: load the model and apply lora adapter, if any
0.00.009.995 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.150 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.155 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.156 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.156 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.157 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.159 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.159 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.160 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.160 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.161 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.161 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.161 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.162 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.162 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.165 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.165 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.165 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.970 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.081 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.858 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.859 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.860 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.860 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.860 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.860 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.861 I llama_model_loader: - type  f32:  194 tensors
0.00.024.861 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.862 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.862 I llama_model_loader: - type q6_K:   13 tensors
0.00.045.067 I llm_load_vocab: special tokens cache size = 25
0.00.050.871 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.874 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.874 I llm_load_print_meta: arch             = gptneox
0.00.050.874 I llm_load_print_meta: vocab type       = BPE
0.00.050.875 I llm_load_print_meta: n_vocab          = 50304
0.00.050.875 I llm_load_print_meta: n_merges         = 50009
0.00.050.875 I llm_load_print_meta: vocab_only       = 0
0.00.050.875 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.875 I llm_load_print_meta: n_embd           = 2048
0.00.050.875 I llm_load_print_meta: n_layer          = 24
0.00.050.878 I llm_load_print_meta: n_head           = 16
0.00.050.879 I llm_load_print_meta: n_head_kv        = 16
0.00.050.879 I llm_load_print_meta: n_rot            = 32
0.00.050.880 I llm_load_print_meta: n_swa            = 0
0.00.050.880 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.880 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.881 I llm_load_print_meta: n_gqa            = 1
0.00.050.881 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.882 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.883 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.883 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.883 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.884 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.884 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.884 I llm_load_print_meta: n_ff             = 8192
0.00.050.885 I llm_load_print_meta: n_expert         = 0
0.00.050.886 I llm_load_print_meta: n_expert_used    = 0
0.00.050.888 I llm_load_print_meta: causal attn      = 1
0.00.050.888 I llm_load_print_meta: pooling type     = 0
0.00.050.888 I llm_load_print_meta: rope type        = 2
0.00.050.888 I llm_load_print_meta: rope scaling     = linear
0.00.050.889 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.890 I llm_load_print_meta: freq_scale_train = 1
0.00.050.891 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.891 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.891 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.891 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.891 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.891 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.891 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.903 I llm_load_print_meta: model type       = 1.4B
0.00.050.903 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.050.904 I llm_load_print_meta: model params     = 1.41 B
0.00.050.904 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.050.904 I llm_load_print_meta: general.name     = 1.4B
0.00.050.905 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.905 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.905 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.905 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.905 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.906 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.906 I llm_load_print_meta: max token length = 1024
0.00.052.845 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.846 I llm_load_tensors: offloading output layer to GPU
0.00.052.846 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.856 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.052.857 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.053.752 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.753 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.753 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.753 I llama_new_context_with_model: n_batch       = 2048
0.00.053.754 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.754 I llama_new_context_with_model: flash_attn    = 0
0.00.053.754 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.754 I llama_new_context_with_model: freq_scale    = 1
0.00.053.755 I ggml_metal_init: allocating
0.00.053.761 I ggml_metal_init: found device: Apple M4
0.00.053.763 I ggml_metal_init: picking default device: Apple M4
0.00.054.305 I ggml_metal_init: using embedded metal library
0.00.056.236 I ggml_metal_init: GPU name:   Apple M4
0.00.056.237 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.238 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.238 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.240 I ggml_metal_init: simdgroup reduction   = true
0.00.056.240 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.240 I ggml_metal_init: has bfloat            = true
0.00.056.240 I ggml_metal_init: use bfloat            = true
0.00.056.240 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.241 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.082.200 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.082.210 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.082.229 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.083.161 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.083.162 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.083.162 I llama_new_context_with_model: graph nodes  = 967
0.00.083.163 I llama_new_context_with_model: graph splits = 2
0.00.083.175 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.624.534 I main: llama threadpool init, n_threads = 4
0.00.624.573 I 
0.00.624.599 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.624.599 I 
0.00.624.839 I sampler seed: 1234
0.00.624.844 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.624.880 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.624.882 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.624.882 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the greatest power to create the future we want to see. I believe that we are in a wonderful time when people from all over the world are coming together to create a better world. I believe that we are going

0.01.374.339 I llama_perf_sampler_print:    sampling time =       1.21 ms /    71 runs   (    0.02 ms per token, 58872.31 tokens per second)
0.01.374.340 I llama_perf_context_print:        load time =     614.53 ms
0.01.374.341 I llama_perf_context_print: prompt eval time =      36.44 ms /     7 tokens (    5.21 ms per token,   192.12 tokens per second)
0.01.374.342 I llama_perf_context_print:        eval time =     710.13 ms /    63 runs   (   11.27 ms per token,    88.72 tokens per second)
0.01.374.342 I llama_perf_context_print:       total time =     749.81 ms /    70 tokens
0.01.374.520 I ggml_metal_free: deallocating

real	0m1.396s
user	0m0.109s
sys	0m0.144s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.037 I build: 4227 (0ca0cc3c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.066 I main: llama backend init
0.00.000.068 I main: load the model and apply lora adapter, if any
0.00.009.111 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.303 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.309 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.311 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.311 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.311 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.312 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.312 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.313 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.313 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.314 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.314 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.316 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.317 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.317 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.320 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.320 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.321 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.301 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.346 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.337 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.338 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.339 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.339 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.339 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.340 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.340 I llama_model_loader: - type  f32:  194 tensors
0.00.025.341 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.341 I llama_model_loader: - type q6_K:   37 tensors
0.00.046.507 I llm_load_vocab: special tokens cache size = 25
0.00.052.507 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.510 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.510 I llm_load_print_meta: arch             = gptneox
0.00.052.510 I llm_load_print_meta: vocab type       = BPE
0.00.052.510 I llm_load_print_meta: n_vocab          = 50304
0.00.052.511 I llm_load_print_meta: n_merges         = 50009
0.00.052.511 I llm_load_print_meta: vocab_only       = 0
0.00.052.511 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.511 I llm_load_print_meta: n_embd           = 2048
0.00.052.511 I llm_load_print_meta: n_layer          = 24
0.00.052.514 I llm_load_print_meta: n_head           = 16
0.00.052.515 I llm_load_print_meta: n_head_kv        = 16
0.00.052.515 I llm_load_print_meta: n_rot            = 32
0.00.052.515 I llm_load_print_meta: n_swa            = 0
0.00.052.516 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.516 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.518 I llm_load_print_meta: n_gqa            = 1
0.00.052.519 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.520 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.520 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.521 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.521 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.521 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.521 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.522 I llm_load_print_meta: n_ff             = 8192
0.00.052.522 I llm_load_print_meta: n_expert         = 0
0.00.052.522 I llm_load_print_meta: n_expert_used    = 0
0.00.052.524 I llm_load_print_meta: causal attn      = 1
0.00.052.525 I llm_load_print_meta: pooling type     = 0
0.00.052.525 I llm_load_print_meta: rope type        = 2
0.00.052.526 I llm_load_print_meta: rope scaling     = linear
0.00.052.526 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.526 I llm_load_print_meta: freq_scale_train = 1
0.00.052.526 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.527 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.531 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.531 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.531 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.531 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.531 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.543 I llm_load_print_meta: model type       = 1.4B
0.00.052.543 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.052.544 I llm_load_print_meta: model params     = 1.41 B
0.00.052.544 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.052.545 I llm_load_print_meta: general.name     = 1.4B
0.00.052.545 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.545 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.545 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.545 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.546 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.052.546 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.546 I llm_load_print_meta: max token length = 1024
0.00.054.643 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.643 I llm_load_tensors: offloading output layer to GPU
0.00.054.643 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.653 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.054.654 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.055.629 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.630 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.630 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.630 I llama_new_context_with_model: n_batch       = 2048
0.00.055.630 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.631 I llama_new_context_with_model: flash_attn    = 0
0.00.055.631 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.631 I llama_new_context_with_model: freq_scale    = 1
0.00.055.632 I ggml_metal_init: allocating
0.00.055.635 I ggml_metal_init: found device: Apple M4
0.00.055.637 I ggml_metal_init: picking default device: Apple M4
0.00.056.215 I ggml_metal_init: using embedded metal library
0.00.058.161 I ggml_metal_init: GPU name:   Apple M4
0.00.058.163 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.165 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.165 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.165 I ggml_metal_init: simdgroup reduction   = true
0.00.058.166 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.166 I ggml_metal_init: has bfloat            = true
0.00.058.166 I ggml_metal_init: use bfloat            = true
0.00.058.166 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.167 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.085.561 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.570 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.590 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.607 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.608 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.608 I llama_new_context_with_model: graph nodes  = 967
0.00.086.609 I llama_new_context_with_model: graph splits = 2
0.00.086.622 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.700.058 I main: llama threadpool init, n_threads = 4
0.00.700.091 I 
0.00.700.145 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.700.146 I 
0.00.700.372 I sampler seed: 1234
0.00.700.376 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.700.408 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.700.409 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.700.409 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.540.633 I llama_perf_sampler_print:    sampling time =       1.20 ms /    71 runs   (    0.02 ms per token, 59117.40 tokens per second)
0.01.540.634 I llama_perf_context_print:        load time =     690.94 ms
0.01.540.635 I llama_perf_context_print: prompt eval time =      38.81 ms /     7 tokens (    5.54 ms per token,   180.38 tokens per second)
0.01.540.636 I llama_perf_context_print:        eval time =     798.40 ms /    63 runs   (   12.67 ms per token,    78.91 tokens per second)
0.01.540.636 I llama_perf_context_print:       total time =     840.58 ms /    70 tokens
0.01.540.803 I ggml_metal_free: deallocating

real	0m1.558s
user	0m0.110s
sys	0m0.157s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4227 (0ca0cc3c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.070 I main: llama backend init
0.00.000.073 I main: load the model and apply lora adapter, if any
0.00.009.974 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.580 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.584 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.586 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.586 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.588 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.588 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.589 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.589 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.590 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.590 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.591 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.591 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.593 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.593 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.595 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.599 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.599 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.510 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.540 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.402 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.403 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.403 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.404 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.404 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.404 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.405 I llama_model_loader: - type  f32:  194 tensors
0.00.025.405 I llama_model_loader: - type q6_K:   98 tensors
0.00.045.685 I llm_load_vocab: special tokens cache size = 25
0.00.051.585 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.587 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.588 I llm_load_print_meta: arch             = gptneox
0.00.051.588 I llm_load_print_meta: vocab type       = BPE
0.00.051.588 I llm_load_print_meta: n_vocab          = 50304
0.00.051.588 I llm_load_print_meta: n_merges         = 50009
0.00.051.588 I llm_load_print_meta: vocab_only       = 0
0.00.051.589 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.589 I llm_load_print_meta: n_embd           = 2048
0.00.051.589 I llm_load_print_meta: n_layer          = 24
0.00.051.591 I llm_load_print_meta: n_head           = 16
0.00.051.592 I llm_load_print_meta: n_head_kv        = 16
0.00.051.593 I llm_load_print_meta: n_rot            = 32
0.00.051.593 I llm_load_print_meta: n_swa            = 0
0.00.051.593 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.593 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.594 I llm_load_print_meta: n_gqa            = 1
0.00.051.595 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.595 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.596 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.596 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.597 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.597 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.597 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.598 I llm_load_print_meta: n_ff             = 8192
0.00.051.598 I llm_load_print_meta: n_expert         = 0
0.00.051.598 I llm_load_print_meta: n_expert_used    = 0
0.00.051.598 I llm_load_print_meta: causal attn      = 1
0.00.051.599 I llm_load_print_meta: pooling type     = 0
0.00.051.601 I llm_load_print_meta: rope type        = 2
0.00.051.601 I llm_load_print_meta: rope scaling     = linear
0.00.051.601 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.602 I llm_load_print_meta: freq_scale_train = 1
0.00.051.602 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.602 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.602 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.602 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.603 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.603 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.603 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.614 I llm_load_print_meta: model type       = 1.4B
0.00.051.615 I llm_load_print_meta: model ftype      = Q6_K
0.00.051.615 I llm_load_print_meta: model params     = 1.41 B
0.00.051.615 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.051.616 I llm_load_print_meta: general.name     = 1.4B
0.00.051.616 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.616 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.616 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.616 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.617 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.617 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.617 I llm_load_print_meta: max token length = 1024
0.00.053.649 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.649 I llm_load_tensors: offloading output layer to GPU
0.00.053.649 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.659 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.053.660 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.054.572 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.572 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.573 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.573 I llama_new_context_with_model: n_batch       = 2048
0.00.054.573 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.573 I llama_new_context_with_model: flash_attn    = 0
0.00.054.574 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.574 I llama_new_context_with_model: freq_scale    = 1
0.00.054.574 I ggml_metal_init: allocating
0.00.054.577 I ggml_metal_init: found device: Apple M4
0.00.054.579 I ggml_metal_init: picking default device: Apple M4
0.00.055.127 I ggml_metal_init: using embedded metal library
0.00.057.019 I ggml_metal_init: GPU name:   Apple M4
0.00.057.020 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.021 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.021 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.021 I ggml_metal_init: simdgroup reduction   = true
0.00.057.023 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.023 I ggml_metal_init: has bfloat            = true
0.00.057.023 I ggml_metal_init: use bfloat            = true
0.00.057.023 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.024 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.084.325 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.339 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.360 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.085.397 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.085.398 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.085.399 I llama_new_context_with_model: graph nodes  = 967
0.00.085.399 I llama_new_context_with_model: graph splits = 2
0.00.085.407 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.775.487 I main: llama threadpool init, n_threads = 4
0.00.775.528 I 
0.00.775.553 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.775.553 I 
0.00.775.784 I sampler seed: 1234
0.00.775.788 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.775.827 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.775.828 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.775.828 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.649.803 I llama_perf_sampler_print:    sampling time =       1.14 ms /    71 runs   (    0.02 ms per token, 62335.38 tokens per second)
0.01.649.803 I llama_perf_context_print:        load time =     765.51 ms
0.01.649.804 I llama_perf_context_print: prompt eval time =      38.52 ms /     7 tokens (    5.50 ms per token,   181.73 tokens per second)
0.01.649.805 I llama_perf_context_print:        eval time =     832.59 ms /    63 runs   (   13.22 ms per token,    75.67 tokens per second)
0.01.649.805 I llama_perf_context_print:       total time =     874.32 ms /    70 tokens
0.01.649.969 I ggml_metal_free: deallocating

real	0m1.671s
user	0m0.109s
sys	0m0.186s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.685 I build: 4227 (0ca0cc3c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.021.164 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.034.663 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.034.671 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.034.673 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.034.674 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.034.675 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.034.675 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.034.676 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.034.678 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.034.678 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.034.679 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.034.679 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.034.680 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.034.681 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.034.681 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.034.684 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.034.689 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.034.689 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.042.343 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.044.244 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.050.979 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.050.981 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.050.981 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.050.982 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.050.982 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.050.983 I llama_model_loader: - type  f32:  194 tensors
0.00.050.983 I llama_model_loader: - type  f16:   98 tensors
0.00.078.626 I llm_load_vocab: special tokens cache size = 25
0.00.085.014 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.085.017 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.085.017 I llm_load_print_meta: arch             = gptneox
0.00.085.017 I llm_load_print_meta: vocab type       = BPE
0.00.085.017 I llm_load_print_meta: n_vocab          = 50304
0.00.085.017 I llm_load_print_meta: n_merges         = 50009
0.00.085.018 I llm_load_print_meta: vocab_only       = 0
0.00.085.018 I llm_load_print_meta: n_ctx_train      = 2048
0.00.085.018 I llm_load_print_meta: n_embd           = 2048
0.00.085.018 I llm_load_print_meta: n_layer          = 24
0.00.085.021 I llm_load_print_meta: n_head           = 16
0.00.085.023 I llm_load_print_meta: n_head_kv        = 16
0.00.085.023 I llm_load_print_meta: n_rot            = 32
0.00.085.023 I llm_load_print_meta: n_swa            = 0
0.00.085.023 I llm_load_print_meta: n_embd_head_k    = 128
0.00.085.023 I llm_load_print_meta: n_embd_head_v    = 128
0.00.085.024 I llm_load_print_meta: n_gqa            = 1
0.00.085.025 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.085.025 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.085.026 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.085.026 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.085.026 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.085.026 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.085.026 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.085.027 I llm_load_print_meta: n_ff             = 8192
0.00.085.027 I llm_load_print_meta: n_expert         = 0
0.00.085.027 I llm_load_print_meta: n_expert_used    = 0
0.00.085.027 I llm_load_print_meta: causal attn      = 1
0.00.085.027 I llm_load_print_meta: pooling type     = 0
0.00.085.028 I llm_load_print_meta: rope type        = 2
0.00.085.029 I llm_load_print_meta: rope scaling     = linear
0.00.085.029 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.085.030 I llm_load_print_meta: freq_scale_train = 1
0.00.085.030 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.085.030 I llm_load_print_meta: rope_finetuned   = unknown
0.00.085.030 I llm_load_print_meta: ssm_d_conv       = 0
0.00.085.030 I llm_load_print_meta: ssm_d_inner      = 0
0.00.085.030 I llm_load_print_meta: ssm_d_state      = 0
0.00.085.030 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.085.031 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.085.037 I llm_load_print_meta: model type       = 1.4B
0.00.085.038 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.085.038 I llm_load_print_meta: model params     = 1.41 B
0.00.085.038 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.085.039 I llm_load_print_meta: general.name     = 1.4B
0.00.085.039 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.085.039 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.085.039 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.085.039 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.085.039 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.085.040 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.085.040 I llm_load_print_meta: max token length = 1024
0.00.087.013 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.087.013 I llm_load_tensors: offloading output layer to GPU
0.00.087.013 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.087.018 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.087.019 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.088.106 I llama_new_context_with_model: n_seq_max     = 1
0.00.088.107 I llama_new_context_with_model: n_ctx         = 128
0.00.088.107 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.088.107 I llama_new_context_with_model: n_batch       = 128
0.00.088.107 I llama_new_context_with_model: n_ubatch      = 128
0.00.088.108 I llama_new_context_with_model: flash_attn    = 0
0.00.088.108 I llama_new_context_with_model: freq_base     = 10000.0
0.00.088.108 I llama_new_context_with_model: freq_scale    = 1
0.00.088.109 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.088.109 I ggml_metal_init: allocating
0.00.088.112 I ggml_metal_init: found device: Apple M4
0.00.088.114 I ggml_metal_init: picking default device: Apple M4
0.00.088.673 I ggml_metal_init: using embedded metal library
0.00.090.721 I ggml_metal_init: GPU name:   Apple M4
0.00.090.723 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.090.723 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.090.723 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.090.724 I ggml_metal_init: simdgroup reduction   = true
0.00.090.724 I ggml_metal_init: simdgroup matrix mul. = true
0.00.090.724 I ggml_metal_init: has bfloat            = true
0.00.090.724 I ggml_metal_init: use bfloat            = true
0.00.090.724 I ggml_metal_init: hasUnifiedMemory      = true
0.00.090.725 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.099.912 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.099.916 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.099.932 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.100.864 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.100.866 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.100.866 I llama_new_context_with_model: graph nodes  = 967
0.00.100.866 I llama_new_context_with_model: graph splits = 2
0.00.100.879 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.059.549 I 
0.01.059.625 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.01.059.634 I perplexity: tokenizing the input ..
0.01.073.967 I perplexity: tokenization took 14.328 ms
0.01.073.981 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.196.655 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.198.341 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.198.393 I llama_perf_context_print:        load time =    1038.37 ms
0.01.198.394 I llama_perf_context_print: prompt eval time =     121.76 ms /   128 tokens (    0.95 ms per token,  1051.24 tokens per second)
0.01.198.396 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.198.396 I llama_perf_context_print:       total time =     138.85 ms /   129 tokens
0.01.199.065 I ggml_metal_free: deallocating

real	0m1.389s
user	0m0.122s
sys	0m0.200s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.114 I build: 4227 (0ca0cc3c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.814 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.020.250 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.020.256 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.020.258 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.020.258 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.020.259 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.020.259 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.020.259 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.020.260 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.020.261 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.020.261 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.020.262 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.020.264 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.020.265 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.020.265 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.020.267 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.020.267 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.020.268 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.026.045 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.027.507 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.033.010 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.033.011 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.033.012 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.033.012 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.033.013 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.033.013 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.033.014 I llama_model_loader: - type  f32:  194 tensors
0.00.033.014 I llama_model_loader: - type q8_0:   98 tensors
0.00.058.052 I llm_load_vocab: special tokens cache size = 25
0.00.063.822 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.063.824 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.063.825 I llm_load_print_meta: arch             = gptneox
0.00.063.825 I llm_load_print_meta: vocab type       = BPE
0.00.063.825 I llm_load_print_meta: n_vocab          = 50304
0.00.063.825 I llm_load_print_meta: n_merges         = 50009
0.00.063.826 I llm_load_print_meta: vocab_only       = 0
0.00.063.826 I llm_load_print_meta: n_ctx_train      = 2048
0.00.063.826 I llm_load_print_meta: n_embd           = 2048
0.00.063.826 I llm_load_print_meta: n_layer          = 24
0.00.063.830 I llm_load_print_meta: n_head           = 16
0.00.063.831 I llm_load_print_meta: n_head_kv        = 16
0.00.063.831 I llm_load_print_meta: n_rot            = 32
0.00.063.831 I llm_load_print_meta: n_swa            = 0
0.00.063.831 I llm_load_print_meta: n_embd_head_k    = 128
0.00.063.832 I llm_load_print_meta: n_embd_head_v    = 128
0.00.063.832 I llm_load_print_meta: n_gqa            = 1
0.00.063.833 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.063.834 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.063.834 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.063.835 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.063.835 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.063.835 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.063.835 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.063.836 I llm_load_print_meta: n_ff             = 8192
0.00.063.836 I llm_load_print_meta: n_expert         = 0
0.00.063.836 I llm_load_print_meta: n_expert_used    = 0
0.00.063.837 I llm_load_print_meta: causal attn      = 1
0.00.063.837 I llm_load_print_meta: pooling type     = 0
0.00.063.837 I llm_load_print_meta: rope type        = 2
0.00.063.837 I llm_load_print_meta: rope scaling     = linear
0.00.063.838 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.063.838 I llm_load_print_meta: freq_scale_train = 1
0.00.063.838 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.063.839 I llm_load_print_meta: rope_finetuned   = unknown
0.00.063.839 I llm_load_print_meta: ssm_d_conv       = 0
0.00.063.839 I llm_load_print_meta: ssm_d_inner      = 0
0.00.063.839 I llm_load_print_meta: ssm_d_state      = 0
0.00.063.839 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.063.839 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.063.851 I llm_load_print_meta: model type       = 1.4B
0.00.063.851 I llm_load_print_meta: model ftype      = Q8_0
0.00.063.852 I llm_load_print_meta: model params     = 1.41 B
0.00.063.852 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.063.852 I llm_load_print_meta: general.name     = 1.4B
0.00.063.855 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.063.855 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.063.855 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.063.855 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.063.855 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.063.856 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.063.856 I llm_load_print_meta: max token length = 1024
0.00.066.001 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.066.002 I llm_load_tensors: offloading output layer to GPU
0.00.066.002 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.066.012 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.066.013 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.066.997 I llama_new_context_with_model: n_seq_max     = 1
0.00.066.998 I llama_new_context_with_model: n_ctx         = 128
0.00.066.998 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.066.998 I llama_new_context_with_model: n_batch       = 128
0.00.066.998 I llama_new_context_with_model: n_ubatch      = 128
0.00.066.998 I llama_new_context_with_model: flash_attn    = 0
0.00.066.999 I llama_new_context_with_model: freq_base     = 10000.0
0.00.066.999 I llama_new_context_with_model: freq_scale    = 1
0.00.067.000 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.067.000 I ggml_metal_init: allocating
0.00.067.006 I ggml_metal_init: found device: Apple M4
0.00.067.008 I ggml_metal_init: picking default device: Apple M4
0.00.067.554 I ggml_metal_init: using embedded metal library
0.00.069.620 I ggml_metal_init: GPU name:   Apple M4
0.00.069.621 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.069.622 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.069.622 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.069.622 I ggml_metal_init: simdgroup reduction   = true
0.00.069.622 I ggml_metal_init: simdgroup matrix mul. = true
0.00.069.623 I ggml_metal_init: has bfloat            = true
0.00.069.623 I ggml_metal_init: use bfloat            = true
0.00.069.623 I ggml_metal_init: hasUnifiedMemory      = true
0.00.069.624 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.079.207 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.079.213 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.079.226 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.080.177 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.080.178 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.080.179 I llama_new_context_with_model: graph nodes  = 967
0.00.080.179 I llama_new_context_with_model: graph splits = 2
0.00.080.191 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.863.365 I 
0.00.863.397 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.863.400 I perplexity: tokenizing the input ..
0.00.871.570 I perplexity: tokenization took 8.167 ms
0.00.871.577 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.993.064 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.00.994.228 I Final estimate: PPL = 10.1362 +/- 3.22437

0.00.994.261 I llama_perf_context_print:        load time =     851.55 ms
0.00.994.262 I llama_perf_context_print: prompt eval time =     121.26 ms /   128 tokens (    0.95 ms per token,  1055.58 tokens per second)
0.00.994.263 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.994.263 I llama_perf_context_print:       total time =     130.89 ms /   129 tokens
0.00.994.648 I ggml_metal_free: deallocating

real	0m1.015s
user	0m0.091s
sys	0m0.158s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.080 I build: 4227 (0ca0cc3c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.464 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.324 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.015.328 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.330 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.335 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.336 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.336 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.337 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.337 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.338 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.338 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.338 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.339 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.339 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.339 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.341 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.341 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.342 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.484 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.590 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.715 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.717 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.717 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.717 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.717 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.718 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.024.718 I llama_model_loader: - type  f32:  194 tensors
0.00.024.719 I llama_model_loader: - type q4_0:   97 tensors
0.00.024.719 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.497 I llm_load_vocab: special tokens cache size = 25
0.00.051.529 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.532 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.532 I llm_load_print_meta: arch             = gptneox
0.00.051.532 I llm_load_print_meta: vocab type       = BPE
0.00.051.533 I llm_load_print_meta: n_vocab          = 50304
0.00.051.533 I llm_load_print_meta: n_merges         = 50009
0.00.051.533 I llm_load_print_meta: vocab_only       = 0
0.00.051.533 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.533 I llm_load_print_meta: n_embd           = 2048
0.00.051.534 I llm_load_print_meta: n_layer          = 24
0.00.051.537 I llm_load_print_meta: n_head           = 16
0.00.051.537 I llm_load_print_meta: n_head_kv        = 16
0.00.051.537 I llm_load_print_meta: n_rot            = 32
0.00.051.538 I llm_load_print_meta: n_swa            = 0
0.00.051.538 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.540 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.540 I llm_load_print_meta: n_gqa            = 1
0.00.051.541 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.542 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.542 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.543 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.543 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.543 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.543 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.544 I llm_load_print_meta: n_ff             = 8192
0.00.051.544 I llm_load_print_meta: n_expert         = 0
0.00.051.544 I llm_load_print_meta: n_expert_used    = 0
0.00.051.544 I llm_load_print_meta: causal attn      = 1
0.00.051.544 I llm_load_print_meta: pooling type     = 0
0.00.051.545 I llm_load_print_meta: rope type        = 2
0.00.051.545 I llm_load_print_meta: rope scaling     = linear
0.00.051.549 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.550 I llm_load_print_meta: freq_scale_train = 1
0.00.051.550 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.550 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.550 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.550 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.550 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.551 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.551 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.562 I llm_load_print_meta: model type       = 1.4B
0.00.051.562 I llm_load_print_meta: model ftype      = Q4_0
0.00.051.563 I llm_load_print_meta: model params     = 1.41 B
0.00.051.563 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.051.563 I llm_load_print_meta: general.name     = 1.4B
0.00.051.564 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.564 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.564 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.564 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.564 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.565 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.565 I llm_load_print_meta: max token length = 1024
0.00.053.063 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.063 I llm_load_tensors: offloading output layer to GPU
0.00.053.063 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.072 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.053.073 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.053.908 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.909 I llama_new_context_with_model: n_ctx         = 128
0.00.053.909 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.909 I llama_new_context_with_model: n_batch       = 128
0.00.053.909 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.910 I llama_new_context_with_model: flash_attn    = 0
0.00.053.910 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.910 I llama_new_context_with_model: freq_scale    = 1
0.00.053.911 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.911 I ggml_metal_init: allocating
0.00.053.916 I ggml_metal_init: found device: Apple M4
0.00.053.918 I ggml_metal_init: picking default device: Apple M4
0.00.054.465 I ggml_metal_init: using embedded metal library
0.00.056.405 I ggml_metal_init: GPU name:   Apple M4
0.00.056.407 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.407 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.407 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.408 I ggml_metal_init: simdgroup reduction   = true
0.00.056.408 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.408 I ggml_metal_init: has bfloat            = true
0.00.056.408 I ggml_metal_init: use bfloat            = true
0.00.056.408 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.409 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.706 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.708 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.722 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.584 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.585 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.585 I llama_new_context_with_model: graph nodes  = 967
0.00.066.586 I llama_new_context_with_model: graph splits = 2
0.00.066.598 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.575.724 I 
0.00.575.760 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.575.766 I perplexity: tokenizing the input ..
0.00.583.735 I perplexity: tokenization took 7.965 ms
0.00.583.738 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.706.265 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.707.429 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.707.453 I llama_perf_context_print:        load time =     566.25 ms
0.00.707.454 I llama_perf_context_print: prompt eval time =     122.30 ms /   128 tokens (    0.96 ms per token,  1046.60 tokens per second)
0.00.707.455 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.707.455 I llama_perf_context_print:       total time =     131.73 ms /   129 tokens
0.00.707.896 I ggml_metal_free: deallocating

real	0m0.726s
user	0m0.078s
sys	0m0.104s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.082 I build: 4227 (0ca0cc3c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.583 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.188 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.014.192 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.194 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.194 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.195 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.195 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.195 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.196 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.199 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.199 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.199 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.200 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.200 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.200 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.202 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.203 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.203 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.260 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.372 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.473 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.474 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.474 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.474 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.475 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.475 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.023.476 I llama_model_loader: - type  f32:  194 tensors
0.00.023.476 I llama_model_loader: - type q4_1:   97 tensors
0.00.023.476 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.345 I llm_load_vocab: special tokens cache size = 25
0.00.050.300 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.303 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.303 I llm_load_print_meta: arch             = gptneox
0.00.050.304 I llm_load_print_meta: vocab type       = BPE
0.00.050.304 I llm_load_print_meta: n_vocab          = 50304
0.00.050.304 I llm_load_print_meta: n_merges         = 50009
0.00.050.304 I llm_load_print_meta: vocab_only       = 0
0.00.050.305 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.305 I llm_load_print_meta: n_embd           = 2048
0.00.050.305 I llm_load_print_meta: n_layer          = 24
0.00.050.307 I llm_load_print_meta: n_head           = 16
0.00.050.308 I llm_load_print_meta: n_head_kv        = 16
0.00.050.308 I llm_load_print_meta: n_rot            = 32
0.00.050.309 I llm_load_print_meta: n_swa            = 0
0.00.050.309 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.309 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.311 I llm_load_print_meta: n_gqa            = 1
0.00.050.311 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.312 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.312 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.313 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.313 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.313 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.313 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.314 I llm_load_print_meta: n_ff             = 8192
0.00.050.314 I llm_load_print_meta: n_expert         = 0
0.00.050.314 I llm_load_print_meta: n_expert_used    = 0
0.00.050.315 I llm_load_print_meta: causal attn      = 1
0.00.050.315 I llm_load_print_meta: pooling type     = 0
0.00.050.315 I llm_load_print_meta: rope type        = 2
0.00.050.315 I llm_load_print_meta: rope scaling     = linear
0.00.050.317 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.319 I llm_load_print_meta: freq_scale_train = 1
0.00.050.319 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.319 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.319 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.319 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.319 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.320 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.320 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.331 I llm_load_print_meta: model type       = 1.4B
0.00.050.332 I llm_load_print_meta: model ftype      = Q4_1
0.00.050.332 I llm_load_print_meta: model params     = 1.41 B
0.00.050.333 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.050.333 I llm_load_print_meta: general.name     = 1.4B
0.00.050.333 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.334 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.334 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.334 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.334 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.334 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.335 I llm_load_print_meta: max token length = 1024
0.00.052.320 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.321 I llm_load_tensors: offloading output layer to GPU
0.00.052.321 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.331 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.052.332 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.053.284 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.285 I llama_new_context_with_model: n_ctx         = 128
0.00.053.285 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.286 I llama_new_context_with_model: n_batch       = 128
0.00.053.286 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.286 I llama_new_context_with_model: flash_attn    = 0
0.00.053.286 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.286 I llama_new_context_with_model: freq_scale    = 1
0.00.053.287 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.287 I ggml_metal_init: allocating
0.00.053.290 I ggml_metal_init: found device: Apple M4
0.00.053.292 I ggml_metal_init: picking default device: Apple M4
0.00.053.838 I ggml_metal_init: using embedded metal library
0.00.055.780 I ggml_metal_init: GPU name:   Apple M4
0.00.055.782 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.782 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.782 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.783 I ggml_metal_init: simdgroup reduction   = true
0.00.055.783 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.783 I ggml_metal_init: has bfloat            = true
0.00.055.783 I ggml_metal_init: use bfloat            = true
0.00.055.783 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.784 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.014 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.016 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.029 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.065.958 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.065.959 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.065.960 I llama_new_context_with_model: graph nodes  = 967
0.00.065.960 I llama_new_context_with_model: graph splits = 2
0.00.065.972 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.672.168 I 
0.00.672.205 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.672.209 I perplexity: tokenizing the input ..
0.00.680.525 I perplexity: tokenization took 8.316 ms
0.00.680.530 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.802.655 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.803.783 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.803.802 I llama_perf_context_print:        load time =     663.58 ms
0.00.803.803 I llama_perf_context_print: prompt eval time =     121.90 ms /   128 tokens (    0.95 ms per token,  1050.02 tokens per second)
0.00.803.804 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.803.805 I llama_perf_context_print:       total time =     131.63 ms /   129 tokens
0.00.804.092 I ggml_metal_free: deallocating

real	0m0.817s
user	0m0.079s
sys	0m0.107s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.082 I build: 4227 (0ca0cc3c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.714 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.556 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.015.560 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.561 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.562 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.562 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.563 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.563 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.564 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.564 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.564 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.565 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.566 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.568 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.568 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.569 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.570 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.570 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.573 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.690 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.746 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.747 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.747 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.748 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.748 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.748 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.024.749 I llama_model_loader: - type  f32:  194 tensors
0.00.024.749 I llama_model_loader: - type q5_0:   97 tensors
0.00.024.749 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.734 I llm_load_vocab: special tokens cache size = 25
0.00.050.754 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.756 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.757 I llm_load_print_meta: arch             = gptneox
0.00.050.757 I llm_load_print_meta: vocab type       = BPE
0.00.050.757 I llm_load_print_meta: n_vocab          = 50304
0.00.050.757 I llm_load_print_meta: n_merges         = 50009
0.00.050.757 I llm_load_print_meta: vocab_only       = 0
0.00.050.758 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.758 I llm_load_print_meta: n_embd           = 2048
0.00.050.758 I llm_load_print_meta: n_layer          = 24
0.00.050.761 I llm_load_print_meta: n_head           = 16
0.00.050.762 I llm_load_print_meta: n_head_kv        = 16
0.00.050.762 I llm_load_print_meta: n_rot            = 32
0.00.050.763 I llm_load_print_meta: n_swa            = 0
0.00.050.763 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.763 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.766 I llm_load_print_meta: n_gqa            = 1
0.00.050.767 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.770 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.770 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.771 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.771 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.771 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.771 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.772 I llm_load_print_meta: n_ff             = 8192
0.00.050.772 I llm_load_print_meta: n_expert         = 0
0.00.050.772 I llm_load_print_meta: n_expert_used    = 0
0.00.050.772 I llm_load_print_meta: causal attn      = 1
0.00.050.772 I llm_load_print_meta: pooling type     = 0
0.00.050.772 I llm_load_print_meta: rope type        = 2
0.00.050.773 I llm_load_print_meta: rope scaling     = linear
0.00.050.773 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.774 I llm_load_print_meta: freq_scale_train = 1
0.00.050.775 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.775 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.775 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.775 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.775 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.775 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.775 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.782 I llm_load_print_meta: model type       = 1.4B
0.00.050.782 I llm_load_print_meta: model ftype      = Q5_0
0.00.050.782 I llm_load_print_meta: model params     = 1.41 B
0.00.050.783 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.050.783 I llm_load_print_meta: general.name     = 1.4B
0.00.050.783 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.784 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.784 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.784 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.784 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.784 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.784 I llm_load_print_meta: max token length = 1024
0.00.052.532 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.533 I llm_load_tensors: offloading output layer to GPU
0.00.052.533 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.538 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.052.538 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.053.407 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.407 I llama_new_context_with_model: n_ctx         = 128
0.00.053.408 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.408 I llama_new_context_with_model: n_batch       = 128
0.00.053.408 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.408 I llama_new_context_with_model: flash_attn    = 0
0.00.053.408 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.409 I llama_new_context_with_model: freq_scale    = 1
0.00.053.409 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.409 I ggml_metal_init: allocating
0.00.053.412 I ggml_metal_init: found device: Apple M4
0.00.053.414 I ggml_metal_init: picking default device: Apple M4
0.00.053.953 I ggml_metal_init: using embedded metal library
0.00.055.872 I ggml_metal_init: GPU name:   Apple M4
0.00.055.873 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.874 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.874 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.874 I ggml_metal_init: simdgroup reduction   = true
0.00.055.874 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.875 I ggml_metal_init: has bfloat            = true
0.00.055.875 I ggml_metal_init: use bfloat            = true
0.00.055.875 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.877 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.208 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.212 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.226 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.112 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.113 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.113 I llama_new_context_with_model: graph nodes  = 967
0.00.066.114 I llama_new_context_with_model: graph splits = 2
0.00.066.121 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.690.707 I 
0.00.690.733 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.690.744 I perplexity: tokenizing the input ..
0.00.698.762 I perplexity: tokenization took 8.016 ms
0.00.698.765 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.834.037 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.835.192 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.835.223 I llama_perf_context_print:        load time =     680.99 ms
0.00.835.224 I llama_perf_context_print: prompt eval time =     135.04 ms /   128 tokens (    1.06 ms per token,   947.85 tokens per second)
0.00.835.225 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.835.225 I llama_perf_context_print:       total time =     144.52 ms /   129 tokens
0.00.835.709 I ggml_metal_free: deallocating

real	0m0.853s
user	0m0.077s
sys	0m0.119s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.088 I build: 4227 (0ca0cc3c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.997 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.944 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.014.949 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.950 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.951 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.951 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.951 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.952 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.952 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.953 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.953 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.955 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.956 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.956 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.956 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.962 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.962 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.963 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.032 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.154 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.258 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.259 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.259 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.260 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.260 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.260 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.261 I llama_model_loader: - type  f32:  194 tensors
0.00.024.261 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.262 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.263 I llm_load_vocab: special tokens cache size = 25
0.00.051.135 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.140 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.140 I llm_load_print_meta: arch             = gptneox
0.00.051.141 I llm_load_print_meta: vocab type       = BPE
0.00.051.143 I llm_load_print_meta: n_vocab          = 50304
0.00.051.143 I llm_load_print_meta: n_merges         = 50009
0.00.051.143 I llm_load_print_meta: vocab_only       = 0
0.00.051.143 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.144 I llm_load_print_meta: n_embd           = 2048
0.00.051.144 I llm_load_print_meta: n_layer          = 24
0.00.051.146 I llm_load_print_meta: n_head           = 16
0.00.051.147 I llm_load_print_meta: n_head_kv        = 16
0.00.051.149 I llm_load_print_meta: n_rot            = 32
0.00.051.149 I llm_load_print_meta: n_swa            = 0
0.00.051.149 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.149 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.150 I llm_load_print_meta: n_gqa            = 1
0.00.051.151 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.151 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.152 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.152 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.152 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.152 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.152 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.153 I llm_load_print_meta: n_ff             = 8192
0.00.051.153 I llm_load_print_meta: n_expert         = 0
0.00.051.153 I llm_load_print_meta: n_expert_used    = 0
0.00.051.154 I llm_load_print_meta: causal attn      = 1
0.00.051.154 I llm_load_print_meta: pooling type     = 0
0.00.051.154 I llm_load_print_meta: rope type        = 2
0.00.051.154 I llm_load_print_meta: rope scaling     = linear
0.00.051.155 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.157 I llm_load_print_meta: freq_scale_train = 1
0.00.051.157 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.157 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.157 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.157 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.157 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.158 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.158 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.169 I llm_load_print_meta: model type       = 1.4B
0.00.051.170 I llm_load_print_meta: model ftype      = Q5_1
0.00.051.171 I llm_load_print_meta: model params     = 1.41 B
0.00.051.172 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.051.172 I llm_load_print_meta: general.name     = 1.4B
0.00.051.172 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.172 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.173 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.173 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.173 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.173 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.173 I llm_load_print_meta: max token length = 1024
0.00.053.244 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.244 I llm_load_tensors: offloading output layer to GPU
0.00.053.244 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.254 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.053.255 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.054.249 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.250 I llama_new_context_with_model: n_ctx         = 128
0.00.054.250 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.250 I llama_new_context_with_model: n_batch       = 128
0.00.054.251 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.251 I llama_new_context_with_model: flash_attn    = 0
0.00.054.251 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.252 I llama_new_context_with_model: freq_scale    = 1
0.00.054.252 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.252 I ggml_metal_init: allocating
0.00.054.258 I ggml_metal_init: found device: Apple M4
0.00.054.261 I ggml_metal_init: picking default device: Apple M4
0.00.054.796 I ggml_metal_init: using embedded metal library
0.00.056.773 I ggml_metal_init: GPU name:   Apple M4
0.00.056.774 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.775 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.775 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.775 I ggml_metal_init: simdgroup reduction   = true
0.00.056.775 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.776 I ggml_metal_init: has bfloat            = true
0.00.056.776 I ggml_metal_init: use bfloat            = true
0.00.056.776 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.777 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.054 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.061 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.086 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.944 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.945 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.946 I llama_new_context_with_model: graph nodes  = 967
0.00.066.946 I llama_new_context_with_model: graph splits = 2
0.00.066.958 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.724.169 I 
0.00.724.201 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.724.206 I perplexity: tokenizing the input ..
0.00.732.548 I perplexity: tokenization took 8.34 ms
0.00.732.553 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.867.670 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.868.914 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.868.935 I llama_perf_context_print:        load time =     715.17 ms
0.00.868.935 I llama_perf_context_print: prompt eval time =     134.88 ms /   128 tokens (    1.05 ms per token,   949.00 tokens per second)
0.00.868.936 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.868.937 I llama_perf_context_print:       total time =     144.77 ms /   129 tokens
0.00.869.346 I ggml_metal_free: deallocating

real	0m0.884s
user	0m0.078s
sys	0m0.124s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.084 I build: 4227 (0ca0cc3c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.350 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.822 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.014.827 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.828 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.829 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.829 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.829 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.830 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.830 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.831 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.831 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.831 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.832 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.832 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.832 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.834 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.834 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.834 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.836 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.870 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.860 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.861 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.862 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.862 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.862 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.863 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.023.863 I llama_model_loader: - type  f32:  194 tensors
0.00.023.864 I llama_model_loader: - type q2_K:   49 tensors
0.00.023.864 I llama_model_loader: - type q3_K:   48 tensors
0.00.023.864 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.845 I llm_load_vocab: special tokens cache size = 25
0.00.049.625 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.628 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.628 I llm_load_print_meta: arch             = gptneox
0.00.049.629 I llm_load_print_meta: vocab type       = BPE
0.00.049.629 I llm_load_print_meta: n_vocab          = 50304
0.00.049.629 I llm_load_print_meta: n_merges         = 50009
0.00.049.629 I llm_load_print_meta: vocab_only       = 0
0.00.049.630 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.630 I llm_load_print_meta: n_embd           = 2048
0.00.049.630 I llm_load_print_meta: n_layer          = 24
0.00.049.633 I llm_load_print_meta: n_head           = 16
0.00.049.634 I llm_load_print_meta: n_head_kv        = 16
0.00.049.634 I llm_load_print_meta: n_rot            = 32
0.00.049.634 I llm_load_print_meta: n_swa            = 0
0.00.049.634 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.634 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.635 I llm_load_print_meta: n_gqa            = 1
0.00.049.636 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.636 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.637 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.639 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.639 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.640 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.640 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.640 I llm_load_print_meta: n_ff             = 8192
0.00.049.641 I llm_load_print_meta: n_expert         = 0
0.00.049.642 I llm_load_print_meta: n_expert_used    = 0
0.00.049.642 I llm_load_print_meta: causal attn      = 1
0.00.049.643 I llm_load_print_meta: pooling type     = 0
0.00.049.643 I llm_load_print_meta: rope type        = 2
0.00.049.643 I llm_load_print_meta: rope scaling     = linear
0.00.049.643 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.644 I llm_load_print_meta: freq_scale_train = 1
0.00.049.644 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.644 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.644 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.645 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.645 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.645 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.645 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.657 I llm_load_print_meta: model type       = 1.4B
0.00.049.658 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.049.658 I llm_load_print_meta: model params     = 1.41 B
0.00.049.659 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.049.659 I llm_load_print_meta: general.name     = 1.4B
0.00.049.659 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.659 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.659 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.659 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.660 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.049.661 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.661 I llm_load_print_meta: max token length = 1024
0.00.051.515 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.515 I llm_load_tensors: offloading output layer to GPU
0.00.051.515 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.525 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.051.526 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.052.450 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.450 I llama_new_context_with_model: n_ctx         = 128
0.00.052.451 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.451 I llama_new_context_with_model: n_batch       = 128
0.00.052.451 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.451 I llama_new_context_with_model: flash_attn    = 0
0.00.052.452 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.452 I llama_new_context_with_model: freq_scale    = 1
0.00.052.452 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.453 I ggml_metal_init: allocating
0.00.052.458 I ggml_metal_init: found device: Apple M4
0.00.052.460 I ggml_metal_init: picking default device: Apple M4
0.00.052.989 I ggml_metal_init: using embedded metal library
0.00.054.981 I ggml_metal_init: GPU name:   Apple M4
0.00.054.982 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.983 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.983 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.983 I ggml_metal_init: simdgroup reduction   = true
0.00.054.984 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.984 I ggml_metal_init: has bfloat            = true
0.00.054.984 I ggml_metal_init: use bfloat            = true
0.00.054.984 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.985 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.900 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.063.904 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.063.919 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.064.859 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.064.860 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.064.860 I llama_new_context_with_model: graph nodes  = 967
0.00.064.860 I llama_new_context_with_model: graph splits = 2
0.00.064.873 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.446.492 I 
0.00.446.530 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.446.536 I perplexity: tokenizing the input ..
0.00.453.846 I perplexity: tokenization took 7.308 ms
0.00.453.849 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.586.171 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.587.335 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.587.366 I llama_perf_context_print:        load time =     437.14 ms
0.00.587.366 I llama_perf_context_print: prompt eval time =     132.07 ms /   128 tokens (    1.03 ms per token,   969.19 tokens per second)
0.00.587.367 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.587.367 I llama_perf_context_print:       total time =     140.88 ms /   129 tokens
0.00.587.912 I ggml_metal_free: deallocating

real	0m0.604s
user	0m0.076s
sys	0m0.075s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.084 I build: 4227 (0ca0cc3c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.128 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.898 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.014.903 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.908 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.909 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.911 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.911 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.911 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.912 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.912 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.913 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.913 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.915 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.915 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.915 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.917 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.917 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.918 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.955 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.057 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.053 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.054 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.055 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.055 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.055 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.055 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.056 I llama_model_loader: - type  f32:  194 tensors
0.00.024.056 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.057 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.057 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.057 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.940 I llm_load_vocab: special tokens cache size = 25
0.00.049.714 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.717 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.718 I llm_load_print_meta: arch             = gptneox
0.00.049.718 I llm_load_print_meta: vocab type       = BPE
0.00.049.718 I llm_load_print_meta: n_vocab          = 50304
0.00.049.718 I llm_load_print_meta: n_merges         = 50009
0.00.049.719 I llm_load_print_meta: vocab_only       = 0
0.00.049.719 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.719 I llm_load_print_meta: n_embd           = 2048
0.00.049.719 I llm_load_print_meta: n_layer          = 24
0.00.049.723 I llm_load_print_meta: n_head           = 16
0.00.049.724 I llm_load_print_meta: n_head_kv        = 16
0.00.049.724 I llm_load_print_meta: n_rot            = 32
0.00.049.724 I llm_load_print_meta: n_swa            = 0
0.00.049.724 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.725 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.725 I llm_load_print_meta: n_gqa            = 1
0.00.049.727 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.728 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.728 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.729 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.729 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.729 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.729 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.730 I llm_load_print_meta: n_ff             = 8192
0.00.049.730 I llm_load_print_meta: n_expert         = 0
0.00.049.730 I llm_load_print_meta: n_expert_used    = 0
0.00.049.730 I llm_load_print_meta: causal attn      = 1
0.00.049.730 I llm_load_print_meta: pooling type     = 0
0.00.049.732 I llm_load_print_meta: rope type        = 2
0.00.049.732 I llm_load_print_meta: rope scaling     = linear
0.00.049.732 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.733 I llm_load_print_meta: freq_scale_train = 1
0.00.049.733 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.733 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.733 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.733 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.734 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.734 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.734 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.745 I llm_load_print_meta: model type       = 1.4B
0.00.049.745 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.049.747 I llm_load_print_meta: model params     = 1.41 B
0.00.049.748 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.049.748 I llm_load_print_meta: general.name     = 1.4B
0.00.049.748 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.748 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.748 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.748 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.749 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.049.749 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.749 I llm_load_print_meta: max token length = 1024
0.00.051.365 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.365 I llm_load_tensors: offloading output layer to GPU
0.00.051.366 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.375 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.051.376 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.052.223 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.224 I llama_new_context_with_model: n_ctx         = 128
0.00.052.224 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.224 I llama_new_context_with_model: n_batch       = 128
0.00.052.225 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.225 I llama_new_context_with_model: flash_attn    = 0
0.00.052.225 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.225 I llama_new_context_with_model: freq_scale    = 1
0.00.052.226 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.226 I ggml_metal_init: allocating
0.00.052.229 I ggml_metal_init: found device: Apple M4
0.00.052.231 I ggml_metal_init: picking default device: Apple M4
0.00.052.773 I ggml_metal_init: using embedded metal library
0.00.054.721 I ggml_metal_init: GPU name:   Apple M4
0.00.054.723 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.723 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.723 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.724 I ggml_metal_init: simdgroup reduction   = true
0.00.054.724 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.724 I ggml_metal_init: has bfloat            = true
0.00.054.724 I ggml_metal_init: use bfloat            = true
0.00.054.724 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.725 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.897 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.063.900 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.063.915 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.064.795 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.064.796 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.064.796 I llama_new_context_with_model: graph nodes  = 967
0.00.064.796 I llama_new_context_with_model: graph splits = 2
0.00.064.808 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.550.112 I 
0.00.550.148 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.550.154 I perplexity: tokenizing the input ..
0.00.557.877 I perplexity: tokenization took 7.721 ms
0.00.557.881 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.689.765 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.690.935 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.690.966 I llama_perf_context_print:        load time =     540.98 ms
0.00.690.967 I llama_perf_context_print: prompt eval time =     131.63 ms /   128 tokens (    1.03 ms per token,   972.43 tokens per second)
0.00.690.968 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.690.968 I llama_perf_context_print:       total time =     140.86 ms /   129 tokens
0.00.691.504 I ggml_metal_free: deallocating

real	0m0.705s
user	0m0.078s
sys	0m0.109s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.082 I build: 4227 (0ca0cc3c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.149 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.845 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.015.849 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.851 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.851 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.851 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.852 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.852 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.853 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.853 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.853 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.854 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.856 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.856 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.857 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.858 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.858 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.859 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.102 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.297 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.475 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.476 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.476 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.477 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.477 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.477 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.478 I llama_model_loader: - type  f32:  194 tensors
0.00.025.478 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.479 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.479 I llama_model_loader: - type q6_K:   13 tensors
0.00.046.164 I llm_load_vocab: special tokens cache size = 25
0.00.052.168 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.171 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.171 I llm_load_print_meta: arch             = gptneox
0.00.052.171 I llm_load_print_meta: vocab type       = BPE
0.00.052.172 I llm_load_print_meta: n_vocab          = 50304
0.00.052.172 I llm_load_print_meta: n_merges         = 50009
0.00.052.172 I llm_load_print_meta: vocab_only       = 0
0.00.052.172 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.172 I llm_load_print_meta: n_embd           = 2048
0.00.052.173 I llm_load_print_meta: n_layer          = 24
0.00.052.176 I llm_load_print_meta: n_head           = 16
0.00.052.177 I llm_load_print_meta: n_head_kv        = 16
0.00.052.177 I llm_load_print_meta: n_rot            = 32
0.00.052.177 I llm_load_print_meta: n_swa            = 0
0.00.052.177 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.177 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.181 I llm_load_print_meta: n_gqa            = 1
0.00.052.181 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.182 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.189 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.191 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.191 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.192 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.192 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.194 I llm_load_print_meta: n_ff             = 8192
0.00.052.194 I llm_load_print_meta: n_expert         = 0
0.00.052.194 I llm_load_print_meta: n_expert_used    = 0
0.00.052.194 I llm_load_print_meta: causal attn      = 1
0.00.052.195 I llm_load_print_meta: pooling type     = 0
0.00.052.195 I llm_load_print_meta: rope type        = 2
0.00.052.195 I llm_load_print_meta: rope scaling     = linear
0.00.052.195 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.195 I llm_load_print_meta: freq_scale_train = 1
0.00.052.196 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.196 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.196 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.196 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.196 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.198 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.198 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.210 I llm_load_print_meta: model type       = 1.4B
0.00.052.210 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.052.210 I llm_load_print_meta: model params     = 1.41 B
0.00.052.211 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.052.211 I llm_load_print_meta: general.name     = 1.4B
0.00.052.211 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.211 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.212 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.212 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.212 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.052.213 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.213 I llm_load_print_meta: max token length = 1024
0.00.053.780 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.780 I llm_load_tensors: offloading output layer to GPU
0.00.053.780 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.790 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.053.791 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.054.657 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.658 I llama_new_context_with_model: n_ctx         = 128
0.00.054.658 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.658 I llama_new_context_with_model: n_batch       = 128
0.00.054.658 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.659 I llama_new_context_with_model: flash_attn    = 0
0.00.054.659 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.659 I llama_new_context_with_model: freq_scale    = 1
0.00.054.660 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.660 I ggml_metal_init: allocating
0.00.054.666 I ggml_metal_init: found device: Apple M4
0.00.054.669 I ggml_metal_init: picking default device: Apple M4
0.00.055.238 I ggml_metal_init: using embedded metal library
0.00.057.176 I ggml_metal_init: GPU name:   Apple M4
0.00.057.178 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.178 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.179 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.179 I ggml_metal_init: simdgroup reduction   = true
0.00.057.179 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.179 I ggml_metal_init: has bfloat            = true
0.00.057.179 I ggml_metal_init: use bfloat            = true
0.00.057.180 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.180 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.106 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.109 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.124 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.969 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.970 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.970 I llama_new_context_with_model: graph nodes  = 967
0.00.066.970 I llama_new_context_with_model: graph splits = 2
0.00.066.983 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.575.005 I 
0.00.575.050 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.575.056 I perplexity: tokenizing the input ..
0.00.582.754 I perplexity: tokenization took 7.697 ms
0.00.582.758 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.717.406 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.718.569 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.718.601 I llama_perf_context_print:        load time =     564.85 ms
0.00.718.602 I llama_perf_context_print: prompt eval time =     134.40 ms /   128 tokens (    1.05 ms per token,   952.37 tokens per second)
0.00.718.603 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.718.604 I llama_perf_context_print:       total time =     143.60 ms /   129 tokens
0.00.719.120 I ggml_metal_free: deallocating

real	0m0.737s
user	0m0.079s
sys	0m0.109s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.082 I build: 4227 (0ca0cc3c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.734 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.527 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.014.532 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.534 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.534 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.535 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.535 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.535 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.536 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.536 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.537 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.537 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.537 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.538 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.538 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.540 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.540 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.541 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.596 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.717 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.830 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.831 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.832 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.832 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.832 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.832 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.023.833 I llama_model_loader: - type  f32:  194 tensors
0.00.023.833 I llama_model_loader: - type q5_K:   61 tensors
0.00.023.833 I llama_model_loader: - type q6_K:   37 tensors
0.00.043.827 I llm_load_vocab: special tokens cache size = 25
0.00.049.684 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.687 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.687 I llm_load_print_meta: arch             = gptneox
0.00.049.687 I llm_load_print_meta: vocab type       = BPE
0.00.049.688 I llm_load_print_meta: n_vocab          = 50304
0.00.049.688 I llm_load_print_meta: n_merges         = 50009
0.00.049.688 I llm_load_print_meta: vocab_only       = 0
0.00.049.688 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.688 I llm_load_print_meta: n_embd           = 2048
0.00.049.689 I llm_load_print_meta: n_layer          = 24
0.00.049.692 I llm_load_print_meta: n_head           = 16
0.00.049.693 I llm_load_print_meta: n_head_kv        = 16
0.00.049.693 I llm_load_print_meta: n_rot            = 32
0.00.049.693 I llm_load_print_meta: n_swa            = 0
0.00.049.693 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.693 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.696 I llm_load_print_meta: n_gqa            = 1
0.00.049.697 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.698 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.698 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.698 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.699 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.699 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.700 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.701 I llm_load_print_meta: n_ff             = 8192
0.00.049.701 I llm_load_print_meta: n_expert         = 0
0.00.049.701 I llm_load_print_meta: n_expert_used    = 0
0.00.049.701 I llm_load_print_meta: causal attn      = 1
0.00.049.702 I llm_load_print_meta: pooling type     = 0
0.00.049.702 I llm_load_print_meta: rope type        = 2
0.00.049.702 I llm_load_print_meta: rope scaling     = linear
0.00.049.702 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.703 I llm_load_print_meta: freq_scale_train = 1
0.00.049.703 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.703 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.703 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.703 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.703 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.703 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.704 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.715 I llm_load_print_meta: model type       = 1.4B
0.00.049.716 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.049.716 I llm_load_print_meta: model params     = 1.41 B
0.00.049.717 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.049.717 I llm_load_print_meta: general.name     = 1.4B
0.00.049.717 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.717 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.717 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.717 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.718 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.049.718 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.718 I llm_load_print_meta: max token length = 1024
0.00.051.674 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.674 I llm_load_tensors: offloading output layer to GPU
0.00.051.675 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.684 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.051.685 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.052.553 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.553 I llama_new_context_with_model: n_ctx         = 128
0.00.052.554 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.554 I llama_new_context_with_model: n_batch       = 128
0.00.052.554 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.554 I llama_new_context_with_model: flash_attn    = 0
0.00.052.554 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.555 I llama_new_context_with_model: freq_scale    = 1
0.00.052.555 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.556 I ggml_metal_init: allocating
0.00.052.558 I ggml_metal_init: found device: Apple M4
0.00.052.560 I ggml_metal_init: picking default device: Apple M4
0.00.053.088 I ggml_metal_init: using embedded metal library
0.00.054.986 I ggml_metal_init: GPU name:   Apple M4
0.00.054.987 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.987 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.988 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.988 I ggml_metal_init: simdgroup reduction   = true
0.00.054.988 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.988 I ggml_metal_init: has bfloat            = true
0.00.054.988 I ggml_metal_init: use bfloat            = true
0.00.054.989 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.989 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.020 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.064.026 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.064.039 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.064.938 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.064.939 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.064.939 I llama_new_context_with_model: graph nodes  = 967
0.00.064.940 I llama_new_context_with_model: graph splits = 2
0.00.064.952 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.661.012 I 
0.00.661.056 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.661.062 I perplexity: tokenizing the input ..
0.00.668.905 I perplexity: tokenization took 7.841 ms
0.00.668.909 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.809.772 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.810.943 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.810.971 I llama_perf_context_print:        load time =     652.27 ms
0.00.810.972 I llama_perf_context_print: prompt eval time =     140.64 ms /   128 tokens (    1.10 ms per token,   910.13 tokens per second)
0.00.810.973 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.810.973 I llama_perf_context_print:       total time =     149.96 ms /   129 tokens
0.00.811.438 I ggml_metal_free: deallocating

real	0m0.826s
user	0m0.077s
sys	0m0.134s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.086 I build: 4227 (0ca0cc3c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.911 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.505 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.509 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.514 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.515 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.515 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.516 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.516 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.518 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.518 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.519 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.519 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.519 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.520 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.520 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.523 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.523 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.523 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.501 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.517 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.388 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.390 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.390 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.390 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.391 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.391 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.392 I llama_model_loader: - type  f32:  194 tensors
0.00.024.392 I llama_model_loader: - type q6_K:   98 tensors
0.00.044.254 I llm_load_vocab: special tokens cache size = 25
0.00.050.178 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.180 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.181 I llm_load_print_meta: arch             = gptneox
0.00.050.181 I llm_load_print_meta: vocab type       = BPE
0.00.050.181 I llm_load_print_meta: n_vocab          = 50304
0.00.050.182 I llm_load_print_meta: n_merges         = 50009
0.00.050.182 I llm_load_print_meta: vocab_only       = 0
0.00.050.182 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.182 I llm_load_print_meta: n_embd           = 2048
0.00.050.182 I llm_load_print_meta: n_layer          = 24
0.00.050.185 I llm_load_print_meta: n_head           = 16
0.00.050.186 I llm_load_print_meta: n_head_kv        = 16
0.00.050.186 I llm_load_print_meta: n_rot            = 32
0.00.050.186 I llm_load_print_meta: n_swa            = 0
0.00.050.187 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.187 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.188 I llm_load_print_meta: n_gqa            = 1
0.00.050.189 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.190 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.190 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.191 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.191 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.191 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.191 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.192 I llm_load_print_meta: n_ff             = 8192
0.00.050.192 I llm_load_print_meta: n_expert         = 0
0.00.050.192 I llm_load_print_meta: n_expert_used    = 0
0.00.050.192 I llm_load_print_meta: causal attn      = 1
0.00.050.192 I llm_load_print_meta: pooling type     = 0
0.00.050.193 I llm_load_print_meta: rope type        = 2
0.00.050.193 I llm_load_print_meta: rope scaling     = linear
0.00.050.193 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.194 I llm_load_print_meta: freq_scale_train = 1
0.00.050.194 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.194 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.194 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.194 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.194 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.195 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.195 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.206 I llm_load_print_meta: model type       = 1.4B
0.00.050.206 I llm_load_print_meta: model ftype      = Q6_K
0.00.050.207 I llm_load_print_meta: model params     = 1.41 B
0.00.050.207 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.050.207 I llm_load_print_meta: general.name     = 1.4B
0.00.050.207 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.208 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.208 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.208 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.208 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.208 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.208 I llm_load_print_meta: max token length = 1024
0.00.051.753 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.753 I llm_load_tensors: offloading output layer to GPU
0.00.051.754 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.763 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.051.764 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.052.593 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.593 I llama_new_context_with_model: n_ctx         = 128
0.00.052.593 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.594 I llama_new_context_with_model: n_batch       = 128
0.00.052.594 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.594 I llama_new_context_with_model: flash_attn    = 0
0.00.052.594 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.595 I llama_new_context_with_model: freq_scale    = 1
0.00.052.595 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.595 I ggml_metal_init: allocating
0.00.052.600 I ggml_metal_init: found device: Apple M4
0.00.052.603 I ggml_metal_init: picking default device: Apple M4
0.00.053.143 I ggml_metal_init: using embedded metal library
0.00.055.093 I ggml_metal_init: GPU name:   Apple M4
0.00.055.094 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.095 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.095 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.095 I ggml_metal_init: simdgroup reduction   = true
0.00.055.096 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.096 I ggml_metal_init: has bfloat            = true
0.00.055.096 I ggml_metal_init: use bfloat            = true
0.00.055.096 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.097 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.130 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.064.132 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.064.146 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.064.970 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.064.971 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.064.972 I llama_new_context_with_model: graph nodes  = 967
0.00.064.972 I llama_new_context_with_model: graph splits = 2
0.00.064.984 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.411.212 I 
0.00.411.246 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.411.250 I perplexity: tokenizing the input ..
0.00.419.276 I perplexity: tokenization took 8.024 ms
0.00.419.279 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.558.772 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.559.947 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.559.974 I llama_perf_context_print:        load time =     401.30 ms
0.00.559.975 I llama_perf_context_print: prompt eval time =     139.27 ms /   128 tokens (    1.09 ms per token,   919.08 tokens per second)
0.00.559.976 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.559.976 I llama_perf_context_print:       total time =     148.76 ms /   129 tokens
0.00.560.345 I ggml_metal_free: deallocating

real	0m0.578s
user	0m0.076s
sys	0m0.101s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.241 I build: 4227 (0ca0cc3c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.021.334 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.037.935 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.037.942 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.037.944 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.037.945 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.037.946 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.037.946 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.037.947 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.037.948 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.037.949 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.037.949 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.037.950 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.037.950 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.037.951 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.037.952 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.037.954 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.037.954 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.037.955 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.045.411 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.047.244 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.053.611 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.053.613 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.053.613 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.053.614 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.053.614 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.053.615 I llama_model_loader: - type  f32:  194 tensors
0.00.053.615 I llama_model_loader: - type  f16:   98 tensors
0.00.080.124 I llm_load_vocab: special tokens cache size = 25
0.00.086.211 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.086.215 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.086.215 I llm_load_print_meta: arch             = gptneox
0.00.086.216 I llm_load_print_meta: vocab type       = BPE
0.00.086.216 I llm_load_print_meta: n_vocab          = 50304
0.00.086.216 I llm_load_print_meta: n_merges         = 50009
0.00.086.216 I llm_load_print_meta: vocab_only       = 0
0.00.086.219 I llm_load_print_meta: n_ctx_train      = 2048
0.00.086.219 I llm_load_print_meta: n_embd           = 2048
0.00.086.219 I llm_load_print_meta: n_layer          = 24
0.00.086.222 I llm_load_print_meta: n_head           = 16
0.00.086.223 I llm_load_print_meta: n_head_kv        = 16
0.00.086.223 I llm_load_print_meta: n_rot            = 32
0.00.086.225 I llm_load_print_meta: n_swa            = 0
0.00.086.225 I llm_load_print_meta: n_embd_head_k    = 128
0.00.086.225 I llm_load_print_meta: n_embd_head_v    = 128
0.00.086.226 I llm_load_print_meta: n_gqa            = 1
0.00.086.227 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.086.232 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.086.233 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.086.233 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.086.235 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.086.235 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.086.235 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.086.236 I llm_load_print_meta: n_ff             = 8192
0.00.086.236 I llm_load_print_meta: n_expert         = 0
0.00.086.238 I llm_load_print_meta: n_expert_used    = 0
0.00.086.239 I llm_load_print_meta: causal attn      = 1
0.00.086.239 I llm_load_print_meta: pooling type     = 0
0.00.086.239 I llm_load_print_meta: rope type        = 2
0.00.086.239 I llm_load_print_meta: rope scaling     = linear
0.00.086.240 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.086.240 I llm_load_print_meta: freq_scale_train = 1
0.00.086.240 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.086.240 I llm_load_print_meta: rope_finetuned   = unknown
0.00.086.241 I llm_load_print_meta: ssm_d_conv       = 0
0.00.086.241 I llm_load_print_meta: ssm_d_inner      = 0
0.00.086.241 I llm_load_print_meta: ssm_d_state      = 0
0.00.086.241 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.086.241 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.086.253 I llm_load_print_meta: model type       = 1.4B
0.00.086.253 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.086.254 I llm_load_print_meta: model params     = 1.41 B
0.00.086.254 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.086.255 I llm_load_print_meta: general.name     = 1.4B
0.00.086.255 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.086.255 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.086.256 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.086.256 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.086.257 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.086.258 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.086.258 I llm_load_print_meta: max token length = 1024
0.00.088.827 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.088.827 I llm_load_tensors: offloading output layer to GPU
0.00.088.828 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.088.838 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.088.839 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.089.753 I llama_new_context_with_model: n_seq_max     = 1
0.00.089.754 I llama_new_context_with_model: n_ctx         = 128
0.00.089.754 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.089.754 I llama_new_context_with_model: n_batch       = 128
0.00.089.754 I llama_new_context_with_model: n_ubatch      = 128
0.00.089.755 I llama_new_context_with_model: flash_attn    = 0
0.00.089.755 I llama_new_context_with_model: freq_base     = 10000.0
0.00.089.755 I llama_new_context_with_model: freq_scale    = 1
0.00.089.756 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.089.756 I ggml_metal_init: allocating
0.00.089.759 I ggml_metal_init: found device: Apple M4
0.00.089.761 I ggml_metal_init: picking default device: Apple M4
0.00.090.346 I ggml_metal_init: using embedded metal library
0.00.092.395 I ggml_metal_init: GPU name:   Apple M4
0.00.092.396 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.092.398 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.092.398 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.092.398 I ggml_metal_init: simdgroup reduction   = true
0.00.092.398 I ggml_metal_init: simdgroup matrix mul. = true
0.00.092.398 I ggml_metal_init: has bfloat            = true
0.00.092.399 I ggml_metal_init: use bfloat            = true
0.00.092.399 I ggml_metal_init: hasUnifiedMemory      = true
0.00.092.399 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.100.857 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.100.859 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.100.872 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.101.729 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.101.731 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.101.731 I llama_new_context_with_model: graph nodes  = 967
0.00.101.731 I llama_new_context_with_model: graph splits = 2
0.00.101.743 I 
0.00.101.773 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.101.774 I compute_imatrix: tokenizing the input ..
0.00.108.867 I compute_imatrix: tokenization took 7.093 ms
0.00.108.869 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.608.084 I compute_imatrix: 1.50 seconds per pass - ETA 0.02 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.610.851 I llama_perf_context_print:        load time =    1586.75 ms
0.01.610.853 I llama_perf_context_print: prompt eval time =    1498.57 ms /   128 tokens (   11.71 ms per token,    85.42 tokens per second)
0.01.610.854 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.610.854 I llama_perf_context_print:       total time =    1589.51 ms /   129 tokens
0.01.611.409 I ggml_metal_free: deallocating

real	0m1.796s
user	0m0.164s
sys	0m0.246s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4227 (0ca0cc3c)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 'Ä'
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x123d04a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x123d04d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x123d05190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x123d05600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x123d05a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x123d05ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x123d06350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x123d067c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x123d06c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x123d070a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x123d07510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x123d07bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x123d086d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x123d08e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x123d09690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x123d09db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x123d0a4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x123d0abf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x123d0b310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x123d0bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x123d0c200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x123d0c920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x123d0d040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x123d0d8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x123d0e000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x123d0e2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x123d0e580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x123d0e9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x123d0f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x123d0f510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x123d0f980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x123d0ff10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x123d10380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x123d10640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x123d10ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x123d11360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x123d11620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x123d11a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x123e045d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x123e04a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x123e04eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x123e05320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x123e05790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x123e05c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x123e06070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x123e064e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x123e06950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x123e06dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x123e07560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x123e079d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x123e07e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x123e082b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x123e08720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x123e08b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x123e09000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x123e09470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x123e099d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x123e09ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x123e0a340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x123e0a7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x123e0ac20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x123e0b090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x123e0b500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x123e0b970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x123e0bde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x123e0c250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x123e0c6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x123e0cb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x123e0cfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x123e0d410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x123e0d880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x123e0dcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x123e0e160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x123e0e5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x123e0ea40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x123e0eeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x123e0f320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x123e0f790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x123e0fc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x123e10070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x123e104e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x123e10950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x123e10dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x123e119a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x123e11c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x123e12160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x123e12660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x123e12b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x123e13060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x123e13560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x123e13a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x123e13f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x123e14460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x123e14960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x123e14e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x123e15360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x123e15860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x123e07080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x123e16260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x123e16700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x123e16ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x123e17040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x123e174e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x123e17980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x123e17e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x123e180e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x123e185b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x123e18ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x123e18fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x123e194b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x123e199b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x123e19eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x123e1a3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x123e1a8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x123e1adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x123e1b2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x123e1b7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x123e1bcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x123e1c1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x123e1c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x123e1cbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x123e1d0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x123e1d5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x123e1dab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x123e1dfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x123e1e4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x123e1e9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x123e1eeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x123e1f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x123e1f8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x123e1fdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x123e202b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x123e207b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x123e20cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x123e211b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x123e216b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x123e21bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x123e220b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x123e225b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x123e22ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x123e22fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x123e234b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x123e239b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x123e23eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x123e243b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x123e248b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x123e24db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x123e252b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x123e257b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x123e25cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x123e261b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x123e266b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x123e26bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x123e270b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x123e27660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x123e27c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x123e281c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x123e28770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x123e28d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x123e29390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x123e299a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x123e29fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x123e2a5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x123e2adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x123e2b250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x123e2b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x123e2bb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x123e2c340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x123e2c890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x123e2cde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x123e2d330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x123e2d880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x123e2ddd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x123e2e320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x123e2e870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x123e2edc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x123e2f310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x123e2f860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x123e2fdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x123e30300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x123e30850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x123e30da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x123e312f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x123e31840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x123e31d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x123e322e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x123e32830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x123e32d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x123e332d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x123e33820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x123e33d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x123e342c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x123e34810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x123e34d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x123e352b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x123e35800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x123e35d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x123e362a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x123e367f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x123e36d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x123e37290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x123e377e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x123e37d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x123e38280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x123e387d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x123e38d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x123e39270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x123e397c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x123e39d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x123e3a260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x123e3a7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x123e3ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x123e3b250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x123e3b7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x123e3bcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x123e3c240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x123e3c790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x123e3cce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x123e3d230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x123e3d780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x123e3dcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x123e3e220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x123e3e770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x123e3ecc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x123e3f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x123e3f600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x123e3faa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x123e3ff40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x123e403e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x123e40880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x123e40d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x123e411c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x123e41660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x123e41b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x123e41fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x123e42440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x123e428e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x123e42e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x123e43550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x123e43c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x123e44390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x123e44ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x123e44d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x123e45380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x123e45990 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
0.00.161.448 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x123c0b9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x123c0be50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x123c0c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x123c0cc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x123c0d170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x123c0d6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x123c0dc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x123c0e160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x123c0e600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x123c0eaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x123c0ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x123c0f200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x123c0fbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x123c103a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x123c10bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x123c112d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x123c119f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x123c12110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x123c12830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x123c131e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x123c13900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x123c14020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x123c14740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x123c14e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x123c15580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x123c15840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x123c15e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x123c16460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x123c16a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x123c17260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x123c17700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x123c179c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x123c18250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x123c18790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x123c18a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x123c18ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x123c19390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x123c19830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x123c19cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x123c1a170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x123c1a610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x123c1aab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x123c1af50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x123c1b3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x123c1b6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x123c1bcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x123c1c2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x123c1c8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x123c1cef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x123c1d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x123c1db10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x123c1e120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x123c1e730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x123c1ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x123c1f530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x123c1f9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x123c1fe70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x123c20130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x123c20740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x123c20f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x123c213d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x123c21870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x123c21d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x123c221b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x123c22650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x123c22af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x123c22f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x123c23430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x123c238d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x123c23d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x123c24210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x123c246b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x123c24b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x123c24ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x123c25490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x123c25930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x123c25dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x123c26270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x123c26710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x123c26bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x123c27050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x123c274f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x123c27990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x123c27e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x123c282d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x123c28770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x123c28c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x123c290b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x123c29550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x123c299f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x123c29e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x123c2a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x123c2a7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x123c2ac70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x123c2b110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x123c2b5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x123c2ba50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x123c2bef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x123c2c390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x123c2c830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x123c2ccd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x123c2d170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x123c2d610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x123c2dab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x123c2df50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x123c2e3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x123c2e890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x123c2ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x123c2f1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x123c2f670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x123c2fb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x123c2ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x123c30450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x123c308f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x123c30d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x123c31230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x123c316d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x123c31b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x123c32010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x123c324b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x123c32950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x123c32df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x123c33290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x123c33730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x123c33bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x123c34070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x123c34510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x123c349b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x123c34e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x123c352f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x123c35790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x123c35c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x123c360d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x123c36570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x123c36a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x123c36eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x123c37350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x123c377f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x123c37c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x123c38130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x123c385d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x123c38a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x123c38f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x123c393b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x123c39850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x123c39cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x123c3a190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x123c3a630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x123c3aad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x123c3af70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x123c3b410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x123c3b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x123c3bd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x123c3c2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x123c3c7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x123c3cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x123c3d290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x123c3d550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x123c3db60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x123c3e170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x123c3e780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x123c3ed90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x123c3f3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x123c3fb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x123c40030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x123c404d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x123c40970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x123c41120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x123c41670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x123c41bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x123c42110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x123c42660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x123c42bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x123c43100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x123c43650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x123c43ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x123c440f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x123c44640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x123c44b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x123c450e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x123c45630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x123c45b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x123c460d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x123c46620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x123c46b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x123c470c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x123c47610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x123c47b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x123c480b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x123c48600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x123c48b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x123c490a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x123c495f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x123c49b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x123c4a090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x123c4a5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x123c4ab30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x123c4b080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x123c4b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x123c4bb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x123c4c070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x123c4c5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x123c4cb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x123c4d060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x123c4d5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x123c4db00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x123c4e050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x123c4e5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x123c4eaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x123c4f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x123c4f590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x123c4fae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x123c50030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x123c50580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x123c50ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x123c51020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x123c51570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x123c51ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x123c52010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x123c52560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x123c52ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x123c53000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x123c53550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x123c53aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x123c53f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x123c543e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x123c54880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x123c54d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x123c551c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x123c55660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x123c55b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x123c55fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x123c56440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x123c568e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x123c56d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x123c57220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x123c576c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x123c57c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x123c58330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x123c58a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x123c59170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x123c59890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x123c59b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x123c5a160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x123c5a770 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x123c0b910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x123c0bd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x123c0c1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x123c0c660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x123c0cad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x123c0cf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x123c0d3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x123c0d820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x123c0dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x123c0e100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x123c0e570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x123c0e9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x123c0f2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x123c0fa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x123c10230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x123c10920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x123c11010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x123c11700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x123c11df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x123c12770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x123c12e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x123c13550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x123c13c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x123c14330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x123c14a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x123c14e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x123c15300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x123c15770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x123c15be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x123c16050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x123c164c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x123c16930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x123c16da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x123c17060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x123c174d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x123c17940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x123c17db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x123c18220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x123c18690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x123c18b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x123c18f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x123c193e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x123c19850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x123c19cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x123c1a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x123c1a5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x123c1aa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x123c1ae80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x123c1b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x123c1b760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x123c1bbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x123c1c040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x123c1c4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x123c1c920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x123c1cd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x123c1d200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x123c1d670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x123c1dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x123c1df50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x123c1e3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x123c1e830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x123c1eca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x123c1f110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x123c1f580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x123c1f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x123c1fe60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x123c202d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x123c20740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x123c20bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x123c21020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x123c21490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x123c21900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x123c21d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x123c221e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x123c22650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x123c22ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x123c22f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x123c233a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x123c23810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x123c23c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x123c240f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x123c24560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x123c249d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x123c24e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x123c252b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x123c25720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x123c25b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x123c26000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x123c26470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x123c268e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x123c26d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x123c271c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x123c27630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x123c27aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x123c27f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x123c28380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x123c287f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x123c28c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x123c290d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x123c29540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x123c299b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x123c29e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x123c2a290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x123c2a700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x123c2ab70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x123c2afe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x123c2b450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x123c2b8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x123c2bd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x123c2c1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x123c2c610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x123c2ca80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x123c2cef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x123c2d360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x123c2d7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x123c2dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x123c2e0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x123c2e520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x123c2e990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x123c2ee00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x123c2f270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x123c2f6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x123c2fb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x123c2ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x123c30430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x123c308a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x123c30d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x123c31180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x123c315f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x123c31a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x123c31ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x123c32340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x123c327b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x123c32c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x123c33090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x123c33500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x123c33970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x123c33de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x123c34250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x123c346c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x123c34b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x123c34fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x123c35410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x123c35880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x123c35cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x123c36160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x123c365d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x123c36a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x123c36eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x123c37320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x123c37790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x123c37c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x123c38070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x123c384e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x123c38950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x123c38dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x123c39230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x123c396a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x123c39b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x123c39f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x123c3a3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x123c3a860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x123c3acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x123c3b140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x123c3b5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x123c3ba20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x123c3be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x123c3c610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x123c3ca80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x123c3cef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x123c3d360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x123c3d7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x123c3dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x123c3e0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x123c3e520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x123c3e990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x123c3ee00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x123c3f270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x123c3f6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x123c3fb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x123c3ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x123c40430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x123c408a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x123c40d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x123c41180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x123c415f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x123c41a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x123c41ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x123c42340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x123c427b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x123c42c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x123c43090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x123c43500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x123c43970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x123c43de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x123c44250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x123c446c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x123c44b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x123c44fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x123c45410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x123c45880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x123c45cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x123c46160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x123c465d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x123c46a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x123c46eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x123c47320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x123c47790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x123c47c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x123c48070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x123c484e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x123c48950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x123c48dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x123c49230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x123c496a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x123c49b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x123c49f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x123c4a3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x123c4a860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x123c4acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x123c4b140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x123c4b5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x123c4ba20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x123c4be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x123c4c300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x123c4c770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x123c4cbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x123c4d050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x123c4d4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x123c4d930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x123c4dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x123c4e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x123c4e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x123c4eaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x123c4ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x123c4f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x123c4f840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x123c4fcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x123c503a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x123c50a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x123c51180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x123c51870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x123c51ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x123c52150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x123c525c0 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.768s
user	0m0.286s
sys	0m0.301s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4227 (0ca0cc3c)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 'Ä'
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14d70d400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14d70d8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14d70de70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14d70e420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14d70e9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14d70ef80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14d70f530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14d70fae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14d710090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14d710590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14d710a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14d710f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14d711ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14d712260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14d712a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14d713190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14d7138b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14d713fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14d7146f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14d714ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14d7155e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14d715d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14d716420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14d716cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14d7173e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14d7176a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14d717cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14d718920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14d718e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14d719120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14d7195c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14d719880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14d71a110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14d71a650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14d71a910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14d71adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14d71b250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14d71b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14d71bb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14d71c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14d71c4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14d71c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14d71ce10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14d71d2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14d71d570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14d71db80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14d71e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14d71eab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14d71f0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14d71f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14d71fce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14d7202f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14d720900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14d720f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14d721700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14d721ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14d722040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14d722300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14d722910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14d723100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14d7233c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14d723860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14d723d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14d7241a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14d724640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14d724ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14d724f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14d725420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14d7258c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14d725d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14d726200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14d7266a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14d726b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14d726fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14d727480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14d727920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14d727dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14d728260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14d728700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14d728ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14d729040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14d7294e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14d729980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14d729e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14d72a2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14d72a760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14d72ac00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14d72b0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14d72b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14d72b9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14d72be80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14d72c320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14d72c7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14d72cc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14d72d100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14d72d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14d72da40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14d71e7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14d72e090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14d72e530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14d72e9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14d72ee70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14d72f310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14d72f7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14d72fc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14d7300f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14d730590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14d730a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14d730ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14d731370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14d731810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14d731cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14d732150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14d7325f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14d732a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14d732f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14d7333d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14d733870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14d733d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14d7341b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14d734650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14d734af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14d734f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14d735430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14d7358d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14d735d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14d736210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14d7366b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14d736b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14d736ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14d737490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14d737930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14d737dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14d738270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14d738710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14d738bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14d739050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14d7394f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14d739990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14d739e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14d73a2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14d73a770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14d73ac10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14d73b0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14d73b550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14d73b9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14d73be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14d73c330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14d73c7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14d73cc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14d73d110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14d73d5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14d73da50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14d73dfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14d73e4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14d73ea40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14d73ef90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14d73f250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14d73f860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14d73fe70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14d740480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14d740a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14d7410a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14d741890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14d741d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14d7421d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14d742670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14d742e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14d743370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14d7438c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14d743e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14d744360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14d7448b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14d744e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14d745350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14d7458a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14d745df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14d746340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14d746890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14d746de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14d747330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14d747880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14d747dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14d748320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14d748870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14d748dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14d749310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14d749860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14d749db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14d74a300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14d74a850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14d74ada0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14d74b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14d74b840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14d74bd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14d74c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14d74c830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14d74cd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14d74d2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14d74d820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14d74dd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14d74e2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14d74e810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14d74ed60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14d74f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14d74f800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14d74fd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14d7502a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14d7507f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14d750d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14d751290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14d7517e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14d751d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14d752280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14d7527d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14d752d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14d753270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14d7537c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14d753d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14d754260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14d7547b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14d754d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14d755250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14d7557a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14d755c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14d7560e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14d756580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14d756a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14d756ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14d757360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14d757800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14d757ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14d758140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14d7585e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14d758a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14d758f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14d7593c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14d759910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14d75a030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14d75a750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14d75ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14d75b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14d75b850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14d75be60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14d75c470 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
0.00.086.214 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14d608d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14d6091e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14d609650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14d609ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14d609f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14d60a3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14d60a810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14d60ac80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14d60b0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14d60b670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14d60bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14d60c160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14d60cc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14d60d430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14d60dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14d60e360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14d60ea80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14d60f1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14d60f8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14d610090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14d6107b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14d610ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14d6115f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14d611d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14d612430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14d6126f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14d6129b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14d612e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14d613290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14d613700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14d613c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14d614110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14d614580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14d614840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14d614cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14d615120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14d615680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14d615b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14d616080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14d616580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14d616a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14d616f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14d617480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14d617980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14d617e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14d6182f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14d618760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14d618bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14d619040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14d6194b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14d619920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14d619d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14d61a200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14d61a670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14d61aae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14d61b2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14d61b750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14d61ba10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14d61c020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14d61c810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14d61ccb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14d61d150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14d61d5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14d61da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14d61df30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14d61e3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14d61e870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14d61ed10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14d61f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14d61f650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14d61faf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14d61ff90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14d620430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14d6208d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14d620d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14d621210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14d6216b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14d621b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14d621ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14d622490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14d622930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14d622dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14d623270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14d623710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14d623bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14d624050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14d6244f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14d624990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14d624e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14d6252d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14d625770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14d625c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14d6260b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14d626550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14d6269f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14d626e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14d627330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14d6277d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14d627c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14d628110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14d6285b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14d628a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14d628ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14d629390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14d629830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14d629cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14d62a170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14d62a610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14d62aab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14d62af50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14d62b3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14d62b890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14d62bd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14d62c1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14d62c670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14d62cb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14d62cfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14d62d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14d62d8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14d62dd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14d62e230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14d62e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14d62eb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14d62f010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14d62f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14d62f950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14d62fdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14d630290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14d630730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14d630bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14d631070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14d631510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14d6319b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14d631e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14d6322f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14d632790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14d632c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14d6330d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14d633570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14d633a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14d633eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14d634350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14d6347f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14d634c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14d635130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14d6355d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14d635a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14d635f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14d6363b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14d636850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14d636cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14d637190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14d637630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14d637b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14d6380d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14d638620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14d638b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14d638e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14d639440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14d639a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14d63a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14d63a670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14d63ac80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14d63b470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14d63b910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14d63bdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14d63c250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14d63ca00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14d63cf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14d63d4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14d63d9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14d63df40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14d63e490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14d63e9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14d63ef30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14d63f480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14d63f9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14d63ff20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14d640470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14d6409c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14d640f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14d641460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14d6419b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14d641f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14d642450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14d6429a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14d642ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14d643440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14d643990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14d643ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14d644430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14d644980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14d644ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14d645420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14d645970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14d645ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14d646410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14d646960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14d646eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14d647400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14d647950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14d647ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14d6483f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14d648940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14d648e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14d6493e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14d649930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14d649e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14d64a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14d64a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14d64ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14d64b3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14d64b910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14d64be60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14d64c3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14d64c900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14d64ce50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14d64d3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14d64d8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14d64de40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14d64e390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14d64e8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14d64ee30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14d64f380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14d64f820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14d64fcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14d650160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14d650600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14d650aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14d650f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14d6513e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14d651880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14d651d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14d6521c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14d652660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14d652b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14d652fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14d6534f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14d653c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14d654330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14d654a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14d655170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14d655430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14d655a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14d656050 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14d608d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14d6091e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14d609650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14d609ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14d609f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14d60a3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14d60a810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14d60ac80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14d60b0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14d60b560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14d60b9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14d60bfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14d60c8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14d60d020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14d60d800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14d60def0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14d60e5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14d60ecd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14d60f3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14d60fd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14d610430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14d610b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14d611210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14d611900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14d611ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14d612460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14d6128d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14d612d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14d6131b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14d613620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14d613a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14d613f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14d614370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14d614630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14d614aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14d614f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14d615380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14d6157f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14d615c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14d6160d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14d616540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14d6169b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14d616e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14d617290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14d617700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14d617b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14d617fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14d618450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14d6188c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14d618d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14d6191a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14d619610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14d619a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14d619ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14d61a360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14d61a7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14d61ac40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14d61b0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14d61b520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14d61b990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14d61be00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14d61c270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14d61c6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14d61cb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14d61cfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14d61d430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14d61d8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14d61dd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14d61e180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14d61e5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14d61ea60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14d61eed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14d61f340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14d61f7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14d61fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14d620090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14d620500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14d620970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14d620de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14d621250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14d6216c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14d621b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14d621fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14d622410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14d622880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14d622cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14d623160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14d6235d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14d623a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14d623eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14d624320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14d624790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14d624c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14d625070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14d6254e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14d625950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14d625dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14d626230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14d6266a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14d626b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14d626f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14d6273f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14d627860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14d627cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14d628140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14d6285b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14d628a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14d628e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14d629300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14d629770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14d629be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14d62a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14d62a4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14d62a930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14d62ada0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14d62b210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14d62b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14d62baf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14d62bf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14d62c3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14d62c840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14d62ccb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14d62d120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14d62d590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14d62da00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14d62de70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14d62e2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14d62e750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14d62ebc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14d62f030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14d62f4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14d62f910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14d62fd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14d6301f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14d630660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14d630ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14d630f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14d6313b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14d631820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14d631c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14d632100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14d632570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14d6329e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14d632e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14d6332c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14d633730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14d633ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14d634010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14d634480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14d6348f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14d634d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14d6351d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14d635640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14d635ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14d635f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14d636390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14d636800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14d636c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14d6370e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14d637550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14d6379c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14d637e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14d6382a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14d638710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14d638b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14d638ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14d639460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14d639be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14d63a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14d63a4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14d63a930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14d63ada0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14d63b210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14d63b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14d63baf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14d63bf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14d63c3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14d63c840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14d63ccb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14d63d120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14d63d590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14d63da00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14d63de70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14d63e2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14d63e750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14d63ebc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14d63f030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14d63f4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14d63f910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14d63fd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14d6401f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14d640660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14d640ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14d640f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14d6413b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14d641820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14d641c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14d642100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14d642570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14d6429e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14d642e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14d6432c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14d643730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14d643ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14d644010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14d644480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14d6448f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14d644d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14d6451d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14d645640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14d645ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14d645f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14d646390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14d646800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14d646c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14d6470e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14d647550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14d6479c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14d647e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14d6482a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14d648710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14d648b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14d648ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14d649460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14d6498d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14d649d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14d64a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14d64a620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14d64aa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14d64af00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14d64b370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14d64b7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14d64bc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14d64c0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14d64c530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14d64c9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14d64ce10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14d64d280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14d64d970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14d64e060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14d64e750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14d64ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14d64f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14d64f720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14d64fb90 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Sorry, I'm a bit rubbish at this


second run: The quick brown fox jumps over the lazy Dog." "Sorry, I'm a bit rubbish at this


single seq run: The quick brown fox jumps over the lazy Dog." "Sorry, I'm a bit rubbish at this

real	0m0.930s
user	0m0.239s
sys	0m0.148s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
