Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:318 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- ARM -mcpu not found, -mcpu=native will be used
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod - Success
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sve
-- Performing Test GGML_MACHINE_SUPPORTS_sve - Failed
-- ARM feature DOTPROD enabled
-- ARM feature MATMUL_INT8 enabled
-- ARM feature FMA enabled
-- ARM feature FP16_VECTOR_ARITHMETIC enabled
-- Adding CPU backend variant ggml-cpu: -mcpu=native+dotprod+i8mm+nosve 
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (2.3s)
-- Generating done (0.2s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m2.570s
user	0m0.915s
sys	0m1.228s
++ nproc
+ make -j10
[  1%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  1%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  1%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  2%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  2%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  3%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  3%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  4%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  5%] Built target sha256
[  6%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o
[  6%] Built target build_info
[  6%] Built target sha1
[  6%] Built target xxhash
[  6%] Linking CXX shared library ../../bin/libggml-base.dylib
[  6%] Built target ggml-base
[  6%] Generate assembly for embedded Metal library
Embedding Metal library
[  6%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[  7%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[  9%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 11%] Linking CXX shared library ../../../bin/libggml-blas.dylib
[ 12%] Linking CXX shared library ../../bin/libggml-cpu.dylib
[ 12%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 13%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 13%] Built target ggml-blas
[ 13%] Built target ggml-cpu
[ 14%] Linking C shared library ../../../bin/libggml-metal.dylib
[ 14%] Built target ggml-metal
[ 15%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 15%] Linking CXX shared library ../../bin/libggml.dylib
[ 15%] Built target ggml
[ 16%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o
[ 20%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o
[ 20%] Linking CXX executable ../../bin/llama-gguf
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 24%] Linking CXX executable ../../bin/llama-gguf-hash
[ 24%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 25%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 26%] Linking CXX shared library ../bin/libllama.dylib
[ 26%] Built target llama-gguf-hash
[ 26%] Built target llama-gguf
[ 26%] Built target llama
[ 27%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 27%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 28%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 29%] Building CXX object common/CMakeFiles/common.dir/chat.cpp.o
[ 29%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 29%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 30%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 31%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 32%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 33%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 34%] Linking C executable ../bin/test-c
[ 34%] Linking CXX executable ../../bin/llama-simple
[ 34%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 34%] Linking CXX executable ../../bin/llama-quantize-stats
[ 34%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 35%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 35%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 35%] Linking CXX executable ../../bin/llama-simple-chat
[ 36%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 36%] Built target llava
[ 36%] Linking CXX static library libcommon.a
[ 36%] Built target llama-quantize-stats
[ 36%] Built target llama-simple
[ 36%] Built target test-c
[ 36%] Linking CXX static library libllava_static.a
[ 37%] Linking CXX shared library ../../bin/libllava_shared.dylib
[ 37%] Built target llama-simple-chat
[ 37%] Built target common
[ 37%] Built target llava_static
[ 37%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-chat.dir/test-chat.cpp.o
[ 41%] Built target llava_shared
[ 41%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 43%] Linking CXX executable ../bin/test-tokenizer-0
[ 44%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 45%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 45%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-chat.dir/get-model.cpp.o
[ 46%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 46%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 46%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 46%] Linking CXX executable ../bin/test-sampling
[ 47%] Linking CXX executable ../bin/test-llama-grammar
[ 47%] Linking CXX executable ../bin/test-grammar-integration
[ 48%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 48%] Linking CXX executable ../bin/test-grammar-parser
[ 48%] Linking CXX executable ../bin/test-chat
[ 49%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 49%] Built target test-tokenizer-1-bpe
[ 49%] Built target test-tokenizer-0
[ 49%] Built target test-tokenizer-1-spm
[ 49%] Linking CXX executable ../bin/test-log
[ 49%] Built target test-sampling
[ 49%] Built target test-grammar-parser
[ 49%] Built target test-grammar-integration
[ 49%] Built target test-llama-grammar
[ 49%] Built target test-chat
[ 49%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 49%] Built target test-json-schema-to-grammar
[ 50%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 56%] Built target test-log
[ 56%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 57%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 57%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 57%] Linking CXX executable ../bin/test-arg-parser
[ 57%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 57%] Linking CXX executable ../bin/test-chat-template
[ 57%] Linking CXX executable ../bin/test-model-load-cancel
[ 58%] Linking CXX executable ../bin/test-gguf
[ 59%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 60%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 61%] Linking CXX executable ../bin/test-barrier
[ 61%] Linking CXX executable ../bin/test-backend-ops
[ 62%] Linking CXX executable ../bin/test-quantize-fns
[ 63%] Linking CXX executable ../bin/test-autorelease
[ 63%] Linking CXX executable ../bin/test-quantize-perf
[ 63%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 63%] Built target test-arg-parser
[ 63%] Built target test-model-load-cancel
[ 63%] Built target test-chat-template
[ 63%] Built target test-gguf
[ 63%] Built target test-barrier
[ 63%] Built target test-backend-ops
[ 64%] Linking CXX executable ../bin/test-rope
[ 64%] Built target test-quantize-fns
[ 64%] Built target test-autorelease
[ 64%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 64%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 64%] Built target test-quantize-perf
[ 65%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 66%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 67%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 68%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 68%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 68%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 69%] Linking CXX executable ../../bin/llama-batched
[ 70%] Linking CXX executable ../../bin/llama-batched-bench
[ 70%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 70%] Linking CXX executable ../../bin/llama-embedding
[ 70%] Linking CXX executable ../../bin/llama-eval-callback
[ 70%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 71%] Linking CXX executable ../../bin/llama-imatrix
[ 71%] Linking CXX executable ../../bin/llama-gguf-split
[ 71%] Built target test-rope
[ 72%] Linking CXX executable ../../bin/llama-gritlm
[ 73%] Linking CXX executable ../../bin/llama-infill
[ 73%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 73%] Built target llama-batched
[ 73%] Built target llama-batched-bench
[ 73%] Built target llama-embedding
[ 73%] Built target llama-gbnf-validator
[ 73%] Built target llama-gguf-split
[ 73%] Built target llama-eval-callback
[ 73%] Built target llama-gritlm
[ 73%] Built target llama-imatrix
[ 74%] Linking CXX executable ../../bin/llama-bench
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 74%] Built target llama-infill
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 74%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 74%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 74%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 75%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 76%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 77%] Linking CXX executable ../../bin/llama-lookup
[ 78%] Linking CXX executable ../../bin/llama-lookup-create
[ 79%] Linking CXX executable ../../bin/llama-lookahead
[ 80%] Linking CXX executable ../../bin/llama-lookup-merge
[ 80%] Linking CXX executable ../../bin/llama-parallel
[ 80%] Linking CXX executable ../../bin/llama-cli
[ 81%] Linking CXX executable ../../bin/llama-lookup-stats
[ 81%] Linking CXX executable ../../bin/llama-passkey
[ 81%] Built target llama-bench
[ 81%] Linking CXX executable ../../bin/llama-perplexity
[ 82%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 82%] Built target llama-lookahead
[ 82%] Built target llama-lookup-create
[ 82%] Built target llama-lookup
[ 82%] Built target llama-lookup-merge
[ 82%] Built target llama-lookup-stats
[ 82%] Built target llama-cli
[ 82%] Built target llama-parallel
[ 82%] Built target llama-passkey
[ 82%] Generating loading.html.hpp
[ 82%] Linking CXX executable ../../bin/llama-quantize
[ 83%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 83%] Built target llama-perplexity
[ 83%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 85%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 85%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 85%] Generating index.html.gz.hpp
[ 86%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 87%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 88%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 89%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 89%] Linking CXX executable ../../bin/llama-retrieval
[ 90%] Linking CXX executable ../../bin/llama-save-load-state
[ 90%] Linking CXX executable ../../bin/llama-speculative
[ 90%] Linking CXX executable ../../bin/llama-speculative-simple
[ 90%] Linking CXX executable ../../bin/llama-tokenize
[ 90%] Building CXX object examples/run/CMakeFiles/llama-run.dir/linenoise.cpp/linenoise.cpp.o
[ 90%] Linking CXX executable ../../bin/llama-tts
[ 90%] Built target llama-quantize
[ 90%] Linking CXX executable ../../bin/llama-gen-docs
[ 91%] Linking CXX executable ../../bin/llama-run
[ 91%] Built target llama-retrieval
[ 91%] Built target llama-speculative
[ 91%] Built target llama-tokenize
[ 92%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 92%] Built target llama-speculative-simple
[ 92%] Built target llama-save-load-state
[ 92%] Built target llama-tts
[ 92%] Built target llama-run
[ 93%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 94%] Built target llama-gen-docs
[ 94%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 94%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 95%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 96%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 97%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 97%] Linking CXX executable ../../bin/llama-cvector-generator
[ 97%] Linking CXX executable ../../bin/llama-export-lora
[ 98%] Linking CXX executable ../../bin/llama-llava-cli
[ 99%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 99%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 99%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Linking CXX executable ../../bin/llama-vdot
[ 99%] Built target llama-convert-llama2c-to-ggml
[ 99%] Built target llama-cvector-generator
[ 99%] Built target llama-export-lora
[ 99%] Built target llama-llava-cli
[ 99%] Built target llama-minicpmv-cli
[ 99%] Built target llama-q8dot
[ 99%] Built target llama-vdot
[ 99%] Built target llama-qwen2vl-cli
[100%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m3.098s
user	0m6.446s
sys	0m9.834s

main: quantize time =  3753.26 ms
main:    total time =  3753.26 ms

main: quantize time =  1947.15 ms
main:    total time =  1947.15 ms

main: quantize time =  3243.11 ms
main:    total time =  3243.11 ms

main: quantize time =  3138.18 ms
main:    total time =  3138.18 ms

main: quantize time =  2301.35 ms
main:    total time =  2301.35 ms

main: quantize time =  5786.31 ms
main:    total time =  5786.31 ms

main: quantize time =  5715.27 ms
main:    total time =  5715.27 ms

main: quantize time =  7260.90 ms
main:    total time =  7260.90 ms

main: quantize time =  5916.16 ms
main:    total time =  5916.17 ms

main: quantize time =  4777.98 ms
main:    total time =  4777.98 ms
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.150 I build: 4612 (0cec062a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.294 I main: llama backend init
0.00.000.301 I main: load the model and apply lora adapter, if any
0.00.066.705 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.079.399 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.079.413 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.079.424 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.079.424 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.079.425 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.079.426 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.079.426 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.079.429 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.079.430 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.079.431 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.079.432 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.079.432 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.079.433 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.079.435 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.079.441 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.079.441 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.079.442 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.086.247 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.088.377 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.095.161 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.095.170 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.095.171 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.095.172 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.095.172 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.095.173 I llama_model_loader: - type  f32:  194 tensors
0.00.095.174 I llama_model_loader: - type  f16:   98 tensors
0.00.095.176 I print_info: file format = GGUF V3 (latest)
0.00.095.178 I print_info: file type   = all F32 (guessed)
0.00.095.182 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.113.256 I load: special tokens cache size = 25
0.00.123.901 I load: token to piece cache size = 0.2984 MB
0.00.123.907 I print_info: arch             = gptneox
0.00.123.908 I print_info: vocab_only       = 0
0.00.123.908 I print_info: n_ctx_train      = 2048
0.00.123.908 I print_info: n_embd           = 2048
0.00.123.909 I print_info: n_layer          = 24
0.00.123.916 I print_info: n_head           = 16
0.00.123.917 I print_info: n_head_kv        = 16
0.00.123.917 I print_info: n_rot            = 32
0.00.123.917 I print_info: n_swa            = 0
0.00.123.917 I print_info: n_embd_head_k    = 128
0.00.123.917 I print_info: n_embd_head_v    = 128
0.00.123.918 I print_info: n_gqa            = 1
0.00.123.919 I print_info: n_embd_k_gqa     = 2048
0.00.123.920 I print_info: n_embd_v_gqa     = 2048
0.00.123.921 I print_info: f_norm_eps       = 1.0e-05
0.00.123.924 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.123.924 I print_info: f_clamp_kqv      = 0.0e+00
0.00.123.925 I print_info: f_max_alibi_bias = 0.0e+00
0.00.123.925 I print_info: f_logit_scale    = 0.0e+00
0.00.123.926 I print_info: n_ff             = 8192
0.00.123.926 I print_info: n_expert         = 0
0.00.123.926 I print_info: n_expert_used    = 0
0.00.123.926 I print_info: causal attn      = 1
0.00.123.927 I print_info: pooling type     = 0
0.00.123.927 I print_info: rope type        = 2
0.00.123.927 I print_info: rope scaling     = linear
0.00.123.928 I print_info: freq_base_train  = 10000.0
0.00.123.928 I print_info: freq_scale_train = 1
0.00.123.928 I print_info: n_ctx_orig_yarn  = 2048
0.00.123.930 I print_info: rope_finetuned   = unknown
0.00.123.931 I print_info: ssm_d_conv       = 0
0.00.123.931 I print_info: ssm_d_inner      = 0
0.00.123.931 I print_info: ssm_d_state      = 0
0.00.123.931 I print_info: ssm_dt_rank      = 0
0.00.123.931 I print_info: ssm_dt_b_c_rms   = 0
0.00.123.931 I print_info: model type       = 1.4B
0.00.123.932 I print_info: model params     = 1.41 B
0.00.123.932 I print_info: general.name     = 1.4B
0.00.123.934 I print_info: vocab type       = BPE
0.00.123.934 I print_info: n_vocab          = 50304
0.00.123.935 I print_info: n_merges         = 50009
0.00.123.935 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.123.935 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.123.935 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.123.937 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.123.937 I print_info: LF token         = 187 'Ċ'
0.00.123.938 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.123.938 I print_info: max token length = 1024
0.00.176.245 I load_tensors: offloading 24 repeating layers to GPU
0.00.176.249 I load_tensors: offloading output layer to GPU
0.00.176.249 I load_tensors: offloaded 25/25 layers to GPU
0.00.176.271 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.176.272 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.176.804 I llama_init_from_model: n_seq_max     = 1
0.00.176.805 I llama_init_from_model: n_ctx         = 2048
0.00.176.805 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.176.805 I llama_init_from_model: n_batch       = 2048
0.00.176.806 I llama_init_from_model: n_ubatch      = 512
0.00.176.806 I llama_init_from_model: flash_attn    = 0
0.00.176.806 I llama_init_from_model: freq_base     = 10000.0
0.00.176.807 I llama_init_from_model: freq_scale    = 1
0.00.176.807 I ggml_metal_init: allocating
0.00.176.828 I ggml_metal_init: found device: Apple M4
0.00.176.834 I ggml_metal_init: picking default device: Apple M4
0.00.177.390 I ggml_metal_init: using embedded metal library
0.00.190.747 I ggml_metal_init: GPU name:   Apple M4
0.00.190.748 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.190.749 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.190.749 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.190.750 I ggml_metal_init: simdgroup reduction   = true
0.00.190.750 I ggml_metal_init: simdgroup matrix mul. = true
0.00.190.750 I ggml_metal_init: has residency sets    = true
0.00.190.750 I ggml_metal_init: has bfloat            = true
0.00.190.750 I ggml_metal_init: use bfloat            = true
0.00.190.750 I ggml_metal_init: hasUnifiedMemory      = true
0.00.190.751 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.215.275 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.244.072 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.244.079 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.244.122 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.248.203 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.248.205 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.248.205 I llama_init_from_model: graph nodes  = 967
0.00.248.205 I llama_init_from_model: graph splits = 2
0.00.248.213 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.248.336 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.248.336 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.314.091 I main: llama threadpool init, n_threads = 4
0.00.314.135 I 
0.00.314.168 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.314.169 I 
0.00.314.294 I sampler seed: 1234
0.00.314.299 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.314.323 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.314.324 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.314.325 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.152.165 I llama_perf_sampler_print:    sampling time =       1.21 ms /    71 runs   (    0.02 ms per token, 58823.53 tokens per second)
0.02.152.166 I llama_perf_context_print:        load time =     246.33 ms
0.02.152.167 I llama_perf_context_print: prompt eval time =      43.61 ms /     7 tokens (    6.23 ms per token,   160.50 tokens per second)
0.02.152.168 I llama_perf_context_print:        eval time =    1791.32 ms /    63 runs   (   28.43 ms per token,    35.17 tokens per second)
0.02.152.168 I llama_perf_context_print:       total time =    1839.12 ms /    70 tokens
0.02.152.438 I ggml_metal_free: deallocating

real	0m2.441s
user	0m0.134s
sys	0m0.146s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.047 I build: 4612 (0cec062a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.084 I main: llama backend init
0.00.000.087 I main: load the model and apply lora adapter, if any
0.00.009.857 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.028.009 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.028.016 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.028.019 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.028.020 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.028.020 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.028.021 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.028.021 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.028.024 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.028.024 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.028.024 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.028.025 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.028.025 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.028.025 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.028.026 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.028.029 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.028.030 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.028.030 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.032.099 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.033.255 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.037.422 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.037.423 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.037.424 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.037.424 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.037.424 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.037.425 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.037.425 I llama_model_loader: - type  f32:  194 tensors
0.00.037.426 I llama_model_loader: - type q8_0:   98 tensors
0.00.037.427 I print_info: file format = GGUF V3 (latest)
0.00.037.427 I print_info: file type   = Q8_0
0.00.037.428 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.045.966 I load: special tokens cache size = 25
0.00.052.243 I load: token to piece cache size = 0.2984 MB
0.00.052.247 I print_info: arch             = gptneox
0.00.052.248 I print_info: vocab_only       = 0
0.00.052.248 I print_info: n_ctx_train      = 2048
0.00.052.251 I print_info: n_embd           = 2048
0.00.052.251 I print_info: n_layer          = 24
0.00.052.255 I print_info: n_head           = 16
0.00.052.256 I print_info: n_head_kv        = 16
0.00.052.256 I print_info: n_rot            = 32
0.00.052.256 I print_info: n_swa            = 0
0.00.052.256 I print_info: n_embd_head_k    = 128
0.00.052.256 I print_info: n_embd_head_v    = 128
0.00.052.257 I print_info: n_gqa            = 1
0.00.052.258 I print_info: n_embd_k_gqa     = 2048
0.00.052.259 I print_info: n_embd_v_gqa     = 2048
0.00.052.259 I print_info: f_norm_eps       = 1.0e-05
0.00.052.260 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.052.260 I print_info: f_clamp_kqv      = 0.0e+00
0.00.052.260 I print_info: f_max_alibi_bias = 0.0e+00
0.00.052.260 I print_info: f_logit_scale    = 0.0e+00
0.00.052.261 I print_info: n_ff             = 8192
0.00.052.261 I print_info: n_expert         = 0
0.00.052.261 I print_info: n_expert_used    = 0
0.00.052.261 I print_info: causal attn      = 1
0.00.052.261 I print_info: pooling type     = 0
0.00.052.262 I print_info: rope type        = 2
0.00.052.262 I print_info: rope scaling     = linear
0.00.052.262 I print_info: freq_base_train  = 10000.0
0.00.052.263 I print_info: freq_scale_train = 1
0.00.052.263 I print_info: n_ctx_orig_yarn  = 2048
0.00.052.263 I print_info: rope_finetuned   = unknown
0.00.052.263 I print_info: ssm_d_conv       = 0
0.00.052.265 I print_info: ssm_d_inner      = 0
0.00.052.265 I print_info: ssm_d_state      = 0
0.00.052.265 I print_info: ssm_dt_rank      = 0
0.00.052.265 I print_info: ssm_dt_b_c_rms   = 0
0.00.052.266 I print_info: model type       = 1.4B
0.00.052.266 I print_info: model params     = 1.41 B
0.00.052.266 I print_info: general.name     = 1.4B
0.00.052.267 I print_info: vocab type       = BPE
0.00.052.267 I print_info: n_vocab          = 50304
0.00.052.268 I print_info: n_merges         = 50009
0.00.052.268 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.052.268 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.052.269 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.052.269 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.052.269 I print_info: LF token         = 187 'Ċ'
0.00.052.269 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.052.270 I print_info: max token length = 1024
0.01.121.560 I load_tensors: offloading 24 repeating layers to GPU
0.01.121.566 I load_tensors: offloading output layer to GPU
0.01.121.567 I load_tensors: offloaded 25/25 layers to GPU
0.01.121.591 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.01.121.594 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.01.122.548 I llama_init_from_model: n_seq_max     = 1
0.01.122.550 I llama_init_from_model: n_ctx         = 2048
0.01.122.550 I llama_init_from_model: n_ctx_per_seq = 2048
0.01.122.551 I llama_init_from_model: n_batch       = 2048
0.01.122.551 I llama_init_from_model: n_ubatch      = 512
0.01.122.551 I llama_init_from_model: flash_attn    = 0
0.01.122.552 I llama_init_from_model: freq_base     = 10000.0
0.01.122.553 I llama_init_from_model: freq_scale    = 1
0.01.122.554 I ggml_metal_init: allocating
0.01.122.564 I ggml_metal_init: found device: Apple M4
0.01.122.571 I ggml_metal_init: picking default device: Apple M4
0.01.123.819 I ggml_metal_init: using embedded metal library
0.01.129.165 I ggml_metal_init: GPU name:   Apple M4
0.01.129.168 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.129.169 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.129.169 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.129.170 I ggml_metal_init: simdgroup reduction   = true
0.01.129.170 I ggml_metal_init: simdgroup matrix mul. = true
0.01.129.170 I ggml_metal_init: has residency sets    = true
0.01.129.171 I ggml_metal_init: has bfloat            = true
0.01.129.171 I ggml_metal_init: use bfloat            = true
0.01.129.171 I ggml_metal_init: hasUnifiedMemory      = true
0.01.129.172 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.145.771 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.199.812 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.01.199.818 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.01.199.853 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.203.925 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.01.203.927 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.01.203.927 I llama_init_from_model: graph nodes  = 967
0.01.203.927 I llama_init_from_model: graph splits = 2
0.01.203.932 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.01.204.065 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.204.066 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.257.660 I main: llama threadpool init, n_threads = 4
0.01.257.710 I 
0.01.257.735 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.257.735 I 
0.01.257.908 I sampler seed: 1234
0.01.257.913 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.257.924 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.257.924 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.257.924 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.366.989 I llama_perf_sampler_print:    sampling time =       1.29 ms /    71 runs   (    0.02 ms per token, 55167.06 tokens per second)
0.02.366.990 I llama_perf_context_print:        load time =    1246.88 ms
0.02.366.991 I llama_perf_context_print: prompt eval time =      39.57 ms /     7 tokens (    5.65 ms per token,   176.92 tokens per second)
0.02.366.992 I llama_perf_context_print:        eval time =    1066.59 ms /    63 runs   (   16.93 ms per token,    59.07 tokens per second)
0.02.366.993 I llama_perf_context_print:       total time =    1110.25 ms /    70 tokens
0.02.367.286 I ggml_metal_free: deallocating

real	0m2.384s
user	0m0.109s
sys	0m0.258s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4612 (0cec062a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.081 I main: llama backend init
0.00.000.084 I main: load the model and apply lora adapter, if any
0.00.012.363 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.020.097 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.020.103 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.020.105 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.020.105 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.020.106 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.020.106 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.020.106 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.020.107 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.020.108 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.020.108 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.020.108 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.020.109 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.020.109 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.020.110 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.020.112 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.020.112 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.020.112 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.024.030 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.025.043 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.028.961 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.028.962 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.028.962 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.028.962 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.028.963 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.028.963 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.028.964 I llama_model_loader: - type  f32:  194 tensors
0.00.028.964 I llama_model_loader: - type q4_0:   97 tensors
0.00.028.964 I llama_model_loader: - type q6_K:    1 tensors
0.00.028.966 I print_info: file format = GGUF V3 (latest)
0.00.028.969 I print_info: file type   = Q4_0
0.00.028.970 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.036.804 I load: special tokens cache size = 25
0.00.042.957 I load: token to piece cache size = 0.2984 MB
0.00.042.961 I print_info: arch             = gptneox
0.00.042.961 I print_info: vocab_only       = 0
0.00.042.961 I print_info: n_ctx_train      = 2048
0.00.042.961 I print_info: n_embd           = 2048
0.00.042.962 I print_info: n_layer          = 24
0.00.042.966 I print_info: n_head           = 16
0.00.042.967 I print_info: n_head_kv        = 16
0.00.042.968 I print_info: n_rot            = 32
0.00.042.970 I print_info: n_swa            = 0
0.00.042.970 I print_info: n_embd_head_k    = 128
0.00.042.971 I print_info: n_embd_head_v    = 128
0.00.042.971 I print_info: n_gqa            = 1
0.00.042.973 I print_info: n_embd_k_gqa     = 2048
0.00.042.974 I print_info: n_embd_v_gqa     = 2048
0.00.042.975 I print_info: f_norm_eps       = 1.0e-05
0.00.042.975 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.042.976 I print_info: f_clamp_kqv      = 0.0e+00
0.00.042.978 I print_info: f_max_alibi_bias = 0.0e+00
0.00.042.978 I print_info: f_logit_scale    = 0.0e+00
0.00.042.979 I print_info: n_ff             = 8192
0.00.042.979 I print_info: n_expert         = 0
0.00.042.979 I print_info: n_expert_used    = 0
0.00.042.979 I print_info: causal attn      = 1
0.00.042.979 I print_info: pooling type     = 0
0.00.042.979 I print_info: rope type        = 2
0.00.042.980 I print_info: rope scaling     = linear
0.00.042.980 I print_info: freq_base_train  = 10000.0
0.00.042.980 I print_info: freq_scale_train = 1
0.00.042.982 I print_info: n_ctx_orig_yarn  = 2048
0.00.042.983 I print_info: rope_finetuned   = unknown
0.00.042.983 I print_info: ssm_d_conv       = 0
0.00.042.983 I print_info: ssm_d_inner      = 0
0.00.042.983 I print_info: ssm_d_state      = 0
0.00.042.983 I print_info: ssm_dt_rank      = 0
0.00.042.983 I print_info: ssm_dt_b_c_rms   = 0
0.00.042.984 I print_info: model type       = 1.4B
0.00.042.988 I print_info: model params     = 1.41 B
0.00.042.988 I print_info: general.name     = 1.4B
0.00.042.989 I print_info: vocab type       = BPE
0.00.042.989 I print_info: n_vocab          = 50304
0.00.042.989 I print_info: n_merges         = 50009
0.00.042.990 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.042.990 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.042.990 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.042.991 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.042.992 I print_info: LF token         = 187 'Ċ'
0.00.042.992 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.042.992 I print_info: max token length = 1024
0.00.585.845 I load_tensors: offloading 24 repeating layers to GPU
0.00.585.865 I load_tensors: offloading output layer to GPU
0.00.585.865 I load_tensors: offloaded 25/25 layers to GPU
0.00.585.898 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.585.899 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.587.060 I llama_init_from_model: n_seq_max     = 1
0.00.587.072 I llama_init_from_model: n_ctx         = 2048
0.00.587.073 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.587.073 I llama_init_from_model: n_batch       = 2048
0.00.587.074 I llama_init_from_model: n_ubatch      = 512
0.00.587.074 I llama_init_from_model: flash_attn    = 0
0.00.587.077 I llama_init_from_model: freq_base     = 10000.0
0.00.587.077 I llama_init_from_model: freq_scale    = 1
0.00.587.079 I ggml_metal_init: allocating
0.00.587.157 I ggml_metal_init: found device: Apple M4
0.00.587.174 I ggml_metal_init: picking default device: Apple M4
0.00.589.028 I ggml_metal_init: using embedded metal library
0.00.594.837 I ggml_metal_init: GPU name:   Apple M4
0.00.594.843 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.594.844 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.594.845 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.594.845 I ggml_metal_init: simdgroup reduction   = true
0.00.594.846 I ggml_metal_init: simdgroup matrix mul. = true
0.00.594.846 I ggml_metal_init: has residency sets    = true
0.00.594.846 I ggml_metal_init: has bfloat            = true
0.00.594.847 I ggml_metal_init: use bfloat            = true
0.00.594.848 I ggml_metal_init: hasUnifiedMemory      = true
0.00.594.856 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.615.360 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.675.784 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.675.792 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.675.834 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.680.467 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.680.469 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.680.469 I llama_init_from_model: graph nodes  = 967
0.00.680.470 I llama_init_from_model: graph splits = 2
0.00.680.475 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.680.590 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.680.590 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.727.922 I main: llama threadpool init, n_threads = 4
0.00.727.967 I 
0.00.727.993 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.727.994 I 
0.00.728.131 I sampler seed: 1234
0.00.728.136 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.728.155 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.728.156 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.728.156 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.444.520 I llama_perf_sampler_print:    sampling time =       1.38 ms /    71 runs   (    0.02 ms per token, 51300.58 tokens per second)
0.01.444.521 I llama_perf_context_print:        load time =     714.64 ms
0.01.444.521 I llama_perf_context_print: prompt eval time =      49.45 ms /     7 tokens (    7.06 ms per token,   141.55 tokens per second)
0.01.444.522 I llama_perf_context_print:        eval time =     663.99 ms /    63 runs   (   10.54 ms per token,    94.88 tokens per second)
0.01.444.523 I llama_perf_context_print:       total time =     717.52 ms /    70 tokens
0.01.444.871 I ggml_metal_free: deallocating

real	0m1.461s
user	0m0.113s
sys	0m0.194s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4612 (0cec062a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.076 I main: llama backend init
0.00.000.078 I main: load the model and apply lora adapter, if any
0.00.009.359 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.124 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.017.129 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.130 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.135 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.136 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.136 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.136 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.137 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.138 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.138 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.138 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.139 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.139 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.140 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.142 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.143 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.143 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.993 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.005 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.732 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.733 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.734 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.734 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.734 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.735 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.025.735 I llama_model_loader: - type  f32:  194 tensors
0.00.025.735 I llama_model_loader: - type q4_1:   97 tensors
0.00.025.736 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.736 I print_info: file format = GGUF V3 (latest)
0.00.025.737 I print_info: file type   = Q4_1
0.00.025.738 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.033.450 I load: special tokens cache size = 25
0.00.039.389 I load: token to piece cache size = 0.2984 MB
0.00.039.391 I print_info: arch             = gptneox
0.00.039.392 I print_info: vocab_only       = 0
0.00.039.392 I print_info: n_ctx_train      = 2048
0.00.039.392 I print_info: n_embd           = 2048
0.00.039.392 I print_info: n_layer          = 24
0.00.039.395 I print_info: n_head           = 16
0.00.039.396 I print_info: n_head_kv        = 16
0.00.039.396 I print_info: n_rot            = 32
0.00.039.396 I print_info: n_swa            = 0
0.00.039.397 I print_info: n_embd_head_k    = 128
0.00.039.397 I print_info: n_embd_head_v    = 128
0.00.039.397 I print_info: n_gqa            = 1
0.00.039.398 I print_info: n_embd_k_gqa     = 2048
0.00.039.401 I print_info: n_embd_v_gqa     = 2048
0.00.039.401 I print_info: f_norm_eps       = 1.0e-05
0.00.039.401 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.402 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.402 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.402 I print_info: f_logit_scale    = 0.0e+00
0.00.039.403 I print_info: n_ff             = 8192
0.00.039.403 I print_info: n_expert         = 0
0.00.039.404 I print_info: n_expert_used    = 0
0.00.039.404 I print_info: causal attn      = 1
0.00.039.405 I print_info: pooling type     = 0
0.00.039.406 I print_info: rope type        = 2
0.00.039.406 I print_info: rope scaling     = linear
0.00.039.406 I print_info: freq_base_train  = 10000.0
0.00.039.407 I print_info: freq_scale_train = 1
0.00.039.407 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.407 I print_info: rope_finetuned   = unknown
0.00.039.407 I print_info: ssm_d_conv       = 0
0.00.039.407 I print_info: ssm_d_inner      = 0
0.00.039.407 I print_info: ssm_d_state      = 0
0.00.039.407 I print_info: ssm_dt_rank      = 0
0.00.039.408 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.408 I print_info: model type       = 1.4B
0.00.039.408 I print_info: model params     = 1.41 B
0.00.039.408 I print_info: general.name     = 1.4B
0.00.039.409 I print_info: vocab type       = BPE
0.00.039.409 I print_info: n_vocab          = 50304
0.00.039.409 I print_info: n_merges         = 50009
0.00.039.410 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.410 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.413 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.414 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.414 I print_info: LF token         = 187 'Ċ'
0.00.039.414 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.414 I print_info: max token length = 1024
0.00.638.503 I load_tensors: offloading 24 repeating layers to GPU
0.00.638.518 I load_tensors: offloading output layer to GPU
0.00.638.519 I load_tensors: offloaded 25/25 layers to GPU
0.00.638.572 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.638.577 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.639.826 I llama_init_from_model: n_seq_max     = 1
0.00.639.834 I llama_init_from_model: n_ctx         = 2048
0.00.639.835 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.639.835 I llama_init_from_model: n_batch       = 2048
0.00.639.836 I llama_init_from_model: n_ubatch      = 512
0.00.639.836 I llama_init_from_model: flash_attn    = 0
0.00.639.838 I llama_init_from_model: freq_base     = 10000.0
0.00.639.839 I llama_init_from_model: freq_scale    = 1
0.00.639.844 I ggml_metal_init: allocating
0.00.639.918 I ggml_metal_init: found device: Apple M4
0.00.639.934 I ggml_metal_init: picking default device: Apple M4
0.00.641.880 I ggml_metal_init: using embedded metal library
0.00.647.910 I ggml_metal_init: GPU name:   Apple M4
0.00.647.915 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.647.916 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.647.917 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.647.919 I ggml_metal_init: simdgroup reduction   = true
0.00.647.919 I ggml_metal_init: simdgroup matrix mul. = true
0.00.647.919 I ggml_metal_init: has residency sets    = true
0.00.647.919 I ggml_metal_init: has bfloat            = true
0.00.647.920 I ggml_metal_init: use bfloat            = true
0.00.647.921 I ggml_metal_init: hasUnifiedMemory      = true
0.00.647.923 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.667.313 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.724.528 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.724.538 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.724.574 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.729.316 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.729.318 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.729.319 I llama_init_from_model: graph nodes  = 967
0.00.729.319 I llama_init_from_model: graph splits = 2
0.00.729.323 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.729.458 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.729.459 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.778.836 I main: llama threadpool init, n_threads = 4
0.00.778.881 I 
0.00.778.908 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.778.910 I 
0.00.779.031 I sampler seed: 1234
0.00.779.036 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.779.092 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.779.094 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.779.094 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.545.154 I llama_perf_sampler_print:    sampling time =       1.21 ms /    71 runs   (    0.02 ms per token, 58872.31 tokens per second)
0.01.545.155 I llama_perf_context_print:        load time =     768.53 ms
0.01.545.155 I llama_perf_context_print: prompt eval time =      49.61 ms /     7 tokens (    7.09 ms per token,   141.10 tokens per second)
0.01.545.156 I llama_perf_context_print:        eval time =     713.73 ms /    63 runs   (   11.33 ms per token,    88.27 tokens per second)
0.01.545.156 I llama_perf_context_print:       total time =     767.27 ms /    70 tokens
0.01.545.379 I ggml_metal_free: deallocating

real	0m1.562s
user	0m0.111s
sys	0m0.207s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4612 (0cec062a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.074 I main: llama backend init
0.00.000.077 I main: load the model and apply lora adapter, if any
0.00.011.292 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.019.218 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.019.223 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.225 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.226 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.226 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.226 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.227 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.227 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.228 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.228 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.228 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.229 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.229 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.230 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.232 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.232 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.232 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.200 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.296 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.028.227 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.028.228 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.028.228 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.028.229 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.028.229 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.028.229 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.028.230 I llama_model_loader: - type  f32:  194 tensors
0.00.028.230 I llama_model_loader: - type q5_0:   97 tensors
0.00.028.230 I llama_model_loader: - type q6_K:    1 tensors
0.00.028.231 I print_info: file format = GGUF V3 (latest)
0.00.028.232 I print_info: file type   = Q5_0
0.00.028.232 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.036.317 I load: special tokens cache size = 25
0.00.042.260 I load: token to piece cache size = 0.2984 MB
0.00.042.262 I print_info: arch             = gptneox
0.00.042.262 I print_info: vocab_only       = 0
0.00.042.263 I print_info: n_ctx_train      = 2048
0.00.042.263 I print_info: n_embd           = 2048
0.00.042.263 I print_info: n_layer          = 24
0.00.042.266 I print_info: n_head           = 16
0.00.042.267 I print_info: n_head_kv        = 16
0.00.042.267 I print_info: n_rot            = 32
0.00.042.267 I print_info: n_swa            = 0
0.00.042.268 I print_info: n_embd_head_k    = 128
0.00.042.268 I print_info: n_embd_head_v    = 128
0.00.042.271 I print_info: n_gqa            = 1
0.00.042.272 I print_info: n_embd_k_gqa     = 2048
0.00.042.273 I print_info: n_embd_v_gqa     = 2048
0.00.042.273 I print_info: f_norm_eps       = 1.0e-05
0.00.042.273 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.042.275 I print_info: f_clamp_kqv      = 0.0e+00
0.00.042.275 I print_info: f_max_alibi_bias = 0.0e+00
0.00.042.275 I print_info: f_logit_scale    = 0.0e+00
0.00.042.276 I print_info: n_ff             = 8192
0.00.042.276 I print_info: n_expert         = 0
0.00.042.276 I print_info: n_expert_used    = 0
0.00.042.277 I print_info: causal attn      = 1
0.00.042.277 I print_info: pooling type     = 0
0.00.042.278 I print_info: rope type        = 2
0.00.042.279 I print_info: rope scaling     = linear
0.00.042.280 I print_info: freq_base_train  = 10000.0
0.00.042.280 I print_info: freq_scale_train = 1
0.00.042.280 I print_info: n_ctx_orig_yarn  = 2048
0.00.042.280 I print_info: rope_finetuned   = unknown
0.00.042.280 I print_info: ssm_d_conv       = 0
0.00.042.281 I print_info: ssm_d_inner      = 0
0.00.042.281 I print_info: ssm_d_state      = 0
0.00.042.281 I print_info: ssm_dt_rank      = 0
0.00.042.281 I print_info: ssm_dt_b_c_rms   = 0
0.00.042.281 I print_info: model type       = 1.4B
0.00.042.285 I print_info: model params     = 1.41 B
0.00.042.285 I print_info: general.name     = 1.4B
0.00.042.286 I print_info: vocab type       = BPE
0.00.042.286 I print_info: n_vocab          = 50304
0.00.042.287 I print_info: n_merges         = 50009
0.00.042.287 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.042.287 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.042.287 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.042.287 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.042.288 I print_info: LF token         = 187 'Ċ'
0.00.042.288 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.042.288 I print_info: max token length = 1024
0.00.660.299 I load_tensors: offloading 24 repeating layers to GPU
0.00.660.320 I load_tensors: offloading output layer to GPU
0.00.660.321 I load_tensors: offloaded 25/25 layers to GPU
0.00.660.354 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.660.355 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.661.618 I llama_init_from_model: n_seq_max     = 1
0.00.661.626 I llama_init_from_model: n_ctx         = 2048
0.00.661.627 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.661.627 I llama_init_from_model: n_batch       = 2048
0.00.661.627 I llama_init_from_model: n_ubatch      = 512
0.00.661.628 I llama_init_from_model: flash_attn    = 0
0.00.661.630 I llama_init_from_model: freq_base     = 10000.0
0.00.661.630 I llama_init_from_model: freq_scale    = 1
0.00.661.635 I ggml_metal_init: allocating
0.00.661.713 I ggml_metal_init: found device: Apple M4
0.00.661.728 I ggml_metal_init: picking default device: Apple M4
0.00.663.566 I ggml_metal_init: using embedded metal library
0.00.669.547 I ggml_metal_init: GPU name:   Apple M4
0.00.669.553 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.669.554 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.669.555 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.669.556 I ggml_metal_init: simdgroup reduction   = true
0.00.669.556 I ggml_metal_init: simdgroup matrix mul. = true
0.00.669.556 I ggml_metal_init: has residency sets    = true
0.00.669.557 I ggml_metal_init: has bfloat            = true
0.00.669.557 I ggml_metal_init: use bfloat            = true
0.00.669.558 I ggml_metal_init: hasUnifiedMemory      = true
0.00.669.560 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.688.394 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.742.853 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.742.860 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.742.902 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.747.461 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.747.462 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.747.463 I llama_init_from_model: graph nodes  = 967
0.00.747.463 I llama_init_from_model: graph splits = 2
0.00.747.469 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.747.601 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.747.602 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.800.772 I main: llama threadpool init, n_threads = 4
0.00.800.821 I 
0.00.800.842 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.800.844 I 
0.00.800.976 I sampler seed: 1234
0.00.800.980 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.800.991 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.800.991 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.800.991 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.635.748 I llama_perf_sampler_print:    sampling time =       1.34 ms /    71 runs   (    0.02 ms per token, 52985.07 tokens per second)
0.01.635.749 I llama_perf_context_print:        load time =     788.48 ms
0.01.635.750 I llama_perf_context_print: prompt eval time =      53.89 ms /     7 tokens (    7.70 ms per token,   129.89 tokens per second)
0.01.635.750 I llama_perf_context_print:        eval time =     777.91 ms /    63 runs   (   12.35 ms per token,    80.99 tokens per second)
0.01.635.750 I llama_perf_context_print:       total time =     835.98 ms /    70 tokens
0.01.636.042 I ggml_metal_free: deallocating

real	0m1.653s
user	0m0.112s
sys	0m0.204s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.043 I build: 4612 (0cec062a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.080 I main: llama backend init
0.00.000.082 I main: load the model and apply lora adapter, if any
0.00.009.111 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.966 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.017.970 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.976 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.976 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.977 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.977 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.979 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.980 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.980 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.981 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.981 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.981 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.982 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.982 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.984 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.984 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.984 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.969 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.244 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.180 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.181 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.182 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.182 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.182 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.183 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.027.183 I llama_model_loader: - type  f32:  194 tensors
0.00.027.183 I llama_model_loader: - type q5_1:   97 tensors
0.00.027.183 I llama_model_loader: - type q6_K:    1 tensors
0.00.027.184 I print_info: file format = GGUF V3 (latest)
0.00.027.184 I print_info: file type   = Q5_1
0.00.027.185 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.034.928 I load: special tokens cache size = 25
0.00.040.991 I load: token to piece cache size = 0.2984 MB
0.00.040.994 I print_info: arch             = gptneox
0.00.040.995 I print_info: vocab_only       = 0
0.00.040.995 I print_info: n_ctx_train      = 2048
0.00.040.995 I print_info: n_embd           = 2048
0.00.040.995 I print_info: n_layer          = 24
0.00.040.998 I print_info: n_head           = 16
0.00.040.999 I print_info: n_head_kv        = 16
0.00.040.999 I print_info: n_rot            = 32
0.00.040.999 I print_info: n_swa            = 0
0.00.040.999 I print_info: n_embd_head_k    = 128
0.00.041.001 I print_info: n_embd_head_v    = 128
0.00.041.002 I print_info: n_gqa            = 1
0.00.041.002 I print_info: n_embd_k_gqa     = 2048
0.00.041.003 I print_info: n_embd_v_gqa     = 2048
0.00.041.004 I print_info: f_norm_eps       = 1.0e-05
0.00.041.005 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.006 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.006 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.006 I print_info: f_logit_scale    = 0.0e+00
0.00.041.007 I print_info: n_ff             = 8192
0.00.041.007 I print_info: n_expert         = 0
0.00.041.007 I print_info: n_expert_used    = 0
0.00.041.007 I print_info: causal attn      = 1
0.00.041.009 I print_info: pooling type     = 0
0.00.041.009 I print_info: rope type        = 2
0.00.041.009 I print_info: rope scaling     = linear
0.00.041.010 I print_info: freq_base_train  = 10000.0
0.00.041.010 I print_info: freq_scale_train = 1
0.00.041.010 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.012 I print_info: rope_finetuned   = unknown
0.00.041.012 I print_info: ssm_d_conv       = 0
0.00.041.012 I print_info: ssm_d_inner      = 0
0.00.041.012 I print_info: ssm_d_state      = 0
0.00.041.012 I print_info: ssm_dt_rank      = 0
0.00.041.012 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.012 I print_info: model type       = 1.4B
0.00.041.013 I print_info: model params     = 1.41 B
0.00.041.013 I print_info: general.name     = 1.4B
0.00.041.014 I print_info: vocab type       = BPE
0.00.041.014 I print_info: n_vocab          = 50304
0.00.041.014 I print_info: n_merges         = 50009
0.00.041.014 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.014 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.015 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.015 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.015 I print_info: LF token         = 187 'Ċ'
0.00.041.015 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.015 I print_info: max token length = 1024
0.00.730.718 I load_tensors: offloading 24 repeating layers to GPU
0.00.730.737 I load_tensors: offloading output layer to GPU
0.00.730.738 I load_tensors: offloaded 25/25 layers to GPU
0.00.730.770 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.730.772 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.731.845 I llama_init_from_model: n_seq_max     = 1
0.00.731.856 I llama_init_from_model: n_ctx         = 2048
0.00.731.856 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.731.857 I llama_init_from_model: n_batch       = 2048
0.00.731.857 I llama_init_from_model: n_ubatch      = 512
0.00.731.858 I llama_init_from_model: flash_attn    = 0
0.00.731.860 I llama_init_from_model: freq_base     = 10000.0
0.00.731.861 I llama_init_from_model: freq_scale    = 1
0.00.731.864 I ggml_metal_init: allocating
0.00.731.942 I ggml_metal_init: found device: Apple M4
0.00.731.958 I ggml_metal_init: picking default device: Apple M4
0.00.733.879 I ggml_metal_init: using embedded metal library
0.00.740.524 I ggml_metal_init: GPU name:   Apple M4
0.00.740.529 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.740.529 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.740.530 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.740.531 I ggml_metal_init: simdgroup reduction   = true
0.00.740.531 I ggml_metal_init: simdgroup matrix mul. = true
0.00.740.531 I ggml_metal_init: has residency sets    = true
0.00.740.531 I ggml_metal_init: has bfloat            = true
0.00.740.532 I ggml_metal_init: use bfloat            = true
0.00.740.533 I ggml_metal_init: hasUnifiedMemory      = true
0.00.740.535 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.757.829 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.817.838 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.817.844 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.817.880 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.822.688 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.822.689 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.822.690 I llama_init_from_model: graph nodes  = 967
0.00.822.690 I llama_init_from_model: graph splits = 2
0.00.822.696 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.822.815 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.822.816 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.870.242 I main: llama threadpool init, n_threads = 4
0.00.870.294 I 
0.00.870.374 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.870.377 I 
0.00.870.543 I sampler seed: 1234
0.00.870.551 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.870.577 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.870.578 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.870.578 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.742.443 I llama_perf_sampler_print:    sampling time =       1.38 ms /    71 runs   (    0.02 ms per token, 51523.95 tokens per second)
0.01.742.443 I llama_perf_context_print:        load time =     860.20 ms
0.01.742.444 I llama_perf_context_print: prompt eval time =      42.14 ms /     7 tokens (    6.02 ms per token,   166.10 tokens per second)
0.01.742.446 I llama_perf_context_print:        eval time =     826.75 ms /    63 runs   (   13.12 ms per token,    76.20 tokens per second)
0.01.742.447 I llama_perf_context_print:       total time =     873.13 ms /    70 tokens
0.01.742.710 I ggml_metal_free: deallocating

real	0m1.762s
user	0m0.111s
sys	0m0.232s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4612 (0cec062a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.075 I main: llama backend init
0.00.000.077 I main: load the model and apply lora adapter, if any
0.00.010.245 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.831 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.837 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.843 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.843 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.843 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.844 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.844 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.847 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.847 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.847 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.848 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.848 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.848 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.852 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.853 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.853 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.854 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.727 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.744 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.615 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.616 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.617 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.617 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.617 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.618 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.618 I llama_model_loader: - type  f32:  194 tensors
0.00.025.619 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.619 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.619 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.620 I print_info: file format = GGUF V3 (latest)
0.00.025.620 I print_info: file type   = Q2_K - Medium
0.00.025.621 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.033.710 I load: special tokens cache size = 25
0.00.039.836 I load: token to piece cache size = 0.2984 MB
0.00.039.839 I print_info: arch             = gptneox
0.00.039.839 I print_info: vocab_only       = 0
0.00.039.839 I print_info: n_ctx_train      = 2048
0.00.039.840 I print_info: n_embd           = 2048
0.00.039.840 I print_info: n_layer          = 24
0.00.039.842 I print_info: n_head           = 16
0.00.039.843 I print_info: n_head_kv        = 16
0.00.039.845 I print_info: n_rot            = 32
0.00.039.846 I print_info: n_swa            = 0
0.00.039.846 I print_info: n_embd_head_k    = 128
0.00.039.846 I print_info: n_embd_head_v    = 128
0.00.039.847 I print_info: n_gqa            = 1
0.00.039.847 I print_info: n_embd_k_gqa     = 2048
0.00.039.848 I print_info: n_embd_v_gqa     = 2048
0.00.039.848 I print_info: f_norm_eps       = 1.0e-05
0.00.039.849 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.849 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.849 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.849 I print_info: f_logit_scale    = 0.0e+00
0.00.039.850 I print_info: n_ff             = 8192
0.00.039.850 I print_info: n_expert         = 0
0.00.039.853 I print_info: n_expert_used    = 0
0.00.039.853 I print_info: causal attn      = 1
0.00.039.853 I print_info: pooling type     = 0
0.00.039.853 I print_info: rope type        = 2
0.00.039.854 I print_info: rope scaling     = linear
0.00.039.854 I print_info: freq_base_train  = 10000.0
0.00.039.854 I print_info: freq_scale_train = 1
0.00.039.854 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.855 I print_info: rope_finetuned   = unknown
0.00.039.855 I print_info: ssm_d_conv       = 0
0.00.039.855 I print_info: ssm_d_inner      = 0
0.00.039.855 I print_info: ssm_d_state      = 0
0.00.039.855 I print_info: ssm_dt_rank      = 0
0.00.039.855 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.856 I print_info: model type       = 1.4B
0.00.039.856 I print_info: model params     = 1.41 B
0.00.039.856 I print_info: general.name     = 1.4B
0.00.039.857 I print_info: vocab type       = BPE
0.00.039.857 I print_info: n_vocab          = 50304
0.00.039.857 I print_info: n_merges         = 50009
0.00.039.857 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.858 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.859 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.860 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.860 I print_info: LF token         = 187 'Ċ'
0.00.039.860 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.860 I print_info: max token length = 1024
0.00.417.310 I load_tensors: offloading 24 repeating layers to GPU
0.00.417.326 I load_tensors: offloading output layer to GPU
0.00.417.327 I load_tensors: offloaded 25/25 layers to GPU
0.00.417.364 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.417.368 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.418.878 I llama_init_from_model: n_seq_max     = 1
0.00.418.883 I llama_init_from_model: n_ctx         = 2048
0.00.418.883 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.418.884 I llama_init_from_model: n_batch       = 2048
0.00.418.884 I llama_init_from_model: n_ubatch      = 512
0.00.418.884 I llama_init_from_model: flash_attn    = 0
0.00.418.887 I llama_init_from_model: freq_base     = 10000.0
0.00.418.887 I llama_init_from_model: freq_scale    = 1
0.00.418.890 I ggml_metal_init: allocating
0.00.418.971 I ggml_metal_init: found device: Apple M4
0.00.418.985 I ggml_metal_init: picking default device: Apple M4
0.00.420.716 I ggml_metal_init: using embedded metal library
0.00.426.302 I ggml_metal_init: GPU name:   Apple M4
0.00.426.320 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.426.321 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.426.321 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.426.322 I ggml_metal_init: simdgroup reduction   = true
0.00.426.322 I ggml_metal_init: simdgroup matrix mul. = true
0.00.426.323 I ggml_metal_init: has residency sets    = true
0.00.426.323 I ggml_metal_init: has bfloat            = true
0.00.426.323 I ggml_metal_init: use bfloat            = true
0.00.426.328 I ggml_metal_init: hasUnifiedMemory      = true
0.00.426.332 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.447.690 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.501.277 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.501.283 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.501.321 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.505.068 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.505.069 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.505.070 I llama_init_from_model: graph nodes  = 967
0.00.505.070 I llama_init_from_model: graph splits = 2
0.00.505.076 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.505.196 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.505.197 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.563.846 I main: llama threadpool init, n_threads = 4
0.00.563.889 I 
0.00.563.913 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.563.913 I 
0.00.564.066 I sampler seed: 1234
0.00.564.070 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.564.081 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.564.081 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.564.081 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.234.169 I llama_perf_sampler_print:    sampling time =       1.34 ms /    71 runs   (    0.02 ms per token, 52866.72 tokens per second)
0.01.234.169 I llama_perf_context_print:        load time =     552.69 ms
0.01.234.170 I llama_perf_context_print: prompt eval time =      35.45 ms /     7 tokens (    5.06 ms per token,   197.47 tokens per second)
0.01.234.171 I llama_perf_context_print:        eval time =     631.77 ms /    63 runs   (   10.03 ms per token,    99.72 tokens per second)
0.01.234.171 I llama_perf_context_print:       total time =     671.24 ms /    70 tokens
0.01.234.375 I ggml_metal_free: deallocating

real	0m1.253s
user	0m0.114s
sys	0m0.163s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.042 I build: 4612 (0cec062a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.075 I main: llama backend init
0.00.000.077 I main: load the model and apply lora adapter, if any
0.00.009.021 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.751 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.756 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.761 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.762 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.762 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.763 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.763 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.764 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.764 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.765 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.765 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.765 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.766 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.766 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.768 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.768 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.768 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.683 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.688 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.570 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.571 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.571 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.571 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.572 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.572 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.573 I llama_model_loader: - type  f32:  194 tensors
0.00.025.573 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.573 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.573 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.574 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.574 I print_info: file format = GGUF V3 (latest)
0.00.025.575 I print_info: file type   = Q3_K - Medium
0.00.025.577 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.033.337 I load: special tokens cache size = 25
0.00.039.201 I load: token to piece cache size = 0.2984 MB
0.00.039.204 I print_info: arch             = gptneox
0.00.039.204 I print_info: vocab_only       = 0
0.00.039.204 I print_info: n_ctx_train      = 2048
0.00.039.204 I print_info: n_embd           = 2048
0.00.039.204 I print_info: n_layer          = 24
0.00.039.207 I print_info: n_head           = 16
0.00.039.208 I print_info: n_head_kv        = 16
0.00.039.208 I print_info: n_rot            = 32
0.00.039.208 I print_info: n_swa            = 0
0.00.039.210 I print_info: n_embd_head_k    = 128
0.00.039.210 I print_info: n_embd_head_v    = 128
0.00.039.211 I print_info: n_gqa            = 1
0.00.039.212 I print_info: n_embd_k_gqa     = 2048
0.00.039.212 I print_info: n_embd_v_gqa     = 2048
0.00.039.213 I print_info: f_norm_eps       = 1.0e-05
0.00.039.213 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.213 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.213 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.214 I print_info: f_logit_scale    = 0.0e+00
0.00.039.214 I print_info: n_ff             = 8192
0.00.039.214 I print_info: n_expert         = 0
0.00.039.215 I print_info: n_expert_used    = 0
0.00.039.215 I print_info: causal attn      = 1
0.00.039.215 I print_info: pooling type     = 0
0.00.039.215 I print_info: rope type        = 2
0.00.039.216 I print_info: rope scaling     = linear
0.00.039.216 I print_info: freq_base_train  = 10000.0
0.00.039.216 I print_info: freq_scale_train = 1
0.00.039.216 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.216 I print_info: rope_finetuned   = unknown
0.00.039.217 I print_info: ssm_d_conv       = 0
0.00.039.217 I print_info: ssm_d_inner      = 0
0.00.039.217 I print_info: ssm_d_state      = 0
0.00.039.217 I print_info: ssm_dt_rank      = 0
0.00.039.219 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.219 I print_info: model type       = 1.4B
0.00.039.219 I print_info: model params     = 1.41 B
0.00.039.219 I print_info: general.name     = 1.4B
0.00.039.220 I print_info: vocab type       = BPE
0.00.039.220 I print_info: n_vocab          = 50304
0.00.039.221 I print_info: n_merges         = 50009
0.00.039.221 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.221 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.222 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.222 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.222 I print_info: LF token         = 187 'Ċ'
0.00.039.223 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.223 I print_info: max token length = 1024
0.00.470.580 I load_tensors: offloading 24 repeating layers to GPU
0.00.470.589 I load_tensors: offloading output layer to GPU
0.00.470.590 I load_tensors: offloaded 25/25 layers to GPU
0.00.470.614 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.470.616 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.471.829 I llama_init_from_model: n_seq_max     = 1
0.00.471.834 I llama_init_from_model: n_ctx         = 2048
0.00.471.835 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.471.836 I llama_init_from_model: n_batch       = 2048
0.00.471.836 I llama_init_from_model: n_ubatch      = 512
0.00.471.836 I llama_init_from_model: flash_attn    = 0
0.00.471.838 I llama_init_from_model: freq_base     = 10000.0
0.00.471.839 I llama_init_from_model: freq_scale    = 1
0.00.471.841 I ggml_metal_init: allocating
0.00.471.892 I ggml_metal_init: found device: Apple M4
0.00.471.905 I ggml_metal_init: picking default device: Apple M4
0.00.473.529 I ggml_metal_init: using embedded metal library
0.00.479.011 I ggml_metal_init: GPU name:   Apple M4
0.00.479.022 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.479.022 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.479.023 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.479.024 I ggml_metal_init: simdgroup reduction   = true
0.00.479.024 I ggml_metal_init: simdgroup matrix mul. = true
0.00.479.025 I ggml_metal_init: has residency sets    = true
0.00.479.025 I ggml_metal_init: has bfloat            = true
0.00.479.025 I ggml_metal_init: use bfloat            = true
0.00.479.027 I ggml_metal_init: hasUnifiedMemory      = true
0.00.479.031 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.500.027 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.559.417 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.559.425 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.559.461 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.564.478 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.564.480 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.564.481 I llama_init_from_model: graph nodes  = 967
0.00.564.481 I llama_init_from_model: graph splits = 2
0.00.564.487 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.564.600 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.564.600 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.622.710 I main: llama threadpool init, n_threads = 4
0.00.622.754 I 
0.00.622.777 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.622.778 I 
0.00.622.927 I sampler seed: 1234
0.00.622.932 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.622.942 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.622.943 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.622.943 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.367.008 I llama_perf_sampler_print:    sampling time =       1.39 ms /    71 runs   (    0.02 ms per token, 51152.74 tokens per second)
0.01.367.008 I llama_perf_context_print:        load time =     612.76 ms
0.01.367.009 I llama_perf_context_print: prompt eval time =      50.01 ms /     7 tokens (    7.14 ms per token,   139.97 tokens per second)
0.01.367.010 I llama_perf_context_print:        eval time =     691.04 ms /    63 runs   (   10.97 ms per token,    91.17 tokens per second)
0.01.367.010 I llama_perf_context_print:       total time =     745.23 ms /    70 tokens
0.01.367.201 I ggml_metal_free: deallocating

real	0m1.382s
user	0m0.110s
sys	0m0.199s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4612 (0cec062a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.074 I main: llama backend init
0.00.000.076 I main: load the model and apply lora adapter, if any
0.00.008.790 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.271 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.276 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.277 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.278 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.278 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.279 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.279 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.280 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.280 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.280 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.283 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.283 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.283 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.284 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.288 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.288 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.288 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.227 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.226 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.136 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.137 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.137 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.137 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.138 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.138 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.139 I llama_model_loader: - type  f32:  194 tensors
0.00.025.139 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.139 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.139 I llama_model_loader: - type q6_K:   13 tensors
0.00.025.140 I print_info: file format = GGUF V3 (latest)
0.00.025.141 I print_info: file type   = Q4_K - Medium
0.00.025.142 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.032.828 I load: special tokens cache size = 25
0.00.038.606 I load: token to piece cache size = 0.2984 MB
0.00.038.609 I print_info: arch             = gptneox
0.00.038.609 I print_info: vocab_only       = 0
0.00.038.609 I print_info: n_ctx_train      = 2048
0.00.038.609 I print_info: n_embd           = 2048
0.00.038.609 I print_info: n_layer          = 24
0.00.038.612 I print_info: n_head           = 16
0.00.038.613 I print_info: n_head_kv        = 16
0.00.038.613 I print_info: n_rot            = 32
0.00.038.614 I print_info: n_swa            = 0
0.00.038.616 I print_info: n_embd_head_k    = 128
0.00.038.616 I print_info: n_embd_head_v    = 128
0.00.038.616 I print_info: n_gqa            = 1
0.00.038.617 I print_info: n_embd_k_gqa     = 2048
0.00.038.618 I print_info: n_embd_v_gqa     = 2048
0.00.038.618 I print_info: f_norm_eps       = 1.0e-05
0.00.038.619 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.619 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.619 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.619 I print_info: f_logit_scale    = 0.0e+00
0.00.038.620 I print_info: n_ff             = 8192
0.00.038.620 I print_info: n_expert         = 0
0.00.038.620 I print_info: n_expert_used    = 0
0.00.038.621 I print_info: causal attn      = 1
0.00.038.623 I print_info: pooling type     = 0
0.00.038.624 I print_info: rope type        = 2
0.00.038.625 I print_info: rope scaling     = linear
0.00.038.625 I print_info: freq_base_train  = 10000.0
0.00.038.625 I print_info: freq_scale_train = 1
0.00.038.626 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.626 I print_info: rope_finetuned   = unknown
0.00.038.626 I print_info: ssm_d_conv       = 0
0.00.038.626 I print_info: ssm_d_inner      = 0
0.00.038.626 I print_info: ssm_d_state      = 0
0.00.038.627 I print_info: ssm_dt_rank      = 0
0.00.038.627 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.628 I print_info: model type       = 1.4B
0.00.038.629 I print_info: model params     = 1.41 B
0.00.038.629 I print_info: general.name     = 1.4B
0.00.038.629 I print_info: vocab type       = BPE
0.00.038.630 I print_info: n_vocab          = 50304
0.00.038.630 I print_info: n_merges         = 50009
0.00.038.630 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.632 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.632 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.632 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.632 I print_info: LF token         = 187 'Ċ'
0.00.038.632 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.633 I print_info: max token length = 1024
0.00.520.383 I load_tensors: offloading 24 repeating layers to GPU
0.00.520.399 I load_tensors: offloading output layer to GPU
0.00.520.399 I load_tensors: offloaded 25/25 layers to GPU
0.00.520.429 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.520.430 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.521.854 I llama_init_from_model: n_seq_max     = 1
0.00.521.863 I llama_init_from_model: n_ctx         = 2048
0.00.521.863 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.521.864 I llama_init_from_model: n_batch       = 2048
0.00.521.864 I llama_init_from_model: n_ubatch      = 512
0.00.521.864 I llama_init_from_model: flash_attn    = 0
0.00.521.865 I llama_init_from_model: freq_base     = 10000.0
0.00.521.870 I llama_init_from_model: freq_scale    = 1
0.00.521.872 I ggml_metal_init: allocating
0.00.521.921 I ggml_metal_init: found device: Apple M4
0.00.521.935 I ggml_metal_init: picking default device: Apple M4
0.00.523.686 I ggml_metal_init: using embedded metal library
0.00.530.229 I ggml_metal_init: GPU name:   Apple M4
0.00.530.234 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.530.235 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.530.236 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.530.237 I ggml_metal_init: simdgroup reduction   = true
0.00.530.237 I ggml_metal_init: simdgroup matrix mul. = true
0.00.530.238 I ggml_metal_init: has residency sets    = true
0.00.530.238 I ggml_metal_init: has bfloat            = true
0.00.530.238 I ggml_metal_init: use bfloat            = true
0.00.530.239 I ggml_metal_init: hasUnifiedMemory      = true
0.00.530.241 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.548.809 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.608.999 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.609.007 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.609.043 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.613.298 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.613.300 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.613.300 I llama_init_from_model: graph nodes  = 967
0.00.613.301 I llama_init_from_model: graph splits = 2
0.00.613.306 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.613.431 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.613.432 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.674.078 I main: llama threadpool init, n_threads = 4
0.00.674.116 I 
0.00.674.139 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.674.140 I 
0.00.674.317 I sampler seed: 1234
0.00.674.321 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.674.362 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.674.364 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.674.364 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.437.884 I llama_perf_sampler_print:    sampling time =       1.47 ms /    71 runs   (    0.02 ms per token, 48365.12 tokens per second)
0.01.437.885 I llama_perf_context_print:        load time =     664.36 ms
0.01.437.888 I llama_perf_context_print: prompt eval time =      57.33 ms /     7 tokens (    8.19 ms per token,   122.11 tokens per second)
0.01.437.889 I llama_perf_context_print:        eval time =     703.67 ms /    63 runs   (   11.17 ms per token,    89.53 tokens per second)
0.01.437.889 I llama_perf_context_print:       total time =     764.73 ms /    70 tokens
0.01.438.183 I ggml_metal_free: deallocating

real	0m1.454s
user	0m0.110s
sys	0m0.200s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.046 I build: 4612 (0cec062a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.079 I main: llama backend init
0.00.000.082 I main: load the model and apply lora adapter, if any
0.00.012.101 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.024.532 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.024.539 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.024.541 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.024.541 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.024.542 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.024.542 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.024.542 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.024.543 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.024.544 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.024.544 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.024.545 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.024.545 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.024.545 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.024.546 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.024.548 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.024.548 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.024.548 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.028.584 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.029.697 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.033.667 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.033.668 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.033.669 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.033.669 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.033.669 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.033.670 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.033.670 I llama_model_loader: - type  f32:  194 tensors
0.00.033.671 I llama_model_loader: - type q5_K:   61 tensors
0.00.033.671 I llama_model_loader: - type q6_K:   37 tensors
0.00.033.672 I print_info: file format = GGUF V3 (latest)
0.00.033.672 I print_info: file type   = Q5_K - Medium
0.00.033.673 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.041.783 I load: special tokens cache size = 25
0.00.047.738 I load: token to piece cache size = 0.2984 MB
0.00.047.742 I print_info: arch             = gptneox
0.00.047.742 I print_info: vocab_only       = 0
0.00.047.742 I print_info: n_ctx_train      = 2048
0.00.047.742 I print_info: n_embd           = 2048
0.00.047.743 I print_info: n_layer          = 24
0.00.047.746 I print_info: n_head           = 16
0.00.047.747 I print_info: n_head_kv        = 16
0.00.047.747 I print_info: n_rot            = 32
0.00.047.748 I print_info: n_swa            = 0
0.00.047.749 I print_info: n_embd_head_k    = 128
0.00.047.749 I print_info: n_embd_head_v    = 128
0.00.047.751 I print_info: n_gqa            = 1
0.00.047.752 I print_info: n_embd_k_gqa     = 2048
0.00.047.752 I print_info: n_embd_v_gqa     = 2048
0.00.047.753 I print_info: f_norm_eps       = 1.0e-05
0.00.047.753 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.047.755 I print_info: f_clamp_kqv      = 0.0e+00
0.00.047.755 I print_info: f_max_alibi_bias = 0.0e+00
0.00.047.755 I print_info: f_logit_scale    = 0.0e+00
0.00.047.755 I print_info: n_ff             = 8192
0.00.047.757 I print_info: n_expert         = 0
0.00.047.757 I print_info: n_expert_used    = 0
0.00.047.757 I print_info: causal attn      = 1
0.00.047.757 I print_info: pooling type     = 0
0.00.047.757 I print_info: rope type        = 2
0.00.047.757 I print_info: rope scaling     = linear
0.00.047.758 I print_info: freq_base_train  = 10000.0
0.00.047.758 I print_info: freq_scale_train = 1
0.00.047.758 I print_info: n_ctx_orig_yarn  = 2048
0.00.047.758 I print_info: rope_finetuned   = unknown
0.00.047.758 I print_info: ssm_d_conv       = 0
0.00.047.759 I print_info: ssm_d_inner      = 0
0.00.047.759 I print_info: ssm_d_state      = 0
0.00.047.759 I print_info: ssm_dt_rank      = 0
0.00.047.759 I print_info: ssm_dt_b_c_rms   = 0
0.00.047.759 I print_info: model type       = 1.4B
0.00.047.759 I print_info: model params     = 1.41 B
0.00.047.761 I print_info: general.name     = 1.4B
0.00.047.761 I print_info: vocab type       = BPE
0.00.047.761 I print_info: n_vocab          = 50304
0.00.047.762 I print_info: n_merges         = 50009
0.00.047.762 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.047.762 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.047.762 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.047.762 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.047.763 I print_info: LF token         = 187 'Ċ'
0.00.047.763 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.047.789 I print_info: max token length = 1024
0.00.623.528 I load_tensors: offloading 24 repeating layers to GPU
0.00.623.541 I load_tensors: offloading output layer to GPU
0.00.623.541 I load_tensors: offloaded 25/25 layers to GPU
0.00.623.569 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.623.570 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.624.656 I llama_init_from_model: n_seq_max     = 1
0.00.624.659 I llama_init_from_model: n_ctx         = 2048
0.00.624.660 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.624.661 I llama_init_from_model: n_batch       = 2048
0.00.624.661 I llama_init_from_model: n_ubatch      = 512
0.00.624.662 I llama_init_from_model: flash_attn    = 0
0.00.624.664 I llama_init_from_model: freq_base     = 10000.0
0.00.624.664 I llama_init_from_model: freq_scale    = 1
0.00.624.667 I ggml_metal_init: allocating
0.00.624.726 I ggml_metal_init: found device: Apple M4
0.00.624.740 I ggml_metal_init: picking default device: Apple M4
0.00.626.641 I ggml_metal_init: using embedded metal library
0.00.631.559 I ggml_metal_init: GPU name:   Apple M4
0.00.631.563 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.631.564 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.631.564 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.631.565 I ggml_metal_init: simdgroup reduction   = true
0.00.631.565 I ggml_metal_init: simdgroup matrix mul. = true
0.00.631.565 I ggml_metal_init: has residency sets    = true
0.00.631.565 I ggml_metal_init: has bfloat            = true
0.00.631.566 I ggml_metal_init: use bfloat            = true
0.00.631.567 I ggml_metal_init: hasUnifiedMemory      = true
0.00.631.569 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.643.225 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.675.507 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.675.515 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.675.552 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.680.607 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.680.609 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.680.609 I llama_init_from_model: graph nodes  = 967
0.00.680.610 I llama_init_from_model: graph splits = 2
0.00.680.620 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.680.744 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.680.745 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.742.661 I main: llama threadpool init, n_threads = 4
0.00.742.708 I 
0.00.742.733 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.742.733 I 
0.00.742.915 I sampler seed: 1234
0.00.742.920 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.742.941 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.742.941 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.742.941 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.582.980 I llama_perf_sampler_print:    sampling time =       1.29 ms /    71 runs   (    0.02 ms per token, 54911.06 tokens per second)
0.01.582.981 I llama_perf_context_print:        load time =     729.62 ms
0.01.582.981 I llama_perf_context_print: prompt eval time =      51.19 ms /     7 tokens (    7.31 ms per token,   136.73 tokens per second)
0.01.582.982 I llama_perf_context_print:        eval time =     786.01 ms /    63 runs   (   12.48 ms per token,    80.15 tokens per second)
0.01.582.982 I llama_perf_context_print:       total time =     841.26 ms /    70 tokens
0.01.583.218 I ggml_metal_free: deallocating

real	0m1.602s
user	0m0.104s
sys	0m0.174s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4612 (0cec062a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.074 I main: llama backend init
0.00.000.076 I main: load the model and apply lora adapter, if any
0.00.009.071 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.851 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.856 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.862 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.862 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.863 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.863 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.864 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.865 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.865 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.867 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.867 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.868 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.868 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.869 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.870 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.871 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.871 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.902 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.973 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.891 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.892 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.892 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.893 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.893 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.893 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.894 I llama_model_loader: - type  f32:  194 tensors
0.00.025.894 I llama_model_loader: - type q6_K:   98 tensors
0.00.025.895 I print_info: file format = GGUF V3 (latest)
0.00.025.895 I print_info: file type   = Q6_K
0.00.025.896 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.033.622 I load: special tokens cache size = 25
0.00.039.411 I load: token to piece cache size = 0.2984 MB
0.00.039.414 I print_info: arch             = gptneox
0.00.039.414 I print_info: vocab_only       = 0
0.00.039.415 I print_info: n_ctx_train      = 2048
0.00.039.415 I print_info: n_embd           = 2048
0.00.039.415 I print_info: n_layer          = 24
0.00.039.418 I print_info: n_head           = 16
0.00.039.419 I print_info: n_head_kv        = 16
0.00.039.419 I print_info: n_rot            = 32
0.00.039.419 I print_info: n_swa            = 0
0.00.039.419 I print_info: n_embd_head_k    = 128
0.00.039.420 I print_info: n_embd_head_v    = 128
0.00.039.420 I print_info: n_gqa            = 1
0.00.039.421 I print_info: n_embd_k_gqa     = 2048
0.00.039.422 I print_info: n_embd_v_gqa     = 2048
0.00.039.422 I print_info: f_norm_eps       = 1.0e-05
0.00.039.423 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.423 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.423 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.423 I print_info: f_logit_scale    = 0.0e+00
0.00.039.424 I print_info: n_ff             = 8192
0.00.039.424 I print_info: n_expert         = 0
0.00.039.424 I print_info: n_expert_used    = 0
0.00.039.425 I print_info: causal attn      = 1
0.00.039.425 I print_info: pooling type     = 0
0.00.039.425 I print_info: rope type        = 2
0.00.039.425 I print_info: rope scaling     = linear
0.00.039.427 I print_info: freq_base_train  = 10000.0
0.00.039.427 I print_info: freq_scale_train = 1
0.00.039.428 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.428 I print_info: rope_finetuned   = unknown
0.00.039.428 I print_info: ssm_d_conv       = 0
0.00.039.428 I print_info: ssm_d_inner      = 0
0.00.039.428 I print_info: ssm_d_state      = 0
0.00.039.428 I print_info: ssm_dt_rank      = 0
0.00.039.428 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.429 I print_info: model type       = 1.4B
0.00.039.429 I print_info: model params     = 1.41 B
0.00.039.429 I print_info: general.name     = 1.4B
0.00.039.430 I print_info: vocab type       = BPE
0.00.039.430 I print_info: n_vocab          = 50304
0.00.039.430 I print_info: n_merges         = 50009
0.00.039.430 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.430 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.431 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.431 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.431 I print_info: LF token         = 187 'Ċ'
0.00.039.431 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.431 I print_info: max token length = 1024
0.00.661.647 I load_tensors: offloading 24 repeating layers to GPU
0.00.661.661 I load_tensors: offloading output layer to GPU
0.00.661.661 I load_tensors: offloaded 25/25 layers to GPU
0.00.661.692 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.661.694 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.662.884 I llama_init_from_model: n_seq_max     = 1
0.00.662.886 I llama_init_from_model: n_ctx         = 2048
0.00.662.887 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.662.887 I llama_init_from_model: n_batch       = 2048
0.00.662.888 I llama_init_from_model: n_ubatch      = 512
0.00.662.889 I llama_init_from_model: flash_attn    = 0
0.00.662.890 I llama_init_from_model: freq_base     = 10000.0
0.00.662.890 I llama_init_from_model: freq_scale    = 1
0.00.662.892 I ggml_metal_init: allocating
0.00.662.912 I ggml_metal_init: found device: Apple M4
0.00.662.921 I ggml_metal_init: picking default device: Apple M4
0.00.664.357 I ggml_metal_init: using embedded metal library
0.00.670.538 I ggml_metal_init: GPU name:   Apple M4
0.00.670.542 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.670.543 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.670.544 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.670.544 I ggml_metal_init: simdgroup reduction   = true
0.00.670.545 I ggml_metal_init: simdgroup matrix mul. = true
0.00.670.545 I ggml_metal_init: has residency sets    = true
0.00.670.545 I ggml_metal_init: has bfloat            = true
0.00.670.546 I ggml_metal_init: use bfloat            = true
0.00.670.547 I ggml_metal_init: hasUnifiedMemory      = true
0.00.670.548 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.687.765 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.741.729 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.741.736 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.741.769 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.745.986 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.745.988 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.745.989 I llama_init_from_model: graph nodes  = 967
0.00.745.989 I llama_init_from_model: graph splits = 2
0.00.745.995 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.746.106 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.746.107 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.810.291 I main: llama threadpool init, n_threads = 4
0.00.810.334 I 
0.00.810.360 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.810.360 I 
0.00.810.513 I sampler seed: 1234
0.00.810.517 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.810.552 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.810.555 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.810.555 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.681.069 I llama_perf_sampler_print:    sampling time =       1.32 ms /    71 runs   (    0.02 ms per token, 53747.16 tokens per second)
0.01.681.070 I llama_perf_context_print:        load time =     800.28 ms
0.01.681.071 I llama_perf_context_print: prompt eval time =      54.34 ms /     7 tokens (    7.76 ms per token,   128.81 tokens per second)
0.01.681.072 I llama_perf_context_print:        eval time =     813.30 ms /    63 runs   (   12.91 ms per token,    77.46 tokens per second)
0.01.681.073 I llama_perf_context_print:       total time =     871.71 ms /    70 tokens
0.01.681.302 I ggml_metal_free: deallocating

real	0m1.700s
user	0m0.108s
sys	0m0.228s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.945 I build: 4612 (0cec062a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.024.604 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.039.104 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.039.109 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.039.111 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.039.111 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.039.112 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.039.116 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.039.117 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.039.118 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.039.119 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.039.119 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.039.120 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.039.120 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.039.121 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.039.121 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.039.123 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.039.124 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.039.124 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.047.127 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.049.120 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.055.848 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.055.850 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.055.850 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.055.851 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.055.851 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.055.851 I llama_model_loader: - type  f32:  194 tensors
0.00.055.852 I llama_model_loader: - type  f16:   98 tensors
0.00.055.853 I print_info: file format = GGUF V3 (latest)
0.00.055.854 I print_info: file type   = all F32 (guessed)
0.00.055.855 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.067.389 I load: special tokens cache size = 25
0.00.074.968 I load: token to piece cache size = 0.2984 MB
0.00.074.971 I print_info: arch             = gptneox
0.00.074.971 I print_info: vocab_only       = 0
0.00.074.971 I print_info: n_ctx_train      = 2048
0.00.074.971 I print_info: n_embd           = 2048
0.00.074.971 I print_info: n_layer          = 24
0.00.074.974 I print_info: n_head           = 16
0.00.074.975 I print_info: n_head_kv        = 16
0.00.074.975 I print_info: n_rot            = 32
0.00.074.977 I print_info: n_swa            = 0
0.00.074.977 I print_info: n_embd_head_k    = 128
0.00.074.978 I print_info: n_embd_head_v    = 128
0.00.074.978 I print_info: n_gqa            = 1
0.00.074.979 I print_info: n_embd_k_gqa     = 2048
0.00.074.980 I print_info: n_embd_v_gqa     = 2048
0.00.074.980 I print_info: f_norm_eps       = 1.0e-05
0.00.074.980 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.074.980 I print_info: f_clamp_kqv      = 0.0e+00
0.00.074.981 I print_info: f_max_alibi_bias = 0.0e+00
0.00.074.981 I print_info: f_logit_scale    = 0.0e+00
0.00.074.981 I print_info: n_ff             = 8192
0.00.074.982 I print_info: n_expert         = 0
0.00.074.982 I print_info: n_expert_used    = 0
0.00.074.982 I print_info: causal attn      = 1
0.00.074.982 I print_info: pooling type     = 0
0.00.074.982 I print_info: rope type        = 2
0.00.074.982 I print_info: rope scaling     = linear
0.00.074.983 I print_info: freq_base_train  = 10000.0
0.00.074.983 I print_info: freq_scale_train = 1
0.00.074.983 I print_info: n_ctx_orig_yarn  = 2048
0.00.074.983 I print_info: rope_finetuned   = unknown
0.00.074.984 I print_info: ssm_d_conv       = 0
0.00.074.984 I print_info: ssm_d_inner      = 0
0.00.074.984 I print_info: ssm_d_state      = 0
0.00.074.984 I print_info: ssm_dt_rank      = 0
0.00.074.984 I print_info: ssm_dt_b_c_rms   = 0
0.00.074.984 I print_info: model type       = 1.4B
0.00.074.985 I print_info: model params     = 1.41 B
0.00.074.985 I print_info: general.name     = 1.4B
0.00.074.985 I print_info: vocab type       = BPE
0.00.074.986 I print_info: n_vocab          = 50304
0.00.074.986 I print_info: n_merges         = 50009
0.00.074.986 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.074.986 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.074.986 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.074.987 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.074.987 I print_info: LF token         = 187 'Ċ'
0.00.074.988 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.074.988 I print_info: max token length = 1024
0.01.412.856 I load_tensors: offloading 24 repeating layers to GPU
0.01.412.866 I load_tensors: offloading output layer to GPU
0.01.412.868 I load_tensors: offloaded 25/25 layers to GPU
0.01.412.895 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.412.897 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.01.413.743 I llama_init_from_model: n_seq_max     = 1
0.01.413.744 I llama_init_from_model: n_ctx         = 128
0.01.413.744 I llama_init_from_model: n_ctx_per_seq = 128
0.01.413.745 I llama_init_from_model: n_batch       = 128
0.01.413.745 I llama_init_from_model: n_ubatch      = 128
0.01.413.745 I llama_init_from_model: flash_attn    = 0
0.01.413.746 I llama_init_from_model: freq_base     = 10000.0
0.01.413.746 I llama_init_from_model: freq_scale    = 1
0.01.413.747 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.413.747 I ggml_metal_init: allocating
0.01.413.769 I ggml_metal_init: found device: Apple M4
0.01.413.775 I ggml_metal_init: picking default device: Apple M4
0.01.414.663 I ggml_metal_init: using embedded metal library
0.01.418.583 I ggml_metal_init: GPU name:   Apple M4
0.01.418.586 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.418.587 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.418.587 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.418.588 I ggml_metal_init: simdgroup reduction   = true
0.01.418.588 I ggml_metal_init: simdgroup matrix mul. = true
0.01.418.588 I ggml_metal_init: has residency sets    = true
0.01.418.588 I ggml_metal_init: has bfloat            = true
0.01.418.588 I ggml_metal_init: use bfloat            = true
0.01.418.589 I ggml_metal_init: hasUnifiedMemory      = true
0.01.418.590 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.429.552 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.431.231 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.431.233 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.431.267 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.432.857 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.432.858 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.432.859 I llama_init_from_model: graph nodes  = 967
0.01.432.859 I llama_init_from_model: graph splits = 2
0.01.432.860 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.432.860 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.467.960 I 
0.01.467.995 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.467.999 I perplexity: tokenizing the input ..
0.01.473.209 I perplexity: tokenization took 5.208 ms
0.01.473.213 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.591.637 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.592.978 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.593.018 I llama_perf_context_print:        load time =    1443.35 ms
0.01.593.019 I llama_perf_context_print: prompt eval time =     118.12 ms /   128 tokens (    0.92 ms per token,  1083.65 tokens per second)
0.01.593.020 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.593.021 I llama_perf_context_print:       total time =     125.06 ms /   129 tokens
0.01.593.422 I ggml_metal_free: deallocating

real	0m1.784s
user	0m0.096s
sys	0m0.247s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.255 I build: 4612 (0cec062a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.226 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.224 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.018.230 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.232 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.232 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.232 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.233 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.233 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.234 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.234 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.235 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.235 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.236 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.236 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.237 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.238 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.239 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.239 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.288 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.314 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.345 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.346 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.347 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.347 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.347 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.348 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.027.348 I llama_model_loader: - type  f32:  194 tensors
0.00.027.349 I llama_model_loader: - type q8_0:   98 tensors
0.00.027.350 I print_info: file format = GGUF V3 (latest)
0.00.027.350 I print_info: file type   = Q8_0
0.00.027.352 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.036.038 I load: special tokens cache size = 25
0.00.042.263 I load: token to piece cache size = 0.2984 MB
0.00.042.268 I print_info: arch             = gptneox
0.00.042.268 I print_info: vocab_only       = 0
0.00.042.268 I print_info: n_ctx_train      = 2048
0.00.042.268 I print_info: n_embd           = 2048
0.00.042.269 I print_info: n_layer          = 24
0.00.042.273 I print_info: n_head           = 16
0.00.042.273 I print_info: n_head_kv        = 16
0.00.042.273 I print_info: n_rot            = 32
0.00.042.274 I print_info: n_swa            = 0
0.00.042.274 I print_info: n_embd_head_k    = 128
0.00.042.274 I print_info: n_embd_head_v    = 128
0.00.042.275 I print_info: n_gqa            = 1
0.00.042.276 I print_info: n_embd_k_gqa     = 2048
0.00.042.279 I print_info: n_embd_v_gqa     = 2048
0.00.042.280 I print_info: f_norm_eps       = 1.0e-05
0.00.042.281 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.042.281 I print_info: f_clamp_kqv      = 0.0e+00
0.00.042.281 I print_info: f_max_alibi_bias = 0.0e+00
0.00.042.281 I print_info: f_logit_scale    = 0.0e+00
0.00.042.282 I print_info: n_ff             = 8192
0.00.042.282 I print_info: n_expert         = 0
0.00.042.282 I print_info: n_expert_used    = 0
0.00.042.284 I print_info: causal attn      = 1
0.00.042.284 I print_info: pooling type     = 0
0.00.042.284 I print_info: rope type        = 2
0.00.042.284 I print_info: rope scaling     = linear
0.00.042.285 I print_info: freq_base_train  = 10000.0
0.00.042.285 I print_info: freq_scale_train = 1
0.00.042.285 I print_info: n_ctx_orig_yarn  = 2048
0.00.042.287 I print_info: rope_finetuned   = unknown
0.00.042.287 I print_info: ssm_d_conv       = 0
0.00.042.288 I print_info: ssm_d_inner      = 0
0.00.042.288 I print_info: ssm_d_state      = 0
0.00.042.288 I print_info: ssm_dt_rank      = 0
0.00.042.288 I print_info: ssm_dt_b_c_rms   = 0
0.00.042.288 I print_info: model type       = 1.4B
0.00.042.289 I print_info: model params     = 1.41 B
0.00.042.289 I print_info: general.name     = 1.4B
0.00.042.290 I print_info: vocab type       = BPE
0.00.042.290 I print_info: n_vocab          = 50304
0.00.042.290 I print_info: n_merges         = 50009
0.00.042.291 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.042.291 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.042.291 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.042.291 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.042.292 I print_info: LF token         = 187 'Ċ'
0.00.042.292 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.042.299 I print_info: max token length = 1024
0.00.769.456 I load_tensors: offloading 24 repeating layers to GPU
0.00.769.459 I load_tensors: offloading output layer to GPU
0.00.769.460 I load_tensors: offloaded 25/25 layers to GPU
0.00.769.480 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.769.482 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.00.770.626 I llama_init_from_model: n_seq_max     = 1
0.00.770.628 I llama_init_from_model: n_ctx         = 128
0.00.770.628 I llama_init_from_model: n_ctx_per_seq = 128
0.00.770.629 I llama_init_from_model: n_batch       = 128
0.00.770.629 I llama_init_from_model: n_ubatch      = 128
0.00.770.629 I llama_init_from_model: flash_attn    = 0
0.00.770.630 I llama_init_from_model: freq_base     = 10000.0
0.00.770.630 I llama_init_from_model: freq_scale    = 1
0.00.770.631 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.770.632 I ggml_metal_init: allocating
0.00.770.664 I ggml_metal_init: found device: Apple M4
0.00.770.675 I ggml_metal_init: picking default device: Apple M4
0.00.771.860 I ggml_metal_init: using embedded metal library
0.00.777.045 I ggml_metal_init: GPU name:   Apple M4
0.00.777.049 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.777.049 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.777.050 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.777.051 I ggml_metal_init: simdgroup reduction   = true
0.00.777.051 I ggml_metal_init: simdgroup matrix mul. = true
0.00.777.051 I ggml_metal_init: has residency sets    = true
0.00.777.051 I ggml_metal_init: has bfloat            = true
0.00.777.052 I ggml_metal_init: use bfloat            = true
0.00.777.052 I ggml_metal_init: hasUnifiedMemory      = true
0.00.777.053 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.792.270 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.795.597 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.795.604 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.795.653 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.798.762 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.798.764 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.798.764 I llama_init_from_model: graph nodes  = 967
0.00.798.765 I llama_init_from_model: graph splits = 2
0.00.798.768 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.798.769 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.828.696 I 
0.00.828.774 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.828.780 I perplexity: tokenizing the input ..
0.00.835.843 I perplexity: tokenization took 7.062 ms
0.00.835.848 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.973.663 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.00.975.019 I Final estimate: PPL = 10.1362 +/- 3.22437

0.00.975.039 I llama_perf_context_print:        load time =     817.46 ms
0.00.975.040 I llama_perf_context_print: prompt eval time =     137.51 ms /   128 tokens (    1.07 ms per token,   930.81 tokens per second)
0.00.975.040 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.975.041 I llama_perf_context_print:       total time =     146.35 ms /   129 tokens
0.00.975.470 I ggml_metal_free: deallocating

real	0m0.992s
user	0m0.077s
sys	0m0.175s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.249 I build: 4612 (0cec062a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.260 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.522 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.017.527 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.528 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.529 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.529 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.530 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.530 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.531 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.531 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.532 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.532 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.532 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.533 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.533 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.536 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.537 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.537 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.503 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.609 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.553 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.554 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.554 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.555 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.555 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.555 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.026.556 I llama_model_loader: - type  f32:  194 tensors
0.00.026.556 I llama_model_loader: - type q4_0:   97 tensors
0.00.026.557 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.557 I print_info: file format = GGUF V3 (latest)
0.00.026.558 I print_info: file type   = Q4_0
0.00.026.559 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.034.647 I load: special tokens cache size = 25
0.00.040.749 I load: token to piece cache size = 0.2984 MB
0.00.040.751 I print_info: arch             = gptneox
0.00.040.752 I print_info: vocab_only       = 0
0.00.040.752 I print_info: n_ctx_train      = 2048
0.00.040.752 I print_info: n_embd           = 2048
0.00.040.752 I print_info: n_layer          = 24
0.00.040.755 I print_info: n_head           = 16
0.00.040.756 I print_info: n_head_kv        = 16
0.00.040.756 I print_info: n_rot            = 32
0.00.040.759 I print_info: n_swa            = 0
0.00.040.759 I print_info: n_embd_head_k    = 128
0.00.040.759 I print_info: n_embd_head_v    = 128
0.00.040.760 I print_info: n_gqa            = 1
0.00.040.761 I print_info: n_embd_k_gqa     = 2048
0.00.040.762 I print_info: n_embd_v_gqa     = 2048
0.00.040.762 I print_info: f_norm_eps       = 1.0e-05
0.00.040.763 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.763 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.763 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.763 I print_info: f_logit_scale    = 0.0e+00
0.00.040.764 I print_info: n_ff             = 8192
0.00.040.764 I print_info: n_expert         = 0
0.00.040.764 I print_info: n_expert_used    = 0
0.00.040.764 I print_info: causal attn      = 1
0.00.040.764 I print_info: pooling type     = 0
0.00.040.765 I print_info: rope type        = 2
0.00.040.765 I print_info: rope scaling     = linear
0.00.040.765 I print_info: freq_base_train  = 10000.0
0.00.040.766 I print_info: freq_scale_train = 1
0.00.040.766 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.766 I print_info: rope_finetuned   = unknown
0.00.040.766 I print_info: ssm_d_conv       = 0
0.00.040.767 I print_info: ssm_d_inner      = 0
0.00.040.767 I print_info: ssm_d_state      = 0
0.00.040.767 I print_info: ssm_dt_rank      = 0
0.00.040.767 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.767 I print_info: model type       = 1.4B
0.00.040.768 I print_info: model params     = 1.41 B
0.00.040.768 I print_info: general.name     = 1.4B
0.00.040.769 I print_info: vocab type       = BPE
0.00.040.769 I print_info: n_vocab          = 50304
0.00.040.770 I print_info: n_merges         = 50009
0.00.040.770 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.770 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.778 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.779 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.780 I print_info: LF token         = 187 'Ċ'
0.00.040.780 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.780 I print_info: max token length = 1024
0.00.570.397 I load_tensors: offloading 24 repeating layers to GPU
0.00.570.414 I load_tensors: offloading output layer to GPU
0.00.570.415 I load_tensors: offloaded 25/25 layers to GPU
0.00.570.451 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.570.452 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.572.040 I llama_init_from_model: n_seq_max     = 1
0.00.572.044 I llama_init_from_model: n_ctx         = 128
0.00.572.045 I llama_init_from_model: n_ctx_per_seq = 128
0.00.572.046 I llama_init_from_model: n_batch       = 128
0.00.572.046 I llama_init_from_model: n_ubatch      = 128
0.00.572.047 I llama_init_from_model: flash_attn    = 0
0.00.572.049 I llama_init_from_model: freq_base     = 10000.0
0.00.572.049 I llama_init_from_model: freq_scale    = 1
0.00.572.050 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.572.052 I ggml_metal_init: allocating
0.00.572.150 I ggml_metal_init: found device: Apple M4
0.00.572.165 I ggml_metal_init: picking default device: Apple M4
0.00.573.961 I ggml_metal_init: using embedded metal library
0.00.580.715 I ggml_metal_init: GPU name:   Apple M4
0.00.580.720 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.580.721 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.580.722 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.580.722 I ggml_metal_init: simdgroup reduction   = true
0.00.580.723 I ggml_metal_init: simdgroup matrix mul. = true
0.00.580.723 I ggml_metal_init: has residency sets    = true
0.00.580.723 I ggml_metal_init: has bfloat            = true
0.00.580.724 I ggml_metal_init: use bfloat            = true
0.00.580.724 I ggml_metal_init: hasUnifiedMemory      = true
0.00.580.726 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.599.356 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.602.943 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.602.959 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.603.018 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.606.300 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.606.301 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.606.302 I llama_init_from_model: graph nodes  = 967
0.00.606.302 I llama_init_from_model: graph splits = 2
0.00.606.305 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.606.306 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.634.515 I 
0.00.634.611 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.634.642 I perplexity: tokenizing the input ..
0.00.641.661 I perplexity: tokenization took 7.015 ms
0.00.641.667 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.774.649 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.775.987 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.776.010 I llama_perf_context_print:        load time =     624.24 ms
0.00.776.011 I llama_perf_context_print: prompt eval time =     132.04 ms /   128 tokens (    1.03 ms per token,   969.42 tokens per second)
0.00.776.012 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.776.012 I llama_perf_context_print:       total time =     141.50 ms /   129 tokens
0.00.776.392 I ggml_metal_free: deallocating

real	0m0.792s
user	0m0.081s
sys	0m0.115s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.098 I build: 4612 (0cec062a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.922 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.294 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.300 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.302 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.305 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.306 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.310 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.310 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.311 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.311 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.312 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.312 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.312 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.313 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.313 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.315 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.315 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.315 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.307 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.479 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.349 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.350 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.350 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.351 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.351 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.351 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.025.352 I llama_model_loader: - type  f32:  194 tensors
0.00.025.352 I llama_model_loader: - type q4_1:   97 tensors
0.00.025.353 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.353 I print_info: file format = GGUF V3 (latest)
0.00.025.354 I print_info: file type   = Q4_1
0.00.025.355 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.032.951 I load: special tokens cache size = 25
0.00.038.938 I load: token to piece cache size = 0.2984 MB
0.00.038.941 I print_info: arch             = gptneox
0.00.038.941 I print_info: vocab_only       = 0
0.00.038.942 I print_info: n_ctx_train      = 2048
0.00.038.942 I print_info: n_embd           = 2048
0.00.038.942 I print_info: n_layer          = 24
0.00.038.945 I print_info: n_head           = 16
0.00.038.946 I print_info: n_head_kv        = 16
0.00.038.946 I print_info: n_rot            = 32
0.00.038.948 I print_info: n_swa            = 0
0.00.038.948 I print_info: n_embd_head_k    = 128
0.00.038.948 I print_info: n_embd_head_v    = 128
0.00.038.949 I print_info: n_gqa            = 1
0.00.038.949 I print_info: n_embd_k_gqa     = 2048
0.00.038.955 I print_info: n_embd_v_gqa     = 2048
0.00.038.956 I print_info: f_norm_eps       = 1.0e-05
0.00.038.956 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.956 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.956 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.957 I print_info: f_logit_scale    = 0.0e+00
0.00.038.959 I print_info: n_ff             = 8192
0.00.038.959 I print_info: n_expert         = 0
0.00.038.960 I print_info: n_expert_used    = 0
0.00.038.960 I print_info: causal attn      = 1
0.00.038.960 I print_info: pooling type     = 0
0.00.038.960 I print_info: rope type        = 2
0.00.038.960 I print_info: rope scaling     = linear
0.00.038.960 I print_info: freq_base_train  = 10000.0
0.00.038.961 I print_info: freq_scale_train = 1
0.00.038.961 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.961 I print_info: rope_finetuned   = unknown
0.00.038.961 I print_info: ssm_d_conv       = 0
0.00.038.962 I print_info: ssm_d_inner      = 0
0.00.038.962 I print_info: ssm_d_state      = 0
0.00.038.962 I print_info: ssm_dt_rank      = 0
0.00.038.963 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.964 I print_info: model type       = 1.4B
0.00.038.964 I print_info: model params     = 1.41 B
0.00.038.964 I print_info: general.name     = 1.4B
0.00.038.965 I print_info: vocab type       = BPE
0.00.038.965 I print_info: n_vocab          = 50304
0.00.038.965 I print_info: n_merges         = 50009
0.00.038.965 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.965 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.965 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.965 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.966 I print_info: LF token         = 187 'Ċ'
0.00.038.966 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.966 I print_info: max token length = 1024
0.00.625.170 I load_tensors: offloading 24 repeating layers to GPU
0.00.625.186 I load_tensors: offloading output layer to GPU
0.00.625.187 I load_tensors: offloaded 25/25 layers to GPU
0.00.625.223 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.625.224 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.626.406 I llama_init_from_model: n_seq_max     = 1
0.00.626.411 I llama_init_from_model: n_ctx         = 128
0.00.626.412 I llama_init_from_model: n_ctx_per_seq = 128
0.00.626.415 I llama_init_from_model: n_batch       = 128
0.00.626.416 I llama_init_from_model: n_ubatch      = 128
0.00.626.416 I llama_init_from_model: flash_attn    = 0
0.00.626.418 I llama_init_from_model: freq_base     = 10000.0
0.00.626.419 I llama_init_from_model: freq_scale    = 1
0.00.626.419 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.626.421 I ggml_metal_init: allocating
0.00.626.551 I ggml_metal_init: found device: Apple M4
0.00.626.565 I ggml_metal_init: picking default device: Apple M4
0.00.628.462 I ggml_metal_init: using embedded metal library
0.00.634.318 I ggml_metal_init: GPU name:   Apple M4
0.00.634.326 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.634.327 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.634.328 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.634.329 I ggml_metal_init: simdgroup reduction   = true
0.00.634.329 I ggml_metal_init: simdgroup matrix mul. = true
0.00.634.337 I ggml_metal_init: has residency sets    = true
0.00.634.338 I ggml_metal_init: has bfloat            = true
0.00.634.338 I ggml_metal_init: use bfloat            = true
0.00.634.340 I ggml_metal_init: hasUnifiedMemory      = true
0.00.634.341 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.653.545 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.656.986 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.656.989 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.657.031 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.660.129 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.660.131 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.660.132 I llama_init_from_model: graph nodes  = 967
0.00.660.132 I llama_init_from_model: graph splits = 2
0.00.660.135 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.660.135 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.684.234 I 
0.00.684.324 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.684.335 I perplexity: tokenizing the input ..
0.00.690.996 I perplexity: tokenization took 6.659 ms
0.00.691.001 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.825.634 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.827.048 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.827.071 I llama_perf_context_print:        load time =     675.30 ms
0.00.827.072 I llama_perf_context_print: prompt eval time =     134.09 ms /   128 tokens (    1.05 ms per token,   954.62 tokens per second)
0.00.827.073 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.827.073 I llama_perf_context_print:       total time =     142.84 ms /   129 tokens
0.00.827.438 I ggml_metal_free: deallocating

real	0m0.842s
user	0m0.078s
sys	0m0.131s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.099 I build: 4612 (0cec062a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.997 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.131 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.018.136 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.143 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.144 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.144 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.145 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.145 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.146 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.146 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.148 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.148 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.149 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.149 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.150 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.151 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.152 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.152 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.034 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.028 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.841 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.842 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.842 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.843 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.843 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.843 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.026.844 I llama_model_loader: - type  f32:  194 tensors
0.00.026.844 I llama_model_loader: - type q5_0:   97 tensors
0.00.026.845 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.845 I print_info: file format = GGUF V3 (latest)
0.00.026.846 I print_info: file type   = Q5_0
0.00.026.847 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.034.868 I load: special tokens cache size = 25
0.00.041.191 I load: token to piece cache size = 0.2984 MB
0.00.041.194 I print_info: arch             = gptneox
0.00.041.194 I print_info: vocab_only       = 0
0.00.041.194 I print_info: n_ctx_train      = 2048
0.00.041.195 I print_info: n_embd           = 2048
0.00.041.195 I print_info: n_layer          = 24
0.00.041.198 I print_info: n_head           = 16
0.00.041.201 I print_info: n_head_kv        = 16
0.00.041.201 I print_info: n_rot            = 32
0.00.041.201 I print_info: n_swa            = 0
0.00.041.202 I print_info: n_embd_head_k    = 128
0.00.041.202 I print_info: n_embd_head_v    = 128
0.00.041.202 I print_info: n_gqa            = 1
0.00.041.203 I print_info: n_embd_k_gqa     = 2048
0.00.041.204 I print_info: n_embd_v_gqa     = 2048
0.00.041.204 I print_info: f_norm_eps       = 1.0e-05
0.00.041.205 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.205 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.205 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.205 I print_info: f_logit_scale    = 0.0e+00
0.00.041.206 I print_info: n_ff             = 8192
0.00.041.206 I print_info: n_expert         = 0
0.00.041.206 I print_info: n_expert_used    = 0
0.00.041.207 I print_info: causal attn      = 1
0.00.041.207 I print_info: pooling type     = 0
0.00.041.207 I print_info: rope type        = 2
0.00.041.207 I print_info: rope scaling     = linear
0.00.041.212 I print_info: freq_base_train  = 10000.0
0.00.041.212 I print_info: freq_scale_train = 1
0.00.041.212 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.213 I print_info: rope_finetuned   = unknown
0.00.041.213 I print_info: ssm_d_conv       = 0
0.00.041.213 I print_info: ssm_d_inner      = 0
0.00.041.213 I print_info: ssm_d_state      = 0
0.00.041.213 I print_info: ssm_dt_rank      = 0
0.00.041.213 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.214 I print_info: model type       = 1.4B
0.00.041.214 I print_info: model params     = 1.41 B
0.00.041.214 I print_info: general.name     = 1.4B
0.00.041.215 I print_info: vocab type       = BPE
0.00.041.215 I print_info: n_vocab          = 50304
0.00.041.215 I print_info: n_merges         = 50009
0.00.041.216 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.216 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.216 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.216 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.216 I print_info: LF token         = 187 'Ċ'
0.00.041.217 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.218 I print_info: max token length = 1024
0.00.664.776 I load_tensors: offloading 24 repeating layers to GPU
0.00.664.792 I load_tensors: offloading output layer to GPU
0.00.664.793 I load_tensors: offloaded 25/25 layers to GPU
0.00.664.826 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.664.828 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.666.152 I llama_init_from_model: n_seq_max     = 1
0.00.666.157 I llama_init_from_model: n_ctx         = 128
0.00.666.158 I llama_init_from_model: n_ctx_per_seq = 128
0.00.666.158 I llama_init_from_model: n_batch       = 128
0.00.666.159 I llama_init_from_model: n_ubatch      = 128
0.00.666.159 I llama_init_from_model: flash_attn    = 0
0.00.666.162 I llama_init_from_model: freq_base     = 10000.0
0.00.666.163 I llama_init_from_model: freq_scale    = 1
0.00.666.163 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.666.166 I ggml_metal_init: allocating
0.00.666.246 I ggml_metal_init: found device: Apple M4
0.00.666.260 I ggml_metal_init: picking default device: Apple M4
0.00.668.001 I ggml_metal_init: using embedded metal library
0.00.674.619 I ggml_metal_init: GPU name:   Apple M4
0.00.674.623 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.674.624 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.674.624 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.674.625 I ggml_metal_init: simdgroup reduction   = true
0.00.674.625 I ggml_metal_init: simdgroup matrix mul. = true
0.00.674.626 I ggml_metal_init: has residency sets    = true
0.00.674.626 I ggml_metal_init: has bfloat            = true
0.00.674.626 I ggml_metal_init: use bfloat            = true
0.00.674.627 I ggml_metal_init: hasUnifiedMemory      = true
0.00.674.628 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.691.914 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.695.263 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.695.267 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.695.310 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.698.403 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.698.405 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.698.406 I llama_init_from_model: graph nodes  = 967
0.00.698.406 I llama_init_from_model: graph splits = 2
0.00.698.409 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.698.409 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.726.654 I 
0.00.726.726 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.726.733 I perplexity: tokenizing the input ..
0.00.733.635 I perplexity: tokenization took 6.9 ms
0.00.733.641 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.869.023 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.870.297 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.870.323 I llama_perf_context_print:        load time =     715.65 ms
0.00.870.324 I llama_perf_context_print: prompt eval time =     134.97 ms /   128 tokens (    1.05 ms per token,   948.34 tokens per second)
0.00.870.325 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.870.325 I llama_perf_context_print:       total time =     143.67 ms /   129 tokens
0.00.870.713 I ggml_metal_free: deallocating

real	0m0.886s
user	0m0.079s
sys	0m0.145s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.091 I build: 4612 (0cec062a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.588 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.019.184 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.019.190 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.194 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.195 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.195 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.195 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.196 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.197 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.197 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.197 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.198 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.198 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.198 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.199 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.201 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.201 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.201 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.979 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.004 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.807 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.808 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.809 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.810 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.810 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.810 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.027.811 I llama_model_loader: - type  f32:  194 tensors
0.00.027.811 I llama_model_loader: - type q5_1:   97 tensors
0.00.027.812 I llama_model_loader: - type q6_K:    1 tensors
0.00.027.812 I print_info: file format = GGUF V3 (latest)
0.00.027.818 I print_info: file type   = Q5_1
0.00.027.820 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.035.852 I load: special tokens cache size = 25
0.00.041.907 I load: token to piece cache size = 0.2984 MB
0.00.041.910 I print_info: arch             = gptneox
0.00.041.910 I print_info: vocab_only       = 0
0.00.041.911 I print_info: n_ctx_train      = 2048
0.00.041.911 I print_info: n_embd           = 2048
0.00.041.911 I print_info: n_layer          = 24
0.00.041.915 I print_info: n_head           = 16
0.00.041.916 I print_info: n_head_kv        = 16
0.00.041.916 I print_info: n_rot            = 32
0.00.041.916 I print_info: n_swa            = 0
0.00.041.916 I print_info: n_embd_head_k    = 128
0.00.041.917 I print_info: n_embd_head_v    = 128
0.00.041.917 I print_info: n_gqa            = 1
0.00.041.918 I print_info: n_embd_k_gqa     = 2048
0.00.041.921 I print_info: n_embd_v_gqa     = 2048
0.00.041.921 I print_info: f_norm_eps       = 1.0e-05
0.00.041.922 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.922 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.922 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.922 I print_info: f_logit_scale    = 0.0e+00
0.00.041.923 I print_info: n_ff             = 8192
0.00.041.923 I print_info: n_expert         = 0
0.00.041.924 I print_info: n_expert_used    = 0
0.00.041.924 I print_info: causal attn      = 1
0.00.041.924 I print_info: pooling type     = 0
0.00.041.924 I print_info: rope type        = 2
0.00.041.924 I print_info: rope scaling     = linear
0.00.041.925 I print_info: freq_base_train  = 10000.0
0.00.041.925 I print_info: freq_scale_train = 1
0.00.041.925 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.925 I print_info: rope_finetuned   = unknown
0.00.041.926 I print_info: ssm_d_conv       = 0
0.00.041.926 I print_info: ssm_d_inner      = 0
0.00.041.926 I print_info: ssm_d_state      = 0
0.00.041.926 I print_info: ssm_dt_rank      = 0
0.00.041.926 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.926 I print_info: model type       = 1.4B
0.00.041.927 I print_info: model params     = 1.41 B
0.00.041.927 I print_info: general.name     = 1.4B
0.00.041.927 I print_info: vocab type       = BPE
0.00.041.928 I print_info: n_vocab          = 50304
0.00.041.928 I print_info: n_merges         = 50009
0.00.041.928 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.928 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.928 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.929 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.929 I print_info: LF token         = 187 'Ċ'
0.00.041.929 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.929 I print_info: max token length = 1024
0.00.405.681 I load_tensors: offloading 24 repeating layers to GPU
0.00.405.687 I load_tensors: offloading output layer to GPU
0.00.405.687 I load_tensors: offloaded 25/25 layers to GPU
0.00.405.713 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.405.715 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.407.155 I llama_init_from_model: n_seq_max     = 1
0.00.407.158 I llama_init_from_model: n_ctx         = 128
0.00.407.158 I llama_init_from_model: n_ctx_per_seq = 128
0.00.407.162 I llama_init_from_model: n_batch       = 128
0.00.407.163 I llama_init_from_model: n_ubatch      = 128
0.00.407.163 I llama_init_from_model: flash_attn    = 0
0.00.407.164 I llama_init_from_model: freq_base     = 10000.0
0.00.407.165 I llama_init_from_model: freq_scale    = 1
0.00.407.174 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.407.176 I ggml_metal_init: allocating
0.00.407.221 I ggml_metal_init: found device: Apple M4
0.00.407.234 I ggml_metal_init: picking default device: Apple M4
0.00.408.649 I ggml_metal_init: using embedded metal library
0.00.414.595 I ggml_metal_init: GPU name:   Apple M4
0.00.414.598 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.414.599 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.414.600 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.414.601 I ggml_metal_init: simdgroup reduction   = true
0.00.414.601 I ggml_metal_init: simdgroup matrix mul. = true
0.00.414.601 I ggml_metal_init: has residency sets    = true
0.00.414.601 I ggml_metal_init: has bfloat            = true
0.00.414.601 I ggml_metal_init: use bfloat            = true
0.00.414.602 I ggml_metal_init: hasUnifiedMemory      = true
0.00.414.604 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.431.390 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.434.793 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.434.800 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.434.843 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.437.996 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.437.998 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.437.998 I llama_init_from_model: graph nodes  = 967
0.00.437.999 I llama_init_from_model: graph splits = 2
0.00.438.002 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.438.002 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.468.648 I 
0.00.468.722 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.468.729 I perplexity: tokenizing the input ..
0.00.475.621 I perplexity: tokenization took 6.888 ms
0.00.475.627 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.620.225 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.621.477 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.621.503 I llama_perf_context_print:        load time =     460.05 ms
0.00.621.504 I llama_perf_context_print: prompt eval time =     143.66 ms /   128 tokens (    1.12 ms per token,   891.01 tokens per second)
0.00.621.505 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.621.505 I llama_perf_context_print:       total time =     152.86 ms /   129 tokens
0.00.621.910 I ggml_metal_free: deallocating

real	0m0.636s
user	0m0.078s
sys	0m0.103s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.100 I build: 4612 (0cec062a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.983 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.759 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.765 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.766 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.767 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.767 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.767 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.768 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.768 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.769 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.769 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.770 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.770 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.770 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.771 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.772 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.773 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.773 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.757 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.868 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.834 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.835 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.835 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.836 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.836 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.836 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.837 I llama_model_loader: - type  f32:  194 tensors
0.00.025.837 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.838 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.838 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.838 I print_info: file format = GGUF V3 (latest)
0.00.025.839 I print_info: file type   = Q2_K - Medium
0.00.025.840 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.033.995 I load: special tokens cache size = 25
0.00.039.950 I load: token to piece cache size = 0.2984 MB
0.00.039.954 I print_info: arch             = gptneox
0.00.039.954 I print_info: vocab_only       = 0
0.00.039.954 I print_info: n_ctx_train      = 2048
0.00.039.954 I print_info: n_embd           = 2048
0.00.039.954 I print_info: n_layer          = 24
0.00.039.957 I print_info: n_head           = 16
0.00.039.958 I print_info: n_head_kv        = 16
0.00.039.958 I print_info: n_rot            = 32
0.00.039.959 I print_info: n_swa            = 0
0.00.039.959 I print_info: n_embd_head_k    = 128
0.00.039.959 I print_info: n_embd_head_v    = 128
0.00.039.960 I print_info: n_gqa            = 1
0.00.039.960 I print_info: n_embd_k_gqa     = 2048
0.00.039.961 I print_info: n_embd_v_gqa     = 2048
0.00.039.962 I print_info: f_norm_eps       = 1.0e-05
0.00.039.962 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.962 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.963 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.963 I print_info: f_logit_scale    = 0.0e+00
0.00.039.964 I print_info: n_ff             = 8192
0.00.039.964 I print_info: n_expert         = 0
0.00.039.964 I print_info: n_expert_used    = 0
0.00.039.964 I print_info: causal attn      = 1
0.00.039.964 I print_info: pooling type     = 0
0.00.039.964 I print_info: rope type        = 2
0.00.039.965 I print_info: rope scaling     = linear
0.00.039.965 I print_info: freq_base_train  = 10000.0
0.00.039.965 I print_info: freq_scale_train = 1
0.00.039.965 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.966 I print_info: rope_finetuned   = unknown
0.00.039.966 I print_info: ssm_d_conv       = 0
0.00.039.966 I print_info: ssm_d_inner      = 0
0.00.039.966 I print_info: ssm_d_state      = 0
0.00.039.966 I print_info: ssm_dt_rank      = 0
0.00.039.966 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.967 I print_info: model type       = 1.4B
0.00.039.967 I print_info: model params     = 1.41 B
0.00.039.967 I print_info: general.name     = 1.4B
0.00.039.968 I print_info: vocab type       = BPE
0.00.039.968 I print_info: n_vocab          = 50304
0.00.039.968 I print_info: n_merges         = 50009
0.00.039.968 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.968 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.969 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.969 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.969 I print_info: LF token         = 187 'Ċ'
0.00.039.969 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.969 I print_info: max token length = 1024
0.00.412.032 I load_tensors: offloading 24 repeating layers to GPU
0.00.412.045 I load_tensors: offloading output layer to GPU
0.00.412.046 I load_tensors: offloaded 25/25 layers to GPU
0.00.412.078 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.412.080 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.413.594 I llama_init_from_model: n_seq_max     = 1
0.00.413.598 I llama_init_from_model: n_ctx         = 128
0.00.413.599 I llama_init_from_model: n_ctx_per_seq = 128
0.00.413.599 I llama_init_from_model: n_batch       = 128
0.00.413.600 I llama_init_from_model: n_ubatch      = 128
0.00.413.600 I llama_init_from_model: flash_attn    = 0
0.00.413.602 I llama_init_from_model: freq_base     = 10000.0
0.00.413.603 I llama_init_from_model: freq_scale    = 1
0.00.413.603 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.413.606 I ggml_metal_init: allocating
0.00.413.659 I ggml_metal_init: found device: Apple M4
0.00.413.672 I ggml_metal_init: picking default device: Apple M4
0.00.415.351 I ggml_metal_init: using embedded metal library
0.00.420.742 I ggml_metal_init: GPU name:   Apple M4
0.00.420.757 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.420.758 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.420.759 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.420.759 I ggml_metal_init: simdgroup reduction   = true
0.00.420.760 I ggml_metal_init: simdgroup matrix mul. = true
0.00.420.760 I ggml_metal_init: has residency sets    = true
0.00.420.760 I ggml_metal_init: has bfloat            = true
0.00.420.760 I ggml_metal_init: use bfloat            = true
0.00.420.762 I ggml_metal_init: hasUnifiedMemory      = true
0.00.420.766 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.442.812 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.446.641 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.446.647 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.446.691 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.449.932 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.449.934 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.449.935 I llama_init_from_model: graph nodes  = 967
0.00.449.936 I llama_init_from_model: graph splits = 2
0.00.449.939 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.449.940 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.481.115 I 
0.00.481.213 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.481.225 I perplexity: tokenizing the input ..
0.00.488.111 I perplexity: tokenization took 6.881 ms
0.00.488.122 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.630.593 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.632.095 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.632.124 I llama_perf_context_print:        load time =     471.12 ms
0.00.632.127 I llama_perf_context_print: prompt eval time =     141.52 ms /   128 tokens (    1.11 ms per token,   904.47 tokens per second)
0.00.632.128 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.632.128 I llama_perf_context_print:       total time =     151.01 ms /   129 tokens
0.00.632.518 I ggml_metal_free: deallocating

real	0m0.648s
user	0m0.081s
sys	0m0.095s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.098 I build: 4612 (0cec062a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.100 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.224 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.230 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.232 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.232 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.233 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.233 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.233 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.236 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.236 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.237 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.237 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.237 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.238 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.238 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.240 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.240 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.241 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.194 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.289 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.224 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.225 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.225 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.226 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.226 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.226 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.227 I llama_model_loader: - type  f32:  194 tensors
0.00.025.227 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.227 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.228 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.228 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.228 I print_info: file format = GGUF V3 (latest)
0.00.025.229 I print_info: file type   = Q3_K - Medium
0.00.025.231 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.033.341 I load: special tokens cache size = 25
0.00.039.394 I load: token to piece cache size = 0.2984 MB
0.00.039.396 I print_info: arch             = gptneox
0.00.039.397 I print_info: vocab_only       = 0
0.00.039.397 I print_info: n_ctx_train      = 2048
0.00.039.397 I print_info: n_embd           = 2048
0.00.039.397 I print_info: n_layer          = 24
0.00.039.401 I print_info: n_head           = 16
0.00.039.401 I print_info: n_head_kv        = 16
0.00.039.402 I print_info: n_rot            = 32
0.00.039.402 I print_info: n_swa            = 0
0.00.039.402 I print_info: n_embd_head_k    = 128
0.00.039.404 I print_info: n_embd_head_v    = 128
0.00.039.405 I print_info: n_gqa            = 1
0.00.039.405 I print_info: n_embd_k_gqa     = 2048
0.00.039.406 I print_info: n_embd_v_gqa     = 2048
0.00.039.407 I print_info: f_norm_eps       = 1.0e-05
0.00.039.407 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.407 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.408 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.408 I print_info: f_logit_scale    = 0.0e+00
0.00.039.409 I print_info: n_ff             = 8192
0.00.039.410 I print_info: n_expert         = 0
0.00.039.410 I print_info: n_expert_used    = 0
0.00.039.410 I print_info: causal attn      = 1
0.00.039.410 I print_info: pooling type     = 0
0.00.039.410 I print_info: rope type        = 2
0.00.039.411 I print_info: rope scaling     = linear
0.00.039.411 I print_info: freq_base_train  = 10000.0
0.00.039.411 I print_info: freq_scale_train = 1
0.00.039.411 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.412 I print_info: rope_finetuned   = unknown
0.00.039.412 I print_info: ssm_d_conv       = 0
0.00.039.412 I print_info: ssm_d_inner      = 0
0.00.039.412 I print_info: ssm_d_state      = 0
0.00.039.412 I print_info: ssm_dt_rank      = 0
0.00.039.412 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.413 I print_info: model type       = 1.4B
0.00.039.414 I print_info: model params     = 1.41 B
0.00.039.415 I print_info: general.name     = 1.4B
0.00.039.415 I print_info: vocab type       = BPE
0.00.039.415 I print_info: n_vocab          = 50304
0.00.039.415 I print_info: n_merges         = 50009
0.00.039.416 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.416 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.416 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.416 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.418 I print_info: LF token         = 187 'Ċ'
0.00.039.418 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.418 I print_info: max token length = 1024
0.00.462.620 I load_tensors: offloading 24 repeating layers to GPU
0.00.462.635 I load_tensors: offloading output layer to GPU
0.00.462.636 I load_tensors: offloaded 25/25 layers to GPU
0.00.462.672 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.462.673 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.464.109 I llama_init_from_model: n_seq_max     = 1
0.00.464.118 I llama_init_from_model: n_ctx         = 128
0.00.464.119 I llama_init_from_model: n_ctx_per_seq = 128
0.00.464.119 I llama_init_from_model: n_batch       = 128
0.00.464.120 I llama_init_from_model: n_ubatch      = 128
0.00.464.120 I llama_init_from_model: flash_attn    = 0
0.00.464.135 I llama_init_from_model: freq_base     = 10000.0
0.00.464.135 I llama_init_from_model: freq_scale    = 1
0.00.464.136 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.464.138 I ggml_metal_init: allocating
0.00.464.256 I ggml_metal_init: found device: Apple M4
0.00.464.271 I ggml_metal_init: picking default device: Apple M4
0.00.466.122 I ggml_metal_init: using embedded metal library
0.00.471.901 I ggml_metal_init: GPU name:   Apple M4
0.00.471.906 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.471.906 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.471.907 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.471.908 I ggml_metal_init: simdgroup reduction   = true
0.00.471.908 I ggml_metal_init: simdgroup matrix mul. = true
0.00.471.909 I ggml_metal_init: has residency sets    = true
0.00.471.909 I ggml_metal_init: has bfloat            = true
0.00.471.909 I ggml_metal_init: use bfloat            = true
0.00.471.910 I ggml_metal_init: hasUnifiedMemory      = true
0.00.471.912 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.491.049 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.494.716 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.494.719 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.494.764 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.497.859 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.497.861 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.497.861 I llama_init_from_model: graph nodes  = 967
0.00.497.862 I llama_init_from_model: graph splits = 2
0.00.497.864 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.497.864 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.529.794 I 
0.00.529.878 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.529.887 I perplexity: tokenizing the input ..
0.00.537.296 I perplexity: tokenization took 7.406 ms
0.00.537.308 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.684.256 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.685.585 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.685.606 I llama_perf_context_print:        load time =     520.68 ms
0.00.685.607 I llama_perf_context_print: prompt eval time =     146.06 ms /   128 tokens (    1.14 ms per token,   876.36 tokens per second)
0.00.685.608 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.685.608 I llama_perf_context_print:       total time =     155.82 ms /   129 tokens
0.00.685.987 I ggml_metal_free: deallocating

real	0m0.700s
user	0m0.080s
sys	0m0.121s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.098 I build: 4612 (0cec062a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.397 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.358 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.017.364 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.365 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.366 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.366 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.367 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.367 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.368 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.368 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.369 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.369 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.369 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.370 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.370 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.371 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.372 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.372 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.268 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.243 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.935 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.936 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.937 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.937 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.937 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.937 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.938 I llama_model_loader: - type  f32:  194 tensors
0.00.025.938 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.938 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.938 I llama_model_loader: - type q6_K:   13 tensors
0.00.025.939 I print_info: file format = GGUF V3 (latest)
0.00.025.939 I print_info: file type   = Q4_K - Medium
0.00.025.940 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.034.062 I load: special tokens cache size = 25
0.00.039.931 I load: token to piece cache size = 0.2984 MB
0.00.039.934 I print_info: arch             = gptneox
0.00.039.934 I print_info: vocab_only       = 0
0.00.039.935 I print_info: n_ctx_train      = 2048
0.00.039.935 I print_info: n_embd           = 2048
0.00.039.935 I print_info: n_layer          = 24
0.00.039.938 I print_info: n_head           = 16
0.00.039.939 I print_info: n_head_kv        = 16
0.00.039.939 I print_info: n_rot            = 32
0.00.039.939 I print_info: n_swa            = 0
0.00.039.939 I print_info: n_embd_head_k    = 128
0.00.039.939 I print_info: n_embd_head_v    = 128
0.00.039.940 I print_info: n_gqa            = 1
0.00.039.941 I print_info: n_embd_k_gqa     = 2048
0.00.039.942 I print_info: n_embd_v_gqa     = 2048
0.00.039.942 I print_info: f_norm_eps       = 1.0e-05
0.00.039.942 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.943 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.943 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.943 I print_info: f_logit_scale    = 0.0e+00
0.00.039.944 I print_info: n_ff             = 8192
0.00.039.944 I print_info: n_expert         = 0
0.00.039.944 I print_info: n_expert_used    = 0
0.00.039.944 I print_info: causal attn      = 1
0.00.039.944 I print_info: pooling type     = 0
0.00.039.944 I print_info: rope type        = 2
0.00.039.945 I print_info: rope scaling     = linear
0.00.039.945 I print_info: freq_base_train  = 10000.0
0.00.039.945 I print_info: freq_scale_train = 1
0.00.039.945 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.946 I print_info: rope_finetuned   = unknown
0.00.039.948 I print_info: ssm_d_conv       = 0
0.00.039.948 I print_info: ssm_d_inner      = 0
0.00.039.948 I print_info: ssm_d_state      = 0
0.00.039.948 I print_info: ssm_dt_rank      = 0
0.00.039.948 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.949 I print_info: model type       = 1.4B
0.00.039.949 I print_info: model params     = 1.41 B
0.00.039.949 I print_info: general.name     = 1.4B
0.00.039.950 I print_info: vocab type       = BPE
0.00.039.950 I print_info: n_vocab          = 50304
0.00.039.950 I print_info: n_merges         = 50009
0.00.039.950 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.950 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.951 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.951 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.955 I print_info: LF token         = 187 'Ċ'
0.00.039.956 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.956 I print_info: max token length = 1024
0.00.511.705 I load_tensors: offloading 24 repeating layers to GPU
0.00.511.722 I load_tensors: offloading output layer to GPU
0.00.511.723 I load_tensors: offloaded 25/25 layers to GPU
0.00.511.758 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.511.759 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.513.142 I llama_init_from_model: n_seq_max     = 1
0.00.513.147 I llama_init_from_model: n_ctx         = 128
0.00.513.148 I llama_init_from_model: n_ctx_per_seq = 128
0.00.513.148 I llama_init_from_model: n_batch       = 128
0.00.513.148 I llama_init_from_model: n_ubatch      = 128
0.00.513.149 I llama_init_from_model: flash_attn    = 0
0.00.513.151 I llama_init_from_model: freq_base     = 10000.0
0.00.513.152 I llama_init_from_model: freq_scale    = 1
0.00.513.153 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.513.155 I ggml_metal_init: allocating
0.00.513.246 I ggml_metal_init: found device: Apple M4
0.00.513.261 I ggml_metal_init: picking default device: Apple M4
0.00.514.996 I ggml_metal_init: using embedded metal library
0.00.521.756 I ggml_metal_init: GPU name:   Apple M4
0.00.521.760 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.521.761 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.521.762 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.521.763 I ggml_metal_init: simdgroup reduction   = true
0.00.521.763 I ggml_metal_init: simdgroup matrix mul. = true
0.00.521.763 I ggml_metal_init: has residency sets    = true
0.00.521.763 I ggml_metal_init: has bfloat            = true
0.00.521.764 I ggml_metal_init: use bfloat            = true
0.00.521.765 I ggml_metal_init: hasUnifiedMemory      = true
0.00.521.776 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.539.616 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.543.141 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.543.147 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.543.191 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.546.280 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.546.282 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.546.282 I llama_init_from_model: graph nodes  = 967
0.00.546.282 I llama_init_from_model: graph splits = 2
0.00.546.285 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.546.285 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.576.925 I 
0.00.577.012 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.577.020 I perplexity: tokenizing the input ..
0.00.584.289 I perplexity: tokenization took 7.266 ms
0.00.584.303 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.733.293 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.734.625 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.734.646 I llama_perf_context_print:        load time =     566.52 ms
0.00.734.647 I llama_perf_context_print: prompt eval time =     148.07 ms /   128 tokens (    1.16 ms per token,   864.46 tokens per second)
0.00.734.648 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.734.648 I llama_perf_context_print:       total time =     157.72 ms /   129 tokens
0.00.735.014 I ggml_metal_free: deallocating

real	0m0.750s
user	0m0.079s
sys	0m0.117s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.097 I build: 4612 (0cec062a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.140 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.129 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.134 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.140 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.141 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.141 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.141 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.142 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.143 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.143 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.144 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.144 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.144 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.145 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.145 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.147 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.147 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.148 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.120 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.212 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.185 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.186 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.186 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.187 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.187 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.187 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.188 I llama_model_loader: - type  f32:  194 tensors
0.00.025.188 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.188 I llama_model_loader: - type q6_K:   37 tensors
0.00.025.189 I print_info: file format = GGUF V3 (latest)
0.00.025.189 I print_info: file type   = Q5_K - Medium
0.00.025.190 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.033.252 I load: special tokens cache size = 25
0.00.039.207 I load: token to piece cache size = 0.2984 MB
0.00.039.210 I print_info: arch             = gptneox
0.00.039.210 I print_info: vocab_only       = 0
0.00.039.210 I print_info: n_ctx_train      = 2048
0.00.039.211 I print_info: n_embd           = 2048
0.00.039.211 I print_info: n_layer          = 24
0.00.039.213 I print_info: n_head           = 16
0.00.039.214 I print_info: n_head_kv        = 16
0.00.039.214 I print_info: n_rot            = 32
0.00.039.215 I print_info: n_swa            = 0
0.00.039.215 I print_info: n_embd_head_k    = 128
0.00.039.215 I print_info: n_embd_head_v    = 128
0.00.039.216 I print_info: n_gqa            = 1
0.00.039.217 I print_info: n_embd_k_gqa     = 2048
0.00.039.217 I print_info: n_embd_v_gqa     = 2048
0.00.039.218 I print_info: f_norm_eps       = 1.0e-05
0.00.039.218 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.220 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.220 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.220 I print_info: f_logit_scale    = 0.0e+00
0.00.039.221 I print_info: n_ff             = 8192
0.00.039.221 I print_info: n_expert         = 0
0.00.039.223 I print_info: n_expert_used    = 0
0.00.039.223 I print_info: causal attn      = 1
0.00.039.223 I print_info: pooling type     = 0
0.00.039.223 I print_info: rope type        = 2
0.00.039.224 I print_info: rope scaling     = linear
0.00.039.224 I print_info: freq_base_train  = 10000.0
0.00.039.224 I print_info: freq_scale_train = 1
0.00.039.224 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.225 I print_info: rope_finetuned   = unknown
0.00.039.225 I print_info: ssm_d_conv       = 0
0.00.039.225 I print_info: ssm_d_inner      = 0
0.00.039.225 I print_info: ssm_d_state      = 0
0.00.039.225 I print_info: ssm_dt_rank      = 0
0.00.039.225 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.229 I print_info: model type       = 1.4B
0.00.039.229 I print_info: model params     = 1.41 B
0.00.039.230 I print_info: general.name     = 1.4B
0.00.039.230 I print_info: vocab type       = BPE
0.00.039.230 I print_info: n_vocab          = 50304
0.00.039.231 I print_info: n_merges         = 50009
0.00.039.231 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.231 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.231 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.231 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.232 I print_info: LF token         = 187 'Ċ'
0.00.039.232 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.232 I print_info: max token length = 1024
0.00.597.626 I load_tensors: offloading 24 repeating layers to GPU
0.00.597.638 I load_tensors: offloading output layer to GPU
0.00.597.638 I load_tensors: offloaded 25/25 layers to GPU
0.00.597.670 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.597.672 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.599.198 I llama_init_from_model: n_seq_max     = 1
0.00.599.204 I llama_init_from_model: n_ctx         = 128
0.00.599.205 I llama_init_from_model: n_ctx_per_seq = 128
0.00.599.206 I llama_init_from_model: n_batch       = 128
0.00.599.206 I llama_init_from_model: n_ubatch      = 128
0.00.599.206 I llama_init_from_model: flash_attn    = 0
0.00.599.208 I llama_init_from_model: freq_base     = 10000.0
0.00.599.208 I llama_init_from_model: freq_scale    = 1
0.00.599.209 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.599.212 I ggml_metal_init: allocating
0.00.599.265 I ggml_metal_init: found device: Apple M4
0.00.599.279 I ggml_metal_init: picking default device: Apple M4
0.00.601.324 I ggml_metal_init: using embedded metal library
0.00.608.143 I ggml_metal_init: GPU name:   Apple M4
0.00.608.150 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.608.150 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.608.151 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.608.152 I ggml_metal_init: simdgroup reduction   = true
0.00.608.152 I ggml_metal_init: simdgroup matrix mul. = true
0.00.608.153 I ggml_metal_init: has residency sets    = true
0.00.608.153 I ggml_metal_init: has bfloat            = true
0.00.608.153 I ggml_metal_init: use bfloat            = true
0.00.608.154 I ggml_metal_init: hasUnifiedMemory      = true
0.00.608.156 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.625.606 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.629.321 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.629.325 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.629.388 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.632.756 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.632.758 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.632.758 I llama_init_from_model: graph nodes  = 967
0.00.632.758 I llama_init_from_model: graph splits = 2
0.00.632.762 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.632.762 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.668.226 I 
0.00.668.315 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.668.323 I perplexity: tokenizing the input ..
0.00.675.244 I perplexity: tokenization took 6.918 ms
0.00.675.250 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.816.673 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.818.194 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.818.217 I llama_perf_context_print:        load time =     659.08 ms
0.00.818.218 I llama_perf_context_print: prompt eval time =     140.48 ms /   128 tokens (    1.10 ms per token,   911.17 tokens per second)
0.00.818.219 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.818.219 I llama_perf_context_print:       total time =     150.00 ms /   129 tokens
0.00.818.593 I ggml_metal_free: deallocating

real	0m0.832s
user	0m0.078s
sys	0m0.139s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.100 I build: 4612 (0cec062a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.430 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.023.197 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.023.202 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.023.204 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.023.205 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.023.207 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.023.207 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.023.207 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.023.208 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.023.209 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.023.211 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.023.211 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.023.211 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.023.216 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.023.216 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.023.218 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.023.219 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.023.219 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.027.075 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.028.160 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.031.999 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.032.000 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.032.000 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.032.001 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.032.001 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.032.001 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.032.002 I llama_model_loader: - type  f32:  194 tensors
0.00.032.002 I llama_model_loader: - type q6_K:   98 tensors
0.00.032.003 I print_info: file format = GGUF V3 (latest)
0.00.032.003 I print_info: file type   = Q6_K
0.00.032.004 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.040.322 I load: special tokens cache size = 25
0.00.046.480 I load: token to piece cache size = 0.2984 MB
0.00.046.490 I print_info: arch             = gptneox
0.00.046.491 I print_info: vocab_only       = 0
0.00.046.492 I print_info: n_ctx_train      = 2048
0.00.046.492 I print_info: n_embd           = 2048
0.00.046.494 I print_info: n_layer          = 24
0.00.046.499 I print_info: n_head           = 16
0.00.046.500 I print_info: n_head_kv        = 16
0.00.046.500 I print_info: n_rot            = 32
0.00.046.500 I print_info: n_swa            = 0
0.00.046.500 I print_info: n_embd_head_k    = 128
0.00.046.501 I print_info: n_embd_head_v    = 128
0.00.046.501 I print_info: n_gqa            = 1
0.00.046.502 I print_info: n_embd_k_gqa     = 2048
0.00.046.503 I print_info: n_embd_v_gqa     = 2048
0.00.046.503 I print_info: f_norm_eps       = 1.0e-05
0.00.046.504 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.046.504 I print_info: f_clamp_kqv      = 0.0e+00
0.00.046.504 I print_info: f_max_alibi_bias = 0.0e+00
0.00.046.505 I print_info: f_logit_scale    = 0.0e+00
0.00.046.505 I print_info: n_ff             = 8192
0.00.046.506 I print_info: n_expert         = 0
0.00.046.506 I print_info: n_expert_used    = 0
0.00.046.506 I print_info: causal attn      = 1
0.00.046.506 I print_info: pooling type     = 0
0.00.046.506 I print_info: rope type        = 2
0.00.046.506 I print_info: rope scaling     = linear
0.00.046.507 I print_info: freq_base_train  = 10000.0
0.00.046.507 I print_info: freq_scale_train = 1
0.00.046.507 I print_info: n_ctx_orig_yarn  = 2048
0.00.046.507 I print_info: rope_finetuned   = unknown
0.00.046.508 I print_info: ssm_d_conv       = 0
0.00.046.508 I print_info: ssm_d_inner      = 0
0.00.046.508 I print_info: ssm_d_state      = 0
0.00.046.509 I print_info: ssm_dt_rank      = 0
0.00.046.509 I print_info: ssm_dt_b_c_rms   = 0
0.00.046.510 I print_info: model type       = 1.4B
0.00.046.510 I print_info: model params     = 1.41 B
0.00.046.510 I print_info: general.name     = 1.4B
0.00.046.510 I print_info: vocab type       = BPE
0.00.046.511 I print_info: n_vocab          = 50304
0.00.046.511 I print_info: n_merges         = 50009
0.00.046.511 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.046.511 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.046.511 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.046.511 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.046.512 I print_info: LF token         = 187 'Ċ'
0.00.046.512 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.046.512 I print_info: max token length = 1024
0.00.592.832 I load_tensors: offloading 24 repeating layers to GPU
0.00.592.842 I load_tensors: offloading output layer to GPU
0.00.592.842 I load_tensors: offloaded 25/25 layers to GPU
0.00.592.872 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.592.875 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.594.327 I llama_init_from_model: n_seq_max     = 1
0.00.594.330 I llama_init_from_model: n_ctx         = 128
0.00.594.330 I llama_init_from_model: n_ctx_per_seq = 128
0.00.594.331 I llama_init_from_model: n_batch       = 128
0.00.594.331 I llama_init_from_model: n_ubatch      = 128
0.00.594.332 I llama_init_from_model: flash_attn    = 0
0.00.594.333 I llama_init_from_model: freq_base     = 10000.0
0.00.594.334 I llama_init_from_model: freq_scale    = 1
0.00.594.335 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.594.337 I ggml_metal_init: allocating
0.00.594.404 I ggml_metal_init: found device: Apple M4
0.00.594.418 I ggml_metal_init: picking default device: Apple M4
0.00.595.818 I ggml_metal_init: using embedded metal library
0.00.601.954 I ggml_metal_init: GPU name:   Apple M4
0.00.601.959 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.601.960 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.601.961 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.601.965 I ggml_metal_init: simdgroup reduction   = true
0.00.601.965 I ggml_metal_init: simdgroup matrix mul. = true
0.00.601.965 I ggml_metal_init: has residency sets    = true
0.00.601.965 I ggml_metal_init: has bfloat            = true
0.00.601.966 I ggml_metal_init: use bfloat            = true
0.00.601.967 I ggml_metal_init: hasUnifiedMemory      = true
0.00.601.970 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.618.432 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.621.766 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.621.769 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.621.816 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.624.992 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.624.993 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.624.994 I llama_init_from_model: graph nodes  = 967
0.00.624.994 I llama_init_from_model: graph splits = 2
0.00.624.997 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.624.997 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.655.584 I 
0.00.655.678 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.655.686 I perplexity: tokenizing the input ..
0.00.662.970 I perplexity: tokenization took 7.28 ms
0.00.662.990 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.804.265 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.805.587 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.805.609 I llama_perf_context_print:        load time =     644.14 ms
0.00.805.610 I llama_perf_context_print: prompt eval time =     140.41 ms /   128 tokens (    1.10 ms per token,   911.62 tokens per second)
0.00.805.611 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.805.611 I llama_perf_context_print:       total time =     150.03 ms /   129 tokens
0.00.805.969 I ggml_metal_free: deallocating

real	0m0.822s
user	0m0.078s
sys	0m0.131s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.291 I build: 4612 (0cec062a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.022.857 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.042.003 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.042.010 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.042.019 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.042.020 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.042.021 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.042.021 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.042.022 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.042.023 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.042.024 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.042.024 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.042.025 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.042.026 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.042.026 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.042.027 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.042.029 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.042.030 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.042.030 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.049.507 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.051.454 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.057.906 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.057.908 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.057.908 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.057.909 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.057.909 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.057.910 I llama_model_loader: - type  f32:  194 tensors
0.00.057.910 I llama_model_loader: - type  f16:   98 tensors
0.00.057.911 I print_info: file format = GGUF V3 (latest)
0.00.057.912 I print_info: file type   = all F32 (guessed)
0.00.057.913 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.069.385 I load: special tokens cache size = 25
0.00.077.177 I load: token to piece cache size = 0.2984 MB
0.00.077.180 I print_info: arch             = gptneox
0.00.077.180 I print_info: vocab_only       = 0
0.00.077.180 I print_info: n_ctx_train      = 2048
0.00.077.181 I print_info: n_embd           = 2048
0.00.077.181 I print_info: n_layer          = 24
0.00.077.184 I print_info: n_head           = 16
0.00.077.185 I print_info: n_head_kv        = 16
0.00.077.186 I print_info: n_rot            = 32
0.00.077.186 I print_info: n_swa            = 0
0.00.077.186 I print_info: n_embd_head_k    = 128
0.00.077.187 I print_info: n_embd_head_v    = 128
0.00.077.187 I print_info: n_gqa            = 1
0.00.077.188 I print_info: n_embd_k_gqa     = 2048
0.00.077.189 I print_info: n_embd_v_gqa     = 2048
0.00.077.189 I print_info: f_norm_eps       = 1.0e-05
0.00.077.190 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.077.190 I print_info: f_clamp_kqv      = 0.0e+00
0.00.077.190 I print_info: f_max_alibi_bias = 0.0e+00
0.00.077.190 I print_info: f_logit_scale    = 0.0e+00
0.00.077.192 I print_info: n_ff             = 8192
0.00.077.192 I print_info: n_expert         = 0
0.00.077.193 I print_info: n_expert_used    = 0
0.00.077.193 I print_info: causal attn      = 1
0.00.077.193 I print_info: pooling type     = 0
0.00.077.193 I print_info: rope type        = 2
0.00.077.193 I print_info: rope scaling     = linear
0.00.077.194 I print_info: freq_base_train  = 10000.0
0.00.077.194 I print_info: freq_scale_train = 1
0.00.077.194 I print_info: n_ctx_orig_yarn  = 2048
0.00.077.194 I print_info: rope_finetuned   = unknown
0.00.077.195 I print_info: ssm_d_conv       = 0
0.00.077.195 I print_info: ssm_d_inner      = 0
0.00.077.195 I print_info: ssm_d_state      = 0
0.00.077.195 I print_info: ssm_dt_rank      = 0
0.00.077.195 I print_info: ssm_dt_b_c_rms   = 0
0.00.077.195 I print_info: model type       = 1.4B
0.00.077.196 I print_info: model params     = 1.41 B
0.00.077.196 I print_info: general.name     = 1.4B
0.00.077.196 I print_info: vocab type       = BPE
0.00.077.197 I print_info: n_vocab          = 50304
0.00.077.197 I print_info: n_merges         = 50009
0.00.077.197 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.077.197 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.077.199 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.077.199 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.077.199 I print_info: LF token         = 187 'Ċ'
0.00.077.199 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.077.199 I print_info: max token length = 1024
0.01.314.611 I load_tensors: offloading 24 repeating layers to GPU
0.01.314.616 I load_tensors: offloading output layer to GPU
0.01.314.617 I load_tensors: offloaded 25/25 layers to GPU
0.01.314.639 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.314.641 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.01.315.564 I llama_init_from_model: n_seq_max     = 1
0.01.315.565 I llama_init_from_model: n_ctx         = 128
0.01.315.565 I llama_init_from_model: n_ctx_per_seq = 128
0.01.315.565 I llama_init_from_model: n_batch       = 128
0.01.315.566 I llama_init_from_model: n_ubatch      = 128
0.01.315.566 I llama_init_from_model: flash_attn    = 0
0.01.315.566 I llama_init_from_model: freq_base     = 10000.0
0.01.315.567 I llama_init_from_model: freq_scale    = 1
0.01.315.567 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.315.568 I ggml_metal_init: allocating
0.01.315.596 I ggml_metal_init: found device: Apple M4
0.01.315.602 I ggml_metal_init: picking default device: Apple M4
0.01.316.584 I ggml_metal_init: using embedded metal library
0.01.320.421 I ggml_metal_init: GPU name:   Apple M4
0.01.320.423 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.320.423 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.320.424 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.320.424 I ggml_metal_init: simdgroup reduction   = true
0.01.320.424 I ggml_metal_init: simdgroup matrix mul. = true
0.01.320.424 I ggml_metal_init: has residency sets    = true
0.01.320.424 I ggml_metal_init: has bfloat            = true
0.01.320.425 I ggml_metal_init: use bfloat            = true
0.01.320.425 I ggml_metal_init: hasUnifiedMemory      = true
0.01.320.429 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.331.055 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.332.715 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.332.717 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.332.742 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.334.441 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.334.442 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.334.442 I llama_init_from_model: graph nodes  = 967
0.01.334.442 I llama_init_from_model: graph splits = 2
0.01.334.444 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.334.444 I 
0.01.334.480 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.334.481 I compute_imatrix: tokenizing the input ..
0.01.338.442 I compute_imatrix: tokenization took 3.961 ms
0.01.338.444 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.606.833 I compute_imatrix: 0.27 seconds per pass - ETA 0.00 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.609.230 I llama_perf_context_print:        load time =    1583.98 ms
0.01.609.231 I llama_perf_context_print: prompt eval time =     266.65 ms /   128 tokens (    2.08 ms per token,   480.02 tokens per second)
0.01.609.232 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.609.233 I llama_perf_context_print:       total time =    1586.37 ms /   129 tokens
0.01.609.727 I ggml_metal_free: deallocating

real	0m1.793s
user	0m0.127s
sys	0m0.251s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4612 (0cec062a)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 'Ċ'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x101204a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x101205160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x101205710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x101205cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x101206270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x101206820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x101206dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x101207380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x101207930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x101207e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x101208330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x101208830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x101209350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x101209b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x10120a310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x10120aa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x10120b150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x10120b870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x10120bf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x10120c760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x10120ce80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x10120d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x10120dcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x10120e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x10120ec80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x10120ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x10120f550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x1012101c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x101210700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x1012109c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x101210e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x101211120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x1012119b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x101211ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1012121b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x101212650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x101212af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x101212f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x101213430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x1012138d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x101213d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x101214210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x1012146b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x101214b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x101214e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x101215420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x101215a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x101216350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x101216960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x101216f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x101217580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x101217b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x1012181a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x1012187b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x101218fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x101219440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x1012198e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x101219ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x10121a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x10121a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x10121ac60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x10121b100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x10121b5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x10121ba40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x10121bee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x10121c380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x10121c820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x10121ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x10121d160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x10121d600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x10121daa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x10121df40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x10121e3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x10121e930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x10121ee80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x10121f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x10121f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x10121fe70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x1012203c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x101220910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x101220e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x1012213b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x101221900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x101221e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x1012223a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x1012228f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x101222e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x101223390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x1012238e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x101223e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x101224380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x1012248d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x101224e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x101225370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x1012258c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x101225e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x101226360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x101216040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x1012267d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x101226f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x1012274d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x101227a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x101227f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x1012284c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x101228a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x101228f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x1012294b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x101229a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x101229f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x10122a4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x10122a9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x10122af40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x10122b490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x10122b930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x10122bdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x10122c270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x10122c710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x10122cbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x10122d050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x10122d4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x10122d990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x10122de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x10122e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x10122e770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x10122ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x10122f0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x10122f550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x10122f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x10122fe90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x101230330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x1012307d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x101230c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x101231110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x1012315b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x101231a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x101231ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x101232390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x101232830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x101232cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x101233170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x101233610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x101233ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x101233f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x1012343f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x101234890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x101234d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x1012351d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x101235670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x101235b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x101235fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x101236450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x1012368f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x101236d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x101237230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x1012376d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x101237b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x101238010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x1012384b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x101238950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x101238df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x101239290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x101239730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x101239bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x10123a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x10123a510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x10123a9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x10123ae50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x10123b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x10123b790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x10123bc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x10123c0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x10123c570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x10123ca10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x10123ceb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x10123d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x10123d7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x10123dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x10123e130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x10123e5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x10123ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x10123ef10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x10123f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x10123f850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x10123fcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x101240190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x101240630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x101240ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x101240f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x101241410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x1012418b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x101241d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x1012421f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x101242690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x101242be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x101243130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x101243680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x101243bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x101243e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x1012444a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x101244ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x1012450c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x1012458b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x101245d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x101246010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x101246620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x101246c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x101247420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x1012478c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x101247d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x101248200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x1012489b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x101248f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x101249450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x1012499a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x101249ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x10124a440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x10124a990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x10124aee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x10124b430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x10124b980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x10124bed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x10124c420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x10124c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x10124cec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x10124d410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x10124d960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x10124deb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x10124e400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x10124e950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x10124eea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x10124f3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x10124f940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x10124fe90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x1012503e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x101250930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x101250e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x1012513d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x101251920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x101251e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x1012523c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x101252910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x101252e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x1012533b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x101253900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x101253e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x1012543a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x1012548f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x101254e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x101255390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x1012558e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x101255e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x101256380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x1012568d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x101256e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x101257370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x1012578c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x101257e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x101258360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x1012588b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x101258e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x101259350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x1012598a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x101259df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x10125a340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x10125a890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x10125ade0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x10125b330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x10125b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x10125bc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x10125c110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x10125c5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x10125ca50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x10125cef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x10125d390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x10125d830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x10125dcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x10125e170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x10125e610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x10125eab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x128604230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x1286046a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x128604b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x128604f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x1286053f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x128606170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x128606890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x128606fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x128607270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x1286076e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x128607ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x1286082f0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.703.445 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.703.449 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12860c1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12860c640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12860cab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12860cf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12860d390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12860d800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12860dc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12860e0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12860e550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12860e9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12860ee30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12860f520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x128610040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x1286107f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x128611000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x128611720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x128611e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x128612560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x128612c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x1286133b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x128613ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x1286141f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x128614910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x128615030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x128615750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x128615a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x128615cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x128616140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x1286165b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x128616a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x128616f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x128617430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x1286178a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x128617b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x128617fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x128618440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x1286189a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x128618ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x1286193a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x1286198a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x128619da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12861a2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12861a7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12861aca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12861b1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12861b610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12861ba80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12861bef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12861c360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12861c7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12861cc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12861d0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12861d520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12861d990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12861de00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12861e5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12861ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12861ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12861f340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12861fb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12861ffd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x128620470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x128620910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x128620db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x128621250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x1286216f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x128621b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x128622030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x1286224d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x128622970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x128622e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x1286232b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x128623750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x128623ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x1286241f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x128624740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x128624c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x1286251e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x128625730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x128625c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x1286261d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x128626720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x128626c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x1286271c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x128627710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x128627c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x1286281b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x128628700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x128628c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x1286291a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x1286296f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x128629c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12862a190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12862a6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12862ac30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12862b180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12862b6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12862bc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12862c170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12862c6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12862cc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12862d160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12862d6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12862dc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12862e150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12862e6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12862ebf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12862f140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12862f690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12862fbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x128630130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x128630680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x128630bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x128631070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x128631510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x1286319b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x128631e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x1286322f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x128632790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x128632c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x1286330d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x128633570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x128633a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x128633eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x128634350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x1286347f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x128634c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x128635130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x1286355d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x128635a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x128635f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x1286363b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x128636850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x128636cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x128637190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x128637630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x128637ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x128637f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x128638410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x1286388b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x128638d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x1286391f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x128639690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x128639b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x128639fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12863a470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12863a910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12863adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12863b250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12863b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12863bb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12863c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12863c4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12863c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12863ce10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12863d2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12863d750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12863dbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12863e090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12863e530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12863e9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12863ee70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12863f310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12863f7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12863fc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x1286400f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x128640590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x128640a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x128640ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x128641370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x128641810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x128641cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x128642150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x1286425f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x128642a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x128642f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x1286433d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x128643870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x128643d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x1286441b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x128644650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x128644af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x128644f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x128645430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x1286458d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x128645d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x128646210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x1286466b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x128646b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x128646ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x128647490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x128647930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x128647dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x128648320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x128648870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x128648dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x128649310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x1286495d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x128649be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12864a1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12864a800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12864aff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12864b490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12864b750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12864bd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12864c370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12864cb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12864d000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12864d4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12864d940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12864e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12864e640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12864eb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12864f0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12864f630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12864fb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x1286500d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x128650620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x128650b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x1286510c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x128651610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x128651b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x1286520b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x128652600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x128652b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x1286530a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x1286535f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x128653b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x128654090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x1286545e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x128654b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x128655080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x1286555d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x128655b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x128656070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x1286565c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x128656b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x128657060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x1286575b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x128657b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x128658050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x1286585a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x128658af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x128659040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x128659590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x128659ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12865a030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12865a580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12865aad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12865b020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12865b570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12865bac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12865c010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12865c560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12865cab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12865d000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12865d550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12865daa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12865dff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12865e540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12865ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12865efe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12865f530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12865fa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12865ffd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x128660520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x128660a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x128660f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x1286613b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x128661850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x128661cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x128662190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x128662630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x128662ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x128662f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x128663410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x1286638b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x128663d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x1286641f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x128664690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x128664b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x128664fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x128665520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x128665c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x128666360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x128666a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1286671a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x128667460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x128667c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x128667f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x128668520 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x10d6046e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x10d604b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x10d604fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x10d605430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x10d6058a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x10d605d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x10d606180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x10d6065f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x10d606a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x10d606ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x10d607340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x10d607a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x10d608580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x10d608d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x10d609540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x10d609c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x10d60a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x10d60aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x10d60b1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x10d60b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x10d60c010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x10d60c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x10d60ce50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x10d60d570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x10d60dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x10d60df50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x10d60e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x10d60e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x10d60eaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x10d60ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x10d60f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x10d60f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x10d60fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x10d610030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x10d6104a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x10d610910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x10d610d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x10d6111f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x10d611660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x10d611ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x10d611f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x10d6123b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x10d612820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x10d612c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x10d613100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x10d613570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x10d6139e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x10d613e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x10d6142c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x10d614730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x10d614ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x10d615010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x10d615480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x10d6158f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x10d615d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x10d6161d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x10d616740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x10d616c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x10d6170b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x10d617520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x10d617990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x10d617e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x10d618270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x10d6186e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x10d618b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x10d618fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x10d619430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x10d6198a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x10d619d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x10d61a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x10d61a5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x10d61aa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x10d61aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x10d61b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x10d61b7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x10d61bc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x10d61c090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x10d61c500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x10d61c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x10d61cde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x10d61d250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x10d61d6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x10d61db30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x10d61dfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x10d61e410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x10d61e880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x10d61ecf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x10d61f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x10d61f5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x10d61fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x10d61feb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x10d620320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x10d620790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x10d620c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x10d621070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x10d6214e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x10d621950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x10d621dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x10d622230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x10d6226a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x10d622b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x10d622f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x10d6233f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x10d623c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x10d623f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x10d6243b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x10d624820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x10d624c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x10d625100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x10d625570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x10d6259e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x10d625e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x10d6262c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x10d626730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x10d626ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x10d627010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x10d627480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x10d6278f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x10d627d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x10d6281d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x10d628640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x10d628ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x10d628f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x10d629390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x10d629800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x10d629c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x10d62a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x10d62a550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x10d62a9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x10d62ae30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x10d62b2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x10d62b710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x10d62bb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x10d62bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x10d62c460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x10d62c8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x10d62cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x10d62d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x10d62d620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x10d62da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x10d62df00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x10d62e370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x10d62e7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x10d62ec50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x10d62f0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x10d62f530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x10d62f9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x10d62fe10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x10d630280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x10d6306f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x10d630b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x10d630fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x10d631440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x10d6318b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x10d631d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x10d632190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x10d632600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x10d632a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x10d632ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x10d633350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x10d6337c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x10d633c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x10d6340a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x10d634510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x10d634980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x10d634df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x10d635260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x10d6356d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x10d635b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x10d635fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x10d636420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x10d636890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x10d636d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x10d637170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x10d6375e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x10d637a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x10d637ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x10d638330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x10d6387a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x10d638c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x10d639080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x10d6394f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x10d639960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x10d639dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x10d63a240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x10d63a6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x10d63ab20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x10d63af90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x10d63b400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x10d63b870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x10d63bce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x10d63c150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x10d63c5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x10d63ca30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x10d63cea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x10d63d310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x10d63d780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x10d63dbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x10d63e060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x10d63e4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x10d63e940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x10d63edb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x10d63f220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x10d63f690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x10d63fb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x10d63ff70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x10d6403e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x10d640850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x10d640cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x10d641130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x10d641cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x10d641f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x10d642230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x10d6426a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x10d642b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x10d642f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x10d6433f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x10d643860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x10d643cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x10d644140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x10d6445b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x10d644a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x10d644e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x10d645300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x10d645770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x10d645be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x10d646050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x10d6464c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x10d646930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x10d646da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x10d647210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x10d647680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x10d647af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x10d647f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x10d6483d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x10d648840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x10d648cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x10d649120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x10d649590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x10d649a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x10d649e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x10d64a2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x10d64a750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x10d64abc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x10d64b030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x10d64b4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x10d64b910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x10d64bd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x10d64c1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x10d64c660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x10d64cad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x10d64cf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x10d64d3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x10d64d820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x10d64dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x10d64e100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x10d64e570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x10d64e9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x10d64ee50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x10d64f2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x10d64f730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x10d64fba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x10d650010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x10d650480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x10d6508f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x10d650d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x10d6511d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x10d651640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x10d651ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x10d651f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x10d652390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x10d652800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x10d652c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x10d6530e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x10d653550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x10d6539c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x10d653e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x10d6542a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x10d654710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x10d654b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x10d654ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x10d655460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x10d6558d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x10d656340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x10d656a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x10d657180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x10d6578a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x10d657b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x10d657fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x10d6585d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x10d658be0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.755s
user	0m0.283s
sys	0m0.308s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4612 (0cec062a)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 'Ċ'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x158710580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x158710c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x158711240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x1587117f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x158711da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x158712350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x158712900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x158712eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x158713460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x158713960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x158713e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x158714360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x158714e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x158715630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x158715e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x158716560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x158716c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x1587173a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x158717ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x158718290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x1587189b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x1587190d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x1587197f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x15871a090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x15871a7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x15871aa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x15871b080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x15871bcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x15871c230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x15871c4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x15871c990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x15871cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x15871d4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x15871da20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x15871dce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x15871e180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x15871e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x15871eac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x15871ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x15871f400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x15871f8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x15871fd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x1587201e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x158720680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x158720940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x158720f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x158721560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x158721e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x158722490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x158722aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x1587230b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x1587236c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x158723cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x1587242e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x158724ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x158724f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x158725410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x1587256d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x158725ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x1587264d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x158726790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x158726c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x1587270d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x158727570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x158727a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x158727eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x158728350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x1587287f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x158728c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x158729130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x1587295d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x158729a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x158729f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x15872a460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x15872a9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x15872af00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x15872b450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x15872b9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x15872bef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x15872c440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x15872c990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x15872cee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x15872d430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x15872d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x15872ded0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x15872e420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x15872e970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x15872eec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x15872f410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x15872f960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x15872feb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x158730400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x158730950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x158730ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x1587313f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x158731940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x158731e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x158721b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x158732300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x158732ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x158733000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x158733550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x158733aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x158733ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x158734540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x158734a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x158734fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x158735530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x158735a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x158735fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x158736520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x158736a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x158736fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x158737460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x158737900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x158737da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x158738240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x1587386e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x158738b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x158739020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x1587394c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x158739960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x158739e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x15873a2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x15873a740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x15873abe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x15873b080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x15873b520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x15873b9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x15873be60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x15873c300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x15873c7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x15873cc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x15873d0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x15873d580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x15873da20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x15873dec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x15873e360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x15873e800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x15873eca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x15873f140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x15873f5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x15873fa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x15873ff20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x1587403c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x158740860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x158740d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x1587411a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x158741640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x158741ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x158741f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x158742420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x1587428c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x158742d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x158743200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x1587436a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x158743b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x158743fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x158744480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x158744920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x158744dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x158745260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x158745700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x158745ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x158746040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x1587464e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x158746980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x158746e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x1587472c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x158747760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x158747c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x1587480a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x158748540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x1587489e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x158748e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x158749320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x1587497c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x158749c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x15874a100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x15874a5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x15874aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x15874aee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x15874b380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x15874b820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x15874bcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x15874c160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x15874c600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x15874caa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x15874cf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x15874d3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x15874d880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x15874dd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x15874e1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x15874e710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x15874ec60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x15874f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x15874f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x15874f9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x15874ffd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x1587505e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x158750bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x1587513e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x158751880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x158751b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x158752150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x158752760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x158752f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x1587533f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x158753890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x158753d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x1587544e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x158754a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x158754f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x1587554d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x158755a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x158755f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x1587564c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x158756a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x158756f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x1587574b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x158757a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x158757f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x1587584a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x1587589f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x158758f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x158759490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x1587599e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x158759f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x15875a480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x15875a9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x15875af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x15875b470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x15875b9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x15875bf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x15875c460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x15875c9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x15875cf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x15875d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x15875d9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x15875def0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x15875e440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x15875e990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x15875eee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x15875f430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x15875f980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x15875fed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x158760420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x158760970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x158760ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x158761410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x158761960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x158761eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x158762400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x158762950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x158762ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x1587633f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x158763940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x158763e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x1587643e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x158764930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x158764e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x1587653d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x158765920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x158765e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x1587663c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x158766910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x158766e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x158767300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x1587677a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x158767c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x1587680e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x158768580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x158768a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x158768ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x158769360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x158769800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x158769ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x15876a140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x15876a5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x15876aa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x15876af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x15876b3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x15876b910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x15876c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x15876c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x15876ce70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x15876d590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x15876d850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x15876e040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x15876e300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x15876e910 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.103.496 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.103.500 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x1586059f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x158605e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x1586062d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x158606740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x158606bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x158607020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x158607490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x158607900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x158607d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x1586081e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x158608650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x158608d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x158609860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x15860a010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x15860a820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x15860af40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x15860b660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x15860bd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x15860c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x15860cbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x15860d2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x15860da10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x15860e130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x15860e850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x15860ef70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x15860f230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x15860f4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x15860f960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x15860fdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x158610240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x1586106b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x158610be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x158611050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x158611310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x158611780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x158611bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x158612060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x1586124d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x158612940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x158612db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x158613220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x158613690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x158613b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x158613f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x1586143e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x158614850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x158614cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x158615130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x1586155a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x158615a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x158615e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x1586162f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x158616760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x158616bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x158617040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x1586174b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x158617a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x158617f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x158618390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x158618800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x158618c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x1586190e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x158619550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x1586199c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x158619e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x15861a2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x15861a710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x15861ab80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x15861aff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x15861b460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x15861b8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x15861bd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x15861c1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x15861c620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x15861ca90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x15861cf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x15861d370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x15861d7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x15861dc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x15861e0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x15861e530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x15861e9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x15861ee10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x15861f280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x15861f6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x15861fb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x15861ffd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x158620440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x1586208b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x158620d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x158621190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x158621600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x158621a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x158621ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x158622350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x1586227c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x158622c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x1586230a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x158623510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x158623980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x158623df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x158624260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x1586246d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x158624b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x158624fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x158625420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x158625890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x158625d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x158626170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x1586265e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x158626a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x158626ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x158627330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x1586277a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x158627c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x158628080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x1586284f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x158628960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x158628dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x158629240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x1586296b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x158629b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x158629f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x15862a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x15862a870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x15862ace0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x15862b150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x15862b5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x15862ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x15862bea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x15862c310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x15862c780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x15862cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x15862d060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x15862d4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x15862d940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x15862ddb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x15862e220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x15862e690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x15862eb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x15862ef70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x15862f3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x15862f850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x15862fcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x158630130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x1586305a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x158630a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x158630e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x1586312f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x158631760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x158631bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x158632040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x1586324b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x158632920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x158632d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x158633200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x158633670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x158633ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x158633f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x1586343c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x158634830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x158634ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x158635110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x158635580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x1586359f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x158635e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x158636610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x1586368d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x158636d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x1586371b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x158637620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x158637a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x158637f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x158638370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x1586387e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x158638c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x1586390c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x158639530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x1586399a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x158639e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x15863a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x15863a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x15863ab60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x15863afd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x15863b440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x15863b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x15863bd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x15863c190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x15863c600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x15863ca70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x15863cee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x15863d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x15863d7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x15863dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x15863e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x15863e510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x15863e980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x15863edf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x15863f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x15863f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x15863fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x15863ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x158640420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x158640890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x158640d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x158641170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x1586415e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x158641a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x158641ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x158642330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x158642ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x1586431a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x158643460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x1586438d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x158643d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x1586441b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x158644620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x158644a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x158644f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x158645370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x1586457e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x158645c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x1586460c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x158646530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x1586469a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x158646e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x158647280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x1586476f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x158647b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x158647fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x158648440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x1586488b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x158648d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x158649190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x158649600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x158649a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x158649ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x15864a350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x15864a7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x15864ac30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x15864b0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x15864b510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x15864b980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x15864bdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x15864c260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x15864c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x15864cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x15864cfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x15864d420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x15864d890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x15864dd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x15864e170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x15864e5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x15864ea50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x15864eec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x15864f330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x15864f7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x15864fc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x158650080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x1586504f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x158650960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x158650dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x158651240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x1586516b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x158651b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x158651f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x158652400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x158652870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x158652ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x158653150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x1586535c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x158653a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x158653ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x158654310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x158654780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x158654bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x158655060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x1586554d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x158655940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x158655db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x158656220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x158656690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x158656b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x158657570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x158657c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x1586583b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x158658ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x158658d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x158659200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x158659800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x158659e10 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x15876e5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x158750290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x15874fc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x1587508a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x158723980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x158723370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x158725990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x158752410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x15871ad30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x158721820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x158722140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x158722750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x158720c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x158722d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x158719d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x158725fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x1587325c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x15876db10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x15871cf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x15871d1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x158752a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x158750eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x15871b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x15871b600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x15871b8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x15876ed70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x15876f030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x15876f2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x15876f5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x15876f870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x15876fb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x15876fdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x1587700b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x158770370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x158770630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x1587708f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x158770bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x158770e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x158771130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x1587713f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x1587716b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x158771970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x158771c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x158771ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x1587721b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x158772470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x158772730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x1587729f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x158772cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x158772f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x158773230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x1587734f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x1587737b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x158773a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x158773d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x158773ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x1587742b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x158774570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x158774830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x158774af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x158774db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x158775070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x158775330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x1587755f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x1587758b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x158775b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x158775e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x1587760f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x1587763b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x158776670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x158776930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x158776bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x158776eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x158777170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x158777430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x1587776f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x1587779b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x158777c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x158777f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x1587781f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x1587784b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x158778770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x158778a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x158778cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x158778fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x158779270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x158779530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x1587797f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x158779ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x158779d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x15877a030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x15877a2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x15877a5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x15877a870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x15877ab30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x15877adf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x15877b0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x15877b370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x15877b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x15877b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x15877bbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x15877be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x15877c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x15877c3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x15877c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x15877c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x15877cc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x15877cef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x15877d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x15877d470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x15877d730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x15877d9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x15877dcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x15877df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x15877e230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x15877e4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x15877e7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x15877ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x15877ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x15877eff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x15877f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x15877f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x15877f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x15877faf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x15877fdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x158780070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x158780330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x1587805f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x1587808b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x158780b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x158780e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x1587810f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x1587813b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x158781670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x158781930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x158781bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x158781eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x158782170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x158782430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x1587826f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x1587829b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x158782c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x158782f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x1587831f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x1587834b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x158783770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x158783a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x158783cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x158783fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x158784270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x158784530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x1587847f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x158784ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x158784d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x158785030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x1587852f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x1587855b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x158785870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x158785b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x158785df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x1587860b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x158786370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x158786630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x1587868f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x158786bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x158786e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x158787130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x1587873f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x1587876b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x158787970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x158787c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x158787ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x1587881b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x158788470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x158788730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x1587889f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x158788cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x158788f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x158789230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x1587894f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x1587897b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x158789a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x158789d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x158789ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x15878a2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x15878a570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x15878a830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x15878aaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x15878adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x15878b070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x15878b330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x15878b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x15878b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x15878bb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x15878be30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x15878c0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x15878c3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x15878c670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x15878c930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x15878cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x15878ceb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x15878d170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x15878d430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x15878d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x15878d9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x15878dc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x15878df30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x15878e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x15878e5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x15878ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x15878f240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x15878f500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x15878f7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x15878fc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x1587900a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x158790510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x158790980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x158790df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x158791260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x1587916d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x158791b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x158791fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x158792420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x158792890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x158792d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x158793170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x1587935e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x158793a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x158793ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x158794330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x1587947a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x158794c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x158795080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x1587954f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x158795960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x158795dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x158796240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x1587966b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x158796b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x158796f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x158797400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x158797870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x158797ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x158798150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x1587985c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x158798a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x158798ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x158799310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x158799780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x158799bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x15879a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x15879a4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x15879a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x15879adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x15879b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x15879b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x15879bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x15879bf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x15879c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x15879c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x15879ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x15879d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x15879d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x15879da10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x15879de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x15879e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x15879e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x15879ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x15879f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x15879f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x15879f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x15879fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x1587a0200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x1587a0670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x1587a0ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1587a0f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x1587a13c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x1587a1830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x1587a1ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x1587a2110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x1587a2580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x1587a29f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x1587a2e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x1587a38d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x1587a3ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x1587a4710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1587a4e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x1587a50f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x1587a58e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1587a5ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x1587a61b0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.967s
user	0m0.235s
sys	0m0.188s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
