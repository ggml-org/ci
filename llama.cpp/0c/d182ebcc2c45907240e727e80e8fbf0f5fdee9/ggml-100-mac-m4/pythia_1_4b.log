Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:49 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:305 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORT_DOTPROD
-- Performing Test GGML_COMPILER_SUPPORT_DOTPROD - Success
-- ARM feature DOTPROD enabled
-- Performing Test GGML_COMPILER_SUPPORT_MATMUL_INT8
-- Performing Test GGML_COMPILER_SUPPORT_MATMUL_INT8 - Success
-- ARM feature MATMUL_INT8 enabled
-- Adding CPU backend variant ggml-cpu: -march=armv8.2a+dotprod+i8mm __ARM_FEATURE_DOTPROD;__ARM_FEATURE_MATMUL_INT8
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (1.4s)
-- Generating done (0.2s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m1.637s
user	0m0.686s
sys	0m0.953s
++ nproc
+ make -j10
[  0%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  1%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  2%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  2%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  4%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  4%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  5%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  6%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  6%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  6%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-aarch64.c.o
[  6%] Built target build_info
[  6%] Built target xxhash
[  6%] Built target sha1
[  6%] Built target sha256
[  7%] Linking CXX shared library libggml-base.dylib
[  7%] Built target ggml-base
[  7%] Generate assembly for embedded Metal library
Embedding Metal library
[  7%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[ 10%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[ 10%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.c.o
[ 10%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[ 11%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[ 12%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 12%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[ 12%] Linking CXX shared library libggml-blas.dylib
[ 13%] Linking CXX shared library libggml-cpu.dylib
[ 13%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 14%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 14%] Built target ggml-cpu
[ 14%] Built target ggml-blas
[ 15%] Linking C shared library libggml-metal.dylib
[ 15%] Built target ggml-metal
[ 15%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 16%] Linking CXX shared library libggml.dylib
[ 16%] Built target ggml
[ 16%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 16%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 20%] Linking CXX executable ../../bin/llama-gguf
[ 21%] Linking CXX executable ../../bin/llama-gguf-hash
[ 22%] Linking CXX shared library libllama.dylib
[ 22%] Built target llama-gguf
[ 22%] Built target llama-gguf-hash
[ 22%] Built target llama
[ 22%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 23%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 23%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 24%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 24%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 25%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 26%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 26%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 26%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 27%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 29%] Linking C executable ../bin/test-c
[ 29%] Linking CXX executable ../../bin/llama-simple
[ 29%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 29%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 29%] Linking CXX executable ../../bin/llama-simple-chat
[ 30%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 31%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 33%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 33%] Linking CXX executable ../../bin/llama-run
[ 33%] Linking CXX executable ../../bin/llama-quantize-stats
[ 33%] Built target llava
[ 34%] Linking CXX static library libcommon.a
[ 34%] Built target test-c
[ 34%] Linking CXX shared library libllava_shared.dylib
[ 34%] Built target llama-simple-chat
[ 35%] Linking CXX static library libllava_static.a
[ 35%] Built target llama-simple
[ 35%] Built target llama-run
[ 35%] Built target llama-quantize-stats
[ 35%] Built target common
[ 35%] Built target llava_static
[ 35%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 35%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 35%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 35%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 35%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 35%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 35%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 36%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 36%] Built target llava_shared
[ 37%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 38%] Linking CXX executable ../bin/test-tokenizer-0
[ 39%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 40%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 41%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 41%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 45%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 45%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 45%] Linking CXX executable ../bin/test-arg-parser
[ 46%] Linking CXX executable ../bin/test-sampling
[ 46%] Linking CXX executable ../bin/test-grammar-parser
[ 46%] Linking CXX executable ../bin/test-chat-template
[ 46%] Linking CXX executable ../bin/test-log
[ 46%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 46%] Built target test-tokenizer-0
[ 47%] Linking CXX executable ../bin/test-grammar-integration
[ 47%] Built target test-tokenizer-1-spm
[ 47%] Built target test-tokenizer-1-bpe
[ 48%] Linking CXX executable ../bin/test-llama-grammar
[ 48%] Built target test-grammar-parser
[ 48%] Built target test-sampling
[ 48%] Built target test-arg-parser
[ 48%] Built target test-chat-template
[ 49%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 49%] Built target test-log
[ 50%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 52%] Built target test-llama-grammar
[ 53%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 53%] Linking CXX executable ../bin/test-model-load-cancel
[ 53%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 56%] Linking CXX executable ../bin/test-autorelease
[ 56%] Built target test-grammar-integration
[ 57%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 57%] Linking CXX executable ../bin/test-barrier
[ 58%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 59%] Linking CXX executable ../bin/test-backend-ops
[ 59%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 59%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 59%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 59%] Linking CXX executable ../bin/test-quantize-perf
[ 60%] Linking CXX executable ../bin/test-quantize-fns
[ 61%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 62%] Linking CXX executable ../bin/test-rope
[ 62%] Built target test-model-load-cancel
[ 62%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 62%] Built target test-autorelease
[ 62%] Built target test-barrier
[ 62%] Built target test-backend-ops
[ 63%] Linking CXX executable ../../bin/llama-batched-bench
[ 63%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 64%] Linking CXX executable ../../bin/llama-batched
[ 66%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 66%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 66%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 67%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 67%] Built target test-quantize-perf
[ 67%] Built target test-quantize-fns
[ 67%] Built target test-rope
[ 67%] Linking CXX executable ../../bin/llama-embedding
[ 67%] Linking CXX executable ../../bin/llama-eval-callback
[ 68%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 69%] Linking CXX executable ../../bin/llama-gguf-split
[ 69%] Built target llama-batched-bench
[ 69%] Built target test-json-schema-to-grammar
[ 70%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 71%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 71%] Built target llama-batched
[ 71%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 71%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 71%] Linking CXX executable ../../bin/llama-infill
[ 71%] Linking CXX executable ../../bin/llama-imatrix
[ 72%] Linking CXX executable ../../bin/llama-gritlm
[ 73%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 74%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 74%] Built target llama-embedding
[ 74%] Built target llama-eval-callback
[ 74%] Built target llama-gguf-split
[ 74%] Built target llama-gbnf-validator
[ 75%] Linking CXX executable ../../bin/llama-bench
[ 75%] Linking CXX executable ../../bin/llama-lookup
[ 75%] Linking CXX executable ../../bin/llama-lookahead
[ 76%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 76%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 76%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 76%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 76%] Built target llama-imatrix
[ 76%] Built target llama-infill
[ 76%] Built target llama-gritlm
[ 77%] Linking CXX executable ../../bin/llama-lookup-create
[ 78%] Linking CXX executable ../../bin/llama-lookup-merge
[ 79%] Linking CXX executable ../../bin/llama-lookup-stats
[ 80%] Linking CXX executable ../../bin/llama-cli
[ 81%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 81%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 81%] Built target llama-lookup
[ 81%] Built target llama-bench
[ 82%] Built target llama-lookahead
[ 82%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 83%] Generating loading.html.hpp
[ 84%] Linking CXX executable ../../bin/llama-parallel
[ 84%] Linking CXX executable ../../bin/llama-passkey
[ 85%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 86%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 86%] Linking CXX executable ../../bin/llama-perplexity
[ 86%] Built target llama-lookup-merge
[ 86%] Built target llama-lookup-create
[ 86%] Built target llama-lookup-stats
[ 86%] Built target llama-cli
[ 86%] Generating index.html.hpp
[ 86%] Linking CXX executable ../../bin/llama-quantize
[ 87%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 88%] Linking CXX executable ../../bin/llama-retrieval
[ 90%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 90%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 90%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 90%] Built target llama-passkey
[ 90%] Built target llama-parallel
[ 90%] Built target llama-perplexity
[ 90%] Linking CXX executable ../../bin/llama-speculative
[ 90%] Built target llama-quantize
[ 90%] Linking CXX executable ../../bin/llama-speculative-simple
[ 91%] Linking CXX executable ../../bin/llama-save-load-state
[ 91%] Linking CXX executable ../../bin/llama-tokenize
[ 92%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 93%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 94%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 94%] Built target llama-retrieval
[ 95%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 95%] Linking CXX executable ../../bin/llama-cvector-generator
[ 95%] Linking CXX executable ../../bin/llama-export-lora
[ 95%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 95%] Built target llama-save-load-state
[ 95%] Built target llama-tokenize
[ 95%] Built target llama-speculative
[ 95%] Built target llama-speculative-simple
[ 95%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 95%] Linking CXX executable ../../bin/llama-llava-cli
[ 96%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 97%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 98%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 98%] Built target llama-convert-llama2c-to-ggml
[ 98%] Built target llama-export-lora
[ 98%] Built target llama-cvector-generator
[ 99%] Linking CXX executable ../../bin/llama-vdot
[ 99%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Built target llama-llava-cli
[ 99%] Built target llama-minicpmv-cli
[ 99%] Built target llama-vdot
[ 99%] Built target llama-q8dot
[ 99%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m2.394s
user	0m5.299s
sys	0m9.070s

main: quantize time =  3778.28 ms
main:    total time =  3778.28 ms

main: quantize time =  2287.06 ms
main:    total time =  2287.06 ms

main: quantize time =  2281.37 ms
main:    total time =  2281.37 ms

main: quantize time =  4167.64 ms
main:    total time =  4167.64 ms

main: quantize time =  2552.08 ms
main:    total time =  2552.08 ms

main: quantize time =  4940.80 ms
main:    total time =  4940.80 ms

main: quantize time =  5653.45 ms
main:    total time =  5653.45 ms

main: quantize time =  6832.92 ms
main:    total time =  6832.92 ms

main: quantize time =  5831.20 ms
main:    total time =  5831.20 ms

main: quantize time =  4454.58 ms
main:    total time =  4454.58 ms
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.104 I build: 4271 (0cd182eb) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.215 I main: llama backend init
0.00.000.221 I main: load the model and apply lora adapter, if any
0.00.087.719 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.098.604 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.098.617 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.098.624 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.098.625 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.098.626 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.098.627 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.098.627 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.098.629 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.098.630 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.098.631 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.098.632 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.098.632 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.098.633 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.098.634 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.098.639 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.098.640 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.098.640 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.105.521 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.107.743 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.114.731 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.114.736 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.114.737 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.114.738 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.114.738 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.114.740 I llama_model_loader: - type  f32:  194 tensors
0.00.114.740 I llama_model_loader: - type  f16:   98 tensors
0.00.153.056 I llm_load_vocab: special tokens cache size = 25
0.00.160.669 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.160.672 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.160.672 I llm_load_print_meta: arch             = gptneox
0.00.160.673 I llm_load_print_meta: vocab type       = BPE
0.00.160.673 I llm_load_print_meta: n_vocab          = 50304
0.00.160.673 I llm_load_print_meta: n_merges         = 50009
0.00.160.673 I llm_load_print_meta: vocab_only       = 0
0.00.160.673 I llm_load_print_meta: n_ctx_train      = 2048
0.00.160.674 I llm_load_print_meta: n_embd           = 2048
0.00.160.674 I llm_load_print_meta: n_layer          = 24
0.00.160.677 I llm_load_print_meta: n_head           = 16
0.00.160.678 I llm_load_print_meta: n_head_kv        = 16
0.00.160.698 I llm_load_print_meta: n_rot            = 32
0.00.160.698 I llm_load_print_meta: n_swa            = 0
0.00.160.699 I llm_load_print_meta: n_embd_head_k    = 128
0.00.160.699 I llm_load_print_meta: n_embd_head_v    = 128
0.00.160.699 I llm_load_print_meta: n_gqa            = 1
0.00.160.700 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.160.701 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.160.701 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.160.702 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.160.702 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.160.702 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.160.702 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.160.703 I llm_load_print_meta: n_ff             = 8192
0.00.160.703 I llm_load_print_meta: n_expert         = 0
0.00.160.703 I llm_load_print_meta: n_expert_used    = 0
0.00.160.704 I llm_load_print_meta: causal attn      = 1
0.00.160.704 I llm_load_print_meta: pooling type     = 0
0.00.160.704 I llm_load_print_meta: rope type        = 2
0.00.160.704 I llm_load_print_meta: rope scaling     = linear
0.00.160.704 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.160.705 I llm_load_print_meta: freq_scale_train = 1
0.00.160.705 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.160.708 I llm_load_print_meta: rope_finetuned   = unknown
0.00.160.708 I llm_load_print_meta: ssm_d_conv       = 0
0.00.160.708 I llm_load_print_meta: ssm_d_inner      = 0
0.00.160.708 I llm_load_print_meta: ssm_d_state      = 0
0.00.160.708 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.160.708 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.160.718 I llm_load_print_meta: model type       = 1.4B
0.00.160.719 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.160.719 I llm_load_print_meta: model params     = 1.41 B
0.00.160.720 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.160.721 I llm_load_print_meta: general.name     = 1.4B
0.00.160.721 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.160.722 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.160.722 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.160.722 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.160.722 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.160.722 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.160.723 I llm_load_print_meta: max token length = 1024
0.00.163.573 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.163.573 I llm_load_tensors: offloading output layer to GPU
0.00.163.573 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.163.592 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.163.593 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.164.593 I llama_new_context_with_model: n_seq_max     = 1
0.00.164.594 I llama_new_context_with_model: n_ctx         = 2048
0.00.164.595 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.164.595 I llama_new_context_with_model: n_batch       = 2048
0.00.164.595 I llama_new_context_with_model: n_ubatch      = 512
0.00.164.595 I llama_new_context_with_model: flash_attn    = 0
0.00.164.596 I llama_new_context_with_model: freq_base     = 10000.0
0.00.164.596 I llama_new_context_with_model: freq_scale    = 1
0.00.164.596 I ggml_metal_init: allocating
0.00.164.599 I ggml_metal_init: found device: Apple M4
0.00.164.602 I ggml_metal_init: picking default device: Apple M4
0.00.165.245 I ggml_metal_init: using embedded metal library
0.00.203.773 I ggml_metal_init: GPU name:   Apple M4
0.00.203.776 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.203.777 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.203.777 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.203.777 I ggml_metal_init: simdgroup reduction   = true
0.00.203.778 I ggml_metal_init: simdgroup matrix mul. = true
0.00.203.778 I ggml_metal_init: has bfloat            = true
0.00.203.778 I ggml_metal_init: use bfloat            = true
0.00.203.779 I ggml_metal_init: hasUnifiedMemory      = true
0.00.203.780 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.264.193 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.264.198 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.264.219 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.265.266 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.265.267 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.265.268 I llama_new_context_with_model: graph nodes  = 967
0.00.265.268 I llama_new_context_with_model: graph splits = 2
0.00.265.290 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.354.885 I main: llama threadpool init, n_threads = 4
0.00.354.914 I 
0.00.354.949 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.354.951 I 
0.00.355.030 I sampler seed: 1234
0.00.355.035 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.355.057 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.355.059 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.355.059 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.204.285 I llama_perf_sampler_print:    sampling time =       1.29 ms /    71 runs   (    0.02 ms per token, 55081.46 tokens per second)
0.02.204.285 I llama_perf_context_print:        load time =     267.15 ms
0.02.204.286 I llama_perf_context_print: prompt eval time =      43.64 ms /     7 tokens (    6.23 ms per token,   160.41 tokens per second)
0.02.204.287 I llama_perf_context_print:        eval time =    1802.66 ms /    63 runs   (   28.61 ms per token,    34.95 tokens per second)
0.02.204.287 I llama_perf_context_print:       total time =    1849.40 ms /    70 tokens
0.02.204.496 I ggml_metal_free: deallocating

real	0m2.555s
user	0m0.154s
sys	0m0.116s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.036 I build: 4271 (0cd182eb) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.066 I main: llama backend init
0.00.000.069 I main: load the model and apply lora adapter, if any
0.00.009.602 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.030.450 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.030.457 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.030.459 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.030.464 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.030.465 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.030.465 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.030.465 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.030.466 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.030.467 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.030.467 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.030.467 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.030.467 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.030.468 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.030.468 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.030.470 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.030.470 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.030.470 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.034.580 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.035.653 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.039.939 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.039.941 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.039.942 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.039.942 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.039.943 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.039.943 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.039.944 I llama_model_loader: - type  f32:  194 tensors
0.00.039.944 I llama_model_loader: - type q8_0:   98 tensors
0.00.065.250 I llm_load_vocab: special tokens cache size = 25
0.00.073.522 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.073.526 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.073.527 I llm_load_print_meta: arch             = gptneox
0.00.073.527 I llm_load_print_meta: vocab type       = BPE
0.00.073.527 I llm_load_print_meta: n_vocab          = 50304
0.00.073.528 I llm_load_print_meta: n_merges         = 50009
0.00.073.528 I llm_load_print_meta: vocab_only       = 0
0.00.073.532 I llm_load_print_meta: n_ctx_train      = 2048
0.00.073.532 I llm_load_print_meta: n_embd           = 2048
0.00.073.532 I llm_load_print_meta: n_layer          = 24
0.00.073.536 I llm_load_print_meta: n_head           = 16
0.00.073.537 I llm_load_print_meta: n_head_kv        = 16
0.00.073.552 I llm_load_print_meta: n_rot            = 32
0.00.073.552 I llm_load_print_meta: n_swa            = 0
0.00.073.552 I llm_load_print_meta: n_embd_head_k    = 128
0.00.073.553 I llm_load_print_meta: n_embd_head_v    = 128
0.00.073.553 I llm_load_print_meta: n_gqa            = 1
0.00.073.554 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.073.555 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.073.556 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.073.557 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.073.557 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.073.557 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.073.557 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.073.558 I llm_load_print_meta: n_ff             = 8192
0.00.073.558 I llm_load_print_meta: n_expert         = 0
0.00.073.559 I llm_load_print_meta: n_expert_used    = 0
0.00.073.559 I llm_load_print_meta: causal attn      = 1
0.00.073.559 I llm_load_print_meta: pooling type     = 0
0.00.073.559 I llm_load_print_meta: rope type        = 2
0.00.073.559 I llm_load_print_meta: rope scaling     = linear
0.00.073.560 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.073.560 I llm_load_print_meta: freq_scale_train = 1
0.00.073.560 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.073.561 I llm_load_print_meta: rope_finetuned   = unknown
0.00.073.561 I llm_load_print_meta: ssm_d_conv       = 0
0.00.073.563 I llm_load_print_meta: ssm_d_inner      = 0
0.00.073.563 I llm_load_print_meta: ssm_d_state      = 0
0.00.073.563 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.073.564 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.073.574 I llm_load_print_meta: model type       = 1.4B
0.00.073.574 I llm_load_print_meta: model ftype      = Q8_0
0.00.073.575 I llm_load_print_meta: model params     = 1.41 B
0.00.073.576 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.073.576 I llm_load_print_meta: general.name     = 1.4B
0.00.073.576 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.073.576 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.073.577 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.073.577 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.073.577 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.073.577 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.073.578 I llm_load_print_meta: max token length = 1024
0.00.076.460 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.076.461 I llm_load_tensors: offloading output layer to GPU
0.00.076.461 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.076.473 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.076.474 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.077.853 I llama_new_context_with_model: n_seq_max     = 1
0.00.077.854 I llama_new_context_with_model: n_ctx         = 2048
0.00.077.854 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.077.854 I llama_new_context_with_model: n_batch       = 2048
0.00.077.855 I llama_new_context_with_model: n_ubatch      = 512
0.00.077.855 I llama_new_context_with_model: flash_attn    = 0
0.00.077.855 I llama_new_context_with_model: freq_base     = 10000.0
0.00.077.856 I llama_new_context_with_model: freq_scale    = 1
0.00.077.857 I ggml_metal_init: allocating
0.00.077.866 I ggml_metal_init: found device: Apple M4
0.00.077.870 I ggml_metal_init: picking default device: Apple M4
0.00.078.750 I ggml_metal_init: using embedded metal library
0.00.082.052 I ggml_metal_init: GPU name:   Apple M4
0.00.082.054 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.082.054 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.082.055 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.082.055 I ggml_metal_init: simdgroup reduction   = true
0.00.082.055 I ggml_metal_init: simdgroup matrix mul. = true
0.00.082.055 I ggml_metal_init: has bfloat            = true
0.00.082.056 I ggml_metal_init: use bfloat            = true
0.00.082.056 I ggml_metal_init: hasUnifiedMemory      = true
0.00.082.057 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.120.393 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.120.408 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.120.436 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.121.527 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.121.528 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.121.529 I llama_new_context_with_model: graph nodes  = 967
0.00.121.529 I llama_new_context_with_model: graph splits = 2
0.00.121.538 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.324.963 I main: llama threadpool init, n_threads = 4
0.01.324.993 I 
0.01.325.025 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.325.025 I 
0.01.325.167 I sampler seed: 1234
0.01.325.171 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.325.181 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.325.181 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.325.181 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.420.076 I llama_perf_sampler_print:    sampling time =       1.23 ms /    71 runs   (    0.02 ms per token, 57676.69 tokens per second)
0.02.420.077 I llama_perf_context_print:        load time =    1315.36 ms
0.02.420.077 I llama_perf_context_print: prompt eval time =      39.76 ms /     7 tokens (    5.68 ms per token,   176.05 tokens per second)
0.02.420.081 I llama_perf_context_print:        eval time =    1052.14 ms /    63 runs   (   16.70 ms per token,    59.88 tokens per second)
0.02.420.082 I llama_perf_context_print:       total time =    1095.12 ms /    70 tokens
0.02.420.268 I ggml_metal_free: deallocating

real	0m2.437s
user	0m0.124s
sys	0m0.222s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.037 I build: 4271 (0cd182eb) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.067 I main: llama backend init
0.00.000.069 I main: load the model and apply lora adapter, if any
0.00.018.848 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.027.769 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.027.774 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.027.776 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.027.777 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.027.777 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.027.777 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.027.778 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.027.779 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.027.779 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.027.779 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.027.782 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.027.782 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.027.783 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.027.783 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.027.786 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.027.786 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.027.786 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.032.338 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.033.594 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.038.133 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.038.135 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.038.135 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.038.136 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.038.136 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.038.136 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.038.137 I llama_model_loader: - type  f32:  194 tensors
0.00.038.137 I llama_model_loader: - type q4_0:   97 tensors
0.00.038.138 I llama_model_loader: - type q6_K:    1 tensors
0.00.066.858 I llm_load_vocab: special tokens cache size = 25
0.00.076.351 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.076.356 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.076.356 I llm_load_print_meta: arch             = gptneox
0.00.076.357 I llm_load_print_meta: vocab type       = BPE
0.00.076.357 I llm_load_print_meta: n_vocab          = 50304
0.00.076.357 I llm_load_print_meta: n_merges         = 50009
0.00.076.358 I llm_load_print_meta: vocab_only       = 0
0.00.076.358 I llm_load_print_meta: n_ctx_train      = 2048
0.00.076.360 I llm_load_print_meta: n_embd           = 2048
0.00.076.360 I llm_load_print_meta: n_layer          = 24
0.00.076.366 I llm_load_print_meta: n_head           = 16
0.00.076.367 I llm_load_print_meta: n_head_kv        = 16
0.00.076.380 I llm_load_print_meta: n_rot            = 32
0.00.076.381 I llm_load_print_meta: n_swa            = 0
0.00.076.382 I llm_load_print_meta: n_embd_head_k    = 128
0.00.076.382 I llm_load_print_meta: n_embd_head_v    = 128
0.00.076.383 I llm_load_print_meta: n_gqa            = 1
0.00.076.384 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.076.385 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.076.386 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.076.386 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.076.387 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.076.387 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.076.387 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.076.388 I llm_load_print_meta: n_ff             = 8192
0.00.076.388 I llm_load_print_meta: n_expert         = 0
0.00.076.388 I llm_load_print_meta: n_expert_used    = 0
0.00.076.389 I llm_load_print_meta: causal attn      = 1
0.00.076.389 I llm_load_print_meta: pooling type     = 0
0.00.076.389 I llm_load_print_meta: rope type        = 2
0.00.076.390 I llm_load_print_meta: rope scaling     = linear
0.00.076.390 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.076.391 I llm_load_print_meta: freq_scale_train = 1
0.00.076.391 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.076.391 I llm_load_print_meta: rope_finetuned   = unknown
0.00.076.392 I llm_load_print_meta: ssm_d_conv       = 0
0.00.076.392 I llm_load_print_meta: ssm_d_inner      = 0
0.00.076.392 I llm_load_print_meta: ssm_d_state      = 0
0.00.076.392 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.076.392 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.076.403 I llm_load_print_meta: model type       = 1.4B
0.00.076.404 I llm_load_print_meta: model ftype      = Q4_0
0.00.076.407 I llm_load_print_meta: model params     = 1.41 B
0.00.076.407 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.076.408 I llm_load_print_meta: general.name     = 1.4B
0.00.076.408 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.076.408 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.076.409 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.076.410 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.076.411 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.076.411 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.076.411 I llm_load_print_meta: max token length = 1024
0.00.079.415 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.079.415 I llm_load_tensors: offloading output layer to GPU
0.00.079.415 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.079.428 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.079.429 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.080.845 I llama_new_context_with_model: n_seq_max     = 1
0.00.080.846 I llama_new_context_with_model: n_ctx         = 2048
0.00.080.847 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.080.847 I llama_new_context_with_model: n_batch       = 2048
0.00.080.847 I llama_new_context_with_model: n_ubatch      = 512
0.00.080.848 I llama_new_context_with_model: flash_attn    = 0
0.00.080.848 I llama_new_context_with_model: freq_base     = 10000.0
0.00.080.848 I llama_new_context_with_model: freq_scale    = 1
0.00.080.849 I ggml_metal_init: allocating
0.00.080.854 I ggml_metal_init: found device: Apple M4
0.00.080.858 I ggml_metal_init: picking default device: Apple M4
0.00.081.792 I ggml_metal_init: using embedded metal library
0.00.085.404 I ggml_metal_init: GPU name:   Apple M4
0.00.085.406 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.085.406 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.085.407 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.085.407 I ggml_metal_init: simdgroup reduction   = true
0.00.085.407 I ggml_metal_init: simdgroup matrix mul. = true
0.00.085.408 I ggml_metal_init: has bfloat            = true
0.00.085.408 I ggml_metal_init: use bfloat            = true
0.00.085.408 I ggml_metal_init: hasUnifiedMemory      = true
0.00.085.409 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.122.131 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.122.145 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.122.169 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.123.331 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.123.333 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.123.333 I llama_new_context_with_model: graph nodes  = 967
0.00.123.334 I llama_new_context_with_model: graph splits = 2
0.00.123.350 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.788.064 I main: llama threadpool init, n_threads = 4
0.00.788.103 I 
0.00.788.136 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.788.137 I 
0.00.788.366 I sampler seed: 1234
0.00.788.370 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.788.392 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.788.392 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.788.392 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.477.480 I llama_perf_sampler_print:    sampling time =       1.20 ms /    71 runs   (    0.02 ms per token, 59117.40 tokens per second)
0.01.477.480 I llama_perf_context_print:        load time =     769.21 ms
0.01.477.481 I llama_perf_context_print: prompt eval time =      47.31 ms /     7 tokens (    6.76 ms per token,   147.97 tokens per second)
0.01.477.482 I llama_perf_context_print:        eval time =     638.84 ms /    63 runs   (   10.14 ms per token,    98.62 tokens per second)
0.01.477.483 I llama_perf_context_print:       total time =     689.42 ms /    70 tokens
0.01.477.681 I ggml_metal_free: deallocating

real	0m1.506s
user	0m0.129s
sys	0m0.166s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.034 I build: 4271 (0cd182eb) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.063 I main: llama backend init
0.00.000.065 I main: load the model and apply lora adapter, if any
0.00.010.128 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.027.687 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.027.691 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.027.692 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.027.697 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.027.697 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.027.697 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.027.698 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.027.698 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.027.699 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.027.699 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.027.699 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.027.699 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.027.700 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.027.700 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.027.701 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.027.702 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.027.702 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.032.064 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.033.237 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.037.469 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.037.471 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.037.471 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.037.471 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.037.471 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.037.472 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.037.472 I llama_model_loader: - type  f32:  194 tensors
0.00.037.472 I llama_model_loader: - type q4_1:   97 tensors
0.00.037.473 I llama_model_loader: - type q6_K:    1 tensors
0.00.062.464 I llm_load_vocab: special tokens cache size = 25
0.00.069.380 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.069.383 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.069.383 I llm_load_print_meta: arch             = gptneox
0.00.069.383 I llm_load_print_meta: vocab type       = BPE
0.00.069.384 I llm_load_print_meta: n_vocab          = 50304
0.00.069.384 I llm_load_print_meta: n_merges         = 50009
0.00.069.384 I llm_load_print_meta: vocab_only       = 0
0.00.069.384 I llm_load_print_meta: n_ctx_train      = 2048
0.00.069.384 I llm_load_print_meta: n_embd           = 2048
0.00.069.385 I llm_load_print_meta: n_layer          = 24
0.00.069.387 I llm_load_print_meta: n_head           = 16
0.00.069.388 I llm_load_print_meta: n_head_kv        = 16
0.00.069.400 I llm_load_print_meta: n_rot            = 32
0.00.069.400 I llm_load_print_meta: n_swa            = 0
0.00.069.400 I llm_load_print_meta: n_embd_head_k    = 128
0.00.069.400 I llm_load_print_meta: n_embd_head_v    = 128
0.00.069.403 I llm_load_print_meta: n_gqa            = 1
0.00.069.404 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.069.405 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.069.405 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.069.406 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.069.406 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.069.406 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.069.406 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.069.407 I llm_load_print_meta: n_ff             = 8192
0.00.069.407 I llm_load_print_meta: n_expert         = 0
0.00.069.407 I llm_load_print_meta: n_expert_used    = 0
0.00.069.407 I llm_load_print_meta: causal attn      = 1
0.00.069.408 I llm_load_print_meta: pooling type     = 0
0.00.069.408 I llm_load_print_meta: rope type        = 2
0.00.069.408 I llm_load_print_meta: rope scaling     = linear
0.00.069.408 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.069.410 I llm_load_print_meta: freq_scale_train = 1
0.00.069.410 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.069.410 I llm_load_print_meta: rope_finetuned   = unknown
0.00.069.410 I llm_load_print_meta: ssm_d_conv       = 0
0.00.069.410 I llm_load_print_meta: ssm_d_inner      = 0
0.00.069.410 I llm_load_print_meta: ssm_d_state      = 0
0.00.069.411 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.069.412 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.069.421 I llm_load_print_meta: model type       = 1.4B
0.00.069.421 I llm_load_print_meta: model ftype      = Q4_1
0.00.069.422 I llm_load_print_meta: model params     = 1.41 B
0.00.069.422 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.069.422 I llm_load_print_meta: general.name     = 1.4B
0.00.069.423 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.069.423 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.069.423 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.069.424 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.069.424 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.069.425 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.069.425 I llm_load_print_meta: max token length = 1024
0.00.071.573 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.071.573 I llm_load_tensors: offloading output layer to GPU
0.00.071.573 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.071.584 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.071.585 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.072.546 I llama_new_context_with_model: n_seq_max     = 1
0.00.072.546 I llama_new_context_with_model: n_ctx         = 2048
0.00.072.547 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.072.547 I llama_new_context_with_model: n_batch       = 2048
0.00.072.547 I llama_new_context_with_model: n_ubatch      = 512
0.00.072.547 I llama_new_context_with_model: flash_attn    = 0
0.00.072.548 I llama_new_context_with_model: freq_base     = 10000.0
0.00.072.548 I llama_new_context_with_model: freq_scale    = 1
0.00.072.548 I ggml_metal_init: allocating
0.00.072.551 I ggml_metal_init: found device: Apple M4
0.00.072.553 I ggml_metal_init: picking default device: Apple M4
0.00.073.145 I ggml_metal_init: using embedded metal library
0.00.075.854 I ggml_metal_init: GPU name:   Apple M4
0.00.075.855 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.075.856 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.075.856 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.075.856 I ggml_metal_init: simdgroup reduction   = true
0.00.075.857 I ggml_metal_init: simdgroup matrix mul. = true
0.00.075.857 I ggml_metal_init: has bfloat            = true
0.00.075.857 I ggml_metal_init: use bfloat            = true
0.00.075.857 I ggml_metal_init: hasUnifiedMemory      = true
0.00.075.858 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.110.841 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.110.847 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.110.866 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.111.914 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.111.916 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.111.916 I llama_new_context_with_model: graph nodes  = 967
0.00.111.916 I llama_new_context_with_model: graph splits = 2
0.00.111.930 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.742.437 I main: llama threadpool init, n_threads = 4
0.00.742.474 I 
0.00.742.503 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.742.505 I 
0.00.742.731 I sampler seed: 1234
0.00.742.735 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.742.753 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.742.754 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.742.754 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.470.474 I llama_perf_sampler_print:    sampling time =       1.06 ms /    71 runs   (    0.01 ms per token, 67234.85 tokens per second)
0.01.470.475 I llama_perf_context_print:        load time =     732.31 ms
0.01.470.476 I llama_perf_context_print: prompt eval time =      39.61 ms /     7 tokens (    5.66 ms per token,   176.72 tokens per second)
0.01.470.477 I llama_perf_context_print:        eval time =     685.29 ms /    63 runs   (   10.88 ms per token,    91.93 tokens per second)
0.01.470.477 I llama_perf_context_print:       total time =     728.04 ms /    70 tokens
0.01.470.659 I ggml_metal_free: deallocating

real	0m1.489s
user	0m0.117s
sys	0m0.167s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.033 I build: 4271 (0cd182eb) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.061 I main: llama backend init
0.00.000.064 I main: load the model and apply lora adapter, if any
0.00.011.471 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.032.761 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.032.765 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.032.772 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.032.772 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.032.773 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.032.773 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.032.773 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.032.774 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.032.774 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.032.775 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.032.775 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.032.775 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.032.775 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.032.776 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.032.777 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.032.778 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.032.778 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.037.436 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.038.629 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.043.177 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.043.178 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.043.178 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.043.178 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.043.179 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.043.179 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.043.179 I llama_model_loader: - type  f32:  194 tensors
0.00.043.180 I llama_model_loader: - type q5_0:   97 tensors
0.00.043.180 I llama_model_loader: - type q6_K:    1 tensors
0.00.069.623 I llm_load_vocab: special tokens cache size = 25
0.00.079.342 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.079.346 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.079.347 I llm_load_print_meta: arch             = gptneox
0.00.079.347 I llm_load_print_meta: vocab type       = BPE
0.00.079.347 I llm_load_print_meta: n_vocab          = 50304
0.00.079.348 I llm_load_print_meta: n_merges         = 50009
0.00.079.348 I llm_load_print_meta: vocab_only       = 0
0.00.079.348 I llm_load_print_meta: n_ctx_train      = 2048
0.00.079.348 I llm_load_print_meta: n_embd           = 2048
0.00.079.349 I llm_load_print_meta: n_layer          = 24
0.00.079.352 I llm_load_print_meta: n_head           = 16
0.00.079.353 I llm_load_print_meta: n_head_kv        = 16
0.00.079.365 I llm_load_print_meta: n_rot            = 32
0.00.079.365 I llm_load_print_meta: n_swa            = 0
0.00.079.366 I llm_load_print_meta: n_embd_head_k    = 128
0.00.079.366 I llm_load_print_meta: n_embd_head_v    = 128
0.00.079.369 I llm_load_print_meta: n_gqa            = 1
0.00.079.371 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.079.372 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.079.372 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.079.373 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.079.373 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.079.373 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.079.374 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.079.375 I llm_load_print_meta: n_ff             = 8192
0.00.079.375 I llm_load_print_meta: n_expert         = 0
0.00.079.375 I llm_load_print_meta: n_expert_used    = 0
0.00.079.375 I llm_load_print_meta: causal attn      = 1
0.00.079.375 I llm_load_print_meta: pooling type     = 0
0.00.079.376 I llm_load_print_meta: rope type        = 2
0.00.079.376 I llm_load_print_meta: rope scaling     = linear
0.00.079.377 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.079.377 I llm_load_print_meta: freq_scale_train = 1
0.00.079.379 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.079.379 I llm_load_print_meta: rope_finetuned   = unknown
0.00.079.379 I llm_load_print_meta: ssm_d_conv       = 0
0.00.079.380 I llm_load_print_meta: ssm_d_inner      = 0
0.00.079.380 I llm_load_print_meta: ssm_d_state      = 0
0.00.079.380 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.079.380 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.079.390 I llm_load_print_meta: model type       = 1.4B
0.00.079.391 I llm_load_print_meta: model ftype      = Q5_0
0.00.079.391 I llm_load_print_meta: model params     = 1.41 B
0.00.079.392 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.079.392 I llm_load_print_meta: general.name     = 1.4B
0.00.079.393 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.079.393 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.079.393 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.079.394 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.079.394 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.079.394 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.079.395 I llm_load_print_meta: max token length = 1024
0.00.082.228 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.082.228 I llm_load_tensors: offloading output layer to GPU
0.00.082.228 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.082.240 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.082.241 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.083.697 I llama_new_context_with_model: n_seq_max     = 1
0.00.083.699 I llama_new_context_with_model: n_ctx         = 2048
0.00.083.699 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.083.699 I llama_new_context_with_model: n_batch       = 2048
0.00.083.700 I llama_new_context_with_model: n_ubatch      = 512
0.00.083.700 I llama_new_context_with_model: flash_attn    = 0
0.00.083.701 I llama_new_context_with_model: freq_base     = 10000.0
0.00.083.701 I llama_new_context_with_model: freq_scale    = 1
0.00.083.702 I ggml_metal_init: allocating
0.00.083.711 I ggml_metal_init: found device: Apple M4
0.00.083.714 I ggml_metal_init: picking default device: Apple M4
0.00.084.544 I ggml_metal_init: using embedded metal library
0.00.088.447 I ggml_metal_init: GPU name:   Apple M4
0.00.088.449 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.088.450 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.088.450 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.088.451 I ggml_metal_init: simdgroup reduction   = true
0.00.088.451 I ggml_metal_init: simdgroup matrix mul. = true
0.00.088.451 I ggml_metal_init: has bfloat            = true
0.00.088.451 I ggml_metal_init: use bfloat            = true
0.00.088.452 I ggml_metal_init: hasUnifiedMemory      = true
0.00.088.453 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.124.497 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.124.504 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.124.525 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.125.590 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.125.593 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.125.593 I llama_new_context_with_model: graph nodes  = 967
0.00.125.593 I llama_new_context_with_model: graph splits = 2
0.00.125.609 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.917.547 I main: llama threadpool init, n_threads = 4
0.00.917.596 I 
0.00.917.638 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.917.640 I 
0.00.917.905 I sampler seed: 1234
0.00.917.910 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.917.975 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.917.977 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.917.977 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.715.926 I llama_perf_sampler_print:    sampling time =       1.19 ms /    71 runs   (    0.02 ms per token, 59714.05 tokens per second)
0.01.715.927 I llama_perf_context_print:        load time =     906.07 ms
0.01.715.927 I llama_perf_context_print: prompt eval time =      49.82 ms /     7 tokens (    7.12 ms per token,   140.51 tokens per second)
0.01.715.931 I llama_perf_context_print:        eval time =     745.18 ms /    63 runs   (   11.83 ms per token,    84.54 tokens per second)
0.01.715.932 I llama_perf_context_print:       total time =     798.38 ms /    70 tokens
0.01.716.124 I ggml_metal_free: deallocating

real	0m1.735s
user	0m0.128s
sys	0m0.196s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.030 I build: 4271 (0cd182eb) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.060 I main: llama backend init
0.00.000.062 I main: load the model and apply lora adapter, if any
0.00.008.170 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.025.277 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.025.281 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.025.283 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.025.283 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.025.284 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.025.284 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.025.284 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.025.285 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.025.286 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.025.286 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.025.286 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.025.286 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.025.287 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.025.287 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.025.289 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.025.289 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.025.289 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.029.961 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.031.161 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.035.928 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.035.930 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.035.930 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.035.930 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.035.931 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.035.931 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.035.932 I llama_model_loader: - type  f32:  194 tensors
0.00.035.932 I llama_model_loader: - type q5_1:   97 tensors
0.00.035.932 I llama_model_loader: - type q6_K:    1 tensors
0.00.065.099 I llm_load_vocab: special tokens cache size = 25
0.00.075.315 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.075.319 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.075.320 I llm_load_print_meta: arch             = gptneox
0.00.075.320 I llm_load_print_meta: vocab type       = BPE
0.00.075.320 I llm_load_print_meta: n_vocab          = 50304
0.00.075.321 I llm_load_print_meta: n_merges         = 50009
0.00.075.321 I llm_load_print_meta: vocab_only       = 0
0.00.075.321 I llm_load_print_meta: n_ctx_train      = 2048
0.00.075.321 I llm_load_print_meta: n_embd           = 2048
0.00.075.322 I llm_load_print_meta: n_layer          = 24
0.00.075.325 I llm_load_print_meta: n_head           = 16
0.00.075.326 I llm_load_print_meta: n_head_kv        = 16
0.00.075.338 I llm_load_print_meta: n_rot            = 32
0.00.075.338 I llm_load_print_meta: n_swa            = 0
0.00.075.339 I llm_load_print_meta: n_embd_head_k    = 128
0.00.075.339 I llm_load_print_meta: n_embd_head_v    = 128
0.00.075.340 I llm_load_print_meta: n_gqa            = 1
0.00.075.341 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.075.342 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.075.342 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.075.343 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.075.343 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.075.343 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.075.343 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.075.344 I llm_load_print_meta: n_ff             = 8192
0.00.075.344 I llm_load_print_meta: n_expert         = 0
0.00.075.345 I llm_load_print_meta: n_expert_used    = 0
0.00.075.345 I llm_load_print_meta: causal attn      = 1
0.00.075.348 I llm_load_print_meta: pooling type     = 0
0.00.075.348 I llm_load_print_meta: rope type        = 2
0.00.075.348 I llm_load_print_meta: rope scaling     = linear
0.00.075.349 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.075.349 I llm_load_print_meta: freq_scale_train = 1
0.00.075.350 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.075.350 I llm_load_print_meta: rope_finetuned   = unknown
0.00.075.350 I llm_load_print_meta: ssm_d_conv       = 0
0.00.075.350 I llm_load_print_meta: ssm_d_inner      = 0
0.00.075.351 I llm_load_print_meta: ssm_d_state      = 0
0.00.075.351 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.075.351 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.075.360 I llm_load_print_meta: model type       = 1.4B
0.00.075.361 I llm_load_print_meta: model ftype      = Q5_1
0.00.075.361 I llm_load_print_meta: model params     = 1.41 B
0.00.075.362 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.075.362 I llm_load_print_meta: general.name     = 1.4B
0.00.075.363 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.075.363 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.075.363 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.075.363 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.075.364 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.075.364 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.075.365 I llm_load_print_meta: max token length = 1024
0.00.077.737 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.077.737 I llm_load_tensors: offloading output layer to GPU
0.00.077.737 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.077.748 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.077.750 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.079.155 I llama_new_context_with_model: n_seq_max     = 1
0.00.079.157 I llama_new_context_with_model: n_ctx         = 2048
0.00.079.157 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.079.157 I llama_new_context_with_model: n_batch       = 2048
0.00.079.158 I llama_new_context_with_model: n_ubatch      = 512
0.00.079.158 I llama_new_context_with_model: flash_attn    = 0
0.00.079.159 I llama_new_context_with_model: freq_base     = 10000.0
0.00.079.159 I llama_new_context_with_model: freq_scale    = 1
0.00.079.160 I ggml_metal_init: allocating
0.00.079.164 I ggml_metal_init: found device: Apple M4
0.00.079.167 I ggml_metal_init: picking default device: Apple M4
0.00.079.975 I ggml_metal_init: using embedded metal library
0.00.083.629 I ggml_metal_init: GPU name:   Apple M4
0.00.083.631 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.083.632 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.083.632 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.083.632 I ggml_metal_init: simdgroup reduction   = true
0.00.083.633 I ggml_metal_init: simdgroup matrix mul. = true
0.00.083.633 I ggml_metal_init: has bfloat            = true
0.00.083.633 I ggml_metal_init: use bfloat            = true
0.00.083.634 I ggml_metal_init: hasUnifiedMemory      = true
0.00.083.635 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.117.484 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.117.492 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.117.512 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.118.542 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.118.543 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.118.544 I llama_new_context_with_model: graph nodes  = 967
0.00.118.544 I llama_new_context_with_model: graph splits = 2
0.00.118.557 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.963.909 I main: llama threadpool init, n_threads = 4
0.00.963.949 I 
0.00.963.977 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.963.977 I 
0.00.964.194 I sampler seed: 1234
0.00.964.198 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.964.243 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.964.245 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.964.245 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.806.493 I llama_perf_sampler_print:    sampling time =       1.19 ms /    71 runs   (    0.02 ms per token, 59764.31 tokens per second)
0.01.806.494 I llama_perf_context_print:        load time =     955.74 ms
0.01.806.495 I llama_perf_context_print: prompt eval time =      42.34 ms /     7 tokens (    6.05 ms per token,   165.31 tokens per second)
0.01.806.496 I llama_perf_context_print:        eval time =     796.93 ms /    63 runs   (   12.65 ms per token,    79.05 tokens per second)
0.01.806.497 I llama_perf_context_print:       total time =     842.59 ms /    70 tokens
0.01.806.678 I ggml_metal_free: deallocating

real	0m1.839s
user	0m0.132s
sys	0m0.176s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.034 I build: 4271 (0cd182eb) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.061 I main: llama backend init
0.00.000.063 I main: load the model and apply lora adapter, if any
0.00.016.816 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.024.025 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.024.029 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.024.031 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.024.032 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.024.032 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.024.032 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.024.033 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.024.034 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.024.034 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.024.034 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.024.035 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.024.035 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.024.036 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.024.038 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.024.040 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.024.040 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.024.040 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.029.178 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.030.538 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.035.794 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.035.796 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.035.796 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.035.797 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.035.797 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.035.797 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.035.798 I llama_model_loader: - type  f32:  194 tensors
0.00.035.798 I llama_model_loader: - type q2_K:   49 tensors
0.00.035.798 I llama_model_loader: - type q3_K:   48 tensors
0.00.035.799 I llama_model_loader: - type q6_K:    1 tensors
0.00.069.968 I llm_load_vocab: special tokens cache size = 25
0.00.080.013 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.080.016 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.080.017 I llm_load_print_meta: arch             = gptneox
0.00.080.017 I llm_load_print_meta: vocab type       = BPE
0.00.080.017 I llm_load_print_meta: n_vocab          = 50304
0.00.080.017 I llm_load_print_meta: n_merges         = 50009
0.00.080.018 I llm_load_print_meta: vocab_only       = 0
0.00.080.018 I llm_load_print_meta: n_ctx_train      = 2048
0.00.080.018 I llm_load_print_meta: n_embd           = 2048
0.00.080.018 I llm_load_print_meta: n_layer          = 24
0.00.080.022 I llm_load_print_meta: n_head           = 16
0.00.080.023 I llm_load_print_meta: n_head_kv        = 16
0.00.080.035 I llm_load_print_meta: n_rot            = 32
0.00.080.036 I llm_load_print_meta: n_swa            = 0
0.00.080.036 I llm_load_print_meta: n_embd_head_k    = 128
0.00.080.036 I llm_load_print_meta: n_embd_head_v    = 128
0.00.080.037 I llm_load_print_meta: n_gqa            = 1
0.00.080.038 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.080.039 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.080.040 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.080.042 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.080.042 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.080.043 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.080.043 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.080.044 I llm_load_print_meta: n_ff             = 8192
0.00.080.044 I llm_load_print_meta: n_expert         = 0
0.00.080.044 I llm_load_print_meta: n_expert_used    = 0
0.00.080.044 I llm_load_print_meta: causal attn      = 1
0.00.080.044 I llm_load_print_meta: pooling type     = 0
0.00.080.045 I llm_load_print_meta: rope type        = 2
0.00.080.045 I llm_load_print_meta: rope scaling     = linear
0.00.080.046 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.080.046 I llm_load_print_meta: freq_scale_train = 1
0.00.080.046 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.080.046 I llm_load_print_meta: rope_finetuned   = unknown
0.00.080.047 I llm_load_print_meta: ssm_d_conv       = 0
0.00.080.047 I llm_load_print_meta: ssm_d_inner      = 0
0.00.080.047 I llm_load_print_meta: ssm_d_state      = 0
0.00.080.047 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.080.047 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.080.057 I llm_load_print_meta: model type       = 1.4B
0.00.080.058 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.080.058 I llm_load_print_meta: model params     = 1.41 B
0.00.080.059 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.080.059 I llm_load_print_meta: general.name     = 1.4B
0.00.080.059 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.080.060 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.080.060 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.080.060 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.080.062 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.080.062 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.080.062 I llm_load_print_meta: max token length = 1024
0.00.082.628 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.082.629 I llm_load_tensors: offloading output layer to GPU
0.00.082.629 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.082.640 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.082.642 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.084.057 I llama_new_context_with_model: n_seq_max     = 1
0.00.084.058 I llama_new_context_with_model: n_ctx         = 2048
0.00.084.058 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.084.059 I llama_new_context_with_model: n_batch       = 2048
0.00.084.059 I llama_new_context_with_model: n_ubatch      = 512
0.00.084.059 I llama_new_context_with_model: flash_attn    = 0
0.00.084.060 I llama_new_context_with_model: freq_base     = 10000.0
0.00.084.060 I llama_new_context_with_model: freq_scale    = 1
0.00.084.061 I ggml_metal_init: allocating
0.00.084.065 I ggml_metal_init: found device: Apple M4
0.00.084.068 I ggml_metal_init: picking default device: Apple M4
0.00.084.781 I ggml_metal_init: using embedded metal library
0.00.088.169 I ggml_metal_init: GPU name:   Apple M4
0.00.088.171 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.088.171 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.088.172 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.088.172 I ggml_metal_init: simdgroup reduction   = true
0.00.088.172 I ggml_metal_init: simdgroup matrix mul. = true
0.00.088.172 I ggml_metal_init: has bfloat            = true
0.00.088.172 I ggml_metal_init: use bfloat            = true
0.00.088.173 I ggml_metal_init: hasUnifiedMemory      = true
0.00.088.174 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.120.056 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.120.064 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.120.081 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.121.120 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.121.121 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.121.121 I llama_new_context_with_model: graph nodes  = 967
0.00.121.122 I llama_new_context_with_model: graph splits = 2
0.00.121.136 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.501.262 I main: llama threadpool init, n_threads = 4
0.00.501.296 I 
0.00.501.325 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.501.325 I 
0.00.501.548 I sampler seed: 1234
0.00.501.552 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.501.591 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.501.594 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.501.594 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.181.155 I llama_perf_sampler_print:    sampling time =       1.18 ms /    71 runs   (    0.02 ms per token, 60425.53 tokens per second)
0.01.181.155 I llama_perf_context_print:        load time =     484.44 ms
0.01.181.156 I llama_perf_context_print: prompt eval time =      35.73 ms /     7 tokens (    5.10 ms per token,   195.94 tokens per second)
0.01.181.157 I llama_perf_context_print:        eval time =     640.92 ms /    63 runs   (   10.17 ms per token,    98.30 tokens per second)
0.01.181.157 I llama_perf_context_print:       total time =     679.90 ms /    70 tokens
0.01.181.333 I ggml_metal_free: deallocating

real	0m1.208s
user	0m0.137s
sys	0m0.122s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.029 I build: 4271 (0cd182eb) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.058 I main: llama backend init
0.00.000.060 I main: load the model and apply lora adapter, if any
0.00.008.373 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.061 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.014.066 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.067 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.068 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.068 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.068 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.069 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.070 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.070 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.070 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.071 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.071 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.071 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.072 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.073 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.074 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.074 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.119 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.171 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.218 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.219 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.219 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.219 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.220 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.220 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.023.221 I llama_model_loader: - type  f32:  194 tensors
0.00.023.221 I llama_model_loader: - type q3_K:   25 tensors
0.00.023.221 I llama_model_loader: - type q4_K:   71 tensors
0.00.023.221 I llama_model_loader: - type q5_K:    1 tensors
0.00.023.222 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.473 I llm_load_vocab: special tokens cache size = 25
0.00.049.330 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.332 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.333 I llm_load_print_meta: arch             = gptneox
0.00.049.333 I llm_load_print_meta: vocab type       = BPE
0.00.049.333 I llm_load_print_meta: n_vocab          = 50304
0.00.049.333 I llm_load_print_meta: n_merges         = 50009
0.00.049.333 I llm_load_print_meta: vocab_only       = 0
0.00.049.334 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.334 I llm_load_print_meta: n_embd           = 2048
0.00.049.334 I llm_load_print_meta: n_layer          = 24
0.00.049.337 I llm_load_print_meta: n_head           = 16
0.00.049.337 I llm_load_print_meta: n_head_kv        = 16
0.00.049.349 I llm_load_print_meta: n_rot            = 32
0.00.049.349 I llm_load_print_meta: n_swa            = 0
0.00.049.349 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.349 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.350 I llm_load_print_meta: n_gqa            = 1
0.00.049.351 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.352 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.352 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.353 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.353 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.353 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.353 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.356 I llm_load_print_meta: n_ff             = 8192
0.00.049.356 I llm_load_print_meta: n_expert         = 0
0.00.049.356 I llm_load_print_meta: n_expert_used    = 0
0.00.049.356 I llm_load_print_meta: causal attn      = 1
0.00.049.357 I llm_load_print_meta: pooling type     = 0
0.00.049.357 I llm_load_print_meta: rope type        = 2
0.00.049.357 I llm_load_print_meta: rope scaling     = linear
0.00.049.357 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.358 I llm_load_print_meta: freq_scale_train = 1
0.00.049.358 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.358 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.358 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.358 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.358 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.359 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.359 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.368 I llm_load_print_meta: model type       = 1.4B
0.00.049.368 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.049.369 I llm_load_print_meta: model params     = 1.41 B
0.00.049.369 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.049.369 I llm_load_print_meta: general.name     = 1.4B
0.00.049.370 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.370 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.370 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.370 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.370 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.049.370 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.371 I llm_load_print_meta: max token length = 1024
0.00.051.282 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.282 I llm_load_tensors: offloading output layer to GPU
0.00.051.282 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.292 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.051.294 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.052.180 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.181 I llama_new_context_with_model: n_ctx         = 2048
0.00.052.182 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.052.182 I llama_new_context_with_model: n_batch       = 2048
0.00.052.182 I llama_new_context_with_model: n_ubatch      = 512
0.00.052.182 I llama_new_context_with_model: flash_attn    = 0
0.00.052.183 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.183 I llama_new_context_with_model: freq_scale    = 1
0.00.052.183 I ggml_metal_init: allocating
0.00.052.186 I ggml_metal_init: found device: Apple M4
0.00.052.188 I ggml_metal_init: picking default device: Apple M4
0.00.052.740 I ggml_metal_init: using embedded metal library
0.00.055.113 I ggml_metal_init: GPU name:   Apple M4
0.00.055.114 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.114 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.115 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.115 I ggml_metal_init: simdgroup reduction   = true
0.00.055.115 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.115 I ggml_metal_init: has bfloat            = true
0.00.055.115 I ggml_metal_init: use bfloat            = true
0.00.055.116 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.116 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.083.593 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.083.601 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.083.620 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.084.626 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.084.628 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.084.628 I llama_new_context_with_model: graph nodes  = 967
0.00.084.628 I llama_new_context_with_model: graph splits = 2
0.00.084.637 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.658.342 I main: llama threadpool init, n_threads = 4
0.00.658.389 I 
0.00.658.423 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.658.424 I 
0.00.658.656 I sampler seed: 1234
0.00.658.660 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.658.697 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.658.701 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.658.701 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.407.484 I llama_perf_sampler_print:    sampling time =       1.14 ms /    71 runs   (    0.02 ms per token, 62062.94 tokens per second)
0.01.407.485 I llama_perf_context_print:        load time =     649.96 ms
0.01.407.485 I llama_perf_context_print: prompt eval time =      40.50 ms /     7 tokens (    5.79 ms per token,   172.85 tokens per second)
0.01.407.486 I llama_perf_context_print:        eval time =     705.39 ms /    63 runs   (   11.20 ms per token,    89.31 tokens per second)
0.01.407.486 I llama_perf_context_print:       total time =     749.15 ms /    70 tokens
0.01.407.677 I ggml_metal_free: deallocating

real	0m1.425s
user	0m0.109s
sys	0m0.134s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.032 I build: 4271 (0cd182eb) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.060 I main: llama backend init
0.00.000.062 I main: load the model and apply lora adapter, if any
0.00.012.699 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.027.397 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.027.402 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.027.403 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.027.404 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.027.405 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.027.405 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.027.406 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.027.406 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.027.407 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.027.407 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.027.407 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.027.408 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.027.408 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.027.408 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.027.410 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.027.411 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.027.411 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.031.630 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.032.767 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.036.941 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.036.942 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.036.942 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.036.942 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.036.943 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.036.943 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.036.944 I llama_model_loader: - type  f32:  194 tensors
0.00.036.944 I llama_model_loader: - type q4_K:   61 tensors
0.00.036.944 I llama_model_loader: - type q5_K:   24 tensors
0.00.036.944 I llama_model_loader: - type q6_K:   13 tensors
0.00.061.534 I llm_load_vocab: special tokens cache size = 25
0.00.069.306 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.069.309 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.069.310 I llm_load_print_meta: arch             = gptneox
0.00.069.310 I llm_load_print_meta: vocab type       = BPE
0.00.069.310 I llm_load_print_meta: n_vocab          = 50304
0.00.069.311 I llm_load_print_meta: n_merges         = 50009
0.00.069.311 I llm_load_print_meta: vocab_only       = 0
0.00.069.311 I llm_load_print_meta: n_ctx_train      = 2048
0.00.069.311 I llm_load_print_meta: n_embd           = 2048
0.00.069.311 I llm_load_print_meta: n_layer          = 24
0.00.069.314 I llm_load_print_meta: n_head           = 16
0.00.069.315 I llm_load_print_meta: n_head_kv        = 16
0.00.069.327 I llm_load_print_meta: n_rot            = 32
0.00.069.329 I llm_load_print_meta: n_swa            = 0
0.00.069.329 I llm_load_print_meta: n_embd_head_k    = 128
0.00.069.330 I llm_load_print_meta: n_embd_head_v    = 128
0.00.069.330 I llm_load_print_meta: n_gqa            = 1
0.00.069.331 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.069.332 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.069.332 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.069.333 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.069.333 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.069.333 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.069.333 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.069.334 I llm_load_print_meta: n_ff             = 8192
0.00.069.334 I llm_load_print_meta: n_expert         = 0
0.00.069.336 I llm_load_print_meta: n_expert_used    = 0
0.00.069.337 I llm_load_print_meta: causal attn      = 1
0.00.069.337 I llm_load_print_meta: pooling type     = 0
0.00.069.338 I llm_load_print_meta: rope type        = 2
0.00.069.338 I llm_load_print_meta: rope scaling     = linear
0.00.069.338 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.069.338 I llm_load_print_meta: freq_scale_train = 1
0.00.069.340 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.069.340 I llm_load_print_meta: rope_finetuned   = unknown
0.00.069.340 I llm_load_print_meta: ssm_d_conv       = 0
0.00.069.341 I llm_load_print_meta: ssm_d_inner      = 0
0.00.069.341 I llm_load_print_meta: ssm_d_state      = 0
0.00.069.341 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.069.341 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.069.351 I llm_load_print_meta: model type       = 1.4B
0.00.069.351 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.069.352 I llm_load_print_meta: model params     = 1.41 B
0.00.069.352 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.069.353 I llm_load_print_meta: general.name     = 1.4B
0.00.069.353 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.069.354 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.069.354 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.069.355 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.069.355 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.069.355 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.069.355 I llm_load_print_meta: max token length = 1024
0.00.071.708 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.071.708 I llm_load_tensors: offloading output layer to GPU
0.00.071.708 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.071.720 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.071.723 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.073.174 I llama_new_context_with_model: n_seq_max     = 1
0.00.073.175 I llama_new_context_with_model: n_ctx         = 2048
0.00.073.175 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.073.176 I llama_new_context_with_model: n_batch       = 2048
0.00.073.176 I llama_new_context_with_model: n_ubatch      = 512
0.00.073.176 I llama_new_context_with_model: flash_attn    = 0
0.00.073.177 I llama_new_context_with_model: freq_base     = 10000.0
0.00.073.178 I llama_new_context_with_model: freq_scale    = 1
0.00.073.178 I ggml_metal_init: allocating
0.00.073.187 I ggml_metal_init: found device: Apple M4
0.00.073.190 I ggml_metal_init: picking default device: Apple M4
0.00.073.992 I ggml_metal_init: using embedded metal library
0.00.077.804 I ggml_metal_init: GPU name:   Apple M4
0.00.077.806 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.077.807 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.077.807 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.077.809 I ggml_metal_init: simdgroup reduction   = true
0.00.077.809 I ggml_metal_init: simdgroup matrix mul. = true
0.00.077.809 I ggml_metal_init: has bfloat            = true
0.00.077.810 I ggml_metal_init: use bfloat            = true
0.00.077.810 I ggml_metal_init: hasUnifiedMemory      = true
0.00.077.811 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.112.122 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.112.128 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.112.146 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.113.137 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.113.138 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.113.139 I llama_new_context_with_model: graph nodes  = 967
0.00.113.139 I llama_new_context_with_model: graph splits = 2
0.00.113.147 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.706.771 I main: llama threadpool init, n_threads = 4
0.00.706.818 I 
0.00.706.844 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.706.844 I 
0.00.707.087 I sampler seed: 1234
0.00.707.091 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.707.131 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.707.132 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.707.132 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.468.264 I llama_perf_sampler_print:    sampling time =       1.27 ms /    71 runs   (    0.02 ms per token, 55729.98 tokens per second)
0.01.468.265 I llama_perf_context_print:        load time =     694.07 ms
0.01.468.266 I llama_perf_context_print: prompt eval time =      47.11 ms /     7 tokens (    6.73 ms per token,   148.59 tokens per second)
0.01.468.266 I llama_perf_context_print:        eval time =     710.93 ms /    63 runs   (   11.28 ms per token,    88.62 tokens per second)
0.01.468.267 I llama_perf_context_print:       total time =     761.50 ms /    70 tokens
0.01.468.441 I ggml_metal_free: deallocating

real	0m1.487s
user	0m0.125s
sys	0m0.152s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.030 I build: 4271 (0cd182eb) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.059 I main: llama backend init
0.00.000.061 I main: load the model and apply lora adapter, if any
0.00.008.731 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.022.256 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.022.261 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.022.263 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.022.265 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.022.265 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.022.265 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.022.265 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.022.266 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.022.266 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.022.267 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.022.267 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.022.267 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.022.268 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.022.268 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.022.270 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.022.270 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.022.271 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.026.575 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.027.650 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.031.826 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.031.827 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.031.828 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.031.828 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.031.828 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.031.828 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.031.829 I llama_model_loader: - type  f32:  194 tensors
0.00.031.829 I llama_model_loader: - type q5_K:   61 tensors
0.00.031.829 I llama_model_loader: - type q6_K:   37 tensors
0.00.055.251 I llm_load_vocab: special tokens cache size = 25
0.00.061.420 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.061.422 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.061.423 I llm_load_print_meta: arch             = gptneox
0.00.061.423 I llm_load_print_meta: vocab type       = BPE
0.00.061.423 I llm_load_print_meta: n_vocab          = 50304
0.00.061.423 I llm_load_print_meta: n_merges         = 50009
0.00.061.424 I llm_load_print_meta: vocab_only       = 0
0.00.061.424 I llm_load_print_meta: n_ctx_train      = 2048
0.00.061.424 I llm_load_print_meta: n_embd           = 2048
0.00.061.424 I llm_load_print_meta: n_layer          = 24
0.00.061.426 I llm_load_print_meta: n_head           = 16
0.00.061.427 I llm_load_print_meta: n_head_kv        = 16
0.00.061.438 I llm_load_print_meta: n_rot            = 32
0.00.061.439 I llm_load_print_meta: n_swa            = 0
0.00.061.439 I llm_load_print_meta: n_embd_head_k    = 128
0.00.061.439 I llm_load_print_meta: n_embd_head_v    = 128
0.00.061.439 I llm_load_print_meta: n_gqa            = 1
0.00.061.440 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.061.441 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.061.441 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.061.442 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.061.442 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.061.442 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.061.442 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.061.443 I llm_load_print_meta: n_ff             = 8192
0.00.061.443 I llm_load_print_meta: n_expert         = 0
0.00.061.443 I llm_load_print_meta: n_expert_used    = 0
0.00.061.444 I llm_load_print_meta: causal attn      = 1
0.00.061.446 I llm_load_print_meta: pooling type     = 0
0.00.061.446 I llm_load_print_meta: rope type        = 2
0.00.061.446 I llm_load_print_meta: rope scaling     = linear
0.00.061.446 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.061.447 I llm_load_print_meta: freq_scale_train = 1
0.00.061.447 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.061.447 I llm_load_print_meta: rope_finetuned   = unknown
0.00.061.447 I llm_load_print_meta: ssm_d_conv       = 0
0.00.061.447 I llm_load_print_meta: ssm_d_inner      = 0
0.00.061.447 I llm_load_print_meta: ssm_d_state      = 0
0.00.061.448 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.061.448 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.061.457 I llm_load_print_meta: model type       = 1.4B
0.00.061.457 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.061.458 I llm_load_print_meta: model params     = 1.41 B
0.00.061.458 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.061.458 I llm_load_print_meta: general.name     = 1.4B
0.00.061.458 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.061.459 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.061.459 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.061.459 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.061.459 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.061.459 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.061.459 I llm_load_print_meta: max token length = 1024
0.00.063.532 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.063.532 I llm_load_tensors: offloading output layer to GPU
0.00.063.533 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.063.543 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.063.544 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.064.431 I llama_new_context_with_model: n_seq_max     = 1
0.00.064.431 I llama_new_context_with_model: n_ctx         = 2048
0.00.064.432 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.064.432 I llama_new_context_with_model: n_batch       = 2048
0.00.064.432 I llama_new_context_with_model: n_ubatch      = 512
0.00.064.432 I llama_new_context_with_model: flash_attn    = 0
0.00.064.433 I llama_new_context_with_model: freq_base     = 10000.0
0.00.064.433 I llama_new_context_with_model: freq_scale    = 1
0.00.064.433 I ggml_metal_init: allocating
0.00.064.439 I ggml_metal_init: found device: Apple M4
0.00.064.442 I ggml_metal_init: picking default device: Apple M4
0.00.064.973 I ggml_metal_init: using embedded metal library
0.00.067.333 I ggml_metal_init: GPU name:   Apple M4
0.00.067.335 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.067.337 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.067.337 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.067.337 I ggml_metal_init: simdgroup reduction   = true
0.00.067.337 I ggml_metal_init: simdgroup matrix mul. = true
0.00.067.337 I ggml_metal_init: has bfloat            = true
0.00.067.338 I ggml_metal_init: use bfloat            = true
0.00.067.338 I ggml_metal_init: hasUnifiedMemory      = true
0.00.067.338 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.097.917 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.097.922 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.097.941 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.099.004 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.099.006 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.099.006 I llama_new_context_with_model: graph nodes  = 967
0.00.099.006 I llama_new_context_with_model: graph splits = 2
0.00.099.015 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.904.529 I main: llama threadpool init, n_threads = 4
0.00.904.618 I 
0.00.904.687 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.904.688 I 
0.00.905.227 I sampler seed: 1234
0.00.905.235 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.905.265 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.905.266 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.905.266 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.759.003 I llama_perf_sampler_print:    sampling time =       1.27 ms /    71 runs   (    0.02 ms per token, 55861.53 tokens per second)
0.01.759.004 I llama_perf_context_print:        load time =     895.78 ms
0.01.759.004 I llama_perf_context_print: prompt eval time =      52.30 ms /     7 tokens (    7.47 ms per token,   133.84 tokens per second)
0.01.759.005 I llama_perf_context_print:        eval time =     798.48 ms /    63 runs   (   12.67 ms per token,    78.90 tokens per second)
0.01.759.006 I llama_perf_context_print:       total time =     854.49 ms /    70 tokens
0.01.759.222 I ggml_metal_free: deallocating

real	0m1.780s
user	0m0.124s
sys	0m0.211s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.032 I build: 4271 (0cd182eb) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.061 I main: llama backend init
0.00.000.063 I main: load the model and apply lora adapter, if any
0.00.009.716 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.175 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.017.183 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.186 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.186 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.187 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.187 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.187 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.190 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.190 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.190 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.191 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.191 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.191 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.192 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.200 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.200 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.200 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.263 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.352 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.418 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.419 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.419 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.419 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.420 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.420 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.026.420 I llama_model_loader: - type  f32:  194 tensors
0.00.026.421 I llama_model_loader: - type q6_K:   98 tensors
0.00.046.659 I llm_load_vocab: special tokens cache size = 25
0.00.052.543 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.546 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.547 I llm_load_print_meta: arch             = gptneox
0.00.052.547 I llm_load_print_meta: vocab type       = BPE
0.00.052.547 I llm_load_print_meta: n_vocab          = 50304
0.00.052.547 I llm_load_print_meta: n_merges         = 50009
0.00.052.548 I llm_load_print_meta: vocab_only       = 0
0.00.052.548 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.548 I llm_load_print_meta: n_embd           = 2048
0.00.052.548 I llm_load_print_meta: n_layer          = 24
0.00.052.551 I llm_load_print_meta: n_head           = 16
0.00.052.552 I llm_load_print_meta: n_head_kv        = 16
0.00.052.564 I llm_load_print_meta: n_rot            = 32
0.00.052.565 I llm_load_print_meta: n_swa            = 0
0.00.052.565 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.565 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.566 I llm_load_print_meta: n_gqa            = 1
0.00.052.567 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.567 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.568 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.568 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.568 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.569 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.569 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.569 I llm_load_print_meta: n_ff             = 8192
0.00.052.569 I llm_load_print_meta: n_expert         = 0
0.00.052.570 I llm_load_print_meta: n_expert_used    = 0
0.00.052.570 I llm_load_print_meta: causal attn      = 1
0.00.052.571 I llm_load_print_meta: pooling type     = 0
0.00.052.573 I llm_load_print_meta: rope type        = 2
0.00.052.573 I llm_load_print_meta: rope scaling     = linear
0.00.052.574 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.574 I llm_load_print_meta: freq_scale_train = 1
0.00.052.574 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.574 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.576 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.576 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.576 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.576 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.576 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.587 I llm_load_print_meta: model type       = 1.4B
0.00.052.587 I llm_load_print_meta: model ftype      = Q6_K
0.00.052.587 I llm_load_print_meta: model params     = 1.41 B
0.00.052.588 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.052.588 I llm_load_print_meta: general.name     = 1.4B
0.00.052.588 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.588 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.589 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.589 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.591 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.052.591 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.591 I llm_load_print_meta: max token length = 1024
0.00.054.575 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.575 I llm_load_tensors: offloading output layer to GPU
0.00.054.576 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.586 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.054.587 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.055.561 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.562 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.562 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.563 I llama_new_context_with_model: n_batch       = 2048
0.00.055.563 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.563 I llama_new_context_with_model: flash_attn    = 0
0.00.055.563 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.564 I llama_new_context_with_model: freq_scale    = 1
0.00.055.564 I ggml_metal_init: allocating
0.00.055.567 I ggml_metal_init: found device: Apple M4
0.00.055.569 I ggml_metal_init: picking default device: Apple M4
0.00.056.111 I ggml_metal_init: using embedded metal library
0.00.058.404 I ggml_metal_init: GPU name:   Apple M4
0.00.058.405 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.406 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.406 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.406 I ggml_metal_init: simdgroup reduction   = true
0.00.058.408 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.408 I ggml_metal_init: has bfloat            = true
0.00.058.408 I ggml_metal_init: use bfloat            = true
0.00.058.409 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.409 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.087.348 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.087.353 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.087.370 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.088.445 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.088.446 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.088.447 I llama_new_context_with_model: graph nodes  = 967
0.00.088.447 I llama_new_context_with_model: graph splits = 2
0.00.088.461 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.751.454 I main: llama threadpool init, n_threads = 4
0.00.751.496 I 
0.00.751.520 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.751.520 I 
0.00.751.681 I sampler seed: 1234
0.00.751.686 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.751.697 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.751.697 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.751.697 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.638.574 I llama_perf_sampler_print:    sampling time =       1.14 ms /    71 runs   (    0.02 ms per token, 62280.70 tokens per second)
0.01.638.575 I llama_perf_context_print:        load time =     741.73 ms
0.01.638.576 I llama_perf_context_print: prompt eval time =      54.31 ms /     7 tokens (    7.76 ms per token,   128.89 tokens per second)
0.01.638.576 I llama_perf_context_print:        eval time =     829.68 ms /    63 runs   (   13.17 ms per token,    75.93 tokens per second)
0.01.638.577 I llama_perf_context_print:       total time =     887.12 ms /    70 tokens
0.01.638.784 I ggml_metal_free: deallocating

real	0m1.658s
user	0m0.110s
sys	0m0.158s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.596 I build: 4271 (0cd182eb) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.023.550 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.035.000 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.035.017 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.035.022 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.035.022 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.035.023 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.035.023 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.035.024 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.035.026 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.035.026 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.035.027 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.035.028 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.035.028 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.035.029 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.035.029 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.035.034 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.035.034 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.035.035 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.044.681 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.046.877 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.054.519 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.054.522 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.054.522 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.054.523 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.054.523 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.054.524 I llama_model_loader: - type  f32:  194 tensors
0.00.054.524 I llama_model_loader: - type  f16:   98 tensors
0.00.084.283 I llm_load_vocab: special tokens cache size = 25
0.00.090.972 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.090.975 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.090.976 I llm_load_print_meta: arch             = gptneox
0.00.090.976 I llm_load_print_meta: vocab type       = BPE
0.00.090.976 I llm_load_print_meta: n_vocab          = 50304
0.00.090.976 I llm_load_print_meta: n_merges         = 50009
0.00.090.977 I llm_load_print_meta: vocab_only       = 0
0.00.090.977 I llm_load_print_meta: n_ctx_train      = 2048
0.00.090.977 I llm_load_print_meta: n_embd           = 2048
0.00.090.977 I llm_load_print_meta: n_layer          = 24
0.00.090.980 I llm_load_print_meta: n_head           = 16
0.00.090.980 I llm_load_print_meta: n_head_kv        = 16
0.00.090.992 I llm_load_print_meta: n_rot            = 32
0.00.090.993 I llm_load_print_meta: n_swa            = 0
0.00.090.993 I llm_load_print_meta: n_embd_head_k    = 128
0.00.090.993 I llm_load_print_meta: n_embd_head_v    = 128
0.00.090.994 I llm_load_print_meta: n_gqa            = 1
0.00.090.994 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.090.995 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.090.996 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.090.996 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.090.996 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.090.996 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.090.996 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.090.997 I llm_load_print_meta: n_ff             = 8192
0.00.090.997 I llm_load_print_meta: n_expert         = 0
0.00.090.997 I llm_load_print_meta: n_expert_used    = 0
0.00.090.998 I llm_load_print_meta: causal attn      = 1
0.00.090.998 I llm_load_print_meta: pooling type     = 0
0.00.090.998 I llm_load_print_meta: rope type        = 2
0.00.090.998 I llm_load_print_meta: rope scaling     = linear
0.00.090.999 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.090.999 I llm_load_print_meta: freq_scale_train = 1
0.00.090.999 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.090.999 I llm_load_print_meta: rope_finetuned   = unknown
0.00.091.000 I llm_load_print_meta: ssm_d_conv       = 0
0.00.091.001 I llm_load_print_meta: ssm_d_inner      = 0
0.00.091.001 I llm_load_print_meta: ssm_d_state      = 0
0.00.091.001 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.091.001 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.091.010 I llm_load_print_meta: model type       = 1.4B
0.00.091.011 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.091.011 I llm_load_print_meta: model params     = 1.41 B
0.00.091.012 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.091.014 I llm_load_print_meta: general.name     = 1.4B
0.00.091.014 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.091.014 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.091.014 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.091.014 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.091.015 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.091.015 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.091.015 I llm_load_print_meta: max token length = 1024
0.00.093.477 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.093.477 I llm_load_tensors: offloading output layer to GPU
0.00.093.477 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.093.488 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.093.489 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.094.393 I llama_new_context_with_model: n_seq_max     = 1
0.00.094.394 I llama_new_context_with_model: n_ctx         = 128
0.00.094.395 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.094.395 I llama_new_context_with_model: n_batch       = 128
0.00.094.395 I llama_new_context_with_model: n_ubatch      = 128
0.00.094.395 I llama_new_context_with_model: flash_attn    = 0
0.00.094.395 I llama_new_context_with_model: freq_base     = 10000.0
0.00.094.396 I llama_new_context_with_model: freq_scale    = 1
0.00.094.396 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.094.396 I ggml_metal_init: allocating
0.00.094.399 I ggml_metal_init: found device: Apple M4
0.00.094.401 I ggml_metal_init: picking default device: Apple M4
0.00.094.991 I ggml_metal_init: using embedded metal library
0.00.097.534 I ggml_metal_init: GPU name:   Apple M4
0.00.097.536 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.097.536 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.097.537 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.097.537 I ggml_metal_init: simdgroup reduction   = true
0.00.097.537 I ggml_metal_init: simdgroup matrix mul. = true
0.00.097.537 I ggml_metal_init: has bfloat            = true
0.00.097.537 I ggml_metal_init: use bfloat            = true
0.00.097.538 I ggml_metal_init: hasUnifiedMemory      = true
0.00.097.538 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.108.195 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.108.197 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.108.210 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.109.031 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.109.032 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.109.032 I llama_new_context_with_model: graph nodes  = 967
0.00.109.033 I llama_new_context_with_model: graph splits = 2
0.00.109.045 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.147.263 I 
0.01.147.317 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.147.380 I perplexity: tokenizing the input ..
0.01.160.514 I perplexity: tokenization took 13.128 ms
0.01.160.542 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.281.658 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.283.605 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.283.630 I llama_perf_context_print:        load time =    1123.70 ms
0.01.283.633 I llama_perf_context_print: prompt eval time =     120.14 ms /   128 tokens (    0.94 ms per token,  1065.47 tokens per second)
0.01.283.637 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.283.641 I llama_perf_context_print:       total time =     136.37 ms /   129 tokens
0.01.284.328 I ggml_metal_free: deallocating

real	0m1.476s
user	0m0.128s
sys	0m0.218s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.125 I build: 4271 (0cd182eb) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.974 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.019.112 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.019.118 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.125 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.126 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.126 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.128 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.129 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.130 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.130 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.131 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.131 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.131 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.135 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.136 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.137 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.137 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.140 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.024.923 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.026.508 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.032.189 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.032.190 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.032.190 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.032.191 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.032.191 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.032.192 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.032.192 I llama_model_loader: - type  f32:  194 tensors
0.00.032.193 I llama_model_loader: - type q8_0:   98 tensors
0.00.056.710 I llm_load_vocab: special tokens cache size = 25
0.00.062.867 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.062.870 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.062.870 I llm_load_print_meta: arch             = gptneox
0.00.062.871 I llm_load_print_meta: vocab type       = BPE
0.00.062.871 I llm_load_print_meta: n_vocab          = 50304
0.00.062.871 I llm_load_print_meta: n_merges         = 50009
0.00.062.871 I llm_load_print_meta: vocab_only       = 0
0.00.062.872 I llm_load_print_meta: n_ctx_train      = 2048
0.00.062.872 I llm_load_print_meta: n_embd           = 2048
0.00.062.872 I llm_load_print_meta: n_layer          = 24
0.00.062.875 I llm_load_print_meta: n_head           = 16
0.00.062.876 I llm_load_print_meta: n_head_kv        = 16
0.00.062.887 I llm_load_print_meta: n_rot            = 32
0.00.062.887 I llm_load_print_meta: n_swa            = 0
0.00.062.888 I llm_load_print_meta: n_embd_head_k    = 128
0.00.062.888 I llm_load_print_meta: n_embd_head_v    = 128
0.00.062.889 I llm_load_print_meta: n_gqa            = 1
0.00.062.890 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.062.891 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.062.891 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.062.892 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.062.893 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.062.893 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.062.893 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.062.894 I llm_load_print_meta: n_ff             = 8192
0.00.062.894 I llm_load_print_meta: n_expert         = 0
0.00.062.895 I llm_load_print_meta: n_expert_used    = 0
0.00.062.895 I llm_load_print_meta: causal attn      = 1
0.00.062.895 I llm_load_print_meta: pooling type     = 0
0.00.062.895 I llm_load_print_meta: rope type        = 2
0.00.062.895 I llm_load_print_meta: rope scaling     = linear
0.00.062.896 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.062.896 I llm_load_print_meta: freq_scale_train = 1
0.00.062.896 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.062.896 I llm_load_print_meta: rope_finetuned   = unknown
0.00.062.896 I llm_load_print_meta: ssm_d_conv       = 0
0.00.062.896 I llm_load_print_meta: ssm_d_inner      = 0
0.00.062.897 I llm_load_print_meta: ssm_d_state      = 0
0.00.062.897 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.062.897 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.062.906 I llm_load_print_meta: model type       = 1.4B
0.00.062.907 I llm_load_print_meta: model ftype      = Q8_0
0.00.062.907 I llm_load_print_meta: model params     = 1.41 B
0.00.062.909 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.062.909 I llm_load_print_meta: general.name     = 1.4B
0.00.062.909 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.062.909 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.062.909 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.062.909 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.062.910 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.062.910 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.062.910 I llm_load_print_meta: max token length = 1024
0.00.064.542 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.064.542 I llm_load_tensors: offloading output layer to GPU
0.00.064.542 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.064.552 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.064.553 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.065.386 I llama_new_context_with_model: n_seq_max     = 1
0.00.065.387 I llama_new_context_with_model: n_ctx         = 128
0.00.065.387 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.065.387 I llama_new_context_with_model: n_batch       = 128
0.00.065.387 I llama_new_context_with_model: n_ubatch      = 128
0.00.065.388 I llama_new_context_with_model: flash_attn    = 0
0.00.065.388 I llama_new_context_with_model: freq_base     = 10000.0
0.00.065.388 I llama_new_context_with_model: freq_scale    = 1
0.00.065.389 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.065.389 I ggml_metal_init: allocating
0.00.065.392 I ggml_metal_init: found device: Apple M4
0.00.065.394 I ggml_metal_init: picking default device: Apple M4
0.00.065.958 I ggml_metal_init: using embedded metal library
0.00.068.411 I ggml_metal_init: GPU name:   Apple M4
0.00.068.412 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.068.413 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.068.413 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.068.413 I ggml_metal_init: simdgroup reduction   = true
0.00.068.414 I ggml_metal_init: simdgroup matrix mul. = true
0.00.068.414 I ggml_metal_init: has bfloat            = true
0.00.068.414 I ggml_metal_init: use bfloat            = true
0.00.068.414 I ggml_metal_init: hasUnifiedMemory      = true
0.00.068.415 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.079.479 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.079.484 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.079.499 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.080.408 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.080.409 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.080.409 I llama_new_context_with_model: graph nodes  = 967
0.00.080.409 I llama_new_context_with_model: graph splits = 2
0.00.080.422 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.857.768 I 
0.00.857.814 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.857.833 I perplexity: tokenizing the input ..
0.00.865.706 I perplexity: tokenization took 7.872 ms
0.00.865.716 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.989.942 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.00.991.288 I Final estimate: PPL = 10.1362 +/- 3.22437

0.00.991.306 I llama_perf_context_print:        load time =     846.79 ms
0.00.991.307 I llama_perf_context_print: prompt eval time =     124.00 ms /   128 tokens (    0.97 ms per token,  1032.26 tokens per second)
0.00.991.308 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.991.309 I llama_perf_context_print:       total time =     133.54 ms /   129 tokens
0.00.991.734 I ggml_metal_free: deallocating

real	0m1.010s
user	0m0.093s
sys	0m0.154s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.084 I build: 4271 (0cd182eb) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.797 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.448 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.015.452 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.454 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.458 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.459 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.459 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.459 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.460 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.461 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.461 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.461 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.462 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.462 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.462 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.464 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.464 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.464 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.375 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.471 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.277 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.278 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.278 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.278 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.279 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.279 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.024.279 I llama_model_loader: - type  f32:  194 tensors
0.00.024.280 I llama_model_loader: - type q4_0:   97 tensors
0.00.024.280 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.173 I llm_load_vocab: special tokens cache size = 25
0.00.049.911 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.914 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.914 I llm_load_print_meta: arch             = gptneox
0.00.049.914 I llm_load_print_meta: vocab type       = BPE
0.00.049.915 I llm_load_print_meta: n_vocab          = 50304
0.00.049.915 I llm_load_print_meta: n_merges         = 50009
0.00.049.915 I llm_load_print_meta: vocab_only       = 0
0.00.049.915 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.915 I llm_load_print_meta: n_embd           = 2048
0.00.049.916 I llm_load_print_meta: n_layer          = 24
0.00.049.918 I llm_load_print_meta: n_head           = 16
0.00.049.919 I llm_load_print_meta: n_head_kv        = 16
0.00.049.930 I llm_load_print_meta: n_rot            = 32
0.00.049.930 I llm_load_print_meta: n_swa            = 0
0.00.049.930 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.930 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.931 I llm_load_print_meta: n_gqa            = 1
0.00.049.932 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.933 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.933 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.934 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.934 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.934 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.934 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.935 I llm_load_print_meta: n_ff             = 8192
0.00.049.935 I llm_load_print_meta: n_expert         = 0
0.00.049.935 I llm_load_print_meta: n_expert_used    = 0
0.00.049.936 I llm_load_print_meta: causal attn      = 1
0.00.049.936 I llm_load_print_meta: pooling type     = 0
0.00.049.936 I llm_load_print_meta: rope type        = 2
0.00.049.936 I llm_load_print_meta: rope scaling     = linear
0.00.049.936 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.938 I llm_load_print_meta: freq_scale_train = 1
0.00.049.938 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.939 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.939 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.939 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.939 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.939 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.939 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.948 I llm_load_print_meta: model type       = 1.4B
0.00.049.949 I llm_load_print_meta: model ftype      = Q4_0
0.00.049.949 I llm_load_print_meta: model params     = 1.41 B
0.00.049.950 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.049.950 I llm_load_print_meta: general.name     = 1.4B
0.00.049.951 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.951 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.951 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.951 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.951 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.049.952 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.952 I llm_load_print_meta: max token length = 1024
0.00.051.498 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.498 I llm_load_tensors: offloading output layer to GPU
0.00.051.499 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.509 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.051.509 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.052.359 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.359 I llama_new_context_with_model: n_ctx         = 128
0.00.052.360 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.360 I llama_new_context_with_model: n_batch       = 128
0.00.052.360 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.360 I llama_new_context_with_model: flash_attn    = 0
0.00.052.360 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.361 I llama_new_context_with_model: freq_scale    = 1
0.00.052.361 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.362 I ggml_metal_init: allocating
0.00.052.368 I ggml_metal_init: found device: Apple M4
0.00.052.370 I ggml_metal_init: picking default device: Apple M4
0.00.052.900 I ggml_metal_init: using embedded metal library
0.00.055.246 I ggml_metal_init: GPU name:   Apple M4
0.00.055.247 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.248 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.248 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.248 I ggml_metal_init: simdgroup reduction   = true
0.00.055.248 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.249 I ggml_metal_init: has bfloat            = true
0.00.055.249 I ggml_metal_init: use bfloat            = true
0.00.055.249 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.250 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.028 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.032 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.047 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.867 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.868 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.868 I llama_new_context_with_model: graph nodes  = 967
0.00.066.869 I llama_new_context_with_model: graph splits = 2
0.00.066.880 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.601.195 I 
0.00.601.226 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.601.239 I perplexity: tokenizing the input ..
0.00.609.194 I perplexity: tokenization took 7.953 ms
0.00.609.204 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.732.080 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.733.423 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.733.513 I llama_perf_context_print:        load time =     591.39 ms
0.00.733.514 I llama_perf_context_print: prompt eval time =     122.65 ms /   128 tokens (    0.96 ms per token,  1043.62 tokens per second)
0.00.733.515 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.733.515 I llama_perf_context_print:       total time =     132.32 ms /   129 tokens
0.00.733.926 I ggml_metal_free: deallocating

real	0m0.749s
user	0m0.079s
sys	0m0.104s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.081 I build: 4271 (0cd182eb) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.739 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.551 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.014.555 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.557 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.559 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.560 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.560 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.560 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.561 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.561 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.562 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.562 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.562 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.563 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.563 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.564 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.565 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.565 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.559 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.606 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.631 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.632 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.632 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.632 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.633 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.633 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.023.634 I llama_model_loader: - type  f32:  194 tensors
0.00.023.634 I llama_model_loader: - type q4_1:   97 tensors
0.00.023.634 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.674 I llm_load_vocab: special tokens cache size = 25
0.00.049.424 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.426 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.427 I llm_load_print_meta: arch             = gptneox
0.00.049.427 I llm_load_print_meta: vocab type       = BPE
0.00.049.427 I llm_load_print_meta: n_vocab          = 50304
0.00.049.428 I llm_load_print_meta: n_merges         = 50009
0.00.049.428 I llm_load_print_meta: vocab_only       = 0
0.00.049.428 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.428 I llm_load_print_meta: n_embd           = 2048
0.00.049.428 I llm_load_print_meta: n_layer          = 24
0.00.049.431 I llm_load_print_meta: n_head           = 16
0.00.049.432 I llm_load_print_meta: n_head_kv        = 16
0.00.049.446 I llm_load_print_meta: n_rot            = 32
0.00.049.446 I llm_load_print_meta: n_swa            = 0
0.00.049.446 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.448 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.448 I llm_load_print_meta: n_gqa            = 1
0.00.049.449 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.450 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.450 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.450 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.451 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.451 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.451 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.451 I llm_load_print_meta: n_ff             = 8192
0.00.049.452 I llm_load_print_meta: n_expert         = 0
0.00.049.452 I llm_load_print_meta: n_expert_used    = 0
0.00.049.452 I llm_load_print_meta: causal attn      = 1
0.00.049.452 I llm_load_print_meta: pooling type     = 0
0.00.049.452 I llm_load_print_meta: rope type        = 2
0.00.049.452 I llm_load_print_meta: rope scaling     = linear
0.00.049.453 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.453 I llm_load_print_meta: freq_scale_train = 1
0.00.049.453 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.453 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.453 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.453 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.454 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.454 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.454 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.463 I llm_load_print_meta: model type       = 1.4B
0.00.049.464 I llm_load_print_meta: model ftype      = Q4_1
0.00.049.465 I llm_load_print_meta: model params     = 1.41 B
0.00.049.465 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.049.466 I llm_load_print_meta: general.name     = 1.4B
0.00.049.467 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.467 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.467 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.467 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.468 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.049.468 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.468 I llm_load_print_meta: max token length = 1024
0.00.051.419 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.420 I llm_load_tensors: offloading output layer to GPU
0.00.051.420 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.430 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.051.431 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.052.346 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.347 I llama_new_context_with_model: n_ctx         = 128
0.00.052.347 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.347 I llama_new_context_with_model: n_batch       = 128
0.00.052.347 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.348 I llama_new_context_with_model: flash_attn    = 0
0.00.052.348 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.348 I llama_new_context_with_model: freq_scale    = 1
0.00.052.349 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.349 I ggml_metal_init: allocating
0.00.052.355 I ggml_metal_init: found device: Apple M4
0.00.052.359 I ggml_metal_init: picking default device: Apple M4
0.00.052.895 I ggml_metal_init: using embedded metal library
0.00.055.218 I ggml_metal_init: GPU name:   Apple M4
0.00.055.219 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.220 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.220 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.220 I ggml_metal_init: simdgroup reduction   = true
0.00.055.220 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.220 I ggml_metal_init: has bfloat            = true
0.00.055.220 I ggml_metal_init: use bfloat            = true
0.00.055.221 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.222 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.884 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.889 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.911 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.759 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.761 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.761 I llama_new_context_with_model: graph nodes  = 967
0.00.066.761 I llama_new_context_with_model: graph splits = 2
0.00.066.773 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.635.137 I 
0.00.635.180 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.635.194 I perplexity: tokenizing the input ..
0.00.642.997 I perplexity: tokenization took 7.802 ms
0.00.643.009 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.765.664 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.766.970 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.766.987 I llama_perf_context_print:        load time =     626.39 ms
0.00.766.988 I llama_perf_context_print: prompt eval time =     122.40 ms /   128 tokens (    0.96 ms per token,  1045.76 tokens per second)
0.00.766.989 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.766.989 I llama_perf_context_print:       total time =     131.85 ms /   129 tokens
0.00.767.293 I ggml_metal_free: deallocating

real	0m0.780s
user	0m0.079s
sys	0m0.111s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.085 I build: 4271 (0cd182eb) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.060 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.859 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.015.863 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.864 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.870 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.870 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.872 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.872 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.873 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.873 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.874 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.874 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.877 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.878 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.878 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.881 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.881 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.881 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.931 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.973 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.034 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.035 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.035 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.035 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.036 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.036 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.025.037 I llama_model_loader: - type  f32:  194 tensors
0.00.025.037 I llama_model_loader: - type q5_0:   97 tensors
0.00.025.037 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.880 I llm_load_vocab: special tokens cache size = 25
0.00.051.743 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.746 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.746 I llm_load_print_meta: arch             = gptneox
0.00.051.747 I llm_load_print_meta: vocab type       = BPE
0.00.051.747 I llm_load_print_meta: n_vocab          = 50304
0.00.051.747 I llm_load_print_meta: n_merges         = 50009
0.00.051.747 I llm_load_print_meta: vocab_only       = 0
0.00.051.747 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.747 I llm_load_print_meta: n_embd           = 2048
0.00.051.748 I llm_load_print_meta: n_layer          = 24
0.00.051.750 I llm_load_print_meta: n_head           = 16
0.00.051.753 I llm_load_print_meta: n_head_kv        = 16
0.00.051.764 I llm_load_print_meta: n_rot            = 32
0.00.051.765 I llm_load_print_meta: n_swa            = 0
0.00.051.765 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.765 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.766 I llm_load_print_meta: n_gqa            = 1
0.00.051.767 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.773 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.775 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.775 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.776 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.776 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.776 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.780 I llm_load_print_meta: n_ff             = 8192
0.00.051.780 I llm_load_print_meta: n_expert         = 0
0.00.051.780 I llm_load_print_meta: n_expert_used    = 0
0.00.051.780 I llm_load_print_meta: causal attn      = 1
0.00.051.780 I llm_load_print_meta: pooling type     = 0
0.00.051.781 I llm_load_print_meta: rope type        = 2
0.00.051.781 I llm_load_print_meta: rope scaling     = linear
0.00.051.781 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.781 I llm_load_print_meta: freq_scale_train = 1
0.00.051.781 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.782 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.782 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.782 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.782 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.782 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.782 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.792 I llm_load_print_meta: model type       = 1.4B
0.00.051.792 I llm_load_print_meta: model ftype      = Q5_0
0.00.051.793 I llm_load_print_meta: model params     = 1.41 B
0.00.051.793 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.051.793 I llm_load_print_meta: general.name     = 1.4B
0.00.051.793 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.794 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.794 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.794 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.794 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.794 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.795 I llm_load_print_meta: max token length = 1024
0.00.053.368 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.368 I llm_load_tensors: offloading output layer to GPU
0.00.053.368 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.378 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.053.379 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.054.187 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.188 I llama_new_context_with_model: n_ctx         = 128
0.00.054.188 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.188 I llama_new_context_with_model: n_batch       = 128
0.00.054.188 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.189 I llama_new_context_with_model: flash_attn    = 0
0.00.054.189 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.189 I llama_new_context_with_model: freq_scale    = 1
0.00.054.190 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.190 I ggml_metal_init: allocating
0.00.054.193 I ggml_metal_init: found device: Apple M4
0.00.054.195 I ggml_metal_init: picking default device: Apple M4
0.00.054.769 I ggml_metal_init: using embedded metal library
0.00.057.091 I ggml_metal_init: GPU name:   Apple M4
0.00.057.093 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.093 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.094 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.094 I ggml_metal_init: simdgroup reduction   = true
0.00.057.094 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.094 I ggml_metal_init: has bfloat            = true
0.00.057.094 I ggml_metal_init: use bfloat            = true
0.00.057.095 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.097 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.589 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.593 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.607 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.514 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.515 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.515 I llama_new_context_with_model: graph nodes  = 967
0.00.068.515 I llama_new_context_with_model: graph splits = 2
0.00.068.527 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.722.531 I 
0.00.722.570 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.722.643 I perplexity: tokenizing the input ..
0.00.730.033 I perplexity: tokenization took 7.389 ms
0.00.730.044 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.864.738 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.866.104 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.866.120 I llama_perf_context_print:        load time =     712.47 ms
0.00.866.122 I llama_perf_context_print: prompt eval time =     134.44 ms /   128 tokens (    1.05 ms per token,   952.13 tokens per second)
0.00.866.123 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.866.123 I llama_perf_context_print:       total time =     143.59 ms /   129 tokens
0.00.866.663 I ggml_metal_free: deallocating

real	0m0.881s
user	0m0.081s
sys	0m0.128s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.081 I build: 4271 (0cd182eb) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.171 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.835 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.014.839 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.842 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.843 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.843 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.844 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.844 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.845 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.845 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.846 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.846 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.846 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.847 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.847 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.850 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.850 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.850 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.761 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.820 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.740 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.742 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.742 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.742 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.743 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.743 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.023.743 I llama_model_loader: - type  f32:  194 tensors
0.00.023.744 I llama_model_loader: - type q5_1:   97 tensors
0.00.023.744 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.618 I llm_load_vocab: special tokens cache size = 25
0.00.049.502 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.505 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.505 I llm_load_print_meta: arch             = gptneox
0.00.049.505 I llm_load_print_meta: vocab type       = BPE
0.00.049.505 I llm_load_print_meta: n_vocab          = 50304
0.00.049.506 I llm_load_print_meta: n_merges         = 50009
0.00.049.506 I llm_load_print_meta: vocab_only       = 0
0.00.049.506 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.506 I llm_load_print_meta: n_embd           = 2048
0.00.049.506 I llm_load_print_meta: n_layer          = 24
0.00.049.509 I llm_load_print_meta: n_head           = 16
0.00.049.510 I llm_load_print_meta: n_head_kv        = 16
0.00.049.521 I llm_load_print_meta: n_rot            = 32
0.00.049.521 I llm_load_print_meta: n_swa            = 0
0.00.049.522 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.522 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.523 I llm_load_print_meta: n_gqa            = 1
0.00.049.523 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.524 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.524 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.525 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.525 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.525 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.525 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.526 I llm_load_print_meta: n_ff             = 8192
0.00.049.526 I llm_load_print_meta: n_expert         = 0
0.00.049.526 I llm_load_print_meta: n_expert_used    = 0
0.00.049.526 I llm_load_print_meta: causal attn      = 1
0.00.049.527 I llm_load_print_meta: pooling type     = 0
0.00.049.527 I llm_load_print_meta: rope type        = 2
0.00.049.527 I llm_load_print_meta: rope scaling     = linear
0.00.049.529 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.529 I llm_load_print_meta: freq_scale_train = 1
0.00.049.529 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.530 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.530 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.530 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.531 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.531 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.531 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.540 I llm_load_print_meta: model type       = 1.4B
0.00.049.541 I llm_load_print_meta: model ftype      = Q5_1
0.00.049.541 I llm_load_print_meta: model params     = 1.41 B
0.00.049.542 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.049.542 I llm_load_print_meta: general.name     = 1.4B
0.00.049.542 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.542 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.542 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.544 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.544 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.049.544 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.544 I llm_load_print_meta: max token length = 1024
0.00.051.108 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.108 I llm_load_tensors: offloading output layer to GPU
0.00.051.108 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.119 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.051.120 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.051.987 I llama_new_context_with_model: n_seq_max     = 1
0.00.051.988 I llama_new_context_with_model: n_ctx         = 128
0.00.051.988 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.051.988 I llama_new_context_with_model: n_batch       = 128
0.00.051.989 I llama_new_context_with_model: n_ubatch      = 128
0.00.051.989 I llama_new_context_with_model: flash_attn    = 0
0.00.051.989 I llama_new_context_with_model: freq_base     = 10000.0
0.00.051.990 I llama_new_context_with_model: freq_scale    = 1
0.00.051.990 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.990 I ggml_metal_init: allocating
0.00.051.996 I ggml_metal_init: found device: Apple M4
0.00.051.998 I ggml_metal_init: picking default device: Apple M4
0.00.052.552 I ggml_metal_init: using embedded metal library
0.00.054.924 I ggml_metal_init: GPU name:   Apple M4
0.00.054.926 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.926 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.927 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.927 I ggml_metal_init: simdgroup reduction   = true
0.00.054.927 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.927 I ggml_metal_init: has bfloat            = true
0.00.054.927 I ggml_metal_init: use bfloat            = true
0.00.054.928 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.928 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.598 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.601 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.615 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.510 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.511 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.511 I llama_new_context_with_model: graph nodes  = 967
0.00.066.512 I llama_new_context_with_model: graph splits = 2
0.00.066.524 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.748.175 I 
0.00.748.213 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.748.227 I perplexity: tokenizing the input ..
0.00.755.810 I perplexity: tokenization took 7.581 ms
0.00.755.823 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.889.876 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.891.305 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.891.314 I llama_perf_context_print:        load time =     739.00 ms
0.00.891.317 I llama_perf_context_print: prompt eval time =     133.82 ms /   128 tokens (    1.05 ms per token,   956.52 tokens per second)
0.00.891.317 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.891.318 I llama_perf_context_print:       total time =     143.14 ms /   129 tokens
0.00.891.680 I ggml_metal_free: deallocating

real	0m0.905s
user	0m0.079s
sys	0m0.125s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.085 I build: 4271 (0cd182eb) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.814 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.531 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.536 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.537 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.538 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.538 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.539 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.539 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.540 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.540 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.540 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.541 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.541 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.541 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.542 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.545 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.546 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.546 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.553 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.635 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.741 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.742 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.743 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.743 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.743 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.744 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.744 I llama_model_loader: - type  f32:  194 tensors
0.00.024.744 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.745 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.745 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.565 I llm_load_vocab: special tokens cache size = 25
0.00.051.456 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.459 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.459 I llm_load_print_meta: arch             = gptneox
0.00.051.459 I llm_load_print_meta: vocab type       = BPE
0.00.051.460 I llm_load_print_meta: n_vocab          = 50304
0.00.051.460 I llm_load_print_meta: n_merges         = 50009
0.00.051.460 I llm_load_print_meta: vocab_only       = 0
0.00.051.460 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.460 I llm_load_print_meta: n_embd           = 2048
0.00.051.461 I llm_load_print_meta: n_layer          = 24
0.00.051.463 I llm_load_print_meta: n_head           = 16
0.00.051.464 I llm_load_print_meta: n_head_kv        = 16
0.00.051.476 I llm_load_print_meta: n_rot            = 32
0.00.051.476 I llm_load_print_meta: n_swa            = 0
0.00.051.476 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.477 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.477 I llm_load_print_meta: n_gqa            = 1
0.00.051.478 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.479 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.479 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.480 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.480 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.480 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.480 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.481 I llm_load_print_meta: n_ff             = 8192
0.00.051.481 I llm_load_print_meta: n_expert         = 0
0.00.051.481 I llm_load_print_meta: n_expert_used    = 0
0.00.051.482 I llm_load_print_meta: causal attn      = 1
0.00.051.483 I llm_load_print_meta: pooling type     = 0
0.00.051.484 I llm_load_print_meta: rope type        = 2
0.00.051.484 I llm_load_print_meta: rope scaling     = linear
0.00.051.484 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.484 I llm_load_print_meta: freq_scale_train = 1
0.00.051.485 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.485 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.486 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.486 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.486 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.487 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.487 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.496 I llm_load_print_meta: model type       = 1.4B
0.00.051.497 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.051.498 I llm_load_print_meta: model params     = 1.41 B
0.00.051.498 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.051.498 I llm_load_print_meta: general.name     = 1.4B
0.00.051.499 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.499 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.499 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.499 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.499 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.500 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.503 I llm_load_print_meta: max token length = 1024
0.00.053.416 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.416 I llm_load_tensors: offloading output layer to GPU
0.00.053.417 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.427 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.053.428 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.054.386 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.387 I llama_new_context_with_model: n_ctx         = 128
0.00.054.387 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.387 I llama_new_context_with_model: n_batch       = 128
0.00.054.387 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.387 I llama_new_context_with_model: flash_attn    = 0
0.00.054.388 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.388 I llama_new_context_with_model: freq_scale    = 1
0.00.054.388 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.389 I ggml_metal_init: allocating
0.00.054.393 I ggml_metal_init: found device: Apple M4
0.00.054.396 I ggml_metal_init: picking default device: Apple M4
0.00.054.920 I ggml_metal_init: using embedded metal library
0.00.057.247 I ggml_metal_init: GPU name:   Apple M4
0.00.057.248 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.249 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.249 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.249 I ggml_metal_init: simdgroup reduction   = true
0.00.057.249 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.250 I ggml_metal_init: has bfloat            = true
0.00.057.250 I ggml_metal_init: use bfloat            = true
0.00.057.250 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.252 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.835 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.837 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.854 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.711 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.712 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.712 I llama_new_context_with_model: graph nodes  = 967
0.00.068.713 I llama_new_context_with_model: graph splits = 2
0.00.068.725 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.398.507 I 
0.00.398.559 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.398.579 I perplexity: tokenizing the input ..
0.00.406.443 I perplexity: tokenization took 7.862 ms
0.00.406.459 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.538.453 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.539.785 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.539.798 I llama_perf_context_print:        load time =     388.69 ms
0.00.539.798 I llama_perf_context_print: prompt eval time =     131.77 ms /   128 tokens (    1.03 ms per token,   971.39 tokens per second)
0.00.539.801 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.539.801 I llama_perf_context_print:       total time =     141.29 ms /   129 tokens
0.00.540.195 I ggml_metal_free: deallocating

real	0m0.554s
user	0m0.080s
sys	0m0.072s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.083 I build: 4271 (0cd182eb) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.883 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.562 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.014.567 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.569 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.570 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.570 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.570 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.571 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.571 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.571 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.572 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.572 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.572 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.573 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.573 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.578 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.578 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.578 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.656 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.786 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.963 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.964 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.964 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.964 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.965 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.965 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.023.966 I llama_model_loader: - type  f32:  194 tensors
0.00.023.966 I llama_model_loader: - type q3_K:   25 tensors
0.00.023.966 I llama_model_loader: - type q4_K:   71 tensors
0.00.023.967 I llama_model_loader: - type q5_K:    1 tensors
0.00.023.967 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.750 I llm_load_vocab: special tokens cache size = 25
0.00.050.707 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.709 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.709 I llm_load_print_meta: arch             = gptneox
0.00.050.710 I llm_load_print_meta: vocab type       = BPE
0.00.050.710 I llm_load_print_meta: n_vocab          = 50304
0.00.050.710 I llm_load_print_meta: n_merges         = 50009
0.00.050.710 I llm_load_print_meta: vocab_only       = 0
0.00.050.711 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.711 I llm_load_print_meta: n_embd           = 2048
0.00.050.711 I llm_load_print_meta: n_layer          = 24
0.00.050.714 I llm_load_print_meta: n_head           = 16
0.00.050.717 I llm_load_print_meta: n_head_kv        = 16
0.00.050.723 I llm_load_print_meta: n_rot            = 32
0.00.050.724 I llm_load_print_meta: n_swa            = 0
0.00.050.724 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.724 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.725 I llm_load_print_meta: n_gqa            = 1
0.00.050.726 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.726 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.727 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.727 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.727 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.727 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.727 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.728 I llm_load_print_meta: n_ff             = 8192
0.00.050.728 I llm_load_print_meta: n_expert         = 0
0.00.050.728 I llm_load_print_meta: n_expert_used    = 0
0.00.050.728 I llm_load_print_meta: causal attn      = 1
0.00.050.729 I llm_load_print_meta: pooling type     = 0
0.00.050.729 I llm_load_print_meta: rope type        = 2
0.00.050.729 I llm_load_print_meta: rope scaling     = linear
0.00.050.729 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.730 I llm_load_print_meta: freq_scale_train = 1
0.00.050.730 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.731 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.731 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.732 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.732 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.733 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.733 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.737 I llm_load_print_meta: model type       = 1.4B
0.00.050.737 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.050.738 I llm_load_print_meta: model params     = 1.41 B
0.00.050.738 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.050.738 I llm_load_print_meta: general.name     = 1.4B
0.00.050.739 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.739 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.739 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.739 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.739 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.740 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.740 I llm_load_print_meta: max token length = 1024
0.00.052.494 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.494 I llm_load_tensors: offloading output layer to GPU
0.00.052.494 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.500 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.052.500 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.053.395 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.396 I llama_new_context_with_model: n_ctx         = 128
0.00.053.396 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.396 I llama_new_context_with_model: n_batch       = 128
0.00.053.396 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.396 I llama_new_context_with_model: flash_attn    = 0
0.00.053.397 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.397 I llama_new_context_with_model: freq_scale    = 1
0.00.053.397 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.398 I ggml_metal_init: allocating
0.00.053.403 I ggml_metal_init: found device: Apple M4
0.00.053.405 I ggml_metal_init: picking default device: Apple M4
0.00.053.938 I ggml_metal_init: using embedded metal library
0.00.056.441 I ggml_metal_init: GPU name:   Apple M4
0.00.056.442 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.442 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.443 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.443 I ggml_metal_init: simdgroup reduction   = true
0.00.056.443 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.443 I ggml_metal_init: has bfloat            = true
0.00.056.443 I ggml_metal_init: use bfloat            = true
0.00.056.444 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.444 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.982 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.997 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.020 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.898 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.899 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.899 I llama_new_context_with_model: graph nodes  = 967
0.00.067.899 I llama_new_context_with_model: graph splits = 2
0.00.067.907 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.498.671 I 
0.00.498.705 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.498.717 I perplexity: tokenizing the input ..
0.00.506.434 I perplexity: tokenization took 7.715 ms
0.00.506.445 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.638.496 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.639.809 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.639.822 I llama_perf_context_print:        load time =     489.78 ms
0.00.639.824 I llama_perf_context_print: prompt eval time =     131.82 ms /   128 tokens (    1.03 ms per token,   970.98 tokens per second)
0.00.639.825 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.639.826 I llama_perf_context_print:       total time =     141.15 ms /   129 tokens
0.00.640.329 I ggml_metal_free: deallocating

real	0m0.654s
user	0m0.080s
sys	0m0.095s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.083 I build: 4271 (0cd182eb) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.154 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.888 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.014.893 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.895 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.895 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.895 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.896 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.896 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.897 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.897 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.897 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.898 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.898 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.898 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.899 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.900 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.901 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.901 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.866 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.980 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.932 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.933 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.934 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.934 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.934 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.934 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.023.935 I llama_model_loader: - type  f32:  194 tensors
0.00.023.935 I llama_model_loader: - type q4_K:   61 tensors
0.00.023.936 I llama_model_loader: - type q5_K:   24 tensors
0.00.023.936 I llama_model_loader: - type q6_K:   13 tensors
0.00.043.896 I llm_load_vocab: special tokens cache size = 25
0.00.049.753 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.755 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.756 I llm_load_print_meta: arch             = gptneox
0.00.049.756 I llm_load_print_meta: vocab type       = BPE
0.00.049.756 I llm_load_print_meta: n_vocab          = 50304
0.00.049.757 I llm_load_print_meta: n_merges         = 50009
0.00.049.757 I llm_load_print_meta: vocab_only       = 0
0.00.049.757 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.757 I llm_load_print_meta: n_embd           = 2048
0.00.049.757 I llm_load_print_meta: n_layer          = 24
0.00.049.760 I llm_load_print_meta: n_head           = 16
0.00.049.761 I llm_load_print_meta: n_head_kv        = 16
0.00.049.772 I llm_load_print_meta: n_rot            = 32
0.00.049.772 I llm_load_print_meta: n_swa            = 0
0.00.049.772 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.772 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.775 I llm_load_print_meta: n_gqa            = 1
0.00.049.776 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.777 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.777 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.777 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.778 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.778 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.778 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.779 I llm_load_print_meta: n_ff             = 8192
0.00.049.779 I llm_load_print_meta: n_expert         = 0
0.00.049.779 I llm_load_print_meta: n_expert_used    = 0
0.00.049.779 I llm_load_print_meta: causal attn      = 1
0.00.049.779 I llm_load_print_meta: pooling type     = 0
0.00.049.779 I llm_load_print_meta: rope type        = 2
0.00.049.779 I llm_load_print_meta: rope scaling     = linear
0.00.049.780 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.781 I llm_load_print_meta: freq_scale_train = 1
0.00.049.781 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.781 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.782 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.782 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.782 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.782 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.782 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.791 I llm_load_print_meta: model type       = 1.4B
0.00.049.791 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.049.792 I llm_load_print_meta: model params     = 1.41 B
0.00.049.793 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.049.793 I llm_load_print_meta: general.name     = 1.4B
0.00.049.793 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.793 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.793 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.794 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.794 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.049.794 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.794 I llm_load_print_meta: max token length = 1024
0.00.051.325 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.325 I llm_load_tensors: offloading output layer to GPU
0.00.051.325 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.335 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.051.336 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.052.167 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.168 I llama_new_context_with_model: n_ctx         = 128
0.00.052.168 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.168 I llama_new_context_with_model: n_batch       = 128
0.00.052.168 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.169 I llama_new_context_with_model: flash_attn    = 0
0.00.052.169 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.169 I llama_new_context_with_model: freq_scale    = 1
0.00.052.170 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.170 I ggml_metal_init: allocating
0.00.052.176 I ggml_metal_init: found device: Apple M4
0.00.052.178 I ggml_metal_init: picking default device: Apple M4
0.00.052.690 I ggml_metal_init: using embedded metal library
0.00.054.993 I ggml_metal_init: GPU name:   Apple M4
0.00.054.994 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.995 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.995 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.995 I ggml_metal_init: simdgroup reduction   = true
0.00.054.995 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.996 I ggml_metal_init: has bfloat            = true
0.00.054.996 I ggml_metal_init: use bfloat            = true
0.00.054.996 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.997 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.704 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.712 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.735 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.583 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.584 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.584 I llama_new_context_with_model: graph nodes  = 967
0.00.066.585 I llama_new_context_with_model: graph splits = 2
0.00.066.596 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.578.237 I 
0.00.578.274 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.578.285 I perplexity: tokenizing the input ..
0.00.586.566 I perplexity: tokenization took 8.278 ms
0.00.586.577 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.720.883 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.722.224 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.722.239 I llama_perf_context_print:        load time =     569.08 ms
0.00.722.240 I llama_perf_context_print: prompt eval time =     134.08 ms /   128 tokens (    1.05 ms per token,   954.65 tokens per second)
0.00.722.241 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.722.242 I llama_perf_context_print:       total time =     144.00 ms /   129 tokens
0.00.722.676 I ggml_metal_free: deallocating

real	0m0.737s
user	0m0.079s
sys	0m0.110s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.083 I build: 4271 (0cd182eb) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.826 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.767 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.014.772 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.773 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.774 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.774 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.774 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.775 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.776 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.776 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.776 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.777 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.777 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.777 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.778 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.781 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.783 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.783 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.846 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.963 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.068 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.070 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.070 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.070 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.071 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.071 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.071 I llama_model_loader: - type  f32:  194 tensors
0.00.024.072 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.072 I llama_model_loader: - type q6_K:   37 tensors
0.00.044.959 I llm_load_vocab: special tokens cache size = 25
0.00.050.958 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.961 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.961 I llm_load_print_meta: arch             = gptneox
0.00.050.961 I llm_load_print_meta: vocab type       = BPE
0.00.050.961 I llm_load_print_meta: n_vocab          = 50304
0.00.050.962 I llm_load_print_meta: n_merges         = 50009
0.00.050.962 I llm_load_print_meta: vocab_only       = 0
0.00.050.962 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.962 I llm_load_print_meta: n_embd           = 2048
0.00.050.962 I llm_load_print_meta: n_layer          = 24
0.00.050.965 I llm_load_print_meta: n_head           = 16
0.00.050.965 I llm_load_print_meta: n_head_kv        = 16
0.00.050.977 I llm_load_print_meta: n_rot            = 32
0.00.050.977 I llm_load_print_meta: n_swa            = 0
0.00.050.978 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.978 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.978 I llm_load_print_meta: n_gqa            = 1
0.00.050.979 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.980 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.981 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.981 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.981 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.981 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.981 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.982 I llm_load_print_meta: n_ff             = 8192
0.00.050.983 I llm_load_print_meta: n_expert         = 0
0.00.050.983 I llm_load_print_meta: n_expert_used    = 0
0.00.050.984 I llm_load_print_meta: causal attn      = 1
0.00.050.984 I llm_load_print_meta: pooling type     = 0
0.00.050.984 I llm_load_print_meta: rope type        = 2
0.00.050.984 I llm_load_print_meta: rope scaling     = linear
0.00.050.984 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.985 I llm_load_print_meta: freq_scale_train = 1
0.00.050.985 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.985 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.985 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.985 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.986 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.986 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.986 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.995 I llm_load_print_meta: model type       = 1.4B
0.00.050.995 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.050.996 I llm_load_print_meta: model params     = 1.41 B
0.00.050.996 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.050.996 I llm_load_print_meta: general.name     = 1.4B
0.00.050.997 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.997 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.997 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.997 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.997 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.998 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.999 I llm_load_print_meta: max token length = 1024
0.00.052.962 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.962 I llm_load_tensors: offloading output layer to GPU
0.00.052.963 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.973 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.052.974 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.053.938 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.939 I llama_new_context_with_model: n_ctx         = 128
0.00.053.939 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.940 I llama_new_context_with_model: n_batch       = 128
0.00.053.940 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.940 I llama_new_context_with_model: flash_attn    = 0
0.00.053.940 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.941 I llama_new_context_with_model: freq_scale    = 1
0.00.053.941 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.941 I ggml_metal_init: allocating
0.00.053.947 I ggml_metal_init: found device: Apple M4
0.00.053.949 I ggml_metal_init: picking default device: Apple M4
0.00.054.496 I ggml_metal_init: using embedded metal library
0.00.057.003 I ggml_metal_init: GPU name:   Apple M4
0.00.057.005 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.005 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.005 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.006 I ggml_metal_init: simdgroup reduction   = true
0.00.057.006 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.006 I ggml_metal_init: has bfloat            = true
0.00.057.006 I ggml_metal_init: use bfloat            = true
0.00.057.007 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.007 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.712 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.714 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.727 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.604 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.605 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.605 I llama_new_context_with_model: graph nodes  = 967
0.00.068.605 I llama_new_context_with_model: graph splits = 2
0.00.068.617 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.665.134 I 
0.00.665.166 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.665.179 I perplexity: tokenizing the input ..
0.00.672.689 I perplexity: tokenization took 7.509 ms
0.00.672.703 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.812.819 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.814.170 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.814.184 I llama_perf_context_print:        load time =     656.30 ms
0.00.814.185 I llama_perf_context_print: prompt eval time =     139.88 ms /   128 tokens (    1.09 ms per token,   915.08 tokens per second)
0.00.814.186 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.814.186 I llama_perf_context_print:       total time =     149.05 ms /   129 tokens
0.00.814.602 I ggml_metal_free: deallocating

real	0m0.828s
user	0m0.080s
sys	0m0.135s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.083 I build: 4271 (0cd182eb) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.235 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.728 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.732 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.733 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.734 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.734 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.734 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.735 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.735 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.736 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.736 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.736 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.737 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.737 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.737 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.740 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.740 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.741 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.813 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.868 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.728 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.729 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.729 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.729 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.730 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.730 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.730 I llama_model_loader: - type  f32:  194 tensors
0.00.024.731 I llama_model_loader: - type q6_K:   98 tensors
0.00.045.454 I llm_load_vocab: special tokens cache size = 25
0.00.051.290 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.292 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.293 I llm_load_print_meta: arch             = gptneox
0.00.051.293 I llm_load_print_meta: vocab type       = BPE
0.00.051.293 I llm_load_print_meta: n_vocab          = 50304
0.00.051.293 I llm_load_print_meta: n_merges         = 50009
0.00.051.293 I llm_load_print_meta: vocab_only       = 0
0.00.051.294 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.294 I llm_load_print_meta: n_embd           = 2048
0.00.051.294 I llm_load_print_meta: n_layer          = 24
0.00.051.296 I llm_load_print_meta: n_head           = 16
0.00.051.297 I llm_load_print_meta: n_head_kv        = 16
0.00.051.309 I llm_load_print_meta: n_rot            = 32
0.00.051.309 I llm_load_print_meta: n_swa            = 0
0.00.051.309 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.309 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.310 I llm_load_print_meta: n_gqa            = 1
0.00.051.311 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.311 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.312 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.313 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.313 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.313 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.313 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.314 I llm_load_print_meta: n_ff             = 8192
0.00.051.314 I llm_load_print_meta: n_expert         = 0
0.00.051.314 I llm_load_print_meta: n_expert_used    = 0
0.00.051.314 I llm_load_print_meta: causal attn      = 1
0.00.051.314 I llm_load_print_meta: pooling type     = 0
0.00.051.315 I llm_load_print_meta: rope type        = 2
0.00.051.315 I llm_load_print_meta: rope scaling     = linear
0.00.051.315 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.315 I llm_load_print_meta: freq_scale_train = 1
0.00.051.316 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.316 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.316 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.316 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.316 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.316 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.316 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.326 I llm_load_print_meta: model type       = 1.4B
0.00.051.326 I llm_load_print_meta: model ftype      = Q6_K
0.00.051.326 I llm_load_print_meta: model params     = 1.41 B
0.00.051.327 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.051.327 I llm_load_print_meta: general.name     = 1.4B
0.00.051.327 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.327 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.327 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.328 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.328 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.328 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.328 I llm_load_print_meta: max token length = 1024
0.00.053.303 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.303 I llm_load_tensors: offloading output layer to GPU
0.00.053.303 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.313 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.053.314 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.054.214 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.215 I llama_new_context_with_model: n_ctx         = 128
0.00.054.215 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.215 I llama_new_context_with_model: n_batch       = 128
0.00.054.216 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.216 I llama_new_context_with_model: flash_attn    = 0
0.00.054.216 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.216 I llama_new_context_with_model: freq_scale    = 1
0.00.054.217 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.217 I ggml_metal_init: allocating
0.00.054.223 I ggml_metal_init: found device: Apple M4
0.00.054.226 I ggml_metal_init: picking default device: Apple M4
0.00.054.770 I ggml_metal_init: using embedded metal library
0.00.057.121 I ggml_metal_init: GPU name:   Apple M4
0.00.057.122 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.123 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.123 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.123 I ggml_metal_init: simdgroup reduction   = true
0.00.057.123 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.124 I ggml_metal_init: has bfloat            = true
0.00.057.124 I ggml_metal_init: use bfloat            = true
0.00.057.124 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.125 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.070 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.079 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.100 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.938 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.939 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.939 I llama_new_context_with_model: graph nodes  = 967
0.00.068.939 I llama_new_context_with_model: graph splits = 2
0.00.068.951 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.382.090 I 
0.00.382.126 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.382.168 I perplexity: tokenizing the input ..
0.00.390.365 I perplexity: tokenization took 8.196 ms
0.00.390.376 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.530.688 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.532.060 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.532.078 I llama_perf_context_print:        load time =     371.85 ms
0.00.532.079 I llama_perf_context_print: prompt eval time =     140.04 ms /   128 tokens (    1.09 ms per token,   914.05 tokens per second)
0.00.532.080 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.532.081 I llama_perf_context_print:       total time =     149.99 ms /   129 tokens
0.00.532.639 I ggml_metal_free: deallocating

real	0m0.548s
user	0m0.079s
sys	0m0.089s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.227 I build: 4271 (0cd182eb) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.021.817 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.034.107 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.034.114 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.034.115 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.034.116 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.034.116 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.034.117 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.034.117 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.034.118 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.034.118 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.034.119 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.034.119 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.034.120 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.034.120 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.034.120 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.034.122 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.034.123 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.034.123 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.043.112 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.045.434 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.052.822 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.052.824 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.052.825 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.052.825 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.052.826 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.052.826 I llama_model_loader: - type  f32:  194 tensors
0.00.052.827 I llama_model_loader: - type  f16:   98 tensors
0.00.082.319 I llm_load_vocab: special tokens cache size = 25
0.00.088.635 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.088.637 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.088.638 I llm_load_print_meta: arch             = gptneox
0.00.088.638 I llm_load_print_meta: vocab type       = BPE
0.00.088.638 I llm_load_print_meta: n_vocab          = 50304
0.00.088.638 I llm_load_print_meta: n_merges         = 50009
0.00.088.639 I llm_load_print_meta: vocab_only       = 0
0.00.088.639 I llm_load_print_meta: n_ctx_train      = 2048
0.00.088.639 I llm_load_print_meta: n_embd           = 2048
0.00.088.639 I llm_load_print_meta: n_layer          = 24
0.00.088.642 I llm_load_print_meta: n_head           = 16
0.00.088.643 I llm_load_print_meta: n_head_kv        = 16
0.00.088.655 I llm_load_print_meta: n_rot            = 32
0.00.088.656 I llm_load_print_meta: n_swa            = 0
0.00.088.656 I llm_load_print_meta: n_embd_head_k    = 128
0.00.088.656 I llm_load_print_meta: n_embd_head_v    = 128
0.00.088.657 I llm_load_print_meta: n_gqa            = 1
0.00.088.657 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.088.658 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.088.661 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.088.662 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.088.663 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.088.663 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.088.664 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.088.664 I llm_load_print_meta: n_ff             = 8192
0.00.088.664 I llm_load_print_meta: n_expert         = 0
0.00.088.665 I llm_load_print_meta: n_expert_used    = 0
0.00.088.665 I llm_load_print_meta: causal attn      = 1
0.00.088.665 I llm_load_print_meta: pooling type     = 0
0.00.088.665 I llm_load_print_meta: rope type        = 2
0.00.088.665 I llm_load_print_meta: rope scaling     = linear
0.00.088.666 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.088.666 I llm_load_print_meta: freq_scale_train = 1
0.00.088.667 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.088.667 I llm_load_print_meta: rope_finetuned   = unknown
0.00.088.667 I llm_load_print_meta: ssm_d_conv       = 0
0.00.088.667 I llm_load_print_meta: ssm_d_inner      = 0
0.00.088.667 I llm_load_print_meta: ssm_d_state      = 0
0.00.088.668 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.088.668 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.088.677 I llm_load_print_meta: model type       = 1.4B
0.00.088.679 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.088.679 I llm_load_print_meta: model params     = 1.41 B
0.00.088.680 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.088.680 I llm_load_print_meta: general.name     = 1.4B
0.00.088.680 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.088.680 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.088.680 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.088.681 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.088.682 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.088.682 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.088.682 I llm_load_print_meta: max token length = 1024
0.00.091.226 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.091.226 I llm_load_tensors: offloading output layer to GPU
0.00.091.226 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.091.237 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.091.238 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.092.198 I llama_new_context_with_model: n_seq_max     = 1
0.00.092.199 I llama_new_context_with_model: n_ctx         = 128
0.00.092.199 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.092.199 I llama_new_context_with_model: n_batch       = 128
0.00.092.200 I llama_new_context_with_model: n_ubatch      = 128
0.00.092.200 I llama_new_context_with_model: flash_attn    = 0
0.00.092.200 I llama_new_context_with_model: freq_base     = 10000.0
0.00.092.200 I llama_new_context_with_model: freq_scale    = 1
0.00.092.201 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.092.201 I ggml_metal_init: allocating
0.00.092.204 I ggml_metal_init: found device: Apple M4
0.00.092.206 I ggml_metal_init: picking default device: Apple M4
0.00.092.772 I ggml_metal_init: using embedded metal library
0.00.095.290 I ggml_metal_init: GPU name:   Apple M4
0.00.095.291 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.095.292 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.095.292 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.095.292 I ggml_metal_init: simdgroup reduction   = true
0.00.095.292 I ggml_metal_init: simdgroup matrix mul. = true
0.00.095.292 I ggml_metal_init: has bfloat            = true
0.00.095.293 I ggml_metal_init: use bfloat            = true
0.00.095.293 I ggml_metal_init: hasUnifiedMemory      = true
0.00.095.293 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.105.904 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.105.907 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.105.920 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.106.774 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.106.776 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.106.776 I llama_new_context_with_model: graph nodes  = 967
0.00.106.776 I llama_new_context_with_model: graph splits = 2
0.00.106.788 I 
0.00.106.819 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.106.820 I compute_imatrix: tokenizing the input ..
0.00.113.728 I compute_imatrix: tokenization took 6.908 ms
0.00.113.730 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.613.185 I compute_imatrix: 1.50 seconds per pass - ETA 0.02 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.616.158 I llama_perf_context_print:        load time =    1591.36 ms
0.01.616.159 I llama_perf_context_print: prompt eval time =    1498.83 ms /   128 tokens (   11.71 ms per token,    85.40 tokens per second)
0.01.616.160 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.616.160 I llama_perf_context_print:       total time =    1594.33 ms /   129 tokens
0.01.616.701 I ggml_metal_free: deallocating

real	0m1.804s
user	0m0.168s
sys	0m0.236s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4271 (0cd182eb)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 'Ä'
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x128f04280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x128f04a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x128f04e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x128f052e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x128f05750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x128f05bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x128f06030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x128f064a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x128f06910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x128f06d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x128f071f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x128f07890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x128f083b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x128f08b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x128f09370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x128f09a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x128f0a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x128f0a8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x128f0aff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x128f0b7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x128f0bee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x128f0c600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x128f0cd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x128f0d5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x128f0dce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x128f0dfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x128f0e260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x128f0e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x128f0ed80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x128f0f1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x128f0f660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x128f0fbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x128f10060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x128f10320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x128f10790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x128f11040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x128f11300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x128f11770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x128f11be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x128f12050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x128f124c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x128f12930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x128f12da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x128f13210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x128f13680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x128f13af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x128f13f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x128f14990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x128f14c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x128f150c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x128f15530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x128f159a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x128f15e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x128f16280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x128f166f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x128f16da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x128f17240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x128f17500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x128f17970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x128f18040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x128f18440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x128f18700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x128f18c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x128f19100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x128f19600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x128f19b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x128f1a000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x128f1a500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x128f1aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x128f1af00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x128f1b400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x128f1b900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x128f1be00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x128f1c300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x128f1c8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x128f1ce60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x128f1d410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x128f1d9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x128f1df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x128f1e520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x128f1ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x128f1f080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x128f1f630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x128f1fbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x128f20190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x128f20740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x128f20cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x128f212a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x128f21850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x128f21e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x128f223b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x128f22960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x128f22f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x128f234c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x128f23a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x128f24020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x128f245d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x128f14580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x128f24d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x128f251a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x128f25610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x128f25bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x128f26170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x128f26720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x128f26cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x128f27280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x128f27830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x128f27de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x128f28390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x128f28940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x128f28ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x128f294a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x128f29a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x128f2a000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x128f2a500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x128f2aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x128f2af00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x128f2b400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x128f2b900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x128f2be00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x128f2c300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x128f2c800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x128f2cd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x128f2d200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x128f2d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x128f2dc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x128f2e100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x128f2e600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x128f2eb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x128f2f000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x128f2f500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x128f2fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x128f2ff00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x128f30400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x128f30900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x128f30e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x128f31300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x128f31800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x128f31d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x128f32200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x128f32700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x128f32c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x128f33100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x128f33600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x128f33b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x128f34000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x128f34500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x128f34a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x128f34f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x128f35400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x128f35900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x128f35e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x128f36300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x128f36800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x128f36d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x128f37200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x128f37700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x128f37c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x128f38100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x128f38600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x128f38b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x128f39000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x128f39500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x128f39a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x128f39f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x128f3a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x128f3a900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x128f3ae00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x128f3b300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x128f3b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x128f3bd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x128f3c200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x128f3c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x128f3cc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x128f3d100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x128f3d600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x128f3db00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x128f3e000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x128f3e500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x128f3ea00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x128f3ef00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x128f3f400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x128f3f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x128f3fe00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x128f40300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x128f40800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x128f40d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x128f41200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x128f41700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x128f41c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x128f42100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x128f42600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x128f42b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x128f43000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x128f435b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x128f43b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x128f44110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x128f446c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x128f44cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x128f452e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x128f458f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x128f460e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x128f46580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x128f46840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x128f46e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x128f47460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x128f47c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x128f480f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x128f48590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x128f48a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x128f491e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x128f49730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x128f49c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x128f4a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x128f4a720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x128f4ac70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x128f4b1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x128f4b710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x128f4bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x128f4c1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x128f4c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x128f4cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x128f4d1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x128f4d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x128f4dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x128f4e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x128f4e6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x128f4ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x128f4f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x128f4f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x128f4fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x128f50170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x128f506c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x128f50c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x128f51160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x128f516b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x128f51c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x128f52150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x128f526a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x128f52bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x128f53140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x128f53690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x128f53be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x128f54130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x128f54680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x128f54bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x128f55120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x128f55670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x128f55bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x128f56110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x128f56660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x128f56bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x128f57100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x128f57650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x128f57ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x128f580f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x128f58640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x128f58b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x128f590e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x128f59630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x128f59b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x128f5a0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x128f5a620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x128f5ab70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x128f5b0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x128f5b610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x128f5bb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x128f5c000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x128f5c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x128f5c940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x128f5cde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x128f5d280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x128f5d720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x128f5dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x128f5e060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x128f5e500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x128f5e9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x128f5ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x128f5f2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x128f5f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x128f5fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x128f600c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x128f60610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x128f60d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x128f61450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x128f61b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x128f62290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x128f62550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x128f62d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x128f63000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x128f63610 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
0.00.193.919 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x128e0a480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x128e0a8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x128e0ad60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x128e0b1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x128e0b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x128e0bab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x128e0bf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x128e0c390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x128e0c800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x128e0cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x128e0d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x128e0d830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x128e0e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x128e0eb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x128e0f310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x128e0fa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x128e10150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x128e10870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x128e10f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x128e11760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x128e11e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x128e125a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x128e12cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x128e133e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x128e13b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x128e13dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x128e14080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x128e144f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x128e14960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x128e14dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x128e15240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x128e15770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x128e15be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x128e15ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x128e16310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x128e16780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x128e16bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x128e17060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x128e174d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x128e17940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x128e17db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x128e18220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x128e18690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x128e18b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x128e18f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x128e193e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x128e19850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x128e19cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x128e1a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x128e1a5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x128e1aa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x128e1ae80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x128e1b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x128e1b760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x128e1bbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x128e1c040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x128e1c5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x128e1cab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x128e1cf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x128e1d390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x128e1d800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x128e1dc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x128e1e0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x128e1e550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x128e1e9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x128e1ee30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x128e1f2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x128e1f710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x128e1fb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x128e1fff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x128e20460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x128e208d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x128e20d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x128e211b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x128e21620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x128e21a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x128e21f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x128e22370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x128e227e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x128e22c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x128e230c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x128e23530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x128e239a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x128e23e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x128e24280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x128e246f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x128e24b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x128e24fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x128e25440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x128e258b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x128e25d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x128e26190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x128e26600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x128e26a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x128e26ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x128e27350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x128e277c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x128e27c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x128e280a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x128e28510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x128e28980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x128e28df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x128e29260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x128e296d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x128e29b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x128e29fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x128e2a420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x128e2a890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x128e2ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x128e2b170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x128e2b5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x128e2ba50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x128e2bec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x128e2c330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x128e2c7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x128e2cc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x128e2d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x128e2d4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x128e2d960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x128e2ddd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x128e2e240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x128e2e6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x128e2eb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x128e2ef90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x128e2f400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x128e2f870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x128e2fce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x128e30150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x128e305c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x128e30a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x128e30ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x128e31310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x128e31780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x128e31bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x128e32060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x128e324d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x128e32940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x128e32db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x128e33220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x128e33690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x128e33b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x128e33f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x128e343e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x128e34850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x128e34cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x128e35130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x128e355a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x128e35a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x128e35e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x128e362f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x128e36760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x128e36bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x128e37040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x128e374b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x128e37920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x128e37d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x128e38200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x128e38670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x128e38ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x128e38f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x128e393c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x128e39830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x128e39ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x128e3a110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x128e3a580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x128e3a9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x128e3ae60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x128e3b2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x128e3b740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x128e3bbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x128e3c020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x128e3c490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x128e3c900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x128e3cd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x128e3d1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x128e3d650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x128e3dac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x128e3df30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x128e3e3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x128e3e810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x128e3ec80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x128e3f0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x128e3f560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x128e3f9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x128e3fe40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x128e402b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x128e40720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x128e40b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x128e41000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x128e41470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x128e418e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x128e41d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x128e421c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x128e42630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x128e42aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x128e42f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x128e43380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x128e437f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x128e43c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x128e440d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x128e44540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x128e449b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x128e44e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x128e45290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x128e45700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x128e45b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x128e45fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x128e46570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x128e469e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x128e46e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x128e479a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x128e47c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x128e47f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x128e48390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x128e48800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x128e48c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x128e490e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x128e49550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x128e499c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x128e49e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x128e4a2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x128e4a710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x128e4ab80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x128e4aff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x128e4b460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x128e4b8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x128e4bd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x128e4c1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x128e4c620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x128e4ca90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x128e4cf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x128e4d370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x128e4d7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x128e4dc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x128e4e0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x128e4e530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x128e4e9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x128e4ee10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x128e4f280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x128e4f6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x128e4fb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x128e4ffd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x128e50440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x128e508b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x128e50d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x128e51190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x128e51600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x128e51a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x128e51ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x128e52350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x128e527c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x128e52c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x128e530a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x128e53510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x128e53980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x128e53df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x128e54260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x128e546d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x128e54b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x128e54fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x128e55420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x128e55890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x128e55d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x128e56170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x128e565e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x128e56a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x128e56ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x128e57330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x128e577a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x128e57c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x128e58080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x128e584f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x128e58960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x128e58dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x128e59240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x128e596b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x128e59b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x128e59f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x128e5a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x128e5a870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x128e5ace0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x128e5b150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x128e5b5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x128e5c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x128e5c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x128e5ce70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x128e5d590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x128e5d850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x128e5dcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x128e5e2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x128e5e8d0 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x128e0a480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x128e0a8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x128e0ad60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x128e0b1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x128e0b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x128e0bab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x128e0bf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x128e0c390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x128e0c800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x128e0cc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x128e0d0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x128e0d6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x128e0dfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x128e0e730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x128e0ef10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x128e0f600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x128e0fcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x128e103e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x128e10ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x128e11450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x128e11b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x128e12230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x128e12920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x128e13010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x128e13700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x128e13b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x128e13fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x128e14450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x128e148c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x128e14d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x128e151a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x128e15610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x128e15a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x128e15d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x128e161b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x128e16620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x128e16a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x128e16f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x128e17370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x128e177e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x128e17c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x128e180c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x128e18530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x128e189a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x128e18e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x128e19280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x128e196f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x128e19b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x128e19fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x128e1a440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x128e1a8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x128e1ad20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x128e1b190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x128e1b600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x128e1ba70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x128e1bee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x128e1c350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x128e1c7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x128e1cc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x128e1d0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x128e1d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x128e1d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x128e1ddf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x128e1e260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x128e1e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x128e1eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x128e1efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x128e1f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x128e1f890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x128e1fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x128e20170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x128e205e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x128e20a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x128e20ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x128e21330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x128e217a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x128e21c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x128e22080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x128e224f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x128e22960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x128e22dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x128e23240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x128e236b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x128e23b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x128e23f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x128e24400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x128e24870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x128e24ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x128e25150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x128e255c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x128e25a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x128e25ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x128e26310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x128e26780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x128e26bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x128e27060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x128e274d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x128e27940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x128e27db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x128e28220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x128e28690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x128e28b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x128e28f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x128e293e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x128e29850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x128e29cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x128e2a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x128e2a5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x128e2aa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x128e2ae80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x128e2b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x128e2b760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x128e2bbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x128e2c040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x128e2c4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x128e2c920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x128e2cd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x128e2d200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x128e2d670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x128e2dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x128e2df50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x128e2e3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x128e2e830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x128e2eca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x128e2f110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x128e2f580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x128e2f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x128e2fe60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x128e302d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x128e30740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x128e30bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x128e31020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x128e31490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x128e31900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x128e31d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x128e321e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x128e32650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x128e32ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x128e32f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x128e333a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x128e33810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x128e33c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x128e340f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x128e34560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x128e349d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x128e34e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x128e352b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x128e35720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x128e35b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x128e36000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x128e36470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x128e368e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x128e36d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x128e371c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x128e37630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x128e37aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x128e37f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x128e38380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x128e387f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x128e38c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x128e390d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x128e39540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x128e399b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x128e39e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x128e3a290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x128e3a700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x128e3ab70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x128e3afe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x128e3b450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x128e3b8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x128e3bd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x128e3c1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x128e3c610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x128e3ca80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x128e3cef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x128e3d360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x128e3d7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x128e3dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x128e3e0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x128e3e520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x128e3e990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x128e3ee00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x128e3f270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x128e3f6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x128e3fb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x128e3ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x128e40430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x128e408a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x128e40d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x128e41180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x128e415f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x128e41a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x128e41ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x128e42340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x128e427b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x128e42c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x128e43090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x128e43500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x128e43970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x128e43de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x128e44250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x128e446c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x128e44b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x128e44fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x128e45410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x128e45880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x128e45cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x128e46160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x128e465d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x128e46a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x128e471c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x128e47630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x128e47aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x128e47f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x128e48380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x128e487f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x128e48c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x128e490d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x128e49540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x128e499b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x128e49e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x128e4a290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x128e4a700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x128e4ab70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x128e4afe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x128e4b450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x128e4b8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x128e4bd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x128e4c1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x128e4c610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x128e4ca80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x128e4cef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x128e4d360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x128e4d7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x128e4dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x128e4e0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x128e4e520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x128e4e990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x128e4ee00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x128e4f270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x128e4f6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x128e4fb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x128e4ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x128e50430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x128e508a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x128e50d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x128e51180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x128e515f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x128e51a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x128e51ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x128e52340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x128e527b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x128e52c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x128e53090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x128e53500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x128e53970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x128e53de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x128e54250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x128e546c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x128e54b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x128e54fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x128e55410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x128e55880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x128e55cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x128e56160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x128e565d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x128e56a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x128e56eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x128e57320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x128e57790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x128e57c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x128e58070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x128e584e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x128e58950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x128e58dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x128e59230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x128e596a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x128e59b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x128e59f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x128e5a3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x128e5a860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x128e5acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x128e5b140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x128e5b9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x128e5c090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x128e5c780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x128e5ce70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x128e5d2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x128e5d750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x128e5dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x128e5e030 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.822s
user	0m0.299s
sys	0m0.298s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4271 (0cd182eb)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 'Ä'
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x15470d370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x15470da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x15470e030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x15470e5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x15470eb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x15470f140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x15470f6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x15470fca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x154710250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x154710750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x154710c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x154711150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x154711c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x154712420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x154712c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x154713350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x154713a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x154714190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x1547148b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x154715080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x1547157a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x154715ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x1547165e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x154716e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x1547175a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x154717860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x154717e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x154718ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x154719020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x1547192e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x154719780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x154719a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x15471a2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x15471a810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x15471aad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x15471af70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x15471b410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x15471b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x15471bd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x15471c1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x15471c690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x15471cb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x15471cfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x15471d470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x15471d730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x15471dd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x15471e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x15471ec70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x15471f280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x15471f890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x15471fea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x1547204b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x154720ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x1547210d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x1547218c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x154721d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x154722200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x1547224c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x154722ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x1547232c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x154723580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x154723a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x154723ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x154724360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x154724800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x154724ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x154725140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x1547255e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x154725a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x154725f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x1547263c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x154726860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x154726d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x154727250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x1547277a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x154727cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x154728240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x154728790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x154728ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x154729230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x154729780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x154729cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x15472a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x15472a770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x15472acc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x15472b210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x15472b760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x15472bcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x15472c200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x15472c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x15472cca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x15472d1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x15472d740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x15472dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x15472e1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x15472e730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x15472ec80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x15471e960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x15472f0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x15472f8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x15472fdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x154730340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x154730890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x154730de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x154731330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x154731880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x154731dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x154732320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x154732870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x154732dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x154733310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x154733860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x154733db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x154734250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x1547346f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x154734b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x154735030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x1547354d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x154735970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x154735e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x1547362b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x154736750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x154736bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x154737090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x154737530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x1547379d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x154737e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x154738310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x1547387b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x154738c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x1547390f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x154739590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x154739a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x154739ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x15473a370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x15473a810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x15473acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x15473b150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x15473b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x15473ba90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x15473bf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x15473c3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x15473c870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x15473cd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x15473d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x15473d650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x15473daf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x15473df90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x15473e430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x15473e8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x15473ed70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x15473f210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x15473f6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x15473fb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x15473fff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x154740490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x154740930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x154740dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x154741270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x154741710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x154741bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x154742050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x1547424f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x154742990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x154742e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x1547432d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x154743770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x154743c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x1547440b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x154744550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x1547449f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x154744e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x154745330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x1547457d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x154745c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x154746110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x1547465b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x154746a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x154746ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x154747390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x154747830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x154747cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x154748170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x154748610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x154748ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x154748f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x1547493f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x154749890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x154749d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x15474a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x15474a670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x15474ab10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x15474afb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x15474b500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x15474ba50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x15474bfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x15474c4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x15474c7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x15474cdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x15474d3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x15474d9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x15474e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x15474e670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x15474e930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x15474ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x15474f550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x15474fd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x1547501e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x154750680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x154750b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x1547512d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x154751820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x154751d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x1547522c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x154752810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x154752d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x1547532b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x154753800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x154753d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x1547542a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x1547547f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x154754d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x154755290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x1547557e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x154755d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x154756280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x1547567d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x154756d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x154757270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x1547577c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x154757d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x154758260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x1547587b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x154758d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x154759250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x1547597a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x154759cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x15475a240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x15475a790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x15475ace0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x15475b230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x15475b780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x15475bcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x15475c220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x15475c770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x15475ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x15475d210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x15475d760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x15475dcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x15475e200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x15475e750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x15475eca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x15475f1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x15475f740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x15475fc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x1547601e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x154760730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x154760c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x1547611d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x154761720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x154761c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x1547621c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x154762710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x154762c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x1547631b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x154763700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x154763c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x1547640f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x154764590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x154764a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x154764ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x154765370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x154765810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x154765cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x154766150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1547665f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x154766a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x154766f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x1547673d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x154767870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x154767d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x1547681b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x154768700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x154768e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x154769540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x154769c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x15476a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x15476a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x15476ae30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x15476b0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x15476b700 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
0.00.097.049 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x156004ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x156005150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x1560055c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x156005a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x156005ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x156006310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x156006780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x156006bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x156007060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x1560075e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x156007a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x1560080d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x156008bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x1560093a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x156009bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x15600a2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x15600a9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x15600b110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x15600b830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x15600c000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x15600c720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x15600ce40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x15600d560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x15600dc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x15600e3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x15600e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x15600e920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x15600ed90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x15600f200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x15600f670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x15600fae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x156010010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x156010480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x156010740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x156010bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x156011020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x156011490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x156011900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x156011d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x1560121e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x156012650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x156012ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x156012f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x1560133a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x156013810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x156013c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x1560140f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x156014560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x1560149d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x156014e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x1560152b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x156015720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x156015b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x156016000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x156016470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x1560168e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x156016e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x156017350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x1560177c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x156017c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x1560180a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x156018510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x156018980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x156018df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x156019260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x1560196d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x156019b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x156019fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x15601a420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x15601a890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x15601ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x15601b170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x15601b5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x15601ba50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x15601bec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x15601c330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x15601c7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x15601cc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x15601d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x15601d4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x15601d960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x15601ddd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x15601e240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x15601e6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x15601eb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x15601ef90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x15601f400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x15601f870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x15601fce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x156020150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x1560205c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x156020a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x156020ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x156021310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x156021780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x156021bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x156022060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x1560224d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x156022940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x156022db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x156023220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x156023690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x156023b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x156023f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x1560243e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x156024850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x156024cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x156025130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x1560255a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x156025a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x156025e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x1560262f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x156026760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x156026bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x156027040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x1560274b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x156027920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x156027d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x156028200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x156028670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x156028ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x156028f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x1560293c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x156029830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x156029ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x15602a110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x15602a580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x15602a9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x15602ae60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x15602b2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x15602b740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x15602bbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x15602c020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x15602c490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x15602c900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x15602cd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x15602d1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x15602d650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x15602dac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x15602df30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x15602e3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x15602e810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x15602ec80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x15602f0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x15602f560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x15602f9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x15602fe40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x1560302b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x156030720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x156030b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x156031000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x156031470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x1560318e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x156031d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x1560321c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x156032630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x156032aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x156032f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x156033380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x1560337f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x156033c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x1560340d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x156034540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x1560349b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x156034e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x156035290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x156035700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x156035b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x156035fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x156036450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x1560368c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x156036d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x1560371a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x156037610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x156037a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x156037ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x156038360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x1560387d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x156038c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x1560390b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x156039520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x156039990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x156039e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x15603a270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x15603a6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x15603ab50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x15603afc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x15603b430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x15603b8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x15603bd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x15603c180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x15603c5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x15603ca60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x15603ced0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x15603d340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x15603d7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x15603dc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x15603e090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x15603e500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x15603e970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x15603ede0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x15603f250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x15603f6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x15603fb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x15603ffa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x156040410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x156040880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x156040e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x156041280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x1560416f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x156042240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x156042500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x1560427c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x156042c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x1560430a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x156043510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x156043980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x156043df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x156044260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x1560446d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x156044b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x156044fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x156045420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x156045890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x156045d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x156046170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x1560465e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x156046a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x156046ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x156047330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x1560477a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x156047c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x156048080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x1560484f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x156048960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x156048dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x156049240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x1560496b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x156049b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x156049f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x15604a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x15604a870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x15604ace0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x15604b150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x15604b5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x15604ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x15604bea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x15604c310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x15604c780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x15604cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x15604d060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x15604d4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x15604d940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x15604ddb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x15604e220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x15604e690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x15604eb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x15604ef70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x15604f3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x15604f850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x15604fcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x156050130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x1560505a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x156050a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x156050e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x1560512f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x156051760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x156051bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x156052040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x1560524b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x156052920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x156052d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x156053200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x156053670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x156053ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x156053f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x1560543c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x156054830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x156054ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x156055110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x156055580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x1560559f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x156055e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x1560568d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x156056ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x156057710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x156057e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x1560580f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x156058560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x156058b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x156059170 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x1562044e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x156204950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x156204dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x156205230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x1562056a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x156205b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x156205f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x1562063f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x156206860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x156206cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x156207140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x156207870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x156208390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x156208b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x156209350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x156209a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x15620a190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x15620a8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x15620afd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x15620b700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x15620be20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x15620c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x15620cc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x15620d380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x15620daa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x15620dd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x15620e020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x15620e490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x15620e900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x15620ed70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x15620f1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x15620f710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x15620fb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x15620fe40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1562102b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x156210720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x156210b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x156211000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x156211470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x1562118e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x156211d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x1562121c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x156212630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x156212aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x156212f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x156213380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x1562137f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x156213c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x1562140d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x156214540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x1562149b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x156214e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x156215290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x156215700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x156215b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x156215fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x156216550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x156216a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x156216ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x156217330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x1562177a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x156217c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x156218080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x1562184f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x156218960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x156218dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x156219240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x1562196b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x156219b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x156219f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x15621a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x15621a870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x15621ace0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x15621b150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x15621b5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x15621ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x15621bea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x15621c310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x15621c780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x15621cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x15621d060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x15621d4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x15621d940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x15621ddb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x15621e220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x15621e690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x15621eb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x15621ef70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x15621f3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x15621f850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x15621fcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x156220130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x1562205a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x156220a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x156220e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x1562212f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x156221760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x156221bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x156222040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x1562224b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x156222920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x156222d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x156223200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x156223670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x156223ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x156223f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x1562243c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x156224830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x156224ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x156225110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x156225580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x1562259f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x156225e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x1562262d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x156226740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x156226bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x156227020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x156227490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x156227900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x156227d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x1562281e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x156228650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x156228ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x156228f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x1562293a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x156229810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x156229c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x15622a0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x15622a560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x15622a9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x15622ae40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x15622b2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x15622b720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x15622bb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x15622c000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x15622c470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x15622c8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x15622cd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x15622d1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x15622d630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x15622daa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x15622df10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x15622e380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x15622e7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x15622ec60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x15622f0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x15622f540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x15622f9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x15622fe20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x156230290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x156230700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x156230b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x156230fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x156231450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x1562318c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x156231d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x1562321a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x156232610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x156232a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x156232ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x156233360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x1562337d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x156233c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x1562340b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x156234520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x156234990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x156234e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x156235270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x1562356e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x156235b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x156235fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x156236430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x1562368a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x156236d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x156237180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x1562375f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x156237a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x156237ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x156238340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x1562387b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x156238c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x156239090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x156239500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x156239970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x156239de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x15623a250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x15623a6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x15623ab30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x156004880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x156004cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x156005160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x1560055d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x156005a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x156005eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x156006320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x156006790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x156006c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x156007070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x1560074e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x156007950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x156007dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x156008230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x1560086a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x156008b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x156008f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x1560093f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x156009860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x156009cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x15600a140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x15600a5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x15600aea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x15600b310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x15600b780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x15600bbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x15600c060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x15600c4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x15600c940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x15600cdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x15600d220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x15600d690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x15600db00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x15600df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x15600e3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x15600e850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x15600ecc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x15600f130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x15600f5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x15600fa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x15600fe80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x1560102f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x156010760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x156010bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x156011040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x1560114b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x156011920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x156011d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x156012200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x156012670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x156012ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x156012f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x1560133c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x156013830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x156013ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x156014110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x156014830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x156014ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x156015110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x156015580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x1560159f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x156015e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x1560162d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x156016740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x156016bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x156017020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x156017490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x156017900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x156017d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x1560181e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x156018650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x156018ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x156018f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x1560193a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x156019810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x156019c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x15601a0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x15601a560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x15601a9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x15601ae40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x15601b2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x15601b720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x15601bb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x15601c000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x15601c470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x15601c8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x15601cd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x15601d1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x15601d630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x15601daa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x15601df10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x15601e380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x15601e7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x15601ec60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x15601f0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x15601f7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x15601feb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x1560205a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x156020c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x156021100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x156021570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1560219e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x156021e50 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.945s
user	0m0.244s
sys	0m0.142s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
