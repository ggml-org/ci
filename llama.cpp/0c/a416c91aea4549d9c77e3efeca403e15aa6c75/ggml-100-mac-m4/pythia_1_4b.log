Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:298 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- ARM -mcpu not found, -mcpu=native will be used
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod - Success
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sve
-- Performing Test GGML_MACHINE_SUPPORTS_sve - Failed
-- ARM feature DOTPROD enabled
-- ARM feature MATMUL_INT8 enabled
-- ARM feature FMA enabled
-- ARM feature FP16_VECTOR_ARITHMETIC enabled
-- Adding CPU backend variant ggml-cpu: -mcpu=native+dotprod+i8mm+nosve 
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (2.3s)
-- Generating done (0.2s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m2.535s
user	0m0.861s
sys	0m1.206s
++ nproc
+ make -j10
[  0%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  1%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  3%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  3%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  3%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  3%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  5%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  6%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  6%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  6%] Built target sha1
[  6%] Built target sha256
[  7%] Linking CXX shared library libggml-base.dylib
[  7%] Built target build_info
[  7%] Built target xxhash
[  7%] Built target ggml-base
[  8%] Generate assembly for embedded Metal library
Embedding Metal library
[  8%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[ 10%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[ 10%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[ 11%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[ 12%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[ 13%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 14%] Linking CXX shared library libggml-blas.dylib
[ 14%] Linking CXX shared library libggml-cpu.dylib
[ 14%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 15%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 15%] Built target ggml-blas
[ 15%] Built target ggml-cpu
[ 15%] Linking C shared library libggml-metal.dylib
[ 15%] Built target ggml-metal
[ 16%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 16%] Linking CXX shared library libggml.dylib
[ 16%] Built target ggml
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 18%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 20%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 21%] Linking CXX executable ../../bin/llama-gguf
[ 21%] Linking CXX executable ../../bin/llama-gguf-hash
[ 22%] Linking CXX shared library libllama.dylib
[ 22%] Built target llama-gguf-hash
[ 22%] Built target llama-gguf
[ 22%] Built target llama
[ 22%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 24%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 24%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 24%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 26%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 26%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 26%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 27%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 28%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 28%] Linking C executable ../bin/test-c
[ 29%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 29%] Linking CXX executable ../../bin/llama-simple-chat
[ 29%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 29%] Linking CXX executable ../../bin/llama-simple
[ 29%] Linking CXX executable ../../bin/llama-quantize-stats
[ 31%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 31%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 31%] Linking CXX static library libcommon.a
[ 31%] Built target llava
[ 31%] Built target llama-quantize-stats
[ 31%] Built target test-c
[ 31%] Built target llama-simple
[ 31%] Built target llama-simple-chat
[ 32%] Linking CXX static library libllava_static.a
[ 32%] Linking CXX shared library libllava_shared.dylib
[ 32%] Built target common
[ 32%] Built target llava_static
[ 32%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 33%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 33%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 33%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 34%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 34%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 35%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 35%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 35%] Built target llava_shared
[ 37%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 37%] Linking CXX executable ../bin/test-tokenizer-0
[ 38%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 39%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 40%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 42%] Linking CXX executable ../bin/test-sampling
[ 42%] Linking CXX executable ../bin/test-grammar-parser
[ 43%] Linking CXX executable ../bin/test-grammar-integration
[ 43%] Linking CXX executable ../bin/test-llama-grammar
[ 44%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 45%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 45%] Built target test-tokenizer-0
[ 45%] Built target test-tokenizer-1-bpe
[ 46%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 46%] Linking CXX executable ../bin/test-log
[ 46%] Built target test-tokenizer-1-spm
[ 46%] Linking CXX executable ../bin/test-arg-parser
[ 46%] Built target test-llama-grammar
[ 46%] Built target test-grammar-parser
[ 46%] Built target test-grammar-integration
[ 46%] Built target test-sampling
[ 46%] Building CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o
[ 47%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 47%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 47%] Built target test-json-schema-to-grammar
[ 48%] Building CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o
[ 48%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 49%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 50%] Built target test-log
[ 50%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 52%] Linking CXX executable ../bin/test-chat-template
[ 52%] Linking CXX executable ../bin/test-gguf
[ 52%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 53%] Linking CXX executable ../bin/test-backend-ops
[ 53%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 55%] Linking CXX executable ../bin/test-autorelease
[ 55%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 55%] Built target test-arg-parser
[ 56%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 57%] Linking CXX executable ../bin/test-model-load-cancel
[ 57%] Linking CXX executable ../bin/test-barrier
[ 57%] Linking CXX executable ../bin/test-quantize-fns
[ 57%] Built target test-gguf
[ 58%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 58%] Built target test-chat-template
[ 58%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 58%] Built target test-backend-ops
[ 59%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 59%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 59%] Built target test-autorelease
[ 59%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 59%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 59%] Built target test-model-load-cancel
[ 59%] Built target test-barrier
[ 59%] Linking CXX executable ../bin/test-rope
[ 60%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 62%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 62%] Linking CXX executable ../../bin/llama-batched-bench
[ 62%] Built target test-quantize-fns
[ 63%] Linking CXX executable ../bin/test-quantize-perf
[ 64%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 65%] Linking CXX executable ../../bin/llama-batched
[ 66%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 67%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 67%] Linking CXX executable ../../bin/llama-embedding
[ 67%] Linking CXX executable ../../bin/llama-eval-callback
[ 67%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 67%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 67%] Linking CXX executable ../../bin/llama-gguf-split
[ 68%] Linking CXX executable ../../bin/llama-gritlm
[ 68%] Built target test-rope
[ 68%] Built target test-quantize-perf
[ 68%] Built target llama-batched-bench
[ 68%] Built target llama-batched
[ 69%] Linking CXX executable ../../bin/llama-imatrix
[ 69%] Built target llama-embedding
[ 69%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 69%] Built target llama-eval-callback
[ 69%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 70%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 70%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 70%] Built target llama-gbnf-validator
[ 70%] Built target llama-gguf-split
[ 70%] Built target llama-gritlm
[ 70%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 71%] Linking CXX executable ../../bin/llama-infill
[ 72%] Linking CXX executable ../../bin/llama-lookahead
[ 72%] Linking CXX executable ../../bin/llama-bench
[ 72%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 73%] Linking CXX executable ../../bin/llama-lookup
[ 73%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 74%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 74%] Built target llama-imatrix
[ 74%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 75%] Linking CXX executable ../../bin/llama-lookup-create
[ 76%] Linking CXX executable ../../bin/llama-lookup-merge
[ 77%] Linking CXX executable ../../bin/llama-lookup-stats
[ 77%] Linking CXX executable ../../bin/llama-cli
[ 77%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 78%] Linking CXX executable ../../bin/llama-parallel
[ 78%] Built target llama-infill
[ 78%] Built target llama-bench
[ 78%] Built target llama-lookup
[ 78%] Built target llama-lookup-create
[ 79%] Linking CXX executable ../../bin/llama-passkey
[ 79%] Built target llama-lookahead
[ 79%] Generating loading.html.hpp
[ 80%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 81%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 82%] Generating index.html.gz.hpp
[ 83%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 83%] Built target llama-lookup-stats
[ 83%] Built target llama-lookup-merge
[ 83%] Built target llama-parallel
[ 83%] Built target llama-cli
[ 83%] Linking CXX executable ../../bin/llama-quantize
[ 83%] Linking CXX executable ../../bin/llama-retrieval
[ 84%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 84%] Linking CXX executable ../../bin/llama-perplexity
[ 85%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 86%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 86%] Built target llama-passkey
[ 86%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 86%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 86%] Linking CXX executable ../../bin/llama-save-load-state
[ 86%] Linking CXX executable ../../bin/llama-run
[ 87%] Linking CXX executable ../../bin/llama-speculative
[ 88%] Linking CXX executable ../../bin/llama-tokenize
[ 89%] Linking CXX executable ../../bin/llama-speculative-simple
[ 89%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 89%] Built target llama-quantize
[ 89%] Built target llama-retrieval
[ 89%] Built target llama-perplexity
[ 90%] Linking CXX executable ../../bin/llama-tts
[ 90%] Built target llama-save-load-state
[ 91%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 91%] Built target llama-run
[ 92%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 93%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 93%] Built target llama-speculative
[ 93%] Built target llama-tokenize
[ 94%] Built target llama-speculative-simple
[ 94%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 94%] Linking CXX executable ../../bin/llama-gen-docs
[ 94%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 94%] Linking CXX executable ../../bin/llama-cvector-generator
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 95%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 95%] Linking CXX executable ../../bin/llama-export-lora
[ 95%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 95%] Built target llama-tts
[ 96%] Linking CXX executable ../../bin/llama-llava-cli
[ 97%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 97%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 98%] Linking CXX executable ../../bin/llama-vdot
[ 98%] Built target llama-convert-llama2c-to-ggml
[ 99%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 99%] Built target llama-gen-docs
[ 99%] Built target llama-cvector-generator
[ 99%] Built target llama-export-lora
[ 99%] Built target llama-llava-cli
[ 99%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Built target llama-qwen2vl-cli
[ 99%] Built target llama-minicpmv-cli
[ 99%] Built target llama-vdot
[ 99%] Built target llama-q8dot
[100%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m2.825s
user	0m5.803s
sys	0m8.690s

main: quantize time =  2943.62 ms
main:    total time =  2943.63 ms

main: quantize time =  3572.55 ms
main:    total time =  3572.55 ms

main: quantize time =  3425.11 ms
main:    total time =  3425.11 ms

main: quantize time =  3387.43 ms
main:    total time =  3387.43 ms

main: quantize time =  2305.42 ms
main:    total time =  2305.42 ms

main: quantize time =  5566.80 ms
main:    total time =  5566.80 ms

main: quantize time =  5897.64 ms
main:    total time =  5897.64 ms

main: quantize time =  6850.05 ms
main:    total time =  6850.06 ms

main: quantize time =  6078.90 ms
main:    total time =  6078.90 ms

main: quantize time =  4631.10 ms
main:    total time =  4631.10 ms
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.111 I build: 4370 (0ca416c9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.241 I main: llama backend init
0.00.000.248 I main: load the model and apply lora adapter, if any
0.00.028.953 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.040.154 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.040.173 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.040.177 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.040.178 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.040.179 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.040.179 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.040.180 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.040.183 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.040.183 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.040.184 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.040.185 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.040.186 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.040.188 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.040.190 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.040.195 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.040.195 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.040.196 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.047.200 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.049.783 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.059.596 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.059.600 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.059.600 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.059.601 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.059.601 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.059.603 I llama_model_loader: - type  f32:  194 tensors
0.00.059.603 I llama_model_loader: - type  f16:   98 tensors
0.00.093.727 I llm_load_vocab: special tokens cache size = 25
0.00.101.273 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.101.275 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.101.276 I llm_load_print_meta: arch             = gptneox
0.00.101.276 I llm_load_print_meta: vocab type       = BPE
0.00.101.276 I llm_load_print_meta: n_vocab          = 50304
0.00.101.276 I llm_load_print_meta: n_merges         = 50009
0.00.101.277 I llm_load_print_meta: vocab_only       = 0
0.00.101.277 I llm_load_print_meta: n_ctx_train      = 2048
0.00.101.277 I llm_load_print_meta: n_embd           = 2048
0.00.101.277 I llm_load_print_meta: n_layer          = 24
0.00.101.279 I llm_load_print_meta: n_head           = 16
0.00.101.280 I llm_load_print_meta: n_head_kv        = 16
0.00.101.282 I llm_load_print_meta: n_rot            = 32
0.00.101.282 I llm_load_print_meta: n_swa            = 0
0.00.101.282 I llm_load_print_meta: n_embd_head_k    = 128
0.00.101.283 I llm_load_print_meta: n_embd_head_v    = 128
0.00.101.285 I llm_load_print_meta: n_gqa            = 1
0.00.101.286 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.101.287 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.101.287 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.101.288 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.101.288 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.101.288 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.101.288 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.101.294 I llm_load_print_meta: n_ff             = 8192
0.00.101.296 I llm_load_print_meta: n_expert         = 0
0.00.101.297 I llm_load_print_meta: n_expert_used    = 0
0.00.101.297 I llm_load_print_meta: causal attn      = 1
0.00.101.297 I llm_load_print_meta: pooling type     = 0
0.00.101.297 I llm_load_print_meta: rope type        = 2
0.00.101.297 I llm_load_print_meta: rope scaling     = linear
0.00.101.298 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.101.298 I llm_load_print_meta: freq_scale_train = 1
0.00.101.298 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.101.299 I llm_load_print_meta: rope_finetuned   = unknown
0.00.101.299 I llm_load_print_meta: ssm_d_conv       = 0
0.00.101.299 I llm_load_print_meta: ssm_d_inner      = 0
0.00.101.299 I llm_load_print_meta: ssm_d_state      = 0
0.00.101.299 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.101.299 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.101.299 I llm_load_print_meta: model type       = 1.4B
0.00.101.300 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.101.300 I llm_load_print_meta: model params     = 1.41 B
0.00.101.301 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.101.301 I llm_load_print_meta: general.name     = 1.4B
0.00.101.301 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.101.302 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.101.302 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.101.302 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.101.302 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.101.303 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.101.303 I llm_load_print_meta: max token length = 1024
0.00.103.996 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.103.996 I llm_load_tensors: offloading output layer to GPU
0.00.103.997 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.104.015 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.104.017 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.105.030 I llama_new_context_with_model: n_seq_max     = 1
0.00.105.031 I llama_new_context_with_model: n_ctx         = 2048
0.00.105.031 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.105.031 I llama_new_context_with_model: n_batch       = 2048
0.00.105.032 I llama_new_context_with_model: n_ubatch      = 512
0.00.105.032 I llama_new_context_with_model: flash_attn    = 0
0.00.105.032 I llama_new_context_with_model: freq_base     = 10000.0
0.00.105.033 I llama_new_context_with_model: freq_scale    = 1
0.00.105.033 I ggml_metal_init: allocating
0.00.105.036 I ggml_metal_init: found device: Apple M4
0.00.105.038 I ggml_metal_init: picking default device: Apple M4
0.00.105.714 I ggml_metal_init: using embedded metal library
0.00.115.186 I ggml_metal_init: GPU name:   Apple M4
0.00.115.188 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.115.188 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.115.189 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.115.189 I ggml_metal_init: simdgroup reduction   = true
0.00.115.189 I ggml_metal_init: simdgroup matrix mul. = true
0.00.115.189 I ggml_metal_init: has bfloat            = true
0.00.115.189 I ggml_metal_init: use bfloat            = true
0.00.115.190 I ggml_metal_init: hasUnifiedMemory      = true
0.00.115.190 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.141.879 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.162.976 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.162.983 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.163.004 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.163.958 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.163.959 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.163.959 I llama_new_context_with_model: graph nodes  = 967
0.00.163.960 I llama_new_context_with_model: graph splits = 2
0.00.163.985 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.164.104 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.164.105 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.241.914 I main: llama threadpool init, n_threads = 4
0.00.241.950 I 
0.00.241.990 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.241.992 I 
0.00.242.064 I sampler seed: 1234
0.00.242.068 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.242.103 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.242.105 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.242.105 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.088.478 I llama_perf_sampler_print:    sampling time =       1.27 ms /    71 runs   (    0.02 ms per token, 55729.98 tokens per second)
0.02.088.478 I llama_perf_context_print:        load time =     212.95 ms
0.02.088.479 I llama_perf_context_print: prompt eval time =      43.83 ms /     7 tokens (    6.26 ms per token,   159.71 tokens per second)
0.02.088.480 I llama_perf_context_print:        eval time =    1799.60 ms /    63 runs   (   28.57 ms per token,    35.01 tokens per second)
0.02.088.481 I llama_perf_context_print:       total time =    1846.57 ms /    70 tokens
0.02.088.651 I ggml_metal_free: deallocating

real	0m2.383s
user	0m0.149s
sys	0m0.099s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.044 I build: 4370 (0ca416c9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.077 I main: llama backend init
0.00.000.080 I main: load the model and apply lora adapter, if any
0.00.010.394 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.034.328 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.034.333 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.034.335 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.034.335 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.034.337 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.034.338 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.034.338 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.034.339 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.034.339 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.034.340 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.034.340 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.034.340 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.034.340 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.034.341 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.034.343 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.034.343 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.034.343 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.038.733 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.039.914 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.044.273 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.044.275 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.044.275 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.044.275 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.044.276 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.044.276 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.044.277 I llama_model_loader: - type  f32:  194 tensors
0.00.044.277 I llama_model_loader: - type q8_0:   98 tensors
0.00.071.306 I llm_load_vocab: special tokens cache size = 25
0.00.079.814 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.079.819 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.079.819 I llm_load_print_meta: arch             = gptneox
0.00.079.820 I llm_load_print_meta: vocab type       = BPE
0.00.079.824 I llm_load_print_meta: n_vocab          = 50304
0.00.079.824 I llm_load_print_meta: n_merges         = 50009
0.00.079.824 I llm_load_print_meta: vocab_only       = 0
0.00.079.825 I llm_load_print_meta: n_ctx_train      = 2048
0.00.079.825 I llm_load_print_meta: n_embd           = 2048
0.00.079.825 I llm_load_print_meta: n_layer          = 24
0.00.079.832 I llm_load_print_meta: n_head           = 16
0.00.079.833 I llm_load_print_meta: n_head_kv        = 16
0.00.079.834 I llm_load_print_meta: n_rot            = 32
0.00.079.834 I llm_load_print_meta: n_swa            = 0
0.00.079.834 I llm_load_print_meta: n_embd_head_k    = 128
0.00.079.834 I llm_load_print_meta: n_embd_head_v    = 128
0.00.079.835 I llm_load_print_meta: n_gqa            = 1
0.00.079.837 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.079.838 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.079.838 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.079.839 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.079.839 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.079.839 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.079.839 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.079.840 I llm_load_print_meta: n_ff             = 8192
0.00.079.841 I llm_load_print_meta: n_expert         = 0
0.00.079.841 I llm_load_print_meta: n_expert_used    = 0
0.00.079.841 I llm_load_print_meta: causal attn      = 1
0.00.079.841 I llm_load_print_meta: pooling type     = 0
0.00.079.842 I llm_load_print_meta: rope type        = 2
0.00.079.842 I llm_load_print_meta: rope scaling     = linear
0.00.079.842 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.079.843 I llm_load_print_meta: freq_scale_train = 1
0.00.079.843 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.079.843 I llm_load_print_meta: rope_finetuned   = unknown
0.00.079.843 I llm_load_print_meta: ssm_d_conv       = 0
0.00.079.844 I llm_load_print_meta: ssm_d_inner      = 0
0.00.079.844 I llm_load_print_meta: ssm_d_state      = 0
0.00.079.844 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.079.844 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.079.844 I llm_load_print_meta: model type       = 1.4B
0.00.079.845 I llm_load_print_meta: model ftype      = Q8_0
0.00.079.846 I llm_load_print_meta: model params     = 1.41 B
0.00.079.846 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.079.847 I llm_load_print_meta: general.name     = 1.4B
0.00.079.847 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.079.847 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.079.848 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.079.848 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.079.848 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.079.849 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.079.849 I llm_load_print_meta: max token length = 1024
0.00.083.096 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.083.097 I llm_load_tensors: offloading output layer to GPU
0.00.083.097 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.083.109 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.083.110 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.084.517 I llama_new_context_with_model: n_seq_max     = 1
0.00.084.518 I llama_new_context_with_model: n_ctx         = 2048
0.00.084.519 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.084.519 I llama_new_context_with_model: n_batch       = 2048
0.00.084.520 I llama_new_context_with_model: n_ubatch      = 512
0.00.084.520 I llama_new_context_with_model: flash_attn    = 0
0.00.084.520 I llama_new_context_with_model: freq_base     = 10000.0
0.00.084.521 I llama_new_context_with_model: freq_scale    = 1
0.00.084.521 I ggml_metal_init: allocating
0.00.084.525 I ggml_metal_init: found device: Apple M4
0.00.084.528 I ggml_metal_init: picking default device: Apple M4
0.00.085.520 I ggml_metal_init: using embedded metal library
0.00.089.428 I ggml_metal_init: GPU name:   Apple M4
0.00.089.431 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.089.431 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.089.432 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.089.432 I ggml_metal_init: simdgroup reduction   = true
0.00.089.432 I ggml_metal_init: simdgroup matrix mul. = true
0.00.089.432 I ggml_metal_init: has bfloat            = true
0.00.089.433 I ggml_metal_init: use bfloat            = true
0.00.089.433 I ggml_metal_init: hasUnifiedMemory      = true
0.00.089.434 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.101.266 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.124.930 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.124.941 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.124.967 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.126.183 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.126.185 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.126.185 I llama_new_context_with_model: graph nodes  = 967
0.00.126.186 I llama_new_context_with_model: graph splits = 2
0.00.126.205 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.126.346 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.126.346 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.331.800 I main: llama threadpool init, n_threads = 4
0.01.331.883 I 
0.01.331.953 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.331.966 I 
0.01.332.498 I sampler seed: 1234
0.01.332.507 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.332.551 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.332.553 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.332.553 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.434.368 I llama_perf_sampler_print:    sampling time =       1.34 ms /    71 runs   (    0.02 ms per token, 52906.11 tokens per second)
0.02.434.368 I llama_perf_context_print:        load time =    1321.40 ms
0.02.434.369 I llama_perf_context_print: prompt eval time =      50.57 ms /     7 tokens (    7.22 ms per token,   138.42 tokens per second)
0.02.434.370 I llama_perf_context_print:        eval time =    1048.20 ms /    63 runs   (   16.64 ms per token,    60.10 tokens per second)
0.02.434.370 I llama_perf_context_print:       total time =    1102.57 ms /    70 tokens
0.02.434.561 I ggml_metal_free: deallocating

real	0m2.454s
user	0m0.135s
sys	0m0.249s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.039 I build: 4370 (0ca416c9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.071 I main: llama backend init
0.00.000.073 I main: load the model and apply lora adapter, if any
0.00.010.672 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.434 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.017.438 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.441 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.441 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.441 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.442 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.442 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.444 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.445 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.446 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.446 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.447 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.447 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.447 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.449 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.449 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.449 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.396 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.438 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.409 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.411 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.412 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.412 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.412 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.413 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.026.413 I llama_model_loader: - type  f32:  194 tensors
0.00.026.413 I llama_model_loader: - type q4_0:   97 tensors
0.00.026.414 I llama_model_loader: - type q6_K:    1 tensors
0.00.048.052 I llm_load_vocab: special tokens cache size = 25
0.00.054.020 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.054.023 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.054.023 I llm_load_print_meta: arch             = gptneox
0.00.054.024 I llm_load_print_meta: vocab type       = BPE
0.00.054.024 I llm_load_print_meta: n_vocab          = 50304
0.00.054.024 I llm_load_print_meta: n_merges         = 50009
0.00.054.024 I llm_load_print_meta: vocab_only       = 0
0.00.054.024 I llm_load_print_meta: n_ctx_train      = 2048
0.00.054.025 I llm_load_print_meta: n_embd           = 2048
0.00.054.025 I llm_load_print_meta: n_layer          = 24
0.00.054.028 I llm_load_print_meta: n_head           = 16
0.00.054.029 I llm_load_print_meta: n_head_kv        = 16
0.00.054.029 I llm_load_print_meta: n_rot            = 32
0.00.054.029 I llm_load_print_meta: n_swa            = 0
0.00.054.029 I llm_load_print_meta: n_embd_head_k    = 128
0.00.054.029 I llm_load_print_meta: n_embd_head_v    = 128
0.00.054.030 I llm_load_print_meta: n_gqa            = 1
0.00.054.031 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.054.032 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.054.032 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.054.035 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.054.035 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.054.035 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.054.035 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.054.036 I llm_load_print_meta: n_ff             = 8192
0.00.054.036 I llm_load_print_meta: n_expert         = 0
0.00.054.036 I llm_load_print_meta: n_expert_used    = 0
0.00.054.036 I llm_load_print_meta: causal attn      = 1
0.00.054.037 I llm_load_print_meta: pooling type     = 0
0.00.054.037 I llm_load_print_meta: rope type        = 2
0.00.054.039 I llm_load_print_meta: rope scaling     = linear
0.00.054.039 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.054.040 I llm_load_print_meta: freq_scale_train = 1
0.00.054.040 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.054.040 I llm_load_print_meta: rope_finetuned   = unknown
0.00.054.040 I llm_load_print_meta: ssm_d_conv       = 0
0.00.054.040 I llm_load_print_meta: ssm_d_inner      = 0
0.00.054.040 I llm_load_print_meta: ssm_d_state      = 0
0.00.054.040 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.054.041 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.054.041 I llm_load_print_meta: model type       = 1.4B
0.00.054.041 I llm_load_print_meta: model ftype      = Q4_0
0.00.054.041 I llm_load_print_meta: model params     = 1.41 B
0.00.054.042 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.054.042 I llm_load_print_meta: general.name     = 1.4B
0.00.054.042 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.054.042 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.054.043 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.054.047 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.054.047 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.054.047 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.054.047 I llm_load_print_meta: max token length = 1024
0.00.056.432 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.056.433 I llm_load_tensors: offloading output layer to GPU
0.00.056.433 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.056.444 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.056.446 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.057.449 I llama_new_context_with_model: n_seq_max     = 1
0.00.057.450 I llama_new_context_with_model: n_ctx         = 2048
0.00.057.450 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.057.450 I llama_new_context_with_model: n_batch       = 2048
0.00.057.450 I llama_new_context_with_model: n_ubatch      = 512
0.00.057.451 I llama_new_context_with_model: flash_attn    = 0
0.00.057.451 I llama_new_context_with_model: freq_base     = 10000.0
0.00.057.451 I llama_new_context_with_model: freq_scale    = 1
0.00.057.452 I ggml_metal_init: allocating
0.00.057.455 I ggml_metal_init: found device: Apple M4
0.00.057.458 I ggml_metal_init: picking default device: Apple M4
0.00.058.184 I ggml_metal_init: using embedded metal library
0.00.060.719 I ggml_metal_init: GPU name:   Apple M4
0.00.060.720 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.060.721 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.060.721 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.060.721 I ggml_metal_init: simdgroup reduction   = true
0.00.060.721 I ggml_metal_init: simdgroup matrix mul. = true
0.00.060.722 I ggml_metal_init: has bfloat            = true
0.00.060.722 I ggml_metal_init: use bfloat            = true
0.00.060.722 I ggml_metal_init: hasUnifiedMemory      = true
0.00.060.723 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.070.395 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.095.083 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.095.091 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.095.118 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.096.278 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.096.280 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.096.281 I llama_new_context_with_model: graph nodes  = 967
0.00.096.281 I llama_new_context_with_model: graph splits = 2
0.00.096.300 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.096.441 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.096.442 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.679.340 I main: llama threadpool init, n_threads = 4
0.00.679.382 I 
0.00.679.408 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.679.408 I 
0.00.679.654 I sampler seed: 1234
0.00.679.659 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.679.702 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.679.707 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.679.707 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.364.878 I llama_perf_sampler_print:    sampling time =       1.21 ms /    71 runs   (    0.02 ms per token, 58677.69 tokens per second)
0.01.364.879 I llama_perf_context_print:        load time =     668.66 ms
0.01.364.879 I llama_perf_context_print: prompt eval time =      43.71 ms /     7 tokens (    6.24 ms per token,   160.16 tokens per second)
0.01.364.880 I llama_perf_context_print:        eval time =     638.44 ms /    63 runs   (   10.13 ms per token,    98.68 tokens per second)
0.01.364.881 I llama_perf_context_print:       total time =     685.54 ms /    70 tokens
0.01.365.100 I ggml_metal_free: deallocating

real	0m1.383s
user	0m0.111s
sys	0m0.148s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.034 I build: 4370 (0ca416c9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.062 I main: llama backend init
0.00.000.064 I main: load the model and apply lora adapter, if any
0.00.008.639 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.224 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.015.228 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.229 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.230 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.230 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.231 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.231 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.232 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.232 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.232 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.233 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.233 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.233 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.234 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.237 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.237 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.237 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.190 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.270 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.128 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.129 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.129 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.129 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.130 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.130 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.024.130 I llama_model_loader: - type  f32:  194 tensors
0.00.024.131 I llama_model_loader: - type q4_1:   97 tensors
0.00.024.131 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.499 I llm_load_vocab: special tokens cache size = 25
0.00.050.302 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.305 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.305 I llm_load_print_meta: arch             = gptneox
0.00.050.306 I llm_load_print_meta: vocab type       = BPE
0.00.050.306 I llm_load_print_meta: n_vocab          = 50304
0.00.050.306 I llm_load_print_meta: n_merges         = 50009
0.00.050.306 I llm_load_print_meta: vocab_only       = 0
0.00.050.306 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.307 I llm_load_print_meta: n_embd           = 2048
0.00.050.307 I llm_load_print_meta: n_layer          = 24
0.00.050.310 I llm_load_print_meta: n_head           = 16
0.00.050.311 I llm_load_print_meta: n_head_kv        = 16
0.00.050.311 I llm_load_print_meta: n_rot            = 32
0.00.050.311 I llm_load_print_meta: n_swa            = 0
0.00.050.311 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.312 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.312 I llm_load_print_meta: n_gqa            = 1
0.00.050.313 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.314 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.317 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.317 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.318 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.318 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.318 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.320 I llm_load_print_meta: n_ff             = 8192
0.00.050.320 I llm_load_print_meta: n_expert         = 0
0.00.050.320 I llm_load_print_meta: n_expert_used    = 0
0.00.050.322 I llm_load_print_meta: causal attn      = 1
0.00.050.322 I llm_load_print_meta: pooling type     = 0
0.00.050.322 I llm_load_print_meta: rope type        = 2
0.00.050.322 I llm_load_print_meta: rope scaling     = linear
0.00.050.323 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.323 I llm_load_print_meta: freq_scale_train = 1
0.00.050.323 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.325 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.325 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.325 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.325 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.325 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.325 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.326 I llm_load_print_meta: model type       = 1.4B
0.00.050.326 I llm_load_print_meta: model ftype      = Q4_1
0.00.050.330 I llm_load_print_meta: model params     = 1.41 B
0.00.050.330 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.050.331 I llm_load_print_meta: general.name     = 1.4B
0.00.050.331 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.331 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.331 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.331 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.332 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.332 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.332 I llm_load_print_meta: max token length = 1024
0.00.052.273 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.274 I llm_load_tensors: offloading output layer to GPU
0.00.052.274 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.284 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.052.286 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.053.179 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.179 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.180 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.180 I llama_new_context_with_model: n_batch       = 2048
0.00.053.180 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.180 I llama_new_context_with_model: flash_attn    = 0
0.00.053.181 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.181 I llama_new_context_with_model: freq_scale    = 1
0.00.053.181 I ggml_metal_init: allocating
0.00.053.184 I ggml_metal_init: found device: Apple M4
0.00.053.186 I ggml_metal_init: picking default device: Apple M4
0.00.053.808 I ggml_metal_init: using embedded metal library
0.00.056.140 I ggml_metal_init: GPU name:   Apple M4
0.00.056.141 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.141 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.142 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.142 I ggml_metal_init: simdgroup reduction   = true
0.00.056.142 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.142 I ggml_metal_init: has bfloat            = true
0.00.056.143 I ggml_metal_init: use bfloat            = true
0.00.056.144 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.145 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.923 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.085.526 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.534 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.557 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.519 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.520 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.520 I llama_new_context_with_model: graph nodes  = 967
0.00.086.521 I llama_new_context_with_model: graph splits = 2
0.00.086.537 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.086.666 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.086.667 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.742.238 I main: llama threadpool init, n_threads = 4
0.00.742.281 I 
0.00.742.326 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.742.327 I 
0.00.742.563 I sampler seed: 1234
0.00.742.567 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.742.613 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.742.617 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.742.617 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.467.753 I llama_perf_sampler_print:    sampling time =       1.09 ms /    71 runs   (    0.02 ms per token, 65257.35 tokens per second)
0.01.467.753 I llama_perf_context_print:        load time =     733.59 ms
0.01.467.754 I llama_perf_context_print: prompt eval time =      43.56 ms /     7 tokens (    6.22 ms per token,   160.69 tokens per second)
0.01.467.755 I llama_perf_context_print:        eval time =     678.77 ms /    63 runs   (   10.77 ms per token,    92.82 tokens per second)
0.01.467.755 I llama_perf_context_print:       total time =     725.52 ms /    70 tokens
0.01.467.966 I ggml_metal_free: deallocating

real	0m1.485s
user	0m0.110s
sys	0m0.159s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.036 I build: 4370 (0ca416c9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.067 I main: llama backend init
0.00.000.069 I main: load the model and apply lora adapter, if any
0.00.009.962 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.377 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.016.382 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.383 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.384 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.384 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.384 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.385 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.385 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.386 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.386 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.386 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.387 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.387 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.387 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.389 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.389 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.389 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.453 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.577 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.571 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.573 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.573 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.573 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.573 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.574 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.025.574 I llama_model_loader: - type  f32:  194 tensors
0.00.025.575 I llama_model_loader: - type q5_0:   97 tensors
0.00.025.575 I llama_model_loader: - type q6_K:    1 tensors
0.00.046.647 I llm_load_vocab: special tokens cache size = 25
0.00.052.668 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.671 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.671 I llm_load_print_meta: arch             = gptneox
0.00.052.671 I llm_load_print_meta: vocab type       = BPE
0.00.052.671 I llm_load_print_meta: n_vocab          = 50304
0.00.052.672 I llm_load_print_meta: n_merges         = 50009
0.00.052.672 I llm_load_print_meta: vocab_only       = 0
0.00.052.672 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.672 I llm_load_print_meta: n_embd           = 2048
0.00.052.672 I llm_load_print_meta: n_layer          = 24
0.00.052.675 I llm_load_print_meta: n_head           = 16
0.00.052.676 I llm_load_print_meta: n_head_kv        = 16
0.00.052.676 I llm_load_print_meta: n_rot            = 32
0.00.052.676 I llm_load_print_meta: n_swa            = 0
0.00.052.676 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.677 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.677 I llm_load_print_meta: n_gqa            = 1
0.00.052.678 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.679 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.679 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.680 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.680 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.680 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.680 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.681 I llm_load_print_meta: n_ff             = 8192
0.00.052.681 I llm_load_print_meta: n_expert         = 0
0.00.052.681 I llm_load_print_meta: n_expert_used    = 0
0.00.052.683 I llm_load_print_meta: causal attn      = 1
0.00.052.684 I llm_load_print_meta: pooling type     = 0
0.00.052.684 I llm_load_print_meta: rope type        = 2
0.00.052.684 I llm_load_print_meta: rope scaling     = linear
0.00.052.684 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.685 I llm_load_print_meta: freq_scale_train = 1
0.00.052.685 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.685 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.685 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.685 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.687 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.687 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.687 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.688 I llm_load_print_meta: model type       = 1.4B
0.00.052.688 I llm_load_print_meta: model ftype      = Q5_0
0.00.052.688 I llm_load_print_meta: model params     = 1.41 B
0.00.052.689 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.052.689 I llm_load_print_meta: general.name     = 1.4B
0.00.052.689 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.689 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.689 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.690 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.690 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.052.690 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.692 I llm_load_print_meta: max token length = 1024
0.00.054.798 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.799 I llm_load_tensors: offloading output layer to GPU
0.00.054.799 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.810 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.054.811 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.055.747 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.748 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.748 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.749 I llama_new_context_with_model: n_batch       = 2048
0.00.055.749 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.749 I llama_new_context_with_model: flash_attn    = 0
0.00.055.749 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.750 I llama_new_context_with_model: freq_scale    = 1
0.00.055.750 I ggml_metal_init: allocating
0.00.055.754 I ggml_metal_init: found device: Apple M4
0.00.055.756 I ggml_metal_init: picking default device: Apple M4
0.00.056.380 I ggml_metal_init: using embedded metal library
0.00.058.705 I ggml_metal_init: GPU name:   Apple M4
0.00.058.706 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.707 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.707 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.707 I ggml_metal_init: simdgroup reduction   = true
0.00.058.707 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.708 I ggml_metal_init: has bfloat            = true
0.00.058.708 I ggml_metal_init: use bfloat            = true
0.00.058.708 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.710 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.658 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.088.901 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.088.906 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.088.924 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.089.978 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.089.980 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.089.980 I llama_new_context_with_model: graph nodes  = 967
0.00.089.981 I llama_new_context_with_model: graph splits = 2
0.00.089.996 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.090.140 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.090.140 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.754.105 I main: llama threadpool init, n_threads = 4
0.00.754.145 I 
0.00.754.177 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.754.179 I 
0.00.754.395 I sampler seed: 1234
0.00.754.399 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.754.443 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.754.462 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.754.462 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.543.516 I llama_perf_sampler_print:    sampling time =       1.23 ms /    71 runs   (    0.02 ms per token, 57583.13 tokens per second)
0.01.543.517 I llama_perf_context_print:        load time =     744.14 ms
0.01.543.517 I llama_perf_context_print: prompt eval time =      43.11 ms /     7 tokens (    6.16 ms per token,   162.39 tokens per second)
0.01.543.518 I llama_perf_context_print:        eval time =     742.93 ms /    63 runs   (   11.79 ms per token,    84.80 tokens per second)
0.01.543.519 I llama_perf_context_print:       total time =     789.41 ms /    70 tokens
0.01.543.706 I ggml_metal_free: deallocating

real	0m1.562s
user	0m0.111s
sys	0m0.163s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.033 I build: 4370 (0ca416c9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.061 I main: llama backend init
0.00.000.063 I main: load the model and apply lora adapter, if any
0.00.008.592 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.237 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.015.241 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.248 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.249 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.249 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.251 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.251 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.252 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.252 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.253 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.253 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.256 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.257 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.257 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.259 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.259 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.260 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.194 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.235 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.073 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.074 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.075 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.075 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.075 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.076 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.076 I llama_model_loader: - type  f32:  194 tensors
0.00.024.076 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.077 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.513 I llm_load_vocab: special tokens cache size = 25
0.00.050.366 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.368 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.369 I llm_load_print_meta: arch             = gptneox
0.00.050.369 I llm_load_print_meta: vocab type       = BPE
0.00.050.369 I llm_load_print_meta: n_vocab          = 50304
0.00.050.369 I llm_load_print_meta: n_merges         = 50009
0.00.050.370 I llm_load_print_meta: vocab_only       = 0
0.00.050.370 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.370 I llm_load_print_meta: n_embd           = 2048
0.00.050.370 I llm_load_print_meta: n_layer          = 24
0.00.050.373 I llm_load_print_meta: n_head           = 16
0.00.050.374 I llm_load_print_meta: n_head_kv        = 16
0.00.050.374 I llm_load_print_meta: n_rot            = 32
0.00.050.374 I llm_load_print_meta: n_swa            = 0
0.00.050.374 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.374 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.375 I llm_load_print_meta: n_gqa            = 1
0.00.050.376 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.378 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.378 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.378 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.379 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.379 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.380 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.381 I llm_load_print_meta: n_ff             = 8192
0.00.050.381 I llm_load_print_meta: n_expert         = 0
0.00.050.381 I llm_load_print_meta: n_expert_used    = 0
0.00.050.382 I llm_load_print_meta: causal attn      = 1
0.00.050.382 I llm_load_print_meta: pooling type     = 0
0.00.050.382 I llm_load_print_meta: rope type        = 2
0.00.050.382 I llm_load_print_meta: rope scaling     = linear
0.00.050.383 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.383 I llm_load_print_meta: freq_scale_train = 1
0.00.050.383 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.383 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.384 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.384 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.384 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.384 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.384 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.384 I llm_load_print_meta: model type       = 1.4B
0.00.050.385 I llm_load_print_meta: model ftype      = Q5_1
0.00.050.385 I llm_load_print_meta: model params     = 1.41 B
0.00.050.386 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.050.386 I llm_load_print_meta: general.name     = 1.4B
0.00.050.388 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.388 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.388 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.388 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.388 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.389 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.389 I llm_load_print_meta: max token length = 1024
0.00.052.395 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.395 I llm_load_tensors: offloading output layer to GPU
0.00.052.395 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.406 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.052.407 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.053.306 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.306 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.307 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.307 I llama_new_context_with_model: n_batch       = 2048
0.00.053.307 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.307 I llama_new_context_with_model: flash_attn    = 0
0.00.053.308 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.308 I llama_new_context_with_model: freq_scale    = 1
0.00.053.308 I ggml_metal_init: allocating
0.00.053.311 I ggml_metal_init: found device: Apple M4
0.00.053.313 I ggml_metal_init: picking default device: Apple M4
0.00.053.921 I ggml_metal_init: using embedded metal library
0.00.056.267 I ggml_metal_init: GPU name:   Apple M4
0.00.056.269 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.269 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.269 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.270 I ggml_metal_init: simdgroup reduction   = true
0.00.056.270 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.270 I ggml_metal_init: has bfloat            = true
0.00.056.270 I ggml_metal_init: use bfloat            = true
0.00.056.271 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.271 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.021 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.084.659 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.665 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.684 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.085.715 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.085.716 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.085.717 I llama_new_context_with_model: graph nodes  = 967
0.00.085.717 I llama_new_context_with_model: graph splits = 2
0.00.085.733 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.085.860 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.085.861 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.717.191 I main: llama threadpool init, n_threads = 4
0.00.717.228 I 
0.00.717.279 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.717.280 I 
0.00.717.515 I sampler seed: 1234
0.00.717.519 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.717.535 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.717.535 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.717.535 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.559.118 I llama_perf_sampler_print:    sampling time =       1.23 ms /    71 runs   (    0.02 ms per token, 57536.47 tokens per second)
0.01.559.119 I llama_perf_context_print:        load time =     708.59 ms
0.01.559.120 I llama_perf_context_print: prompt eval time =      42.32 ms /     7 tokens (    6.05 ms per token,   165.40 tokens per second)
0.01.559.120 I llama_perf_context_print:        eval time =     796.28 ms /    63 runs   (   12.64 ms per token,    79.12 tokens per second)
0.01.559.121 I llama_perf_context_print:       total time =     841.93 ms /    70 tokens
0.01.559.310 I ggml_metal_free: deallocating

real	0m1.576s
user	0m0.110s
sys	0m0.165s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.038 I build: 4370 (0ca416c9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.069 I main: llama backend init
0.00.000.071 I main: load the model and apply lora adapter, if any
0.00.010.055 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.557 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.562 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.563 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.564 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.564 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.565 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.565 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.566 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.566 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.567 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.569 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.569 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.570 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.570 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.572 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.572 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.573 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.562 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.668 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.631 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.632 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.633 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.633 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.633 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.634 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.634 I llama_model_loader: - type  f32:  194 tensors
0.00.024.634 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.635 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.635 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.760 I llm_load_vocab: special tokens cache size = 25
0.00.051.583 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.586 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.586 I llm_load_print_meta: arch             = gptneox
0.00.051.586 I llm_load_print_meta: vocab type       = BPE
0.00.051.587 I llm_load_print_meta: n_vocab          = 50304
0.00.051.587 I llm_load_print_meta: n_merges         = 50009
0.00.051.587 I llm_load_print_meta: vocab_only       = 0
0.00.051.587 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.587 I llm_load_print_meta: n_embd           = 2048
0.00.051.588 I llm_load_print_meta: n_layer          = 24
0.00.051.590 I llm_load_print_meta: n_head           = 16
0.00.051.591 I llm_load_print_meta: n_head_kv        = 16
0.00.051.591 I llm_load_print_meta: n_rot            = 32
0.00.051.591 I llm_load_print_meta: n_swa            = 0
0.00.051.591 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.592 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.592 I llm_load_print_meta: n_gqa            = 1
0.00.051.593 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.594 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.594 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.595 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.595 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.595 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.595 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.596 I llm_load_print_meta: n_ff             = 8192
0.00.051.596 I llm_load_print_meta: n_expert         = 0
0.00.051.596 I llm_load_print_meta: n_expert_used    = 0
0.00.051.596 I llm_load_print_meta: causal attn      = 1
0.00.051.597 I llm_load_print_meta: pooling type     = 0
0.00.051.597 I llm_load_print_meta: rope type        = 2
0.00.051.599 I llm_load_print_meta: rope scaling     = linear
0.00.051.600 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.600 I llm_load_print_meta: freq_scale_train = 1
0.00.051.600 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.600 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.600 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.600 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.601 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.601 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.601 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.601 I llm_load_print_meta: model type       = 1.4B
0.00.051.602 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.051.602 I llm_load_print_meta: model params     = 1.41 B
0.00.051.603 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.051.603 I llm_load_print_meta: general.name     = 1.4B
0.00.051.603 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.603 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.604 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.604 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.604 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.604 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.605 I llm_load_print_meta: max token length = 1024
0.00.053.517 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.518 I llm_load_tensors: offloading output layer to GPU
0.00.053.518 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.528 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.053.529 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.054.435 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.436 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.436 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.436 I llama_new_context_with_model: n_batch       = 2048
0.00.054.437 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.437 I llama_new_context_with_model: flash_attn    = 0
0.00.054.437 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.438 I llama_new_context_with_model: freq_scale    = 1
0.00.054.438 I ggml_metal_init: allocating
0.00.054.445 I ggml_metal_init: found device: Apple M4
0.00.054.448 I ggml_metal_init: picking default device: Apple M4
0.00.055.038 I ggml_metal_init: using embedded metal library
0.00.057.395 I ggml_metal_init: GPU name:   Apple M4
0.00.057.397 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.397 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.397 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.398 I ggml_metal_init: simdgroup reduction   = true
0.00.057.398 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.398 I ggml_metal_init: has bfloat            = true
0.00.057.398 I ggml_metal_init: use bfloat            = true
0.00.057.398 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.399 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.129 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.086.165 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.170 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.187 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.186 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.187 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.188 I llama_new_context_with_model: graph nodes  = 967
0.00.087.188 I llama_new_context_with_model: graph splits = 2
0.00.087.204 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.087.352 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.353 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.441.412 I main: llama threadpool init, n_threads = 4
0.00.441.454 I 
0.00.441.494 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.441.496 I 
0.00.441.734 I sampler seed: 1234
0.00.441.738 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.441.777 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.441.779 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.441.779 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.117.618 I llama_perf_sampler_print:    sampling time =       1.27 ms /    71 runs   (    0.02 ms per token, 55729.98 tokens per second)
0.01.117.619 I llama_perf_context_print:        load time =     431.35 ms
0.01.117.619 I llama_perf_context_print: prompt eval time =      35.86 ms /     7 tokens (    5.12 ms per token,   195.21 tokens per second)
0.01.117.620 I llama_perf_context_print:        eval time =     637.36 ms /    63 runs   (   10.12 ms per token,    98.85 tokens per second)
0.01.117.620 I llama_perf_context_print:       total time =     676.21 ms /    70 tokens
0.01.117.841 I ggml_metal_free: deallocating

real	0m1.135s
user	0m0.111s
sys	0m0.110s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.037 I build: 4370 (0ca416c9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.065 I main: llama backend init
0.00.000.067 I main: load the model and apply lora adapter, if any
0.00.009.114 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.778 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.015.784 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.785 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.786 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.786 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.788 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.789 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.789 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.790 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.790 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.790 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.792 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.793 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.793 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.797 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.797 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.797 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.663 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.732 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.723 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.724 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.724 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.725 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.725 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.725 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.726 I llama_model_loader: - type  f32:  194 tensors
0.00.024.726 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.727 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.727 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.727 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.811 I llm_load_vocab: special tokens cache size = 25
0.00.052.011 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.015 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.015 I llm_load_print_meta: arch             = gptneox
0.00.052.017 I llm_load_print_meta: vocab type       = BPE
0.00.052.017 I llm_load_print_meta: n_vocab          = 50304
0.00.052.018 I llm_load_print_meta: n_merges         = 50009
0.00.052.018 I llm_load_print_meta: vocab_only       = 0
0.00.052.018 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.018 I llm_load_print_meta: n_embd           = 2048
0.00.052.018 I llm_load_print_meta: n_layer          = 24
0.00.052.022 I llm_load_print_meta: n_head           = 16
0.00.052.022 I llm_load_print_meta: n_head_kv        = 16
0.00.052.022 I llm_load_print_meta: n_rot            = 32
0.00.052.024 I llm_load_print_meta: n_swa            = 0
0.00.052.024 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.024 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.025 I llm_load_print_meta: n_gqa            = 1
0.00.052.025 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.028 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.030 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.030 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.030 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.030 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.031 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.031 I llm_load_print_meta: n_ff             = 8192
0.00.052.031 I llm_load_print_meta: n_expert         = 0
0.00.052.031 I llm_load_print_meta: n_expert_used    = 0
0.00.052.032 I llm_load_print_meta: causal attn      = 1
0.00.052.032 I llm_load_print_meta: pooling type     = 0
0.00.052.032 I llm_load_print_meta: rope type        = 2
0.00.052.032 I llm_load_print_meta: rope scaling     = linear
0.00.052.032 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.033 I llm_load_print_meta: freq_scale_train = 1
0.00.052.033 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.033 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.033 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.033 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.033 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.033 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.033 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.034 I llm_load_print_meta: model type       = 1.4B
0.00.052.034 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.052.034 I llm_load_print_meta: model params     = 1.41 B
0.00.052.035 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.052.035 I llm_load_print_meta: general.name     = 1.4B
0.00.052.035 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.035 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.036 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.036 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.036 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.052.036 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.036 I llm_load_print_meta: max token length = 1024
0.00.053.924 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.925 I llm_load_tensors: offloading output layer to GPU
0.00.053.926 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.936 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.053.937 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.054.834 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.835 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.835 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.835 I llama_new_context_with_model: n_batch       = 2048
0.00.054.836 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.836 I llama_new_context_with_model: flash_attn    = 0
0.00.054.838 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.838 I llama_new_context_with_model: freq_scale    = 1
0.00.054.838 I ggml_metal_init: allocating
0.00.054.841 I ggml_metal_init: found device: Apple M4
0.00.054.843 I ggml_metal_init: picking default device: Apple M4
0.00.055.475 I ggml_metal_init: using embedded metal library
0.00.058.206 I ggml_metal_init: GPU name:   Apple M4
0.00.058.207 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.208 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.208 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.209 I ggml_metal_init: simdgroup reduction   = true
0.00.058.209 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.209 I ggml_metal_init: has bfloat            = true
0.00.058.209 I ggml_metal_init: use bfloat            = true
0.00.058.209 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.210 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.335 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.087.771 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.087.787 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.087.813 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.088.807 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.088.808 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.088.809 I llama_new_context_with_model: graph nodes  = 967
0.00.088.809 I llama_new_context_with_model: graph splits = 2
0.00.088.823 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.088.963 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.088.964 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.520.383 I main: llama threadpool init, n_threads = 4
0.00.520.421 I 
0.00.520.452 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.520.453 I 
0.00.520.680 I sampler seed: 1234
0.00.520.685 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.520.724 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.520.728 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.520.728 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.269.643 I llama_perf_sampler_print:    sampling time =       1.45 ms /    71 runs   (    0.02 ms per token, 49100.97 tokens per second)
0.01.269.644 I llama_perf_context_print:        load time =     511.26 ms
0.01.269.645 I llama_perf_context_print: prompt eval time =      40.38 ms /     7 tokens (    5.77 ms per token,   173.35 tokens per second)
0.01.269.646 I llama_perf_context_print:        eval time =     705.53 ms /    63 runs   (   11.20 ms per token,    89.29 tokens per second)
0.01.269.646 I llama_perf_context_print:       total time =     749.26 ms /    70 tokens
0.01.269.827 I ggml_metal_free: deallocating

real	0m1.288s
user	0m0.110s
sys	0m0.123s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.033 I build: 4370 (0ca416c9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.063 I main: llama backend init
0.00.000.065 I main: load the model and apply lora adapter, if any
0.00.011.361 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.019.796 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.019.800 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.802 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.802 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.803 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.807 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.808 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.809 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.809 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.809 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.810 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.812 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.812 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.813 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.817 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.817 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.819 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.768 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.820 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.028.736 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.028.738 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.028.738 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.028.738 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.028.738 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.028.739 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.028.739 I llama_model_loader: - type  f32:  194 tensors
0.00.028.740 I llama_model_loader: - type q4_K:   61 tensors
0.00.028.740 I llama_model_loader: - type q5_K:   24 tensors
0.00.028.740 I llama_model_loader: - type q6_K:   13 tensors
0.00.049.211 I llm_load_vocab: special tokens cache size = 25
0.00.055.152 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.055.155 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.055.155 I llm_load_print_meta: arch             = gptneox
0.00.055.156 I llm_load_print_meta: vocab type       = BPE
0.00.055.156 I llm_load_print_meta: n_vocab          = 50304
0.00.055.156 I llm_load_print_meta: n_merges         = 50009
0.00.055.156 I llm_load_print_meta: vocab_only       = 0
0.00.055.157 I llm_load_print_meta: n_ctx_train      = 2048
0.00.055.157 I llm_load_print_meta: n_embd           = 2048
0.00.055.157 I llm_load_print_meta: n_layer          = 24
0.00.055.160 I llm_load_print_meta: n_head           = 16
0.00.055.161 I llm_load_print_meta: n_head_kv        = 16
0.00.055.161 I llm_load_print_meta: n_rot            = 32
0.00.055.161 I llm_load_print_meta: n_swa            = 0
0.00.055.161 I llm_load_print_meta: n_embd_head_k    = 128
0.00.055.161 I llm_load_print_meta: n_embd_head_v    = 128
0.00.055.162 I llm_load_print_meta: n_gqa            = 1
0.00.055.163 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.055.164 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.055.165 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.055.166 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.055.166 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.055.168 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.055.168 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.055.169 I llm_load_print_meta: n_ff             = 8192
0.00.055.169 I llm_load_print_meta: n_expert         = 0
0.00.055.170 I llm_load_print_meta: n_expert_used    = 0
0.00.055.170 I llm_load_print_meta: causal attn      = 1
0.00.055.170 I llm_load_print_meta: pooling type     = 0
0.00.055.170 I llm_load_print_meta: rope type        = 2
0.00.055.170 I llm_load_print_meta: rope scaling     = linear
0.00.055.171 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.055.171 I llm_load_print_meta: freq_scale_train = 1
0.00.055.171 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.055.171 I llm_load_print_meta: rope_finetuned   = unknown
0.00.055.171 I llm_load_print_meta: ssm_d_conv       = 0
0.00.055.171 I llm_load_print_meta: ssm_d_inner      = 0
0.00.055.172 I llm_load_print_meta: ssm_d_state      = 0
0.00.055.172 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.055.172 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.055.172 I llm_load_print_meta: model type       = 1.4B
0.00.055.173 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.055.173 I llm_load_print_meta: model params     = 1.41 B
0.00.055.174 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.055.174 I llm_load_print_meta: general.name     = 1.4B
0.00.055.174 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.055.174 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.055.175 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.055.175 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.055.175 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.055.177 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.055.177 I llm_load_print_meta: max token length = 1024
0.00.057.181 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.057.181 I llm_load_tensors: offloading output layer to GPU
0.00.057.182 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.057.192 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.057.193 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.058.134 I llama_new_context_with_model: n_seq_max     = 1
0.00.058.135 I llama_new_context_with_model: n_ctx         = 2048
0.00.058.135 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.058.136 I llama_new_context_with_model: n_batch       = 2048
0.00.058.136 I llama_new_context_with_model: n_ubatch      = 512
0.00.058.136 I llama_new_context_with_model: flash_attn    = 0
0.00.058.136 I llama_new_context_with_model: freq_base     = 10000.0
0.00.058.137 I llama_new_context_with_model: freq_scale    = 1
0.00.058.137 I ggml_metal_init: allocating
0.00.058.142 I ggml_metal_init: found device: Apple M4
0.00.058.144 I ggml_metal_init: picking default device: Apple M4
0.00.058.741 I ggml_metal_init: using embedded metal library
0.00.061.086 I ggml_metal_init: GPU name:   Apple M4
0.00.061.087 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.061.087 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.061.088 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.061.088 I ggml_metal_init: simdgroup reduction   = true
0.00.061.088 I ggml_metal_init: simdgroup matrix mul. = true
0.00.061.088 I ggml_metal_init: has bfloat            = true
0.00.061.088 I ggml_metal_init: use bfloat            = true
0.00.061.089 I ggml_metal_init: hasUnifiedMemory      = true
0.00.061.089 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.070.874 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.092.717 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.092.724 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.092.746 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.093.845 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.093.846 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.093.847 I llama_new_context_with_model: graph nodes  = 967
0.00.093.847 I llama_new_context_with_model: graph splits = 2
0.00.093.864 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.094.011 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.094.011 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.624.864 I main: llama threadpool init, n_threads = 4
0.00.624.907 I 
0.00.624.940 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.624.941 I 
0.00.625.161 I sampler seed: 1234
0.00.625.166 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.625.221 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.625.225 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.625.226 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.395.107 I llama_perf_sampler_print:    sampling time =       1.26 ms /    71 runs   (    0.02 ms per token, 56304.52 tokens per second)
0.01.395.108 I llama_perf_context_print:        load time =     613.50 ms
0.01.395.109 I llama_perf_context_print: prompt eval time =      49.67 ms /     7 tokens (    7.10 ms per token,   140.94 tokens per second)
0.01.395.109 I llama_perf_context_print:        eval time =     717.18 ms /    63 runs   (   11.38 ms per token,    87.84 tokens per second)
0.01.395.111 I llama_perf_context_print:       total time =     770.25 ms /    70 tokens
0.01.395.281 I ggml_metal_free: deallocating

real	0m1.416s
user	0m0.111s
sys	0m0.143s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.035 I build: 4370 (0ca416c9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.065 I main: llama backend init
0.00.000.067 I main: load the model and apply lora adapter, if any
0.00.008.623 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.312 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.017.316 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.318 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.318 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.319 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.319 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.320 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.320 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.322 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.323 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.323 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.323 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.324 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.325 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.328 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.328 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.329 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.264 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.361 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.283 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.284 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.284 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.285 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.285 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.285 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.026.286 I llama_model_loader: - type  f32:  194 tensors
0.00.026.286 I llama_model_loader: - type q5_K:   61 tensors
0.00.026.287 I llama_model_loader: - type q6_K:   37 tensors
0.00.046.747 I llm_load_vocab: special tokens cache size = 25
0.00.052.658 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.661 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.662 I llm_load_print_meta: arch             = gptneox
0.00.052.662 I llm_load_print_meta: vocab type       = BPE
0.00.052.662 I llm_load_print_meta: n_vocab          = 50304
0.00.052.662 I llm_load_print_meta: n_merges         = 50009
0.00.052.663 I llm_load_print_meta: vocab_only       = 0
0.00.052.663 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.663 I llm_load_print_meta: n_embd           = 2048
0.00.052.663 I llm_load_print_meta: n_layer          = 24
0.00.052.666 I llm_load_print_meta: n_head           = 16
0.00.052.666 I llm_load_print_meta: n_head_kv        = 16
0.00.052.667 I llm_load_print_meta: n_rot            = 32
0.00.052.667 I llm_load_print_meta: n_swa            = 0
0.00.052.667 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.669 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.670 I llm_load_print_meta: n_gqa            = 1
0.00.052.671 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.672 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.672 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.672 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.673 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.673 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.673 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.674 I llm_load_print_meta: n_ff             = 8192
0.00.052.674 I llm_load_print_meta: n_expert         = 0
0.00.052.674 I llm_load_print_meta: n_expert_used    = 0
0.00.052.676 I llm_load_print_meta: causal attn      = 1
0.00.052.677 I llm_load_print_meta: pooling type     = 0
0.00.052.678 I llm_load_print_meta: rope type        = 2
0.00.052.678 I llm_load_print_meta: rope scaling     = linear
0.00.052.678 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.678 I llm_load_print_meta: freq_scale_train = 1
0.00.052.679 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.679 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.679 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.679 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.679 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.679 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.679 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.680 I llm_load_print_meta: model type       = 1.4B
0.00.052.684 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.052.684 I llm_load_print_meta: model params     = 1.41 B
0.00.052.685 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.052.685 I llm_load_print_meta: general.name     = 1.4B
0.00.052.686 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.686 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.686 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.686 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.686 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.052.687 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.687 I llm_load_print_meta: max token length = 1024
0.00.054.668 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.669 I llm_load_tensors: offloading output layer to GPU
0.00.054.669 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.679 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.054.680 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.055.579 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.579 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.580 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.580 I llama_new_context_with_model: n_batch       = 2048
0.00.055.580 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.580 I llama_new_context_with_model: flash_attn    = 0
0.00.055.581 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.581 I llama_new_context_with_model: freq_scale    = 1
0.00.055.581 I ggml_metal_init: allocating
0.00.055.584 I ggml_metal_init: found device: Apple M4
0.00.055.586 I ggml_metal_init: picking default device: Apple M4
0.00.056.183 I ggml_metal_init: using embedded metal library
0.00.058.504 I ggml_metal_init: GPU name:   Apple M4
0.00.058.505 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.507 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.508 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.508 I ggml_metal_init: simdgroup reduction   = true
0.00.058.508 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.508 I ggml_metal_init: has bfloat            = true
0.00.058.508 I ggml_metal_init: use bfloat            = true
0.00.058.509 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.509 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.185 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.088.445 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.088.454 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.088.483 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.089.587 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.089.589 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.089.589 I llama_new_context_with_model: graph nodes  = 967
0.00.089.589 I llama_new_context_with_model: graph splits = 2
0.00.089.605 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.089.746 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.089.747 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.718.927 I main: llama threadpool init, n_threads = 4
0.00.718.966 I 
0.00.718.995 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.718.995 I 
0.00.719.236 I sampler seed: 1234
0.00.719.241 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.719.282 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.719.299 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.719.299 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.572.511 I llama_perf_sampler_print:    sampling time =       1.17 ms /    71 runs   (    0.02 ms per token, 60528.56 tokens per second)
0.01.572.512 I llama_perf_context_print:        load time =     710.30 ms
0.01.572.513 I llama_perf_context_print: prompt eval time =      51.50 ms /     7 tokens (    7.36 ms per token,   135.94 tokens per second)
0.01.572.514 I llama_perf_context_print:        eval time =     798.74 ms /    63 runs   (   12.68 ms per token,    78.87 tokens per second)
0.01.572.514 I llama_perf_context_print:       total time =     853.59 ms /    70 tokens
0.01.572.680 I ggml_metal_free: deallocating

real	0m1.589s
user	0m0.110s
sys	0m0.168s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.033 I build: 4370 (0ca416c9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.061 I main: llama backend init
0.00.000.063 I main: load the model and apply lora adapter, if any
0.00.009.590 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.570 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.574 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.576 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.582 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.582 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.582 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.584 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.585 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.585 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.585 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.586 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.586 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.590 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.590 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.592 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.593 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.594 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.612 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.649 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.565 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.566 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.566 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.567 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.567 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.567 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.568 I llama_model_loader: - type  f32:  194 tensors
0.00.025.568 I llama_model_loader: - type q6_K:   98 tensors
0.00.045.969 I llm_load_vocab: special tokens cache size = 25
0.00.051.784 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.787 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.787 I llm_load_print_meta: arch             = gptneox
0.00.051.788 I llm_load_print_meta: vocab type       = BPE
0.00.051.788 I llm_load_print_meta: n_vocab          = 50304
0.00.051.788 I llm_load_print_meta: n_merges         = 50009
0.00.051.788 I llm_load_print_meta: vocab_only       = 0
0.00.051.788 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.789 I llm_load_print_meta: n_embd           = 2048
0.00.051.789 I llm_load_print_meta: n_layer          = 24
0.00.051.791 I llm_load_print_meta: n_head           = 16
0.00.051.792 I llm_load_print_meta: n_head_kv        = 16
0.00.051.792 I llm_load_print_meta: n_rot            = 32
0.00.051.792 I llm_load_print_meta: n_swa            = 0
0.00.051.793 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.793 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.793 I llm_load_print_meta: n_gqa            = 1
0.00.051.794 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.795 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.796 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.796 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.796 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.796 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.797 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.797 I llm_load_print_meta: n_ff             = 8192
0.00.051.798 I llm_load_print_meta: n_expert         = 0
0.00.051.798 I llm_load_print_meta: n_expert_used    = 0
0.00.051.798 I llm_load_print_meta: causal attn      = 1
0.00.051.798 I llm_load_print_meta: pooling type     = 0
0.00.051.798 I llm_load_print_meta: rope type        = 2
0.00.051.798 I llm_load_print_meta: rope scaling     = linear
0.00.051.800 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.800 I llm_load_print_meta: freq_scale_train = 1
0.00.051.800 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.801 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.801 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.801 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.802 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.803 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.803 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.803 I llm_load_print_meta: model type       = 1.4B
0.00.051.803 I llm_load_print_meta: model ftype      = Q6_K
0.00.051.804 I llm_load_print_meta: model params     = 1.41 B
0.00.051.804 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.051.804 I llm_load_print_meta: general.name     = 1.4B
0.00.051.805 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.805 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.805 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.805 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.805 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.806 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.806 I llm_load_print_meta: max token length = 1024
0.00.053.890 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.890 I llm_load_tensors: offloading output layer to GPU
0.00.053.890 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.901 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.053.902 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.054.791 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.792 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.792 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.792 I llama_new_context_with_model: n_batch       = 2048
0.00.054.793 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.793 I llama_new_context_with_model: flash_attn    = 0
0.00.054.793 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.793 I llama_new_context_with_model: freq_scale    = 1
0.00.054.794 I ggml_metal_init: allocating
0.00.054.797 I ggml_metal_init: found device: Apple M4
0.00.054.799 I ggml_metal_init: picking default device: Apple M4
0.00.055.394 I ggml_metal_init: using embedded metal library
0.00.057.705 I ggml_metal_init: GPU name:   Apple M4
0.00.057.706 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.706 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.707 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.707 I ggml_metal_init: simdgroup reduction   = true
0.00.057.707 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.707 I ggml_metal_init: has bfloat            = true
0.00.057.708 I ggml_metal_init: use bfloat            = true
0.00.057.708 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.709 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.362 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.087.145 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.087.150 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.087.169 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.088.266 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.088.267 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.088.268 I llama_new_context_with_model: graph nodes  = 967
0.00.088.268 I llama_new_context_with_model: graph splits = 2
0.00.088.285 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.088.439 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.088.439 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.744.607 I main: llama threadpool init, n_threads = 4
0.00.744.641 I 
0.00.744.667 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.744.669 I 
0.00.744.824 I sampler seed: 1234
0.00.744.828 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.744.862 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.744.866 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.744.866 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.626.058 I llama_perf_sampler_print:    sampling time =       1.22 ms /    71 runs   (    0.02 ms per token, 58388.16 tokens per second)
0.01.626.059 I llama_perf_context_print:        load time =     735.01 ms
0.01.626.060 I llama_perf_context_print: prompt eval time =      54.32 ms /     7 tokens (    7.76 ms per token,   128.86 tokens per second)
0.01.626.060 I llama_perf_context_print:        eval time =     823.75 ms /    63 runs   (   13.08 ms per token,    76.48 tokens per second)
0.01.626.061 I llama_perf_context_print:       total time =     881.45 ms /    70 tokens
0.01.626.261 I ggml_metal_free: deallocating

real	0m1.644s
user	0m0.109s
sys	0m0.159s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.580 I build: 4370 (0ca416c9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.025.057 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.037.004 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.037.010 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.037.012 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.037.018 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.037.018 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.037.018 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.037.019 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.037.020 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.037.020 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.037.021 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.037.021 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.037.022 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.037.022 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.037.022 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.037.024 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.037.024 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.037.025 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.045.281 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.047.489 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.054.552 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.054.554 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.054.555 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.054.555 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.054.556 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.054.556 I llama_model_loader: - type  f32:  194 tensors
0.00.054.557 I llama_model_loader: - type  f16:   98 tensors
0.00.085.008 I llm_load_vocab: special tokens cache size = 25
0.00.091.449 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.091.452 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.091.452 I llm_load_print_meta: arch             = gptneox
0.00.091.453 I llm_load_print_meta: vocab type       = BPE
0.00.091.453 I llm_load_print_meta: n_vocab          = 50304
0.00.091.453 I llm_load_print_meta: n_merges         = 50009
0.00.091.453 I llm_load_print_meta: vocab_only       = 0
0.00.091.453 I llm_load_print_meta: n_ctx_train      = 2048
0.00.091.453 I llm_load_print_meta: n_embd           = 2048
0.00.091.453 I llm_load_print_meta: n_layer          = 24
0.00.091.456 I llm_load_print_meta: n_head           = 16
0.00.091.457 I llm_load_print_meta: n_head_kv        = 16
0.00.091.457 I llm_load_print_meta: n_rot            = 32
0.00.091.457 I llm_load_print_meta: n_swa            = 0
0.00.091.458 I llm_load_print_meta: n_embd_head_k    = 128
0.00.091.458 I llm_load_print_meta: n_embd_head_v    = 128
0.00.091.458 I llm_load_print_meta: n_gqa            = 1
0.00.091.459 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.091.460 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.091.460 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.091.460 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.091.461 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.091.461 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.091.461 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.091.462 I llm_load_print_meta: n_ff             = 8192
0.00.091.462 I llm_load_print_meta: n_expert         = 0
0.00.091.462 I llm_load_print_meta: n_expert_used    = 0
0.00.091.462 I llm_load_print_meta: causal attn      = 1
0.00.091.462 I llm_load_print_meta: pooling type     = 0
0.00.091.462 I llm_load_print_meta: rope type        = 2
0.00.091.462 I llm_load_print_meta: rope scaling     = linear
0.00.091.463 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.091.463 I llm_load_print_meta: freq_scale_train = 1
0.00.091.463 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.091.463 I llm_load_print_meta: rope_finetuned   = unknown
0.00.091.464 I llm_load_print_meta: ssm_d_conv       = 0
0.00.091.464 I llm_load_print_meta: ssm_d_inner      = 0
0.00.091.464 I llm_load_print_meta: ssm_d_state      = 0
0.00.091.466 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.091.466 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.091.466 I llm_load_print_meta: model type       = 1.4B
0.00.091.467 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.091.467 I llm_load_print_meta: model params     = 1.41 B
0.00.091.472 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.091.472 I llm_load_print_meta: general.name     = 1.4B
0.00.091.472 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.091.472 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.091.473 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.091.473 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.091.473 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.091.473 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.091.473 I llm_load_print_meta: max token length = 1024
0.00.094.039 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.094.040 I llm_load_tensors: offloading output layer to GPU
0.00.094.040 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.094.050 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.094.051 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.094.990 I llama_new_context_with_model: n_seq_max     = 1
0.00.094.991 I llama_new_context_with_model: n_ctx         = 128
0.00.094.991 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.094.992 I llama_new_context_with_model: n_batch       = 128
0.00.094.992 I llama_new_context_with_model: n_ubatch      = 128
0.00.094.992 I llama_new_context_with_model: flash_attn    = 0
0.00.094.992 I llama_new_context_with_model: freq_base     = 10000.0
0.00.094.993 I llama_new_context_with_model: freq_scale    = 1
0.00.094.993 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.094.993 I ggml_metal_init: allocating
0.00.094.996 I ggml_metal_init: found device: Apple M4
0.00.094.998 I ggml_metal_init: picking default device: Apple M4
0.00.095.637 I ggml_metal_init: using embedded metal library
0.00.098.231 I ggml_metal_init: GPU name:   Apple M4
0.00.098.232 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.098.233 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.098.233 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.098.233 I ggml_metal_init: simdgroup reduction   = true
0.00.098.233 I ggml_metal_init: simdgroup matrix mul. = true
0.00.098.234 I ggml_metal_init: has bfloat            = true
0.00.098.234 I ggml_metal_init: use bfloat            = true
0.00.098.234 I ggml_metal_init: hasUnifiedMemory      = true
0.00.098.235 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.107.596 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.108.902 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.108.904 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.108.917 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.109.746 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.109.747 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.109.747 I llama_new_context_with_model: graph nodes  = 967
0.00.109.748 I llama_new_context_with_model: graph splits = 2
0.00.109.755 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.109.755 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.108.798 I 
0.01.108.831 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.108.869 I perplexity: tokenizing the input ..
0.01.121.092 I perplexity: tokenization took 12.222 ms
0.01.121.097 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.242.190 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.244.324 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.244.358 I llama_perf_context_print:        load time =    1083.73 ms
0.01.244.367 I llama_perf_context_print: prompt eval time =     120.72 ms /   128 tokens (    0.94 ms per token,  1060.35 tokens per second)
0.01.244.368 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.244.371 I llama_perf_context_print:       total time =     135.56 ms /   129 tokens
0.01.245.030 I ggml_metal_free: deallocating

real	0m1.438s
user	0m0.123s
sys	0m0.212s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.129 I build: 4370 (0ca416c9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.012.451 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.020.782 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.020.788 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.020.790 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.020.791 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.020.791 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.020.791 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.020.792 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.020.793 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.020.793 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.020.793 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.020.794 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.020.794 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.020.794 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.020.795 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.020.797 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.020.797 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.020.798 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.026.200 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.027.712 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.033.185 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.033.187 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.033.187 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.033.188 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.033.188 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.033.189 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.033.189 I llama_model_loader: - type  f32:  194 tensors
0.00.033.190 I llama_model_loader: - type q8_0:   98 tensors
0.00.058.882 I llm_load_vocab: special tokens cache size = 25
0.00.065.143 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.065.148 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.065.148 I llm_load_print_meta: arch             = gptneox
0.00.065.149 I llm_load_print_meta: vocab type       = BPE
0.00.065.149 I llm_load_print_meta: n_vocab          = 50304
0.00.065.149 I llm_load_print_meta: n_merges         = 50009
0.00.065.149 I llm_load_print_meta: vocab_only       = 0
0.00.065.149 I llm_load_print_meta: n_ctx_train      = 2048
0.00.065.149 I llm_load_print_meta: n_embd           = 2048
0.00.065.149 I llm_load_print_meta: n_layer          = 24
0.00.065.153 I llm_load_print_meta: n_head           = 16
0.00.065.153 I llm_load_print_meta: n_head_kv        = 16
0.00.065.153 I llm_load_print_meta: n_rot            = 32
0.00.065.154 I llm_load_print_meta: n_swa            = 0
0.00.065.154 I llm_load_print_meta: n_embd_head_k    = 128
0.00.065.154 I llm_load_print_meta: n_embd_head_v    = 128
0.00.065.154 I llm_load_print_meta: n_gqa            = 1
0.00.065.155 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.065.156 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.065.156 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.065.157 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.065.157 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.065.157 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.065.157 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.065.157 I llm_load_print_meta: n_ff             = 8192
0.00.065.158 I llm_load_print_meta: n_expert         = 0
0.00.065.158 I llm_load_print_meta: n_expert_used    = 0
0.00.065.158 I llm_load_print_meta: causal attn      = 1
0.00.065.158 I llm_load_print_meta: pooling type     = 0
0.00.065.158 I llm_load_print_meta: rope type        = 2
0.00.065.158 I llm_load_print_meta: rope scaling     = linear
0.00.065.159 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.065.159 I llm_load_print_meta: freq_scale_train = 1
0.00.065.159 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.065.159 I llm_load_print_meta: rope_finetuned   = unknown
0.00.065.160 I llm_load_print_meta: ssm_d_conv       = 0
0.00.065.160 I llm_load_print_meta: ssm_d_inner      = 0
0.00.065.160 I llm_load_print_meta: ssm_d_state      = 0
0.00.065.160 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.065.160 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.065.160 I llm_load_print_meta: model type       = 1.4B
0.00.065.161 I llm_load_print_meta: model ftype      = Q8_0
0.00.065.161 I llm_load_print_meta: model params     = 1.41 B
0.00.065.162 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.065.162 I llm_load_print_meta: general.name     = 1.4B
0.00.065.162 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.065.162 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.065.162 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.065.162 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.065.163 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.065.163 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.065.163 I llm_load_print_meta: max token length = 1024
0.00.067.255 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.067.256 I llm_load_tensors: offloading output layer to GPU
0.00.067.256 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.067.266 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.067.267 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.068.196 I llama_new_context_with_model: n_seq_max     = 1
0.00.068.197 I llama_new_context_with_model: n_ctx         = 128
0.00.068.197 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.068.198 I llama_new_context_with_model: n_batch       = 128
0.00.068.198 I llama_new_context_with_model: n_ubatch      = 128
0.00.068.198 I llama_new_context_with_model: flash_attn    = 0
0.00.068.198 I llama_new_context_with_model: freq_base     = 10000.0
0.00.068.198 I llama_new_context_with_model: freq_scale    = 1
0.00.068.200 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.068.200 I ggml_metal_init: allocating
0.00.068.204 I ggml_metal_init: found device: Apple M4
0.00.068.205 I ggml_metal_init: picking default device: Apple M4
0.00.068.834 I ggml_metal_init: using embedded metal library
0.00.071.624 I ggml_metal_init: GPU name:   Apple M4
0.00.071.627 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.071.627 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.071.627 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.071.628 I ggml_metal_init: simdgroup reduction   = true
0.00.071.628 I ggml_metal_init: simdgroup matrix mul. = true
0.00.071.628 I ggml_metal_init: has bfloat            = true
0.00.071.628 I ggml_metal_init: use bfloat            = true
0.00.071.628 I ggml_metal_init: hasUnifiedMemory      = true
0.00.071.629 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.080.601 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.082.120 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.082.122 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.082.135 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.083.070 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.083.072 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.083.072 I llama_new_context_with_model: graph nodes  = 967
0.00.083.072 I llama_new_context_with_model: graph splits = 2
0.00.083.085 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.083.086 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.927.760 I 
0.00.927.795 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.927.832 I perplexity: tokenizing the input ..
0.00.935.695 I perplexity: tokenization took 7.861 ms
0.00.935.698 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.059.943 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.061.199 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.061.220 I llama_perf_context_print:        load time =     915.31 ms
0.01.061.223 I llama_perf_context_print: prompt eval time =     124.02 ms /   128 tokens (    0.97 ms per token,  1032.07 tokens per second)
0.01.061.224 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.061.224 I llama_perf_context_print:       total time =     133.46 ms /   129 tokens
0.01.061.585 I ggml_metal_free: deallocating

real	0m1.080s
user	0m0.092s
sys	0m0.151s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.085 I build: 4370 (0ca416c9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.205 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.998 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.015.002 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.008 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.009 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.009 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.009 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.010 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.012 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.013 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.013 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.013 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.014 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.014 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.015 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.016 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.016 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.016 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.846 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.941 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.786 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.787 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.787 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.788 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.788 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.788 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.023.789 I llama_model_loader: - type  f32:  194 tensors
0.00.023.789 I llama_model_loader: - type q4_0:   97 tensors
0.00.023.789 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.196 I llm_load_vocab: special tokens cache size = 25
0.00.049.945 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.948 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.948 I llm_load_print_meta: arch             = gptneox
0.00.049.949 I llm_load_print_meta: vocab type       = BPE
0.00.049.949 I llm_load_print_meta: n_vocab          = 50304
0.00.049.949 I llm_load_print_meta: n_merges         = 50009
0.00.049.949 I llm_load_print_meta: vocab_only       = 0
0.00.049.949 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.949 I llm_load_print_meta: n_embd           = 2048
0.00.049.950 I llm_load_print_meta: n_layer          = 24
0.00.049.953 I llm_load_print_meta: n_head           = 16
0.00.049.954 I llm_load_print_meta: n_head_kv        = 16
0.00.049.954 I llm_load_print_meta: n_rot            = 32
0.00.049.954 I llm_load_print_meta: n_swa            = 0
0.00.049.954 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.954 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.955 I llm_load_print_meta: n_gqa            = 1
0.00.049.956 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.956 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.957 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.957 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.957 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.958 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.958 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.958 I llm_load_print_meta: n_ff             = 8192
0.00.049.959 I llm_load_print_meta: n_expert         = 0
0.00.049.959 I llm_load_print_meta: n_expert_used    = 0
0.00.049.959 I llm_load_print_meta: causal attn      = 1
0.00.049.961 I llm_load_print_meta: pooling type     = 0
0.00.049.961 I llm_load_print_meta: rope type        = 2
0.00.049.961 I llm_load_print_meta: rope scaling     = linear
0.00.049.963 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.965 I llm_load_print_meta: freq_scale_train = 1
0.00.049.965 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.966 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.966 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.966 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.966 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.966 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.966 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.966 I llm_load_print_meta: model type       = 1.4B
0.00.049.970 I llm_load_print_meta: model ftype      = Q4_0
0.00.049.971 I llm_load_print_meta: model params     = 1.41 B
0.00.049.971 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.049.971 I llm_load_print_meta: general.name     = 1.4B
0.00.049.975 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.977 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.977 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.978 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.978 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.049.978 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.978 I llm_load_print_meta: max token length = 1024
0.00.051.735 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.736 I llm_load_tensors: offloading output layer to GPU
0.00.051.736 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.742 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.051.742 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.052.695 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.696 I llama_new_context_with_model: n_ctx         = 128
0.00.052.696 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.697 I llama_new_context_with_model: n_batch       = 128
0.00.052.697 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.697 I llama_new_context_with_model: flash_attn    = 0
0.00.052.697 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.698 I llama_new_context_with_model: freq_scale    = 1
0.00.052.698 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.698 I ggml_metal_init: allocating
0.00.052.702 I ggml_metal_init: found device: Apple M4
0.00.052.704 I ggml_metal_init: picking default device: Apple M4
0.00.053.340 I ggml_metal_init: using embedded metal library
0.00.055.812 I ggml_metal_init: GPU name:   Apple M4
0.00.055.814 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.814 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.814 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.815 I ggml_metal_init: simdgroup reduction   = true
0.00.055.815 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.815 I ggml_metal_init: has bfloat            = true
0.00.055.815 I ggml_metal_init: use bfloat            = true
0.00.055.815 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.816 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.990 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.067.295 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.299 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.316 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.214 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.216 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.216 I llama_new_context_with_model: graph nodes  = 967
0.00.068.216 I llama_new_context_with_model: graph splits = 2
0.00.068.227 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.228 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.611.403 I 
0.00.611.436 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.611.448 I perplexity: tokenizing the input ..
0.00.619.584 I perplexity: tokenization took 8.134 ms
0.00.619.591 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.741.892 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.743.079 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.743.094 I llama_perf_context_print:        load time =     602.19 ms
0.00.743.096 I llama_perf_context_print: prompt eval time =     122.08 ms /   128 tokens (    0.95 ms per token,  1048.53 tokens per second)
0.00.743.096 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.743.097 I llama_perf_context_print:       total time =     131.69 ms /   129 tokens
0.00.743.606 I ggml_metal_free: deallocating

real	0m0.759s
user	0m0.078s
sys	0m0.095s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.094 I build: 4370 (0ca416c9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.841 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.809 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.014.814 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.815 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.816 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.816 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.816 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.816 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.817 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.819 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.820 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.820 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.820 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.821 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.821 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.822 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.823 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.823 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.652 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.754 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.591 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.592 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.593 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.593 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.593 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.594 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.023.594 I llama_model_loader: - type  f32:  194 tensors
0.00.023.595 I llama_model_loader: - type q4_1:   97 tensors
0.00.023.595 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.786 I llm_load_vocab: special tokens cache size = 25
0.00.050.753 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.756 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.756 I llm_load_print_meta: arch             = gptneox
0.00.050.757 I llm_load_print_meta: vocab type       = BPE
0.00.050.757 I llm_load_print_meta: n_vocab          = 50304
0.00.050.757 I llm_load_print_meta: n_merges         = 50009
0.00.050.757 I llm_load_print_meta: vocab_only       = 0
0.00.050.758 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.758 I llm_load_print_meta: n_embd           = 2048
0.00.050.758 I llm_load_print_meta: n_layer          = 24
0.00.050.761 I llm_load_print_meta: n_head           = 16
0.00.050.762 I llm_load_print_meta: n_head_kv        = 16
0.00.050.762 I llm_load_print_meta: n_rot            = 32
0.00.050.762 I llm_load_print_meta: n_swa            = 0
0.00.050.762 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.762 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.765 I llm_load_print_meta: n_gqa            = 1
0.00.050.766 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.767 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.767 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.768 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.768 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.768 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.768 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.769 I llm_load_print_meta: n_ff             = 8192
0.00.050.769 I llm_load_print_meta: n_expert         = 0
0.00.050.769 I llm_load_print_meta: n_expert_used    = 0
0.00.050.769 I llm_load_print_meta: causal attn      = 1
0.00.050.770 I llm_load_print_meta: pooling type     = 0
0.00.050.770 I llm_load_print_meta: rope type        = 2
0.00.050.770 I llm_load_print_meta: rope scaling     = linear
0.00.050.770 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.771 I llm_load_print_meta: freq_scale_train = 1
0.00.050.772 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.772 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.772 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.772 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.772 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.772 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.772 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.773 I llm_load_print_meta: model type       = 1.4B
0.00.050.773 I llm_load_print_meta: model ftype      = Q4_1
0.00.050.774 I llm_load_print_meta: model params     = 1.41 B
0.00.050.774 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.050.774 I llm_load_print_meta: general.name     = 1.4B
0.00.050.775 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.775 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.775 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.775 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.776 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.776 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.776 I llm_load_print_meta: max token length = 1024
0.00.052.779 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.779 I llm_load_tensors: offloading output layer to GPU
0.00.052.780 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.790 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.052.791 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.053.686 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.687 I llama_new_context_with_model: n_ctx         = 128
0.00.053.687 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.687 I llama_new_context_with_model: n_batch       = 128
0.00.053.687 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.687 I llama_new_context_with_model: flash_attn    = 0
0.00.053.688 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.688 I llama_new_context_with_model: freq_scale    = 1
0.00.053.688 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.689 I ggml_metal_init: allocating
0.00.053.692 I ggml_metal_init: found device: Apple M4
0.00.053.694 I ggml_metal_init: picking default device: Apple M4
0.00.054.260 I ggml_metal_init: using embedded metal library
0.00.056.604 I ggml_metal_init: GPU name:   Apple M4
0.00.056.605 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.606 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.606 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.607 I ggml_metal_init: simdgroup reduction   = true
0.00.056.607 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.607 I ggml_metal_init: has bfloat            = true
0.00.056.607 I ggml_metal_init: use bfloat            = true
0.00.056.607 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.608 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.452 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.067.677 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.679 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.692 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.536 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.537 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.537 I llama_new_context_with_model: graph nodes  = 967
0.00.068.538 I llama_new_context_with_model: graph splits = 2
0.00.068.550 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.551 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.669.256 I 
0.00.669.303 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.669.316 I perplexity: tokenizing the input ..
0.00.677.321 I perplexity: tokenization took 8.003 ms
0.00.677.324 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.800.095 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.801.262 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.801.280 I llama_perf_context_print:        load time =     660.41 ms
0.00.801.281 I llama_perf_context_print: prompt eval time =     122.54 ms /   128 tokens (    0.96 ms per token,  1044.55 tokens per second)
0.00.801.282 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.801.282 I llama_perf_context_print:       total time =     132.03 ms /   129 tokens
0.00.801.816 I ggml_metal_free: deallocating

real	0m0.816s
user	0m0.079s
sys	0m0.109s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.086 I build: 4370 (0ca416c9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.582 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.374 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.015.378 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.380 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.380 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.381 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.381 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.381 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.382 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.382 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.383 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.383 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.383 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.384 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.384 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.386 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.386 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.387 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.272 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.361 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.259 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.260 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.260 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.260 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.261 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.261 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.024.261 I llama_model_loader: - type  f32:  194 tensors
0.00.024.262 I llama_model_loader: - type q5_0:   97 tensors
0.00.024.262 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.538 I llm_load_vocab: special tokens cache size = 25
0.00.050.433 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.436 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.436 I llm_load_print_meta: arch             = gptneox
0.00.050.437 I llm_load_print_meta: vocab type       = BPE
0.00.050.437 I llm_load_print_meta: n_vocab          = 50304
0.00.050.437 I llm_load_print_meta: n_merges         = 50009
0.00.050.437 I llm_load_print_meta: vocab_only       = 0
0.00.050.437 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.438 I llm_load_print_meta: n_embd           = 2048
0.00.050.438 I llm_load_print_meta: n_layer          = 24
0.00.050.440 I llm_load_print_meta: n_head           = 16
0.00.050.441 I llm_load_print_meta: n_head_kv        = 16
0.00.050.441 I llm_load_print_meta: n_rot            = 32
0.00.050.441 I llm_load_print_meta: n_swa            = 0
0.00.050.442 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.442 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.442 I llm_load_print_meta: n_gqa            = 1
0.00.050.443 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.446 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.447 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.447 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.447 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.448 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.448 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.448 I llm_load_print_meta: n_ff             = 8192
0.00.050.449 I llm_load_print_meta: n_expert         = 0
0.00.050.449 I llm_load_print_meta: n_expert_used    = 0
0.00.050.449 I llm_load_print_meta: causal attn      = 1
0.00.050.449 I llm_load_print_meta: pooling type     = 0
0.00.050.449 I llm_load_print_meta: rope type        = 2
0.00.050.458 I llm_load_print_meta: rope scaling     = linear
0.00.050.461 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.461 I llm_load_print_meta: freq_scale_train = 1
0.00.050.462 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.462 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.462 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.462 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.462 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.462 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.464 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.464 I llm_load_print_meta: model type       = 1.4B
0.00.050.464 I llm_load_print_meta: model ftype      = Q5_0
0.00.050.464 I llm_load_print_meta: model params     = 1.41 B
0.00.050.465 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.050.465 I llm_load_print_meta: general.name     = 1.4B
0.00.050.465 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.466 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.466 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.466 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.466 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.466 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.467 I llm_load_print_meta: max token length = 1024
0.00.052.423 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.424 I llm_load_tensors: offloading output layer to GPU
0.00.052.424 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.434 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.052.435 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.053.320 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.321 I llama_new_context_with_model: n_ctx         = 128
0.00.053.321 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.321 I llama_new_context_with_model: n_batch       = 128
0.00.053.321 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.321 I llama_new_context_with_model: flash_attn    = 0
0.00.053.322 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.322 I llama_new_context_with_model: freq_scale    = 1
0.00.053.322 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.323 I ggml_metal_init: allocating
0.00.053.327 I ggml_metal_init: found device: Apple M4
0.00.053.330 I ggml_metal_init: picking default device: Apple M4
0.00.053.903 I ggml_metal_init: using embedded metal library
0.00.056.186 I ggml_metal_init: GPU name:   Apple M4
0.00.056.188 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.188 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.189 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.189 I ggml_metal_init: simdgroup reduction   = true
0.00.056.189 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.189 I ggml_metal_init: has bfloat            = true
0.00.056.189 I ggml_metal_init: use bfloat            = true
0.00.056.190 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.190 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.673 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.066.935 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.939 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.956 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.891 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.892 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.893 I llama_new_context_with_model: graph nodes  = 967
0.00.067.893 I llama_new_context_with_model: graph splits = 2
0.00.067.905 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.906 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.722.758 I 
0.00.722.797 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.722.810 I perplexity: tokenizing the input ..
0.00.730.825 I perplexity: tokenization took 8.014 ms
0.00.730.829 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.866.144 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.867.394 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.867.409 I llama_perf_context_print:        load time =     713.17 ms
0.00.867.409 I llama_perf_context_print: prompt eval time =     135.09 ms /   128 tokens (    1.06 ms per token,   947.52 tokens per second)
0.00.867.410 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.867.411 I llama_perf_context_print:       total time =     144.65 ms /   129 tokens
0.00.867.894 I ggml_metal_free: deallocating

real	0m0.883s
user	0m0.078s
sys	0m0.120s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.089 I build: 4370 (0ca416c9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.274 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.483 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.015.487 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.493 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.494 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.494 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.494 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.495 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.497 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.498 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.498 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.498 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.499 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.499 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.500 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.501 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.502 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.502 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.395 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.550 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.521 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.522 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.523 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.523 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.523 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.524 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.524 I llama_model_loader: - type  f32:  194 tensors
0.00.024.525 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.525 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.604 I llm_load_vocab: special tokens cache size = 25
0.00.051.436 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.440 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.441 I llm_load_print_meta: arch             = gptneox
0.00.051.441 I llm_load_print_meta: vocab type       = BPE
0.00.051.441 I llm_load_print_meta: n_vocab          = 50304
0.00.051.442 I llm_load_print_meta: n_merges         = 50009
0.00.051.442 I llm_load_print_meta: vocab_only       = 0
0.00.051.442 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.442 I llm_load_print_meta: n_embd           = 2048
0.00.051.442 I llm_load_print_meta: n_layer          = 24
0.00.051.445 I llm_load_print_meta: n_head           = 16
0.00.051.445 I llm_load_print_meta: n_head_kv        = 16
0.00.051.446 I llm_load_print_meta: n_rot            = 32
0.00.051.448 I llm_load_print_meta: n_swa            = 0
0.00.051.448 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.448 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.449 I llm_load_print_meta: n_gqa            = 1
0.00.051.450 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.450 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.451 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.451 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.451 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.452 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.452 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.452 I llm_load_print_meta: n_ff             = 8192
0.00.051.453 I llm_load_print_meta: n_expert         = 0
0.00.051.453 I llm_load_print_meta: n_expert_used    = 0
0.00.051.453 I llm_load_print_meta: causal attn      = 1
0.00.051.453 I llm_load_print_meta: pooling type     = 0
0.00.051.453 I llm_load_print_meta: rope type        = 2
0.00.051.453 I llm_load_print_meta: rope scaling     = linear
0.00.051.455 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.456 I llm_load_print_meta: freq_scale_train = 1
0.00.051.456 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.456 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.456 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.456 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.456 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.457 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.457 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.457 I llm_load_print_meta: model type       = 1.4B
0.00.051.457 I llm_load_print_meta: model ftype      = Q5_1
0.00.051.458 I llm_load_print_meta: model params     = 1.41 B
0.00.051.458 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.051.458 I llm_load_print_meta: general.name     = 1.4B
0.00.051.459 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.459 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.459 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.459 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.460 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.460 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.460 I llm_load_print_meta: max token length = 1024
0.00.053.532 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.532 I llm_load_tensors: offloading output layer to GPU
0.00.053.532 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.543 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.053.544 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.054.551 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.552 I llama_new_context_with_model: n_ctx         = 128
0.00.054.552 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.552 I llama_new_context_with_model: n_batch       = 128
0.00.054.552 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.553 I llama_new_context_with_model: flash_attn    = 0
0.00.054.553 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.553 I llama_new_context_with_model: freq_scale    = 1
0.00.054.553 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.554 I ggml_metal_init: allocating
0.00.054.557 I ggml_metal_init: found device: Apple M4
0.00.054.559 I ggml_metal_init: picking default device: Apple M4
0.00.055.117 I ggml_metal_init: using embedded metal library
0.00.057.455 I ggml_metal_init: GPU name:   Apple M4
0.00.057.457 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.457 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.457 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.457 I ggml_metal_init: simdgroup reduction   = true
0.00.057.458 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.458 I ggml_metal_init: has bfloat            = true
0.00.057.458 I ggml_metal_init: use bfloat            = true
0.00.057.458 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.459 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.317 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.068.568 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.573 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.588 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.069.483 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.069.484 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.069.484 I llama_new_context_with_model: graph nodes  = 967
0.00.069.484 I llama_new_context_with_model: graph splits = 2
0.00.069.497 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.069.498 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.653.108 I 
0.00.653.153 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.653.164 I perplexity: tokenizing the input ..
0.00.660.867 I perplexity: tokenization took 7.702 ms
0.00.660.871 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.795.727 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.796.892 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.796.910 I llama_perf_context_print:        load time =     643.83 ms
0.00.796.911 I llama_perf_context_print: prompt eval time =     134.63 ms /   128 tokens (    1.05 ms per token,   950.75 tokens per second)
0.00.796.913 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.796.914 I llama_perf_context_print:       total time =     143.80 ms /   129 tokens
0.00.797.371 I ggml_metal_free: deallocating

real	0m0.812s
user	0m0.079s
sys	0m0.113s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.086 I build: 4370 (0ca416c9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.629 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.146 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.151 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.153 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.153 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.154 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.154 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.154 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.155 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.157 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.157 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.158 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.158 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.158 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.159 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.160 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.160 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.161 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.051 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.177 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.096 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.097 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.097 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.098 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.098 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.098 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.099 I llama_model_loader: - type  f32:  194 tensors
0.00.024.099 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.099 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.100 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.353 I llm_load_vocab: special tokens cache size = 25
0.00.050.280 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.282 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.282 I llm_load_print_meta: arch             = gptneox
0.00.050.283 I llm_load_print_meta: vocab type       = BPE
0.00.050.283 I llm_load_print_meta: n_vocab          = 50304
0.00.050.283 I llm_load_print_meta: n_merges         = 50009
0.00.050.284 I llm_load_print_meta: vocab_only       = 0
0.00.050.284 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.284 I llm_load_print_meta: n_embd           = 2048
0.00.050.284 I llm_load_print_meta: n_layer          = 24
0.00.050.287 I llm_load_print_meta: n_head           = 16
0.00.050.287 I llm_load_print_meta: n_head_kv        = 16
0.00.050.287 I llm_load_print_meta: n_rot            = 32
0.00.050.288 I llm_load_print_meta: n_swa            = 0
0.00.050.288 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.288 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.289 I llm_load_print_meta: n_gqa            = 1
0.00.050.290 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.292 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.293 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.293 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.293 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.294 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.294 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.294 I llm_load_print_meta: n_ff             = 8192
0.00.050.295 I llm_load_print_meta: n_expert         = 0
0.00.050.295 I llm_load_print_meta: n_expert_used    = 0
0.00.050.296 I llm_load_print_meta: causal attn      = 1
0.00.050.296 I llm_load_print_meta: pooling type     = 0
0.00.050.297 I llm_load_print_meta: rope type        = 2
0.00.050.297 I llm_load_print_meta: rope scaling     = linear
0.00.050.297 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.297 I llm_load_print_meta: freq_scale_train = 1
0.00.050.298 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.298 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.298 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.298 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.298 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.298 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.299 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.299 I llm_load_print_meta: model type       = 1.4B
0.00.050.299 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.050.299 I llm_load_print_meta: model params     = 1.41 B
0.00.050.301 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.050.301 I llm_load_print_meta: general.name     = 1.4B
0.00.050.301 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.302 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.302 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.302 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.302 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.303 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.303 I llm_load_print_meta: max token length = 1024
0.00.052.130 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.130 I llm_load_tensors: offloading output layer to GPU
0.00.052.130 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.140 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.052.141 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.052.985 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.986 I llama_new_context_with_model: n_ctx         = 128
0.00.052.986 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.986 I llama_new_context_with_model: n_batch       = 128
0.00.052.986 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.986 I llama_new_context_with_model: flash_attn    = 0
0.00.052.987 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.987 I llama_new_context_with_model: freq_scale    = 1
0.00.052.987 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.988 I ggml_metal_init: allocating
0.00.052.991 I ggml_metal_init: found device: Apple M4
0.00.052.993 I ggml_metal_init: picking default device: Apple M4
0.00.053.539 I ggml_metal_init: using embedded metal library
0.00.055.840 I ggml_metal_init: GPU name:   Apple M4
0.00.055.842 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.842 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.842 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.843 I ggml_metal_init: simdgroup reduction   = true
0.00.055.843 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.843 I ggml_metal_init: has bfloat            = true
0.00.055.843 I ggml_metal_init: use bfloat            = true
0.00.055.844 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.844 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.436 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.066.691 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.701 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.722 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.603 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.604 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.604 I llama_new_context_with_model: graph nodes  = 967
0.00.067.605 I llama_new_context_with_model: graph splits = 2
0.00.067.617 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.618 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.393.890 I 
0.00.393.935 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.393.946 I perplexity: tokenizing the input ..
0.00.401.597 I perplexity: tokenization took 7.651 ms
0.00.401.601 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.534.568 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.535.815 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.535.835 I llama_perf_context_print:        load time =     384.25 ms
0.00.535.836 I llama_perf_context_print: prompt eval time =     132.73 ms /   128 tokens (    1.04 ms per token,   964.37 tokens per second)
0.00.535.836 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.535.837 I llama_perf_context_print:       total time =     141.95 ms /   129 tokens
0.00.536.345 I ggml_metal_free: deallocating

real	0m0.551s
user	0m0.077s
sys	0m0.076s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.084 I build: 4370 (0ca416c9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.688 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.594 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.014.599 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.600 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.601 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.601 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.601 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.602 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.603 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.603 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.603 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.604 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.604 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.604 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.605 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.606 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.607 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.607 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.403 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.450 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.233 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.234 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.235 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.235 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.235 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.235 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.023.236 I llama_model_loader: - type  f32:  194 tensors
0.00.023.236 I llama_model_loader: - type q3_K:   25 tensors
0.00.023.236 I llama_model_loader: - type q4_K:   71 tensors
0.00.023.236 I llama_model_loader: - type q5_K:    1 tensors
0.00.023.237 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.537 I llm_load_vocab: special tokens cache size = 25
0.00.049.487 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.491 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.491 I llm_load_print_meta: arch             = gptneox
0.00.049.491 I llm_load_print_meta: vocab type       = BPE
0.00.049.491 I llm_load_print_meta: n_vocab          = 50304
0.00.049.492 I llm_load_print_meta: n_merges         = 50009
0.00.049.492 I llm_load_print_meta: vocab_only       = 0
0.00.049.494 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.494 I llm_load_print_meta: n_embd           = 2048
0.00.049.494 I llm_load_print_meta: n_layer          = 24
0.00.049.497 I llm_load_print_meta: n_head           = 16
0.00.049.497 I llm_load_print_meta: n_head_kv        = 16
0.00.049.498 I llm_load_print_meta: n_rot            = 32
0.00.049.498 I llm_load_print_meta: n_swa            = 0
0.00.049.498 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.498 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.503 I llm_load_print_meta: n_gqa            = 1
0.00.049.504 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.505 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.505 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.506 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.506 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.506 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.507 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.507 I llm_load_print_meta: n_ff             = 8192
0.00.049.508 I llm_load_print_meta: n_expert         = 0
0.00.049.508 I llm_load_print_meta: n_expert_used    = 0
0.00.049.508 I llm_load_print_meta: causal attn      = 1
0.00.049.508 I llm_load_print_meta: pooling type     = 0
0.00.049.508 I llm_load_print_meta: rope type        = 2
0.00.049.509 I llm_load_print_meta: rope scaling     = linear
0.00.049.509 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.509 I llm_load_print_meta: freq_scale_train = 1
0.00.049.509 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.510 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.510 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.512 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.512 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.512 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.512 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.512 I llm_load_print_meta: model type       = 1.4B
0.00.049.513 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.049.513 I llm_load_print_meta: model params     = 1.41 B
0.00.049.513 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.049.513 I llm_load_print_meta: general.name     = 1.4B
0.00.049.514 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.514 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.514 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.514 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.516 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.049.516 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.516 I llm_load_print_meta: max token length = 1024
0.00.051.396 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.397 I llm_load_tensors: offloading output layer to GPU
0.00.051.397 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.407 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.051.408 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.052.293 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.294 I llama_new_context_with_model: n_ctx         = 128
0.00.052.294 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.295 I llama_new_context_with_model: n_batch       = 128
0.00.052.295 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.295 I llama_new_context_with_model: flash_attn    = 0
0.00.052.295 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.296 I llama_new_context_with_model: freq_scale    = 1
0.00.052.296 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.296 I ggml_metal_init: allocating
0.00.052.302 I ggml_metal_init: found device: Apple M4
0.00.052.304 I ggml_metal_init: picking default device: Apple M4
0.00.052.864 I ggml_metal_init: using embedded metal library
0.00.055.191 I ggml_metal_init: GPU name:   Apple M4
0.00.055.192 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.193 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.193 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.193 I ggml_metal_init: simdgroup reduction   = true
0.00.055.193 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.193 I ggml_metal_init: has bfloat            = true
0.00.055.194 I ggml_metal_init: use bfloat            = true
0.00.055.194 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.195 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.653 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.065.915 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.918 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.932 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.781 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.782 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.782 I llama_new_context_with_model: graph nodes  = 967
0.00.066.783 I llama_new_context_with_model: graph splits = 2
0.00.066.795 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.796 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.474.024 I 
0.00.474.075 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.474.086 I perplexity: tokenizing the input ..
0.00.481.910 I perplexity: tokenization took 7.821 ms
0.00.481.913 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.613.741 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.614.928 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.614.948 I llama_perf_context_print:        load time =     465.33 ms
0.00.614.949 I llama_perf_context_print: prompt eval time =     131.60 ms /   128 tokens (    1.03 ms per token,   972.61 tokens per second)
0.00.614.950 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.614.950 I llama_perf_context_print:       total time =     140.93 ms /   129 tokens
0.00.615.336 I ggml_metal_free: deallocating

real	0m0.628s
user	0m0.078s
sys	0m0.083s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.101 I build: 4370 (0ca416c9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.899 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.714 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.014.719 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.721 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.721 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.722 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.722 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.722 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.723 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.723 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.724 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.724 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.725 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.725 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.725 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.728 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.728 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.729 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.639 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.677 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.603 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.604 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.604 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.605 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.605 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.605 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.023.606 I llama_model_loader: - type  f32:  194 tensors
0.00.023.606 I llama_model_loader: - type q4_K:   61 tensors
0.00.023.607 I llama_model_loader: - type q5_K:   24 tensors
0.00.023.607 I llama_model_loader: - type q6_K:   13 tensors
0.00.044.635 I llm_load_vocab: special tokens cache size = 25
0.00.050.455 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.458 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.458 I llm_load_print_meta: arch             = gptneox
0.00.050.458 I llm_load_print_meta: vocab type       = BPE
0.00.050.459 I llm_load_print_meta: n_vocab          = 50304
0.00.050.459 I llm_load_print_meta: n_merges         = 50009
0.00.050.459 I llm_load_print_meta: vocab_only       = 0
0.00.050.459 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.459 I llm_load_print_meta: n_embd           = 2048
0.00.050.460 I llm_load_print_meta: n_layer          = 24
0.00.050.462 I llm_load_print_meta: n_head           = 16
0.00.050.463 I llm_load_print_meta: n_head_kv        = 16
0.00.050.464 I llm_load_print_meta: n_rot            = 32
0.00.050.469 I llm_load_print_meta: n_swa            = 0
0.00.050.469 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.469 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.470 I llm_load_print_meta: n_gqa            = 1
0.00.050.471 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.471 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.472 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.473 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.474 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.474 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.475 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.475 I llm_load_print_meta: n_ff             = 8192
0.00.050.476 I llm_load_print_meta: n_expert         = 0
0.00.050.476 I llm_load_print_meta: n_expert_used    = 0
0.00.050.476 I llm_load_print_meta: causal attn      = 1
0.00.050.476 I llm_load_print_meta: pooling type     = 0
0.00.050.476 I llm_load_print_meta: rope type        = 2
0.00.050.477 I llm_load_print_meta: rope scaling     = linear
0.00.050.477 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.477 I llm_load_print_meta: freq_scale_train = 1
0.00.050.477 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.478 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.478 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.479 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.479 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.479 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.479 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.479 I llm_load_print_meta: model type       = 1.4B
0.00.050.480 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.050.480 I llm_load_print_meta: model params     = 1.41 B
0.00.050.481 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.050.481 I llm_load_print_meta: general.name     = 1.4B
0.00.050.482 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.482 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.482 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.482 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.482 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.483 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.483 I llm_load_print_meta: max token length = 1024
0.00.052.309 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.310 I llm_load_tensors: offloading output layer to GPU
0.00.052.310 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.315 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.052.316 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.053.246 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.247 I llama_new_context_with_model: n_ctx         = 128
0.00.053.247 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.247 I llama_new_context_with_model: n_batch       = 128
0.00.053.247 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.248 I llama_new_context_with_model: flash_attn    = 0
0.00.053.248 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.248 I llama_new_context_with_model: freq_scale    = 1
0.00.053.249 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.249 I ggml_metal_init: allocating
0.00.053.255 I ggml_metal_init: found device: Apple M4
0.00.053.257 I ggml_metal_init: picking default device: Apple M4
0.00.053.801 I ggml_metal_init: using embedded metal library
0.00.056.178 I ggml_metal_init: GPU name:   Apple M4
0.00.056.179 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.180 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.180 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.182 I ggml_metal_init: simdgroup reduction   = true
0.00.056.182 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.182 I ggml_metal_init: has bfloat            = true
0.00.056.184 I ggml_metal_init: use bfloat            = true
0.00.056.184 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.190 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.541 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.066.767 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.769 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.782 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.584 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.585 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.586 I llama_new_context_with_model: graph nodes  = 967
0.00.067.586 I llama_new_context_with_model: graph splits = 2
0.00.067.593 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.594 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.565.569 I 
0.00.565.609 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.565.621 I perplexity: tokenizing the input ..
0.00.573.554 I perplexity: tokenization took 7.932 ms
0.00.573.559 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.707.946 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.709.123 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.709.137 I llama_perf_context_print:        load time =     556.66 ms
0.00.709.138 I llama_perf_context_print: prompt eval time =     134.16 ms /   128 tokens (    1.05 ms per token,   954.08 tokens per second)
0.00.709.139 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.709.139 I llama_perf_context_print:       total time =     143.57 ms /   129 tokens
0.00.709.590 I ggml_metal_free: deallocating

real	0m0.724s
user	0m0.078s
sys	0m0.101s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.087 I build: 4370 (0ca416c9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.803 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.661 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.666 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.667 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.668 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.668 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.669 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.669 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.670 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.670 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.670 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.671 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.671 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.672 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.672 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.676 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.677 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.677 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.588 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.683 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.608 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.609 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.609 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.610 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.610 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.610 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.611 I llama_model_loader: - type  f32:  194 tensors
0.00.024.611 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.611 I llama_model_loader: - type q6_K:   37 tensors
0.00.045.685 I llm_load_vocab: special tokens cache size = 25
0.00.051.513 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.515 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.516 I llm_load_print_meta: arch             = gptneox
0.00.051.516 I llm_load_print_meta: vocab type       = BPE
0.00.051.517 I llm_load_print_meta: n_vocab          = 50304
0.00.051.517 I llm_load_print_meta: n_merges         = 50009
0.00.051.517 I llm_load_print_meta: vocab_only       = 0
0.00.051.517 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.517 I llm_load_print_meta: n_embd           = 2048
0.00.051.517 I llm_load_print_meta: n_layer          = 24
0.00.051.520 I llm_load_print_meta: n_head           = 16
0.00.051.521 I llm_load_print_meta: n_head_kv        = 16
0.00.051.521 I llm_load_print_meta: n_rot            = 32
0.00.051.521 I llm_load_print_meta: n_swa            = 0
0.00.051.521 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.522 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.522 I llm_load_print_meta: n_gqa            = 1
0.00.051.523 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.524 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.524 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.525 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.525 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.525 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.525 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.526 I llm_load_print_meta: n_ff             = 8192
0.00.051.526 I llm_load_print_meta: n_expert         = 0
0.00.051.526 I llm_load_print_meta: n_expert_used    = 0
0.00.051.526 I llm_load_print_meta: causal attn      = 1
0.00.051.526 I llm_load_print_meta: pooling type     = 0
0.00.051.527 I llm_load_print_meta: rope type        = 2
0.00.051.527 I llm_load_print_meta: rope scaling     = linear
0.00.051.528 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.530 I llm_load_print_meta: freq_scale_train = 1
0.00.051.530 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.530 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.531 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.531 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.531 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.531 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.531 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.531 I llm_load_print_meta: model type       = 1.4B
0.00.051.532 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.051.532 I llm_load_print_meta: model params     = 1.41 B
0.00.051.533 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.051.533 I llm_load_print_meta: general.name     = 1.4B
0.00.051.533 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.533 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.535 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.536 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.536 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.536 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.537 I llm_load_print_meta: max token length = 1024
0.00.053.629 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.630 I llm_load_tensors: offloading output layer to GPU
0.00.053.630 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.641 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.053.642 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.054.541 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.542 I llama_new_context_with_model: n_ctx         = 128
0.00.054.542 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.542 I llama_new_context_with_model: n_batch       = 128
0.00.054.542 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.542 I llama_new_context_with_model: flash_attn    = 0
0.00.054.543 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.543 I llama_new_context_with_model: freq_scale    = 1
0.00.054.543 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.544 I ggml_metal_init: allocating
0.00.054.547 I ggml_metal_init: found device: Apple M4
0.00.054.549 I ggml_metal_init: picking default device: Apple M4
0.00.055.104 I ggml_metal_init: using embedded metal library
0.00.057.455 I ggml_metal_init: GPU name:   Apple M4
0.00.057.456 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.456 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.457 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.457 I ggml_metal_init: simdgroup reduction   = true
0.00.057.457 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.457 I ggml_metal_init: has bfloat            = true
0.00.057.458 I ggml_metal_init: use bfloat            = true
0.00.057.458 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.459 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.320 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.068.585 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.588 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.602 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.069.518 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.069.519 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.069.519 I llama_new_context_with_model: graph nodes  = 967
0.00.069.520 I llama_new_context_with_model: graph splits = 2
0.00.069.532 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.069.533 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.655.618 I 
0.00.655.661 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.655.673 I perplexity: tokenizing the input ..
0.00.663.910 I perplexity: tokenization took 8.235 ms
0.00.663.914 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.803.973 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.805.390 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.805.404 I llama_perf_context_print:        load time =     645.81 ms
0.00.805.408 I llama_perf_context_print: prompt eval time =     139.82 ms /   128 tokens (    1.09 ms per token,   915.44 tokens per second)
0.00.805.408 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.805.409 I llama_perf_context_print:       total time =     149.79 ms /   129 tokens
0.00.805.740 I ggml_metal_free: deallocating

real	0m0.821s
user	0m0.079s
sys	0m0.120s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.101 I build: 4370 (0ca416c9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.962 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.855 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.014.859 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.860 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.861 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.861 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.862 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.862 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.863 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.863 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.863 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.864 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.864 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.864 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.865 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.866 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.867 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.867 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.802 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.873 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.835 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.836 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.836 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.836 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.837 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.837 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.023.837 I llama_model_loader: - type  f32:  194 tensors
0.00.023.838 I llama_model_loader: - type q6_K:   98 tensors
0.00.045.491 I llm_load_vocab: special tokens cache size = 25
0.00.051.392 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.398 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.398 I llm_load_print_meta: arch             = gptneox
0.00.051.398 I llm_load_print_meta: vocab type       = BPE
0.00.051.399 I llm_load_print_meta: n_vocab          = 50304
0.00.051.399 I llm_load_print_meta: n_merges         = 50009
0.00.051.399 I llm_load_print_meta: vocab_only       = 0
0.00.051.399 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.399 I llm_load_print_meta: n_embd           = 2048
0.00.051.399 I llm_load_print_meta: n_layer          = 24
0.00.051.404 I llm_load_print_meta: n_head           = 16
0.00.051.404 I llm_load_print_meta: n_head_kv        = 16
0.00.051.405 I llm_load_print_meta: n_rot            = 32
0.00.051.405 I llm_load_print_meta: n_swa            = 0
0.00.051.407 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.407 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.407 I llm_load_print_meta: n_gqa            = 1
0.00.051.408 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.409 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.409 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.409 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.410 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.410 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.410 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.410 I llm_load_print_meta: n_ff             = 8192
0.00.051.411 I llm_load_print_meta: n_expert         = 0
0.00.051.411 I llm_load_print_meta: n_expert_used    = 0
0.00.051.411 I llm_load_print_meta: causal attn      = 1
0.00.051.411 I llm_load_print_meta: pooling type     = 0
0.00.051.412 I llm_load_print_meta: rope type        = 2
0.00.051.416 I llm_load_print_meta: rope scaling     = linear
0.00.051.445 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.447 I llm_load_print_meta: freq_scale_train = 1
0.00.051.448 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.448 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.448 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.448 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.448 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.448 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.448 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.448 I llm_load_print_meta: model type       = 1.4B
0.00.051.449 I llm_load_print_meta: model ftype      = Q6_K
0.00.051.449 I llm_load_print_meta: model params     = 1.41 B
0.00.051.449 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.051.450 I llm_load_print_meta: general.name     = 1.4B
0.00.051.450 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.451 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.452 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.452 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.452 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.452 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.452 I llm_load_print_meta: max token length = 1024
0.00.053.572 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.573 I llm_load_tensors: offloading output layer to GPU
0.00.053.573 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.584 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.053.585 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.054.463 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.463 I llama_new_context_with_model: n_ctx         = 128
0.00.054.464 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.464 I llama_new_context_with_model: n_batch       = 128
0.00.054.464 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.464 I llama_new_context_with_model: flash_attn    = 0
0.00.054.465 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.465 I llama_new_context_with_model: freq_scale    = 1
0.00.054.466 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.466 I ggml_metal_init: allocating
0.00.054.471 I ggml_metal_init: found device: Apple M4
0.00.054.473 I ggml_metal_init: picking default device: Apple M4
0.00.055.076 I ggml_metal_init: using embedded metal library
0.00.057.468 I ggml_metal_init: GPU name:   Apple M4
0.00.057.470 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.470 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.471 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.471 I ggml_metal_init: simdgroup reduction   = true
0.00.057.471 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.471 I ggml_metal_init: has bfloat            = true
0.00.057.472 I ggml_metal_init: use bfloat            = true
0.00.057.472 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.473 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.562 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.068.849 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.851 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.874 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.069.760 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.069.761 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.069.761 I llama_new_context_with_model: graph nodes  = 967
0.00.069.762 I llama_new_context_with_model: graph splits = 2
0.00.069.775 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.069.776 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.368.821 I 
0.00.368.863 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.368.875 I perplexity: tokenizing the input ..
0.00.376.376 I perplexity: tokenization took 7.498 ms
0.00.376.380 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.515.707 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.517.277 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.517.290 I llama_perf_context_print:        load time =     359.85 ms
0.00.517.292 I llama_perf_context_print: prompt eval time =     139.09 ms /   128 tokens (    1.09 ms per token,   920.23 tokens per second)
0.00.517.293 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.517.293 I llama_perf_context_print:       total time =     148.47 ms /   129 tokens
0.00.517.692 I ggml_metal_free: deallocating

real	0m0.533s
user	0m0.080s
sys	0m0.059s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.375 I build: 4370 (0ca416c9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.023.449 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.035.517 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.035.527 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.035.530 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.035.531 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.035.532 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.035.533 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.035.533 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.035.535 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.035.535 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.035.537 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.035.537 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.035.538 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.035.539 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.035.540 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.035.543 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.035.544 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.035.544 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.044.425 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.046.648 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.053.817 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.053.818 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.053.819 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.053.819 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.053.820 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.053.820 I llama_model_loader: - type  f32:  194 tensors
0.00.053.821 I llama_model_loader: - type  f16:   98 tensors
0.00.083.020 I llm_load_vocab: special tokens cache size = 25
0.00.089.851 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.089.854 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.089.854 I llm_load_print_meta: arch             = gptneox
0.00.089.855 I llm_load_print_meta: vocab type       = BPE
0.00.089.855 I llm_load_print_meta: n_vocab          = 50304
0.00.089.855 I llm_load_print_meta: n_merges         = 50009
0.00.089.855 I llm_load_print_meta: vocab_only       = 0
0.00.089.855 I llm_load_print_meta: n_ctx_train      = 2048
0.00.089.855 I llm_load_print_meta: n_embd           = 2048
0.00.089.855 I llm_load_print_meta: n_layer          = 24
0.00.089.859 I llm_load_print_meta: n_head           = 16
0.00.089.860 I llm_load_print_meta: n_head_kv        = 16
0.00.089.860 I llm_load_print_meta: n_rot            = 32
0.00.089.860 I llm_load_print_meta: n_swa            = 0
0.00.089.861 I llm_load_print_meta: n_embd_head_k    = 128
0.00.089.861 I llm_load_print_meta: n_embd_head_v    = 128
0.00.089.861 I llm_load_print_meta: n_gqa            = 1
0.00.089.862 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.089.863 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.089.863 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.089.864 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.089.864 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.089.864 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.089.864 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.089.867 I llm_load_print_meta: n_ff             = 8192
0.00.089.867 I llm_load_print_meta: n_expert         = 0
0.00.089.867 I llm_load_print_meta: n_expert_used    = 0
0.00.089.868 I llm_load_print_meta: causal attn      = 1
0.00.089.868 I llm_load_print_meta: pooling type     = 0
0.00.089.868 I llm_load_print_meta: rope type        = 2
0.00.089.868 I llm_load_print_meta: rope scaling     = linear
0.00.089.868 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.089.868 I llm_load_print_meta: freq_scale_train = 1
0.00.089.869 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.089.869 I llm_load_print_meta: rope_finetuned   = unknown
0.00.089.869 I llm_load_print_meta: ssm_d_conv       = 0
0.00.089.869 I llm_load_print_meta: ssm_d_inner      = 0
0.00.089.869 I llm_load_print_meta: ssm_d_state      = 0
0.00.089.869 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.089.869 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.089.870 I llm_load_print_meta: model type       = 1.4B
0.00.089.870 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.089.872 I llm_load_print_meta: model params     = 1.41 B
0.00.089.872 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.089.872 I llm_load_print_meta: general.name     = 1.4B
0.00.089.872 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.089.873 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.089.874 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.089.874 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.089.874 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.089.875 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.089.875 I llm_load_print_meta: max token length = 1024
0.00.092.513 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.092.514 I llm_load_tensors: offloading output layer to GPU
0.00.092.514 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.092.524 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.092.525 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.093.466 I llama_new_context_with_model: n_seq_max     = 1
0.00.093.467 I llama_new_context_with_model: n_ctx         = 128
0.00.093.467 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.093.467 I llama_new_context_with_model: n_batch       = 128
0.00.093.467 I llama_new_context_with_model: n_ubatch      = 128
0.00.093.468 I llama_new_context_with_model: flash_attn    = 0
0.00.093.468 I llama_new_context_with_model: freq_base     = 10000.0
0.00.093.468 I llama_new_context_with_model: freq_scale    = 1
0.00.093.469 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.093.469 I ggml_metal_init: allocating
0.00.093.478 I ggml_metal_init: found device: Apple M4
0.00.093.481 I ggml_metal_init: picking default device: Apple M4
0.00.094.135 I ggml_metal_init: using embedded metal library
0.00.096.712 I ggml_metal_init: GPU name:   Apple M4
0.00.096.713 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.096.714 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.096.714 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.096.714 I ggml_metal_init: simdgroup reduction   = true
0.00.096.715 I ggml_metal_init: simdgroup matrix mul. = true
0.00.096.715 I ggml_metal_init: has bfloat            = true
0.00.096.715 I ggml_metal_init: use bfloat            = true
0.00.096.715 I ggml_metal_init: hasUnifiedMemory      = true
0.00.096.716 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.105.968 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.107.512 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.107.514 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.107.528 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.108.444 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.108.445 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.108.445 I llama_new_context_with_model: graph nodes  = 967
0.00.108.446 I llama_new_context_with_model: graph splits = 2
0.00.108.459 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.108.460 I 
0.00.108.493 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.108.494 I compute_imatrix: tokenizing the input ..
0.00.115.467 I compute_imatrix: tokenization took 6.973 ms
0.00.115.469 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.643.483 I compute_imatrix: 1.53 seconds per pass - ETA 0.02 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.646.333 I llama_perf_context_print:        load time =    1620.03 ms
0.01.646.334 I llama_perf_context_print: prompt eval time =    1527.34 ms /   128 tokens (   11.93 ms per token,    83.81 tokens per second)
0.01.646.335 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.646.336 I llama_perf_context_print:       total time =    1622.87 ms /   129 tokens
0.01.647.032 I ggml_metal_free: deallocating

real	0m1.835s
user	0m0.172s
sys	0m0.255s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4370 (0ca416c9)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 'Ä'
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12d10a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12d10a990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12d10af40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12d10b4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12d10baa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12d10c050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12d10c600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12d10cbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12d10d160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12d10d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12d10db60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12d10e060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12d10eb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12d10f330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12d10fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12d110260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12d110980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12d1110a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12d1117c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12d111f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12d1126b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12d112dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12d1134f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12d113d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12d1144b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12d114770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12d114d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12d1159f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12d115f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12d1161f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12d116690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12d116950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12d1171e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12d117720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12d1179e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12d117e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12d118320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12d1187c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12d118c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12d119100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12d1195a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12d119a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12d119ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12d11a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12d11a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12d11ac50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12d11b260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12d11bb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12d11c190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12d11c7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12d11cdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12d11d3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12d11d9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12d11dfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12d11e7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12d11ec70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12d11f110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12d11f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12d11f9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12d1201d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12d120490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12d120930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12d120dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12d121270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12d121710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12d121bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12d122050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12d1224f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12d122990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12d122e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12d1232d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12d123770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12d123c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12d124160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12d1246b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12d124c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12d125150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12d1256a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12d125bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12d126140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12d126690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12d126be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12d127130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12d127680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12d127bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12d128120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12d128670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12d128bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12d129110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12d129660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12d129bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12d12a100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12d12a650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12d12aba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12d12b0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12d12b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12d12bb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12d11b870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12d12c000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12d12c7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12d12cd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12d12d250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12d12d7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12d12dcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12d12e240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12d12e790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12d12ece0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12d12f230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12d12f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12d12fcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12d130220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12d130770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12d130cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12d131160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12d131600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12d131aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12d131f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12d1323e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12d132880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12d132d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12d1331c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12d133660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12d133b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12d133fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12d134440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12d1348e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12d134d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12d135220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12d1356c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12d135b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12d136000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12d1364a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12d136940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12d136de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12d137280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12d137720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12d137bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12d138060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12d138500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12d1389a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12d138e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12d1392e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12d139780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12d139c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12d13a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12d13a560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12d13aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12d13aea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12d13b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12d13b7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12d13bc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12d13c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12d13c5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12d13ca60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12d13cf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12d13d3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12d13d840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12d13dce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12d13e180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12d13e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12d13eac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12d13ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12d13f400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12d13f8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12d13fd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12d1401e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12d140680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12d140b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12d140fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12d141460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12d141900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12d141da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12d142240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12d1426e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12d142b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12d143020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12d1434c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12d143960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12d143e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12d1442a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12d144740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12d144be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12d145080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12d145520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12d1459c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12d145e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12d146300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12d1467a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12d146c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12d1470e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12d147580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12d147a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12d147ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12d148410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12d148960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12d148eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12d149400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12d1496c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12d149cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12d14a2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12d14a8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12d14b0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12d14b580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12d14b840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12d14be50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12d14c460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12d14cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12d14d0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12d14d590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12d14da30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12d14e1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12d14e730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12d14ec80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12d14f1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12d14f720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12d14fc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12d1501c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12d150710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12d150c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12d1511b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12d151700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12d151c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12d1521a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12d1526f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12d152c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12d153190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12d1536e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12d153c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12d154180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12d1546d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12d154c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12d155170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12d1556c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12d155c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12d156160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12d1566b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12d156c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12d157150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12d1576a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12d157bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12d158140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12d158690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12d158be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12d159130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12d159680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12d159bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12d15a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12d15a670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12d15abc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12d15b110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12d15b660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12d15bbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12d15c100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12d15c650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12d15cba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12d15d0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12d15d640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12d15db90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12d15e0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12d15e630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12d15eb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12d15f0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12d15f620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12d15fb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12d1600c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12d160610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12d160b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12d161000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12d1614a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12d161940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12d161de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12d162280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12d162720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12d162bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12d163060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12d163500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12d1639a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12d163e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12d1642e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12d164780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12d164c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12d1650c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12d165610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12d165d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12d166450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12d166b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12d167290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12d167550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12d167d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12d168000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12d168610 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
0.00.144.115 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.144.118 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x10e004dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x10e005230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x10e0056a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x10e005b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x10e005f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x10e0063f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x10e006860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x10e006cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x10e007140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x10e0075b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x10e007a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x10e0080e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x10e008c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x10e0093b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x10e009bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x10e00a2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x10e00aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x10e00b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x10e00b840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x10e00c010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x10e00c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x10e00ce50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x10e00d570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x10e00dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x10e00e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x10e00e670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x10e00e930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x10e00eda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x10e00f210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x10e00f680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x10e00faf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x10e010020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x10e010490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x10e010750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x10e010bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x10e011030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x10e0114a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x10e011910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x10e011d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x10e0121f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x10e012660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x10e012ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x10e012f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x10e0133b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x10e013820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x10e013c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x10e014100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x10e014570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x10e0149e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x10e014e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x10e0152c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x10e015730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x10e015ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x10e016010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x10e016480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x10e0168f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x10e016e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x10e017360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x10e0177d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x10e017c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x10e0180b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x10e018520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x10e018990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x10e018e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x10e019270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x10e0196e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x10e019b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x10e019fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x10e01a430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x10e01a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x10e01ad10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x10e01b180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x10e01b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x10e01ba60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x10e01bed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x10e01c340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x10e01c7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x10e01cc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x10e01d090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x10e01d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x10e01d970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x10e01dde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x10e01e250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x10e01e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x10e01eb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x10e01efa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x10e01f410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x10e01f880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x10e01fcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x10e020160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x10e0205d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x10e020a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x10e020eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x10e021320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x10e021790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x10e021c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x10e022070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x10e0224e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x10e022950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x10e022dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x10e023230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x10e0236a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x10e023b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x10e023f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x10e0243f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x10e024860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x10e024cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x10e025140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x10e0255b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x10e025a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x10e025e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x10e026300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x10e026770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x10e026be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x10e027050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x10e0274c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x10e027930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x10e027da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x10e028210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x10e028680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x10e028af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x10e028f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x10e0293d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x10e029840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x10e029cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x10e02a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x10e02a590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x10e02aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x10e02ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x10e02b2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x10e02b750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x10e02bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x10e02c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x10e02c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x10e02c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x10e02cd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x10e02d1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x10e02d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x10e02dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x10e02df40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x10e02e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x10e02e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x10e02ec90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x10e02f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x10e02f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x10e02f9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x10e02fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x10e0302c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x10e030730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x10e030ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x10e031010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x10e031480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x10e0318f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x10e031d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x10e0321d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x10e032640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x10e032ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x10e032f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x10e033390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x10e033800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x10e033c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x10e0340e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x10e034550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x10e0349c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x10e034e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x10e0352a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x10e035710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x10e035b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x10e035ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x10e036460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x10e0368d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x10e036d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x10e0371b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x10e037620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x10e037a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x10e037f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x10e038370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x10e704230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x10e7046a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x10e704b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x10e704f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x10e7053f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x10e705860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x10e705cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x10e706140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x10e7065b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x10e706a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x10e706e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x10e707300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x10e707770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x10e707be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x10e708050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x10e7084c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x10e708930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x10e708da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x10e709210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x10e709680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x10e709af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x10e709f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x10e70a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x10e70a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x10e70acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x10e70b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x10e70b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x10e70ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x10e70be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x10e70c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x10e70c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x10e70cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x10e70d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x10e70dbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x10e70de90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x10e70e150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x10e70e5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x10e70ea30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x10e70eea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x10e70f310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x10e70f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x10e70fbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x10e710060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x10e7104d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x10e710940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x10e710db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x10e711220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x10e711690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x10e711b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x10e711f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x10e7123e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x10e712850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x10e712cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x10e713130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x10e7135a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x10e713a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x10e713e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x10e7142f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x10e714760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x10e714bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x10e715040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x10e7154b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x10e715920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x10e715d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x10e716200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x10e716670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x10e716ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x10e716f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x10e7173c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x10e717830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x10e717ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x10e718110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x10e718580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x10e7189f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x10e718e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x10e7192d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x10e719740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x10e719bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x10e71a020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x10e71a490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x10e71a900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x10e71ad70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x10e71b1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x10e71b650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x10e71bac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x10e71bf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x10e71c3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x10e71c810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x10e71cc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x10e71d0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x10e71d560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x10e71d9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x10e71de40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x10e71e2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x10e71e720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x10e71eb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x10e71f000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x10e71f470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x10e71f8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x10e71fd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x10e7201c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x10e720630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x10e720aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x10e720f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x10e721380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x10e7217f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x10e722260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x10e722980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x10e7230a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x10e7237c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x10e723a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x10e723ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x10e7244f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x10e724b00 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x10e004ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x10e005150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x10e0055c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x10e005a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x10e005ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x10e006310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x10e006780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x10e006bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x10e007060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x10e0074d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x10e007940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x10e007f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x10e008810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x10e008f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x10e009770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x10e009e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x10e00a550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x10e00ac40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x10e00b330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x10e00bcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x10e00c3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x10e00ca90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x10e00d180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x10e00d870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x10e00df60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x10e00e3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x10e00e840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x10e00ecb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x10e00f120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x10e00f590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x10e00fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x10e00fe70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x10e0102e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x10e0105a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x10e010a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x10e010e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x10e0112f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x10e011760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x10e011bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x10e012040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x10e0124b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x10e012920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x10e012d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x10e013200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x10e013670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x10e013ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x10e013f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x10e0143c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x10e014830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x10e014ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x10e015110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x10e015580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x10e0159f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x10e015e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x10e0162d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x10e016740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x10e016bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x10e017020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x10e017490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x10e017900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x10e017d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x10e0181e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x10e018650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x10e018ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x10e018f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x10e0193a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x10e019810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x10e019c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x10e01a0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x10e01a560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x10e01a9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x10e01ae40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x10e01b2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x10e01b720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x10e01bb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x10e01c000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x10e01c470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x10e01c8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x10e01cd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x10e01d1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x10e01d630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x10e01daa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x10e01df10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x10e01e380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x10e01e7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x10e01ec60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x10e01f0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x10e01f540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x10e01f9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x10e01fe20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x10e020290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x10e020700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x10e020b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x10e020fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x10e021450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x10e0218c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x10e021d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x10e0221a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x10e022610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x10e022a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x10e022ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x10e023360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x10e0237d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x10e023c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x10e0240b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x10e024520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x10e024990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x10e024e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x10e025270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x10e0256e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x10e025b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x10e025fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x10e026430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x10e0268a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x10e026d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x10e027180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x10e0275f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x10e027a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x10e027ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x10e028340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x10e0287b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x10e028c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x10e029090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x10e029500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x10e029970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x10e029de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x10e02a250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x10e02a6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x10e02ab30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x10e02afa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x10e02b410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x10e02b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x10e02bcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x10e02c160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x10e02c5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x10e02ca40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x10e02ceb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x10e02d320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x10e02d790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x10e02dc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x10e02e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x10e02e4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x10e02e950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x10e02edc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x10e02f230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x10e02f6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x10e02fb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x10e02ff80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x10e0303f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x10e030860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x10e030cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x10e031140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x10e0315b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x10e031a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x10e031e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x10e032300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x10e032770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x10e032be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x10e033050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x10e0334c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x10e033930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x10e033da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x10e034210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x10e034680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x10e034af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x10e034f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x10e0353d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x10e035840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x10e035cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x10e036120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x10e036590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x10e036a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x10e036e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x10e0372e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x10e037750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x10e037bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x10e038030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x10e0384a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x10e038910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x10e038e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x10e0393a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x10e0398b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x10e039dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x10e03a2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x10e03a7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x10e03acf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x10e03b200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x10e03b710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x10e03bc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x10e03c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x10e03c640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x10e03cb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x10e03d060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x10e03d800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x10e03dd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x10e03e2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x10e03e7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x10e03eab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x10e03f0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x10e03f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x10e03fce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x10e0404d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x10e040970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x10e040c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x10e041240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x10e041850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x10e042040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x10e0424e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x10e042980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x10e042e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x10e0435d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x10e043b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x10e044070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x10e0445c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x10e044b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x10e045060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x10e0455b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x10e045b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x10e046050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x10e0465a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x10e046af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x10e047040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x10e047590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x10e047ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x10e048030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x10e048580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x10e048ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x10e049020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x10e049570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x10e049ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x10e04a010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x10e04a560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x10e04aab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x10e04b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x10e04b550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x10e04baa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x10e04bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x10e04c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x10e04ca90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x10e04cfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x10e04d530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x10e04da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x10e04dfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x10e04e520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x10e04ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x10e04efc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x10e04f510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x10e04fa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x10e04ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x10e050500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x10e050a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x10e050fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x10e0514f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x10e051a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x10e051f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x10e0524e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x10e052a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x10e052f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x10e0534d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x10e053a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x10e053f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x10e0544c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x10e054a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x10e054f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x10e0554b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x10e055a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x10e055f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x10e0563f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x10e056890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x10e056d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x10e0571d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x10e057670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x10e057b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x10e057fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x10e058450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x10e0588f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x10e058d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x10e059230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x10e0596d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x10e059b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x10e05a010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x10e05a4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x10e05aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x10e05b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x10e05b840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x10e05bf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x10e05c680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x10e05c940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x10e05d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x10e05d3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x10e05da00 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.796s
user	0m0.292s
sys	0m0.298s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4370 (0ca416c9)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 'Ä'
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x135f103e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x135f10af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x135f110a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x135f11650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x135f11c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x135f121b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x135f12760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x135f12d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x135f132c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x135f137c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x135f13cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x135f141c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x135f14ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x135f15490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x135f15ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x135f163c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x135f16ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x135f17200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x135f17920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x135f180f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x135f18810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x135f18f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x135f19650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x135f19ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x135f1a610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x135f1a8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x135f1aee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x135f1bb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x135f1c090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x135f1c350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x135f1c7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x135f1cab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x135f1d340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x135f1d880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x135f1db40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x135f1dfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x135f1e480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x135f1e920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x135f1edc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x135f1f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x135f1f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x135f1fba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x135f20040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x135f204e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x135f207a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x135f20db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x135f213c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x135f21ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x135f222f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x135f22900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x135f22f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x135f23520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x135f23b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x135f24140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x135f24930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x135f24dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x135f25270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x135f25530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x135f25b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x135f26330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x135f265f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x135f26a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x135f26f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x135f273d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x135f27870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x135f27d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x135f281b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x135f28650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x135f28af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x135f28f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x135f29430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x135f298d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x135f29d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x135f2a2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x135f2a810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x135f2ad60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x135f2b2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x135f2b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x135f2bd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x135f2c2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x135f2c7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x135f2cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x135f2d290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x135f2d7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x135f2dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x135f2e280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x135f2e7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x135f2ed20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x135f2f270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x135f2f7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x135f2fd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x135f30260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x135f307b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x135f30d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x135f31250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x135f317a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x135f31cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x135f219d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x135f32160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x135f32910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x135f32e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x135f333b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x135f33900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x135f33e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x135f343a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x135f348f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x135f34e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x135f35390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x135f358e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x135f35e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x135f36380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x135f368d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x135f36e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x135f372c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x135f37760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x135f37c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x135f380a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x135f38540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x135f389e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x135f38e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x135f39320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x135f397c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x135f39c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x135f3a100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x135f3a5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x135f3aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x135f3aee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x135f3b380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x135f3b820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x135f3bcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x135f3c160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x135f3c600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x135f3caa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x135f3cf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x135f3d3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x135f3d880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x135f3dd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x135f3e1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x135f3e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x135f3eb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x135f3efa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x135f3f440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x135f3f8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x135f3fd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x135f40220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x135f406c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x135f40b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x135f41000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x135f414a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x135f41940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x135f41de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x135f42280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x135f42720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x135f42bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x135f43060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x135f43500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x135f439a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x135f43e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x135f442e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x135f44780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x135f44c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x135f450c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x135f45560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x135f45a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x135f45ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x135f46340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x135f467e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x135f46c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x135f47120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x135f475c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x135f47a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x135f47f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x135f483a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x135f48840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x135f48ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x135f49180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x135f49620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x135f49ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x135f49f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x135f4a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x135f4a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x135f4ad40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x135f4b1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x135f4b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x135f4bb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x135f4bfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x135f4c460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x135f4c900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x135f4cda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x135f4d240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x135f4d6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x135f4db80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x135f4e020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x135f4e570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x135f4eac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x135f4f010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x135f4f560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x135f4f820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x135f4fe30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x135f50440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x135f50a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x135f51240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x135f516e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x135f519a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x135f51fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x135f525c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x135f52db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x135f53250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x135f536f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x135f53b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x135f54340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x135f54890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x135f54de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x135f55330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x135f55880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x135f55dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x135f56320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x135f56870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x135f56dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x135f57310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x135f57860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x135f57db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x135f58300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x135f58850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x135f58da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x135f592f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x135f59840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x135f59d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x135f5a2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x135f5a830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x135f5ad80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x135f5b2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x135f5b820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x135f5bd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x135f5c2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x135f5c810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x135f5cd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x135f5d2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x135f5d800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x135f5dd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x135f5e2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x135f5e7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x135f5ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x135f5f290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x135f5f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x135f5fd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x135f60280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x135f607d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x135f60d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x135f61270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x135f617c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x135f61d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x135f62260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x135f627b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x135f62d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x135f63250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x135f637a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x135f63cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x135f64240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x135f64790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x135f64ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x135f65230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x135f65780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x135f65cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x135f66220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x135f66770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x135f66cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x135f67160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x135f67600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x135f67aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x135f67f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x135f683e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x135f68880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x135f68d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x135f691c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x135f69660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x135f69b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x135f69fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x135f6a440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x135f6a8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x135f6ad80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x135f6b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x135f6b770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x135f6be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x135f6c5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x135f6ccd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x135f6d3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x135f6d6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x135f6dea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x135f6e160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x135f6e770 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
0.00.087.979 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.982 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x135f11a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x135f11ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x135f12330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x135f127a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x135f12c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x135f13080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x135f0fa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x135f10590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x135f2ae80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x135f2b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x135f2b760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x135f2bbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x135f2c350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x135f2cad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x135f2d2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x135f2d9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x135f2e090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x135f2e780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x135f2ee70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x135f2f7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x135f2fee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x135f305d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x135f30cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x135f313b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x135f31aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x135f31f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x135f32380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x135f327f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x135f32c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x135f330d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x135f33540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x135f339b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x135f33e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x135f340e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x135f34550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x135f349c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x135f34e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x135f352a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x135f35710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x135f35b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x135f35ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x135f36460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x135f368d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x135f36d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x135f371b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x135f37620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x135f37a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x135f37f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x135f38370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x135f387e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x135f38c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x135f390c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x135f39530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x135f399a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x135f39e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x135f3a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x135f3a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x135f3ab60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x135f3afd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x135f3b440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x135f3b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x135f3bd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x135f3c190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x135f3c600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x135f3ca70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x135f3cee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x135f3d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x135f3d7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x135f3dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x135f3e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x135f3e510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x135f3e980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x135f3edf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x135f3f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x135f3f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x135f3fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x135f3ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x135f40420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x135f40890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x135f40d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x135f41170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x135f415e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x135f41a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x135f41ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x135f42330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x135f427a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x135f42c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x135f43080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x135f434f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x135f43960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x135f43dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x135f44240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x135f446b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x135f44b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x135f44f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x135f45400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x135f45870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x135f45ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x135f46150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x135f465c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x135f46a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x135f46ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x135f47310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x135f47780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x135f47bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x135f48060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x135f484d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x135f48940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x135f48db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x135f49220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x135f49690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x135f49b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x135f49f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x135f4a3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x135f4a850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x135f4acc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x135f4b130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x135f4b5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x135f4ba10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x135f4be80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x135f4c2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x135f4c760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x135f4cbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x135f4d040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x135f4d4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x135f4d920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x135f4dd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x135f4e200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x135f4e670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x135f4eae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x135f4ef50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x135f4f3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x135f4f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x135f4fca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x135f50110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x135f50580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x135f509f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x135f50e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x135f512d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x135f51740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x135f51bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x135f52020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x135f52490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x135f52900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x135f52d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x135f531e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x135f53650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x135f53ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x135f53f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x135f543a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x135f54810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x135f54c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x135f550f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x135f55560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x135f559d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x135f55e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x135f562b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x135f56720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x135f56b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x135f57000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x135f57470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x135f578e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x135f57d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x135f581c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x135f58630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x135f58aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x135f58f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x135f59380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x135f597f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x135f59c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x135f5a0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x135f5a540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x135f5a9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x135f5ae20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x135f5b290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x135f5b700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x135f5bb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x135f5bfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x135f5c450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x135f5c8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x135f5cd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x135f5d1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x135f5d610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x135f5da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x135f5def0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x135f5e360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x135f5e7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x135f5ec40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x135f5f0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x135f5f520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x135f5f990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x135f5fe00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x135f60270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x135f606e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x135f60b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x135f60fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x135f61430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x135f618a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x135f61d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x135f62180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x135f625f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x135f62a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x135f62ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x135f63340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x135f637b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x135f63c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x135f64090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x135f64500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x135f64970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x135f64de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x135f65560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x135f659d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x135f65e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x135f662b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x135f66720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x135f66b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x135f67000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x135f67470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x135f678e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x135f67d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x135f681c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x135f68630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x135f68aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x135f68f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x135f69380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x135f697f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x135f69c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x135f6a0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x135f6a540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x135f6a9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x135f6ae20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x135f6b290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x135f6b700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x135f6bb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x135f6bfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x135f6c450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x135f6c8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x135f6cd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x135f6d1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x135f6d610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x135f6da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x135f6def0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x135f6e360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x135f6e7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x135f1da20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x135f1de90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x135f1e300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x135f1e770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x135f1ebe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x135f1f050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x135f1f4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x135f1f930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x135f1fda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x135f20210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x135f20680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x135f20af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x135f20f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x135f213d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x135f21840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x135f21cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x135f22120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x135f22590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x135f22a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x135f22e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x135f232e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x135f23750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x135f23bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x135f24030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x135f244a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x135f24910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x135f24d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x135f251f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x135f25660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x135f25ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x135f25f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x135f263b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x135f26820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x135f26c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x135f27100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x135f27570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x135f279e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x135f27e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x135f282c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x135f289b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x135f290a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x135f29790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x135f29e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x135f2a2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x135f2a760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x135f1c230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x135f1c6a0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x1706044e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x170604950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x170604dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x170605230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x1706056a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x170605b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x170605f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x1706063f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x170606860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x170606cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x170607140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x170607860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x170608380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x170608b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x170609340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x170609a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x17060a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x17060a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x17060afc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x17060b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x17060be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x17060c530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x17060cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x17060d370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x17060da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x17060dd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x17060e010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x17060e480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x17060e8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x17060ed60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x17060f1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x17060f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x17060fb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x17060fe30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1706102a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x170610710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x170610b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x170610ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x170611460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x1706118d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x170611d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x1706121b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x170612620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x170612a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x170612f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x170613370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x1706137e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x170613c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x1706140c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x170614530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x1706149a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x170614e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x170615280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x1706156f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x170615b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x170615fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x170616540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x170616a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x170616eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x170617320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x170617790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x170617c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x170618070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x1706184e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x170618950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x170618dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x170619230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x1706196a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x170619b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x170619f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x17061a3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x17061a860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x17061acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x17061b140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x17061b5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x17061ba20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x17061be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x17061c300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x17061c770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x17061cbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x17061d050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x17061d4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x17061d930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x17061dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x17061e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x17061e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x17061eaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x17061ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x17061f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x17061f840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x17061fcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x170620120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x170620590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x170620a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x170620e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x1706212e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x170621750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x170621bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x170622030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x1706224a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x170622910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x170622d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x1706231f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x170623660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x170623ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x170623f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x1706243b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x170624820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x170624c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x170625100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x170625570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x1706259e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x170625e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x1706262c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x170626730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x170626ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x170627010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x170627480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x1706278f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x170627d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x1706281d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x170628640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x170628ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x170628f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x170629390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x170629800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x170629c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x17062a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x17062a550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x17062a9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x17062ae30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x17062b2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x17062b710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x17062bb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x17062bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x17062c460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x17062c8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x17062cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x17062d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x17062d620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x17062da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x17062df00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x17062e370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x17062e7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x17062ec50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x17062f0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x17062f530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x17062f9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x17062fe10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x170630280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x1706306f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x170630b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x170630fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x170631440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x1706318b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x170631d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x170632190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x170632600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x170632a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x170632ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x170633350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x1706337c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x170633c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x1706340a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x170634510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x170634980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x170634df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x170635260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x1706356d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x170635b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x170635fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x170636420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x170636890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x170636d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x170637170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x1706375e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x170637a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x170637ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x170638330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x1706387a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x170638c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x170639080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x1706394f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x170639960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x170639dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x17063a240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x17063a6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x17063ab20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x17063af90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x17063b400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x17063b870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x17063bce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x17063c150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x17063c5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x17063ca30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x17063cea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x17063d310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x17063d780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x17063dbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x17063e060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x17063e4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x17063e940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x17063edb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x17063f220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x17063f690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x17063fb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x17063ff70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x170640500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x170640970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x170640de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x170641930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x170641bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x170641eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x170642320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x170642790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x170642c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x170643070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x1706434e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x170643950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x170643dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x170644230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x1706446a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x170644b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x170644f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x1706453f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x170645860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x170645cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x170646140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x1706465b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x170646a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x170646e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x170647300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x170647770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x170647be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x170648050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x1706484c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x170648930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x170648da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x170649210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x170649680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x170649af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x170649f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x17064a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x17064a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x17064b1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x17064b470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x17064b8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x17064bd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x17064c1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x17064c630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x17064caa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x17064cf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x17064d380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x17064d7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x17064dc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x17064e0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x17064e540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x17064e9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x17064ee20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x17064f290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x17064f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x17064fb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x17064ffe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x170650450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x1706508c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x170650d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x1706511a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x170651610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x170651a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x170651ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x170652360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x1706527d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x170652c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x1706530b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x170653520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x170653990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x170653e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x170654270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x1706546e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x170654b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x170654fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x170655430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x1706558a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x170656310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x170656a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x170657150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x170657870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x170657b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x170657fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1706585a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x170658bb0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.931s
user	0m0.243s
sys	0m0.147s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
