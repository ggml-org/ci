Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:298 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- ARM -mcpu not found, -mcpu=native will be used
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod - Success
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sve
-- Performing Test GGML_MACHINE_SUPPORTS_sve - Failed
-- ARM feature DOTPROD enabled
-- ARM feature MATMUL_INT8 enabled
-- ARM feature FMA enabled
-- ARM feature FP16_VECTOR_ARITHMETIC enabled
-- Adding CPU backend variant ggml-cpu: -mcpu=native+dotprod+i8mm+nosve 
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (2.4s)
-- Generating done (0.2s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m2.655s
user	0m0.906s
sys	0m1.239s
++ nproc
+ make -j10
[  0%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  1%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  1%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  1%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  3%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  3%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  3%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  4%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  5%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  5%] Built target build_info
[  5%] Built target sha256
[  5%] Built target sha1
[  5%] Linking CXX shared library libggml-base.dylib
[  5%] Built target xxhash
[  5%] Built target ggml-base
[  6%] Generate assembly for embedded Metal library
Embedding Metal library
[  7%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[  8%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[ 11%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[ 11%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[ 11%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[ 11%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[ 12%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 12%] Linking CXX shared library libggml-blas.dylib
[ 12%] Linking CXX shared library libggml-cpu.dylib
[ 13%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 13%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 13%] Built target ggml-blas
[ 13%] Built target ggml-cpu
[ 13%] Linking C shared library libggml-metal.dylib
[ 13%] Built target ggml-metal
[ 13%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 14%] Linking CXX shared library libggml.dylib
[ 14%] Built target ggml
[ 14%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o
[ 16%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 22%] Linking CXX executable ../../bin/llama-gguf-hash
[ 23%] Linking CXX executable ../../bin/llama-gguf
[ 23%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 25%] Linking CXX shared library libllama.dylib
[ 25%] Built target llama-gguf
[ 25%] Built target llama-gguf-hash
[ 25%] Built target llama
[ 26%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 27%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 29%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 29%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 29%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 30%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 31%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 31%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 31%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 32%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 32%] Linking C executable ../bin/test-c
[ 32%] Linking CXX executable ../../bin/llama-simple-chat
[ 32%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 34%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 34%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 34%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 34%] Linking CXX executable ../../bin/llama-simple
[ 34%] Linking CXX executable ../../bin/llama-quantize-stats
[ 34%] Built target llava
[ 35%] Linking CXX static library libcommon.a
[ 36%] Linking CXX shared library libllava_shared.dylib
[ 36%] Linking CXX static library libllava_static.a
[ 36%] Built target llama-quantize-stats
[ 36%] Built target llama-simple-chat
[ 36%] Built target test-c
[ 36%] Built target llama-simple
[ 36%] Built target llava_static
[ 36%] Built target common
[ 37%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 37%] Built target llava_shared
[ 37%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 45%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 45%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 46%] Linking CXX executable ../bin/test-tokenizer-0
[ 46%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 46%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 47%] Linking CXX executable ../bin/test-grammar-integration
[ 47%] Linking CXX executable ../bin/test-grammar-parser
[ 47%] Linking CXX executable ../bin/test-sampling
[ 48%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 49%] Linking CXX executable ../bin/test-log
[ 49%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 49%] Linking CXX executable ../bin/test-llama-grammar
[ 50%] Linking CXX executable ../bin/test-arg-parser
[ 50%] Built target test-tokenizer-1-spm
[ 50%] Built target test-tokenizer-0
[ 50%] Built target test-grammar-parser
[ 50%] Built target test-sampling
[ 50%] Built target test-json-schema-to-grammar
[ 50%] Built target test-tokenizer-1-bpe
[ 50%] Built target test-grammar-integration
[ 50%] Built target test-log
[ 50%] Built target test-llama-grammar
[ 51%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 53%] Built target test-arg-parser
[ 54%] Building CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 56%] Linking CXX executable ../bin/test-chat-template
[ 56%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 57%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 58%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 59%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 59%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 59%] Linking CXX executable ../bin/test-gguf
[ 60%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 61%] Linking CXX executable ../bin/test-backend-ops
[ 61%] Linking CXX executable ../bin/test-model-load-cancel
[ 61%] Linking CXX executable ../bin/test-quantize-perf
[ 61%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 61%] Linking CXX executable ../bin/test-barrier
[ 62%] Linking CXX executable ../bin/test-autorelease
[ 62%] Linking CXX executable ../bin/test-quantize-fns
[ 62%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 62%] Built target test-chat-template
[ 63%] Linking CXX executable ../../bin/llama-batched-bench
[ 63%] Built target test-gguf
[ 63%] Built target test-model-load-cancel
[ 64%] Linking CXX executable ../bin/test-rope
[ 64%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 64%] Built target test-autorelease
[ 64%] Built target test-barrier
[ 64%] Built target test-backend-ops
[ 64%] Built target test-quantize-fns
[ 64%] Built target test-quantize-perf
[ 64%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 64%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 64%] Built target llama-batched-bench
[ 65%] Linking CXX executable ../../bin/llama-batched
[ 65%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 65%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 65%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 65%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 65%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 66%] Linking CXX executable ../../bin/llama-embedding
[ 67%] Linking CXX executable ../../bin/llama-eval-callback
[ 67%] Built target test-rope
[ 67%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 68%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 69%] Linking CXX executable ../../bin/llama-gritlm
[ 70%] Linking CXX executable ../../bin/llama-gguf-split
[ 71%] Linking CXX executable ../../bin/llama-infill
[ 72%] Linking CXX executable ../../bin/llama-imatrix
[ 73%] Linking CXX executable ../../bin/llama-bench
[ 73%] Built target llama-batched
[ 73%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 73%] Built target llama-eval-callback
[ 73%] Built target llama-gbnf-validator
[ 74%] Linking CXX executable ../../bin/llama-lookahead
[ 74%] Built target llama-gguf-split
[ 74%] Built target llama-gritlm
[ 74%] Built target llama-infill
[ 74%] Built target llama-embedding
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 74%] Built target llama-imatrix
[ 74%] Built target llama-bench
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 74%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 75%] Linking CXX executable ../../bin/llama-lookup
[ 75%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 76%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 76%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 77%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 78%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 78%] Built target llama-lookahead
[ 79%] Linking CXX executable ../../bin/llama-lookup-create
[ 80%] Linking CXX executable ../../bin/llama-lookup-merge
[ 81%] Linking CXX executable ../../bin/llama-cli
[ 81%] Linking CXX executable ../../bin/llama-parallel
[ 82%] Linking CXX executable ../../bin/llama-lookup-stats
[ 82%] Linking CXX executable ../../bin/llama-passkey
[ 82%] Linking CXX executable ../../bin/llama-perplexity
[ 82%] Linking CXX executable ../../bin/llama-quantize
[ 83%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 83%] Built target llama-lookup
[ 83%] Generating loading.html.hpp
[ 83%] Built target llama-lookup-merge
[ 83%] Linking CXX executable ../../bin/llama-retrieval
[ 84%] Built target llama-cli
[ 84%] Built target llama-perplexity
[ 84%] Generating index.html.gz.hpp
[ 84%] Built target llama-passkey
[ 84%] Built target llama-quantize
[ 84%] Built target llama-parallel
[ 84%] Built target llama-lookup-stats
[ 84%] Built target llama-lookup-create
[ 85%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 86%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 87%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 88%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 89%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 90%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 90%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 90%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 90%] Built target llama-retrieval
[ 90%] Linking CXX executable ../../bin/llama-save-load-state
[ 90%] Linking CXX executable ../../bin/llama-speculative-simple
[ 90%] Linking CXX executable ../../bin/llama-speculative
[ 90%] Linking CXX executable ../../bin/llama-run
[ 90%] Linking CXX executable ../../bin/llama-tokenize
[ 91%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 91%] Linking CXX executable ../../bin/llama-tts
[ 92%] Linking CXX executable ../../bin/llama-gen-docs
[ 92%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 93%] Linking CXX executable ../../bin/llama-cvector-generator
[ 93%] Built target llama-speculative
[ 93%] Built target llama-run
[ 93%] Built target llama-save-load-state
[ 93%] Built target llama-speculative-simple
[ 93%] Built target llama-tts
[ 93%] Built target llama-tokenize
[ 93%] Built target llama-convert-llama2c-to-ggml
[ 93%] Built target llama-gen-docs
[ 93%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 93%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 95%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 96%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 97%] Linking CXX executable ../../bin/llama-export-lora
[ 98%] Linking CXX executable ../../bin/llama-llava-cli
[ 99%] Built target llama-cvector-generator
[ 99%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 99%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 99%] Linking CXX executable ../../bin/llama-vdot
[ 99%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Built target llama-export-lora
[ 99%] Built target llama-qwen2vl-cli
[ 99%] Built target llama-llava-cli
[ 99%] Built target llama-q8dot
[ 99%] Built target llama-vdot
[ 99%] Built target llama-minicpmv-cli
[100%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m2.937s
user	0m6.138s
sys	0m10.188s

main: quantize time =  3060.85 ms
main:    total time =  3060.85 ms

main: quantize time =  4448.63 ms
main:    total time =  4448.63 ms

main: quantize time =  2769.37 ms
main:    total time =  2769.37 ms

main: quantize time =  1955.71 ms
main:    total time =  1955.71 ms

main: quantize time =  2604.95 ms
main:    total time =  2604.95 ms

main: quantize time =  5434.32 ms
main:    total time =  5434.32 ms

main: quantize time =  5579.22 ms
main:    total time =  5579.22 ms

main: quantize time =  6939.10 ms
main:    total time =  6939.10 ms

main: quantize time =  5883.26 ms
main:    total time =  5883.26 ms

main: quantize time =  4487.62 ms
main:    total time =  4487.62 ms
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.120 I build: 4399 (0ccae21e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.233 I main: llama backend init
0.00.000.240 I main: load the model and apply lora adapter, if any
0.00.030.453 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.041.106 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.041.120 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.041.124 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.041.124 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.041.125 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.041.126 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.041.127 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.041.129 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.041.129 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.041.130 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.041.131 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.041.131 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.041.132 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.041.133 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.041.138 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.041.139 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.041.140 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.048.073 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.050.270 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.059.520 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.059.528 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.059.529 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.059.529 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.059.530 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.059.531 I llama_model_loader: - type  f32:  194 tensors
0.00.059.531 I llama_model_loader: - type  f16:   98 tensors
0.00.093.914 I llm_load_vocab: special tokens cache size = 25
0.00.101.063 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.101.066 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.101.066 I llm_load_print_meta: arch             = gptneox
0.00.101.066 I llm_load_print_meta: vocab type       = BPE
0.00.101.066 I llm_load_print_meta: n_vocab          = 50304
0.00.101.066 I llm_load_print_meta: n_merges         = 50009
0.00.101.067 I llm_load_print_meta: vocab_only       = 0
0.00.101.067 I llm_load_print_meta: n_ctx_train      = 2048
0.00.101.067 I llm_load_print_meta: n_embd           = 2048
0.00.101.067 I llm_load_print_meta: n_layer          = 24
0.00.101.071 I llm_load_print_meta: n_head           = 16
0.00.101.083 I llm_load_print_meta: n_head_kv        = 16
0.00.101.083 I llm_load_print_meta: n_rot            = 32
0.00.101.084 I llm_load_print_meta: n_swa            = 0
0.00.101.084 I llm_load_print_meta: n_embd_head_k    = 128
0.00.101.084 I llm_load_print_meta: n_embd_head_v    = 128
0.00.101.085 I llm_load_print_meta: n_gqa            = 1
0.00.101.085 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.101.086 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.101.087 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.101.087 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.101.089 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.101.089 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.101.090 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.101.091 I llm_load_print_meta: n_ff             = 8192
0.00.101.091 I llm_load_print_meta: n_expert         = 0
0.00.101.091 I llm_load_print_meta: n_expert_used    = 0
0.00.101.092 I llm_load_print_meta: causal attn      = 1
0.00.101.092 I llm_load_print_meta: pooling type     = 0
0.00.101.092 I llm_load_print_meta: rope type        = 2
0.00.101.092 I llm_load_print_meta: rope scaling     = linear
0.00.101.092 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.101.093 I llm_load_print_meta: freq_scale_train = 1
0.00.101.093 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.101.093 I llm_load_print_meta: rope_finetuned   = unknown
0.00.101.093 I llm_load_print_meta: ssm_d_conv       = 0
0.00.101.093 I llm_load_print_meta: ssm_d_inner      = 0
0.00.101.094 I llm_load_print_meta: ssm_d_state      = 0
0.00.101.094 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.101.094 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.101.094 I llm_load_print_meta: model type       = 1.4B
0.00.101.104 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.101.105 I llm_load_print_meta: model params     = 1.41 B
0.00.101.105 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.101.105 I llm_load_print_meta: general.name     = 1.4B
0.00.101.106 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.101.106 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.101.106 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.101.106 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.101.108 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.101.108 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.101.108 I llm_load_print_meta: max token length = 1024
0.00.103.753 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.103.754 I llm_load_tensors: offloading output layer to GPU
0.00.103.754 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.103.780 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.103.781 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.104.798 I llama_new_context_with_model: n_seq_max     = 1
0.00.104.799 I llama_new_context_with_model: n_ctx         = 2048
0.00.104.799 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.104.799 I llama_new_context_with_model: n_batch       = 2048
0.00.104.799 I llama_new_context_with_model: n_ubatch      = 512
0.00.104.800 I llama_new_context_with_model: flash_attn    = 0
0.00.104.800 I llama_new_context_with_model: freq_base     = 10000.0
0.00.104.800 I llama_new_context_with_model: freq_scale    = 1
0.00.104.801 I ggml_metal_init: allocating
0.00.104.811 I ggml_metal_init: found device: Apple M4
0.00.104.815 I ggml_metal_init: picking default device: Apple M4
0.00.105.537 I ggml_metal_init: using embedded metal library
0.00.115.209 I ggml_metal_init: GPU name:   Apple M4
0.00.115.211 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.115.212 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.115.212 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.115.212 I ggml_metal_init: simdgroup reduction   = true
0.00.115.212 I ggml_metal_init: simdgroup matrix mul. = true
0.00.115.212 I ggml_metal_init: has bfloat            = true
0.00.115.212 I ggml_metal_init: use bfloat            = true
0.00.115.213 I ggml_metal_init: hasUnifiedMemory      = true
0.00.115.214 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.139.607 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.160.237 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.160.245 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.160.265 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.161.249 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.161.251 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.161.252 I llama_new_context_with_model: graph nodes  = 967
0.00.161.252 I llama_new_context_with_model: graph splits = 2
0.00.161.276 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.161.409 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.161.410 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.245.087 I main: llama threadpool init, n_threads = 4
0.00.245.131 I 
0.00.245.171 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.245.172 I 
0.00.245.438 I sampler seed: 1234
0.00.245.442 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.245.466 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.245.467 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.245.468 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.088.381 I llama_perf_sampler_print:    sampling time =       1.24 ms /    71 runs   (    0.02 ms per token, 57211.93 tokens per second)
0.02.088.382 I llama_perf_context_print:        load time =     214.62 ms
0.02.088.383 I llama_perf_context_print: prompt eval time =      44.01 ms /     7 tokens (    6.29 ms per token,   159.05 tokens per second)
0.02.088.389 I llama_perf_context_print:        eval time =    1796.01 ms /    63 runs   (   28.51 ms per token,    35.08 tokens per second)
0.02.088.390 I llama_perf_context_print:       total time =    1843.30 ms /    70 tokens
0.02.088.608 I ggml_metal_free: deallocating

real	0m2.399s
user	0m0.147s
sys	0m0.107s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.043 I build: 4399 (0ccae21e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.074 I main: llama backend init
0.00.000.077 I main: load the model and apply lora adapter, if any
0.00.009.857 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.031.093 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.031.099 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.031.101 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.031.101 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.031.103 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.031.104 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.031.106 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.031.110 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.031.110 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.031.110 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.031.110 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.031.111 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.031.111 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.031.111 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.031.116 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.031.116 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.031.116 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.035.135 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.036.211 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.040.353 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.040.356 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.040.357 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.040.357 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.040.358 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.040.358 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.040.359 I llama_model_loader: - type  f32:  194 tensors
0.00.040.359 I llama_model_loader: - type q8_0:   98 tensors
0.00.065.433 I llm_load_vocab: special tokens cache size = 25
0.00.072.697 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.072.701 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.072.701 I llm_load_print_meta: arch             = gptneox
0.00.072.701 I llm_load_print_meta: vocab type       = BPE
0.00.072.702 I llm_load_print_meta: n_vocab          = 50304
0.00.072.702 I llm_load_print_meta: n_merges         = 50009
0.00.072.702 I llm_load_print_meta: vocab_only       = 0
0.00.072.704 I llm_load_print_meta: n_ctx_train      = 2048
0.00.072.705 I llm_load_print_meta: n_embd           = 2048
0.00.072.705 I llm_load_print_meta: n_layer          = 24
0.00.072.709 I llm_load_print_meta: n_head           = 16
0.00.072.723 I llm_load_print_meta: n_head_kv        = 16
0.00.072.724 I llm_load_print_meta: n_rot            = 32
0.00.072.724 I llm_load_print_meta: n_swa            = 0
0.00.072.724 I llm_load_print_meta: n_embd_head_k    = 128
0.00.072.724 I llm_load_print_meta: n_embd_head_v    = 128
0.00.072.725 I llm_load_print_meta: n_gqa            = 1
0.00.072.726 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.072.726 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.072.727 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.072.727 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.072.727 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.072.728 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.072.728 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.072.729 I llm_load_print_meta: n_ff             = 8192
0.00.072.729 I llm_load_print_meta: n_expert         = 0
0.00.072.729 I llm_load_print_meta: n_expert_used    = 0
0.00.072.730 I llm_load_print_meta: causal attn      = 1
0.00.072.731 I llm_load_print_meta: pooling type     = 0
0.00.072.731 I llm_load_print_meta: rope type        = 2
0.00.072.731 I llm_load_print_meta: rope scaling     = linear
0.00.072.732 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.072.732 I llm_load_print_meta: freq_scale_train = 1
0.00.072.732 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.072.732 I llm_load_print_meta: rope_finetuned   = unknown
0.00.072.732 I llm_load_print_meta: ssm_d_conv       = 0
0.00.072.732 I llm_load_print_meta: ssm_d_inner      = 0
0.00.072.733 I llm_load_print_meta: ssm_d_state      = 0
0.00.072.733 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.072.733 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.072.733 I llm_load_print_meta: model type       = 1.4B
0.00.072.744 I llm_load_print_meta: model ftype      = Q8_0
0.00.072.745 I llm_load_print_meta: model params     = 1.41 B
0.00.072.745 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.072.745 I llm_load_print_meta: general.name     = 1.4B
0.00.072.746 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.072.746 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.072.746 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.072.746 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.072.746 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.072.747 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.072.747 I llm_load_print_meta: max token length = 1024
0.00.075.307 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.075.308 I llm_load_tensors: offloading output layer to GPU
0.00.075.308 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.075.320 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.075.321 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.076.376 I llama_new_context_with_model: n_seq_max     = 1
0.00.076.378 I llama_new_context_with_model: n_ctx         = 2048
0.00.076.378 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.076.378 I llama_new_context_with_model: n_batch       = 2048
0.00.076.378 I llama_new_context_with_model: n_ubatch      = 512
0.00.076.378 I llama_new_context_with_model: flash_attn    = 0
0.00.076.379 I llama_new_context_with_model: freq_base     = 10000.0
0.00.076.379 I llama_new_context_with_model: freq_scale    = 1
0.00.076.380 I ggml_metal_init: allocating
0.00.076.384 I ggml_metal_init: found device: Apple M4
0.00.076.386 I ggml_metal_init: picking default device: Apple M4
0.00.077.236 I ggml_metal_init: using embedded metal library
0.00.080.152 I ggml_metal_init: GPU name:   Apple M4
0.00.080.154 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.080.154 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.080.154 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.080.155 I ggml_metal_init: simdgroup reduction   = true
0.00.080.155 I ggml_metal_init: simdgroup matrix mul. = true
0.00.080.155 I ggml_metal_init: has bfloat            = true
0.00.080.155 I ggml_metal_init: use bfloat            = true
0.00.080.156 I ggml_metal_init: hasUnifiedMemory      = true
0.00.080.156 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.091.573 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.117.926 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.117.935 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.117.960 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.119.116 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.119.119 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.119.119 I llama_new_context_with_model: graph nodes  = 967
0.00.119.119 I llama_new_context_with_model: graph splits = 2
0.00.119.141 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.119.281 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.119.282 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.341.307 I main: llama threadpool init, n_threads = 4
0.01.341.344 I 
0.01.341.380 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.341.381 I 
0.01.341.612 I sampler seed: 1234
0.01.341.620 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.341.634 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.341.634 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.341.634 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.428.473 I llama_perf_sampler_print:    sampling time =       1.15 ms /    71 runs   (    0.02 ms per token, 61739.13 tokens per second)
0.02.428.474 I llama_perf_context_print:        load time =    1331.44 ms
0.02.428.475 I llama_perf_context_print: prompt eval time =      42.33 ms /     7 tokens (    6.05 ms per token,   165.37 tokens per second)
0.02.428.477 I llama_perf_context_print:        eval time =    1041.66 ms /    63 runs   (   16.53 ms per token,    60.48 tokens per second)
0.02.428.477 I llama_perf_context_print:       total time =    1087.17 ms /    70 tokens
0.02.428.744 I ggml_metal_free: deallocating

real	0m2.449s
user	0m0.121s
sys	0m0.244s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.046 I build: 4399 (0ccae21e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.078 I main: llama backend init
0.00.000.080 I main: load the model and apply lora adapter, if any
0.00.011.036 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.667 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.016.671 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.674 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.675 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.675 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.675 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.676 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.677 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.677 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.677 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.678 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.678 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.678 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.679 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.680 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.681 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.681 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.785 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.891 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.948 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.949 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.949 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.950 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.950 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.951 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.025.951 I llama_model_loader: - type  f32:  194 tensors
0.00.025.952 I llama_model_loader: - type q4_0:   97 tensors
0.00.025.952 I llama_model_loader: - type q6_K:    1 tensors
0.00.047.592 I llm_load_vocab: special tokens cache size = 25
0.00.053.661 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.053.664 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.053.664 I llm_load_print_meta: arch             = gptneox
0.00.053.664 I llm_load_print_meta: vocab type       = BPE
0.00.053.665 I llm_load_print_meta: n_vocab          = 50304
0.00.053.665 I llm_load_print_meta: n_merges         = 50009
0.00.053.665 I llm_load_print_meta: vocab_only       = 0
0.00.053.665 I llm_load_print_meta: n_ctx_train      = 2048
0.00.053.665 I llm_load_print_meta: n_embd           = 2048
0.00.053.665 I llm_load_print_meta: n_layer          = 24
0.00.053.671 I llm_load_print_meta: n_head           = 16
0.00.053.685 I llm_load_print_meta: n_head_kv        = 16
0.00.053.686 I llm_load_print_meta: n_rot            = 32
0.00.053.686 I llm_load_print_meta: n_swa            = 0
0.00.053.686 I llm_load_print_meta: n_embd_head_k    = 128
0.00.053.686 I llm_load_print_meta: n_embd_head_v    = 128
0.00.053.687 I llm_load_print_meta: n_gqa            = 1
0.00.053.688 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.053.688 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.053.689 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.053.691 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.053.692 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.053.693 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.053.693 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.053.693 I llm_load_print_meta: n_ff             = 8192
0.00.053.694 I llm_load_print_meta: n_expert         = 0
0.00.053.695 I llm_load_print_meta: n_expert_used    = 0
0.00.053.695 I llm_load_print_meta: causal attn      = 1
0.00.053.695 I llm_load_print_meta: pooling type     = 0
0.00.053.695 I llm_load_print_meta: rope type        = 2
0.00.053.695 I llm_load_print_meta: rope scaling     = linear
0.00.053.695 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.053.696 I llm_load_print_meta: freq_scale_train = 1
0.00.053.696 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.053.696 I llm_load_print_meta: rope_finetuned   = unknown
0.00.053.696 I llm_load_print_meta: ssm_d_conv       = 0
0.00.053.696 I llm_load_print_meta: ssm_d_inner      = 0
0.00.053.696 I llm_load_print_meta: ssm_d_state      = 0
0.00.053.696 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.053.696 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.053.697 I llm_load_print_meta: model type       = 1.4B
0.00.053.708 I llm_load_print_meta: model ftype      = Q4_0
0.00.053.708 I llm_load_print_meta: model params     = 1.41 B
0.00.053.709 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.053.710 I llm_load_print_meta: general.name     = 1.4B
0.00.053.710 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.053.710 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.053.711 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.053.711 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.053.711 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.053.711 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.053.711 I llm_load_print_meta: max token length = 1024
0.00.056.126 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.056.127 I llm_load_tensors: offloading output layer to GPU
0.00.056.127 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.056.140 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.056.140 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.057.129 I llama_new_context_with_model: n_seq_max     = 1
0.00.057.130 I llama_new_context_with_model: n_ctx         = 2048
0.00.057.130 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.057.130 I llama_new_context_with_model: n_batch       = 2048
0.00.057.130 I llama_new_context_with_model: n_ubatch      = 512
0.00.057.131 I llama_new_context_with_model: flash_attn    = 0
0.00.057.131 I llama_new_context_with_model: freq_base     = 10000.0
0.00.057.132 I llama_new_context_with_model: freq_scale    = 1
0.00.057.132 I ggml_metal_init: allocating
0.00.057.140 I ggml_metal_init: found device: Apple M4
0.00.057.142 I ggml_metal_init: picking default device: Apple M4
0.00.057.908 I ggml_metal_init: using embedded metal library
0.00.060.453 I ggml_metal_init: GPU name:   Apple M4
0.00.060.454 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.060.455 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.060.455 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.060.455 I ggml_metal_init: simdgroup reduction   = true
0.00.060.455 I ggml_metal_init: simdgroup matrix mul. = true
0.00.060.455 I ggml_metal_init: has bfloat            = true
0.00.060.456 I ggml_metal_init: use bfloat            = true
0.00.060.456 I ggml_metal_init: hasUnifiedMemory      = true
0.00.060.457 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.070.465 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.094.883 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.094.892 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.094.917 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.095.995 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.095.999 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.095.999 I llama_new_context_with_model: graph nodes  = 967
0.00.095.999 I llama_new_context_with_model: graph splits = 2
0.00.096.020 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.096.160 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.096.161 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.690.133 I main: llama threadpool init, n_threads = 4
0.00.690.177 I 
0.00.690.202 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.690.204 I 
0.00.690.446 I sampler seed: 1234
0.00.690.450 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.690.461 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.690.461 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.690.463 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.368.885 I llama_perf_sampler_print:    sampling time =       1.27 ms /    71 runs   (    0.02 ms per token, 55773.76 tokens per second)
0.01.368.885 I llama_perf_context_print:        load time =     679.09 ms
0.01.368.886 I llama_perf_context_print: prompt eval time =      39.90 ms /     7 tokens (    5.70 ms per token,   175.44 tokens per second)
0.01.368.886 I llama_perf_context_print:        eval time =     635.37 ms /    63 runs   (   10.09 ms per token,    99.16 tokens per second)
0.01.368.887 I llama_perf_context_print:       total time =     678.75 ms /    70 tokens
0.01.369.117 I ggml_metal_free: deallocating

real	0m1.388s
user	0m0.113s
sys	0m0.149s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.039 I build: 4399 (0ccae21e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.068 I main: llama backend init
0.00.000.070 I main: load the model and apply lora adapter, if any
0.00.008.766 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.017 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.021 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.027 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.028 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.028 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.028 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.030 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.031 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.031 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.031 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.032 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.032 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.036 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.037 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.038 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.039 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.039 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.026 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.113 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.970 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.972 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.972 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.973 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.973 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.973 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.024.974 I llama_model_loader: - type  f32:  194 tensors
0.00.024.974 I llama_model_loader: - type q4_1:   97 tensors
0.00.024.974 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.727 I llm_load_vocab: special tokens cache size = 25
0.00.051.621 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.623 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.624 I llm_load_print_meta: arch             = gptneox
0.00.051.624 I llm_load_print_meta: vocab type       = BPE
0.00.051.624 I llm_load_print_meta: n_vocab          = 50304
0.00.051.625 I llm_load_print_meta: n_merges         = 50009
0.00.051.625 I llm_load_print_meta: vocab_only       = 0
0.00.051.625 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.625 I llm_load_print_meta: n_embd           = 2048
0.00.051.625 I llm_load_print_meta: n_layer          = 24
0.00.051.628 I llm_load_print_meta: n_head           = 16
0.00.051.641 I llm_load_print_meta: n_head_kv        = 16
0.00.051.642 I llm_load_print_meta: n_rot            = 32
0.00.051.642 I llm_load_print_meta: n_swa            = 0
0.00.051.643 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.643 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.644 I llm_load_print_meta: n_gqa            = 1
0.00.051.644 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.645 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.646 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.646 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.646 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.646 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.646 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.647 I llm_load_print_meta: n_ff             = 8192
0.00.051.647 I llm_load_print_meta: n_expert         = 0
0.00.051.647 I llm_load_print_meta: n_expert_used    = 0
0.00.051.647 I llm_load_print_meta: causal attn      = 1
0.00.051.649 I llm_load_print_meta: pooling type     = 0
0.00.051.649 I llm_load_print_meta: rope type        = 2
0.00.051.649 I llm_load_print_meta: rope scaling     = linear
0.00.051.650 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.650 I llm_load_print_meta: freq_scale_train = 1
0.00.051.650 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.650 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.650 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.651 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.651 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.651 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.651 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.651 I llm_load_print_meta: model type       = 1.4B
0.00.051.661 I llm_load_print_meta: model ftype      = Q4_1
0.00.051.661 I llm_load_print_meta: model params     = 1.41 B
0.00.051.662 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.051.662 I llm_load_print_meta: general.name     = 1.4B
0.00.051.662 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.662 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.662 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.662 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.663 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.664 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.664 I llm_load_print_meta: max token length = 1024
0.00.053.710 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.710 I llm_load_tensors: offloading output layer to GPU
0.00.053.711 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.721 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.053.722 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.054.653 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.654 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.654 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.654 I llama_new_context_with_model: n_batch       = 2048
0.00.054.654 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.655 I llama_new_context_with_model: flash_attn    = 0
0.00.054.655 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.655 I llama_new_context_with_model: freq_scale    = 1
0.00.054.656 I ggml_metal_init: allocating
0.00.054.659 I ggml_metal_init: found device: Apple M4
0.00.054.661 I ggml_metal_init: picking default device: Apple M4
0.00.055.284 I ggml_metal_init: using embedded metal library
0.00.057.663 I ggml_metal_init: GPU name:   Apple M4
0.00.057.665 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.665 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.665 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.666 I ggml_metal_init: simdgroup reduction   = true
0.00.057.666 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.666 I ggml_metal_init: has bfloat            = true
0.00.057.666 I ggml_metal_init: use bfloat            = true
0.00.057.667 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.667 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.689 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.086.868 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.873 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.891 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.877 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.879 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.879 I llama_new_context_with_model: graph nodes  = 967
0.00.087.880 I llama_new_context_with_model: graph splits = 2
0.00.087.896 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.088.043 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.088.043 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.744.822 I main: llama threadpool init, n_threads = 4
0.00.744.872 I 
0.00.744.909 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.744.910 I 
0.00.745.144 I sampler seed: 1234
0.00.745.152 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.745.183 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.745.186 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.745.186 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.469.699 I llama_perf_sampler_print:    sampling time =       1.34 ms /    71 runs   (    0.02 ms per token, 52788.10 tokens per second)
0.01.469.700 I llama_perf_context_print:        load time =     736.05 ms
0.01.469.701 I llama_perf_context_print: prompt eval time =      42.34 ms /     7 tokens (    6.05 ms per token,   165.31 tokens per second)
0.01.469.701 I llama_perf_context_print:        eval time =     679.84 ms /    63 runs   (   10.79 ms per token,    92.67 tokens per second)
0.01.469.703 I llama_perf_context_print:       total time =     724.88 ms /    70 tokens
0.01.469.930 I ggml_metal_free: deallocating

real	0m1.486s
user	0m0.110s
sys	0m0.156s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.037 I build: 4399 (0ccae21e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.070 I main: llama backend init
0.00.000.072 I main: load the model and apply lora adapter, if any
0.00.016.117 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.030.469 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.030.474 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.030.479 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.030.480 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.030.480 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.030.482 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.030.482 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.030.483 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.030.483 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.030.484 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.030.487 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.030.487 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.030.488 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.030.489 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.030.490 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.030.491 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.030.491 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.034.938 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.036.274 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.041.324 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.041.326 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.041.326 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.041.327 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.041.327 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.041.327 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.041.328 I llama_model_loader: - type  f32:  194 tensors
0.00.041.328 I llama_model_loader: - type q5_0:   97 tensors
0.00.041.328 I llama_model_loader: - type q6_K:    1 tensors
0.00.071.436 I llm_load_vocab: special tokens cache size = 25
0.00.081.873 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.081.877 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.081.877 I llm_load_print_meta: arch             = gptneox
0.00.081.878 I llm_load_print_meta: vocab type       = BPE
0.00.081.878 I llm_load_print_meta: n_vocab          = 50304
0.00.081.878 I llm_load_print_meta: n_merges         = 50009
0.00.081.878 I llm_load_print_meta: vocab_only       = 0
0.00.081.879 I llm_load_print_meta: n_ctx_train      = 2048
0.00.081.879 I llm_load_print_meta: n_embd           = 2048
0.00.081.879 I llm_load_print_meta: n_layer          = 24
0.00.081.883 I llm_load_print_meta: n_head           = 16
0.00.081.896 I llm_load_print_meta: n_head_kv        = 16
0.00.081.896 I llm_load_print_meta: n_rot            = 32
0.00.081.899 I llm_load_print_meta: n_swa            = 0
0.00.081.900 I llm_load_print_meta: n_embd_head_k    = 128
0.00.081.900 I llm_load_print_meta: n_embd_head_v    = 128
0.00.081.901 I llm_load_print_meta: n_gqa            = 1
0.00.081.902 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.081.903 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.081.904 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.081.904 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.081.904 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.081.905 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.081.905 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.081.908 I llm_load_print_meta: n_ff             = 8192
0.00.081.908 I llm_load_print_meta: n_expert         = 0
0.00.081.908 I llm_load_print_meta: n_expert_used    = 0
0.00.081.908 I llm_load_print_meta: causal attn      = 1
0.00.081.908 I llm_load_print_meta: pooling type     = 0
0.00.081.909 I llm_load_print_meta: rope type        = 2
0.00.081.909 I llm_load_print_meta: rope scaling     = linear
0.00.081.910 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.081.910 I llm_load_print_meta: freq_scale_train = 1
0.00.081.911 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.081.911 I llm_load_print_meta: rope_finetuned   = unknown
0.00.081.911 I llm_load_print_meta: ssm_d_conv       = 0
0.00.081.911 I llm_load_print_meta: ssm_d_inner      = 0
0.00.081.912 I llm_load_print_meta: ssm_d_state      = 0
0.00.081.912 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.081.912 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.081.913 I llm_load_print_meta: model type       = 1.4B
0.00.081.924 I llm_load_print_meta: model ftype      = Q5_0
0.00.081.924 I llm_load_print_meta: model params     = 1.41 B
0.00.081.925 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.081.925 I llm_load_print_meta: general.name     = 1.4B
0.00.081.926 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.081.926 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.081.926 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.081.927 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.081.927 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.081.927 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.081.928 I llm_load_print_meta: max token length = 1024
0.00.084.780 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.084.781 I llm_load_tensors: offloading output layer to GPU
0.00.084.781 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.084.792 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.084.793 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.086.143 I llama_new_context_with_model: n_seq_max     = 1
0.00.086.144 I llama_new_context_with_model: n_ctx         = 2048
0.00.086.144 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.086.145 I llama_new_context_with_model: n_batch       = 2048
0.00.086.145 I llama_new_context_with_model: n_ubatch      = 512
0.00.086.145 I llama_new_context_with_model: flash_attn    = 0
0.00.086.146 I llama_new_context_with_model: freq_base     = 10000.0
0.00.086.146 I llama_new_context_with_model: freq_scale    = 1
0.00.086.147 I ggml_metal_init: allocating
0.00.086.151 I ggml_metal_init: found device: Apple M4
0.00.086.154 I ggml_metal_init: picking default device: Apple M4
0.00.086.975 I ggml_metal_init: using embedded metal library
0.00.090.673 I ggml_metal_init: GPU name:   Apple M4
0.00.090.676 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.090.676 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.090.677 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.090.677 I ggml_metal_init: simdgroup reduction   = true
0.00.090.677 I ggml_metal_init: simdgroup matrix mul. = true
0.00.090.678 I ggml_metal_init: has bfloat            = true
0.00.090.678 I ggml_metal_init: use bfloat            = true
0.00.090.678 I ggml_metal_init: hasUnifiedMemory      = true
0.00.090.681 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.103.273 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.126.641 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.126.646 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.126.668 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.127.665 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.127.666 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.127.666 I llama_new_context_with_model: graph nodes  = 967
0.00.127.667 I llama_new_context_with_model: graph splits = 2
0.00.127.679 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.127.820 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.127.820 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.819.149 I main: llama threadpool init, n_threads = 4
0.00.819.195 I 
0.00.819.238 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.819.238 I 
0.00.819.538 I sampler seed: 1234
0.00.819.543 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.819.569 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.819.571 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.819.571 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.624.698 I llama_perf_sampler_print:    sampling time =       1.21 ms /    71 runs   (    0.02 ms per token, 58774.83 tokens per second)
0.01.624.698 I llama_perf_context_print:        load time =     803.03 ms
0.01.624.699 I llama_perf_context_print: prompt eval time =      50.60 ms /     7 tokens (    7.23 ms per token,   138.33 tokens per second)
0.01.624.699 I llama_perf_context_print:        eval time =     751.57 ms /    63 runs   (   11.93 ms per token,    83.82 tokens per second)
0.01.624.700 I llama_perf_context_print:       total time =     805.55 ms /    70 tokens
0.01.624.897 I ggml_metal_free: deallocating

real	0m1.652s
user	0m0.137s
sys	0m0.182s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.036 I build: 4399 (0ccae21e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.065 I main: llama backend init
0.00.000.067 I main: load the model and apply lora adapter, if any
0.00.008.749 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.029.975 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.029.979 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.029.980 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.029.984 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.029.985 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.029.985 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.029.986 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.029.986 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.029.987 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.029.987 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.029.987 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.029.987 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.029.988 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.029.989 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.029.991 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.029.991 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.029.991 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.034.101 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.035.203 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.039.283 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.039.284 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.039.285 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.039.285 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.039.285 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.039.285 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.039.286 I llama_model_loader: - type  f32:  194 tensors
0.00.039.286 I llama_model_loader: - type q5_1:   97 tensors
0.00.039.287 I llama_model_loader: - type q6_K:    1 tensors
0.00.064.415 I llm_load_vocab: special tokens cache size = 25
0.00.071.731 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.071.734 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.071.734 I llm_load_print_meta: arch             = gptneox
0.00.071.735 I llm_load_print_meta: vocab type       = BPE
0.00.071.735 I llm_load_print_meta: n_vocab          = 50304
0.00.071.735 I llm_load_print_meta: n_merges         = 50009
0.00.071.735 I llm_load_print_meta: vocab_only       = 0
0.00.071.735 I llm_load_print_meta: n_ctx_train      = 2048
0.00.071.735 I llm_load_print_meta: n_embd           = 2048
0.00.071.736 I llm_load_print_meta: n_layer          = 24
0.00.071.738 I llm_load_print_meta: n_head           = 16
0.00.071.750 I llm_load_print_meta: n_head_kv        = 16
0.00.071.751 I llm_load_print_meta: n_rot            = 32
0.00.071.751 I llm_load_print_meta: n_swa            = 0
0.00.071.751 I llm_load_print_meta: n_embd_head_k    = 128
0.00.071.751 I llm_load_print_meta: n_embd_head_v    = 128
0.00.071.752 I llm_load_print_meta: n_gqa            = 1
0.00.071.752 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.071.754 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.071.755 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.071.755 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.071.755 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.071.755 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.071.755 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.071.756 I llm_load_print_meta: n_ff             = 8192
0.00.071.756 I llm_load_print_meta: n_expert         = 0
0.00.071.756 I llm_load_print_meta: n_expert_used    = 0
0.00.071.757 I llm_load_print_meta: causal attn      = 1
0.00.071.757 I llm_load_print_meta: pooling type     = 0
0.00.071.757 I llm_load_print_meta: rope type        = 2
0.00.071.757 I llm_load_print_meta: rope scaling     = linear
0.00.071.758 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.071.758 I llm_load_print_meta: freq_scale_train = 1
0.00.071.758 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.071.758 I llm_load_print_meta: rope_finetuned   = unknown
0.00.071.758 I llm_load_print_meta: ssm_d_conv       = 0
0.00.071.758 I llm_load_print_meta: ssm_d_inner      = 0
0.00.071.760 I llm_load_print_meta: ssm_d_state      = 0
0.00.071.760 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.071.761 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.071.761 I llm_load_print_meta: model type       = 1.4B
0.00.071.771 I llm_load_print_meta: model ftype      = Q5_1
0.00.071.772 I llm_load_print_meta: model params     = 1.41 B
0.00.071.773 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.071.773 I llm_load_print_meta: general.name     = 1.4B
0.00.071.773 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.071.773 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.071.773 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.071.774 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.071.774 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.071.774 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.071.774 I llm_load_print_meta: max token length = 1024
0.00.073.950 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.073.951 I llm_load_tensors: offloading output layer to GPU
0.00.073.951 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.073.961 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.073.962 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.074.929 I llama_new_context_with_model: n_seq_max     = 1
0.00.074.930 I llama_new_context_with_model: n_ctx         = 2048
0.00.074.930 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.074.930 I llama_new_context_with_model: n_batch       = 2048
0.00.074.931 I llama_new_context_with_model: n_ubatch      = 512
0.00.074.931 I llama_new_context_with_model: flash_attn    = 0
0.00.074.931 I llama_new_context_with_model: freq_base     = 10000.0
0.00.074.932 I llama_new_context_with_model: freq_scale    = 1
0.00.074.932 I ggml_metal_init: allocating
0.00.074.938 I ggml_metal_init: found device: Apple M4
0.00.074.943 I ggml_metal_init: picking default device: Apple M4
0.00.075.582 I ggml_metal_init: using embedded metal library
0.00.078.422 I ggml_metal_init: GPU name:   Apple M4
0.00.078.423 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.078.424 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.078.424 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.078.425 I ggml_metal_init: simdgroup reduction   = true
0.00.078.425 I ggml_metal_init: simdgroup matrix mul. = true
0.00.078.425 I ggml_metal_init: has bfloat            = true
0.00.078.425 I ggml_metal_init: use bfloat            = true
0.00.078.425 I ggml_metal_init: hasUnifiedMemory      = true
0.00.078.426 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.089.205 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.110.401 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.110.407 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.110.425 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.111.450 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.111.452 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.111.452 I llama_new_context_with_model: graph nodes  = 967
0.00.111.452 I llama_new_context_with_model: graph splits = 2
0.00.111.468 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.111.608 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.111.609 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.836.480 I main: llama threadpool init, n_threads = 4
0.00.836.520 I 
0.00.836.550 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.836.550 I 
0.00.836.782 I sampler seed: 1234
0.00.836.787 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.836.818 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.836.819 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.836.819 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.677.205 I llama_perf_sampler_print:    sampling time =       1.25 ms /    71 runs   (    0.02 ms per token, 56800.00 tokens per second)
0.01.677.206 I llama_perf_context_print:        load time =     827.73 ms
0.01.677.207 I llama_perf_context_print: prompt eval time =      42.30 ms /     7 tokens (    6.04 ms per token,   165.47 tokens per second)
0.01.677.208 I llama_perf_context_print:        eval time =     795.00 ms /    63 runs   (   12.62 ms per token,    79.25 tokens per second)
0.01.677.208 I llama_perf_context_print:       total time =     840.73 ms /    70 tokens
0.01.677.434 I ggml_metal_free: deallocating

real	0m1.694s
user	0m0.119s
sys	0m0.163s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.039 I build: 4399 (0ccae21e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.068 I main: llama backend init
0.00.000.070 I main: load the model and apply lora adapter, if any
0.00.010.305 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.821 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.826 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.827 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.828 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.828 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.829 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.829 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.831 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.831 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.831 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.832 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.832 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.832 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.833 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.834 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.834 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.835 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.757 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.819 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.753 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.754 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.754 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.755 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.755 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.755 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.756 I llama_model_loader: - type  f32:  194 tensors
0.00.024.756 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.757 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.757 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.433 I llm_load_vocab: special tokens cache size = 25
0.00.051.370 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.373 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.374 I llm_load_print_meta: arch             = gptneox
0.00.051.374 I llm_load_print_meta: vocab type       = BPE
0.00.051.374 I llm_load_print_meta: n_vocab          = 50304
0.00.051.374 I llm_load_print_meta: n_merges         = 50009
0.00.051.374 I llm_load_print_meta: vocab_only       = 0
0.00.051.375 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.375 I llm_load_print_meta: n_embd           = 2048
0.00.051.375 I llm_load_print_meta: n_layer          = 24
0.00.051.378 I llm_load_print_meta: n_head           = 16
0.00.051.386 I llm_load_print_meta: n_head_kv        = 16
0.00.051.386 I llm_load_print_meta: n_rot            = 32
0.00.051.388 I llm_load_print_meta: n_swa            = 0
0.00.051.388 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.388 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.389 I llm_load_print_meta: n_gqa            = 1
0.00.051.390 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.390 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.391 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.391 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.395 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.396 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.397 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.398 I llm_load_print_meta: n_ff             = 8192
0.00.051.398 I llm_load_print_meta: n_expert         = 0
0.00.051.398 I llm_load_print_meta: n_expert_used    = 0
0.00.051.398 I llm_load_print_meta: causal attn      = 1
0.00.051.399 I llm_load_print_meta: pooling type     = 0
0.00.051.399 I llm_load_print_meta: rope type        = 2
0.00.051.399 I llm_load_print_meta: rope scaling     = linear
0.00.051.399 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.399 I llm_load_print_meta: freq_scale_train = 1
0.00.051.400 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.400 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.400 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.400 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.400 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.400 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.400 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.401 I llm_load_print_meta: model type       = 1.4B
0.00.051.405 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.051.406 I llm_load_print_meta: model params     = 1.41 B
0.00.051.406 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.051.406 I llm_load_print_meta: general.name     = 1.4B
0.00.051.407 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.408 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.408 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.408 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.408 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.408 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.410 I llm_load_print_meta: max token length = 1024
0.00.053.177 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.178 I llm_load_tensors: offloading output layer to GPU
0.00.053.178 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.184 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.053.184 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.054.089 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.090 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.090 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.090 I llama_new_context_with_model: n_batch       = 2048
0.00.054.090 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.090 I llama_new_context_with_model: flash_attn    = 0
0.00.054.091 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.091 I llama_new_context_with_model: freq_scale    = 1
0.00.054.091 I ggml_metal_init: allocating
0.00.054.095 I ggml_metal_init: found device: Apple M4
0.00.054.097 I ggml_metal_init: picking default device: Apple M4
0.00.054.714 I ggml_metal_init: using embedded metal library
0.00.057.051 I ggml_metal_init: GPU name:   Apple M4
0.00.057.052 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.053 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.053 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.053 I ggml_metal_init: simdgroup reduction   = true
0.00.057.054 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.054 I ggml_metal_init: has bfloat            = true
0.00.057.054 I ggml_metal_init: use bfloat            = true
0.00.057.054 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.055 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.977 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.086.619 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.629 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.654 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.772 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.773 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.773 I llama_new_context_with_model: graph nodes  = 967
0.00.087.774 I llama_new_context_with_model: graph splits = 2
0.00.087.788 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.087.915 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.915 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.439.710 I main: llama threadpool init, n_threads = 4
0.00.439.753 I 
0.00.439.796 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.439.797 I 
0.00.440.021 I sampler seed: 1234
0.00.440.025 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.440.052 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.440.053 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.440.053 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.120.816 I llama_perf_sampler_print:    sampling time =       1.17 ms /    71 runs   (    0.02 ms per token, 60631.94 tokens per second)
0.01.120.817 I llama_perf_context_print:        load time =     429.40 ms
0.01.120.818 I llama_perf_context_print: prompt eval time =      35.80 ms /     7 tokens (    5.11 ms per token,   195.54 tokens per second)
0.01.120.818 I llama_perf_context_print:        eval time =     642.02 ms /    63 runs   (   10.19 ms per token,    98.13 tokens per second)
0.01.120.818 I llama_perf_context_print:       total time =     681.11 ms /    70 tokens
0.01.121.025 I ggml_metal_free: deallocating

real	0m1.138s
user	0m0.110s
sys	0m0.110s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.038 I build: 4399 (0ccae21e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.067 I main: llama backend init
0.00.000.069 I main: load the model and apply lora adapter, if any
0.00.008.837 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.644 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.014.648 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.650 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.650 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.651 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.651 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.651 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.652 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.653 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.653 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.655 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.656 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.656 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.656 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.658 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.658 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.659 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.583 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.639 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.520 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.521 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.522 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.522 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.522 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.522 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.023.523 I llama_model_loader: - type  f32:  194 tensors
0.00.023.523 I llama_model_loader: - type q3_K:   25 tensors
0.00.023.523 I llama_model_loader: - type q4_K:   71 tensors
0.00.023.524 I llama_model_loader: - type q5_K:    1 tensors
0.00.023.524 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.154 I llm_load_vocab: special tokens cache size = 25
0.00.050.071 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.074 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.074 I llm_load_print_meta: arch             = gptneox
0.00.050.074 I llm_load_print_meta: vocab type       = BPE
0.00.050.075 I llm_load_print_meta: n_vocab          = 50304
0.00.050.075 I llm_load_print_meta: n_merges         = 50009
0.00.050.075 I llm_load_print_meta: vocab_only       = 0
0.00.050.075 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.075 I llm_load_print_meta: n_embd           = 2048
0.00.050.075 I llm_load_print_meta: n_layer          = 24
0.00.050.078 I llm_load_print_meta: n_head           = 16
0.00.050.091 I llm_load_print_meta: n_head_kv        = 16
0.00.050.091 I llm_load_print_meta: n_rot            = 32
0.00.050.091 I llm_load_print_meta: n_swa            = 0
0.00.050.091 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.091 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.092 I llm_load_print_meta: n_gqa            = 1
0.00.050.093 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.094 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.094 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.095 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.095 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.095 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.095 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.096 I llm_load_print_meta: n_ff             = 8192
0.00.050.096 I llm_load_print_meta: n_expert         = 0
0.00.050.097 I llm_load_print_meta: n_expert_used    = 0
0.00.050.097 I llm_load_print_meta: causal attn      = 1
0.00.050.097 I llm_load_print_meta: pooling type     = 0
0.00.050.098 I llm_load_print_meta: rope type        = 2
0.00.050.098 I llm_load_print_meta: rope scaling     = linear
0.00.050.098 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.098 I llm_load_print_meta: freq_scale_train = 1
0.00.050.099 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.100 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.100 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.100 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.100 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.100 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.101 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.101 I llm_load_print_meta: model type       = 1.4B
0.00.050.111 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.050.112 I llm_load_print_meta: model params     = 1.41 B
0.00.050.113 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.050.113 I llm_load_print_meta: general.name     = 1.4B
0.00.050.113 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.114 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.114 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.114 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.115 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.115 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.115 I llm_load_print_meta: max token length = 1024
0.00.052.065 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.066 I llm_load_tensors: offloading output layer to GPU
0.00.052.066 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.077 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.052.078 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.052.948 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.949 I llama_new_context_with_model: n_ctx         = 2048
0.00.052.949 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.052.949 I llama_new_context_with_model: n_batch       = 2048
0.00.052.950 I llama_new_context_with_model: n_ubatch      = 512
0.00.052.950 I llama_new_context_with_model: flash_attn    = 0
0.00.052.950 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.950 I llama_new_context_with_model: freq_scale    = 1
0.00.052.951 I ggml_metal_init: allocating
0.00.052.954 I ggml_metal_init: found device: Apple M4
0.00.052.956 I ggml_metal_init: picking default device: Apple M4
0.00.053.546 I ggml_metal_init: using embedded metal library
0.00.055.917 I ggml_metal_init: GPU name:   Apple M4
0.00.055.919 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.919 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.919 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.920 I ggml_metal_init: simdgroup reduction   = true
0.00.055.920 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.920 I ggml_metal_init: has bfloat            = true
0.00.055.920 I ggml_metal_init: use bfloat            = true
0.00.055.920 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.921 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.836 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.085.677 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.686 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.707 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.693 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.694 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.695 I llama_new_context_with_model: graph nodes  = 967
0.00.086.695 I llama_new_context_with_model: graph splits = 2
0.00.086.710 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.086.839 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.086.840 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.519.982 I main: llama threadpool init, n_threads = 4
0.00.520.016 I 
0.00.520.047 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.520.049 I 
0.00.520.210 I sampler seed: 1234
0.00.520.215 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.520.224 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.520.224 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.520.225 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.265.612 I llama_perf_sampler_print:    sampling time =       1.34 ms /    71 runs   (    0.02 ms per token, 53064.28 tokens per second)
0.01.265.613 I llama_perf_context_print:        load time =     511.14 ms
0.01.265.615 I llama_perf_context_print: prompt eval time =      40.38 ms /     7 tokens (    5.77 ms per token,   173.34 tokens per second)
0.01.265.616 I llama_perf_context_print:        eval time =     702.22 ms /    63 runs   (   11.15 ms per token,    89.72 tokens per second)
0.01.265.616 I llama_perf_context_print:       total time =     745.63 ms /    70 tokens
0.01.265.853 I ggml_metal_free: deallocating

real	0m1.281s
user	0m0.111s
sys	0m0.112s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4399 (0ccae21e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.068 I main: llama backend init
0.00.000.070 I main: load the model and apply lora adapter, if any
0.00.008.964 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.497 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.014.502 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.504 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.505 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.505 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.505 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.506 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.507 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.507 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.507 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.508 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.508 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.508 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.509 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.510 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.511 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.511 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.396 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.443 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.292 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.294 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.294 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.294 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.295 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.295 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.023.295 I llama_model_loader: - type  f32:  194 tensors
0.00.023.296 I llama_model_loader: - type q4_K:   61 tensors
0.00.023.296 I llama_model_loader: - type q5_K:   24 tensors
0.00.023.296 I llama_model_loader: - type q6_K:   13 tensors
0.00.044.049 I llm_load_vocab: special tokens cache size = 25
0.00.049.887 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.890 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.891 I llm_load_print_meta: arch             = gptneox
0.00.049.891 I llm_load_print_meta: vocab type       = BPE
0.00.049.891 I llm_load_print_meta: n_vocab          = 50304
0.00.049.891 I llm_load_print_meta: n_merges         = 50009
0.00.049.891 I llm_load_print_meta: vocab_only       = 0
0.00.049.892 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.892 I llm_load_print_meta: n_embd           = 2048
0.00.049.892 I llm_load_print_meta: n_layer          = 24
0.00.049.895 I llm_load_print_meta: n_head           = 16
0.00.049.907 I llm_load_print_meta: n_head_kv        = 16
0.00.049.907 I llm_load_print_meta: n_rot            = 32
0.00.049.908 I llm_load_print_meta: n_swa            = 0
0.00.049.908 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.908 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.909 I llm_load_print_meta: n_gqa            = 1
0.00.049.909 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.911 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.912 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.912 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.913 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.913 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.913 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.913 I llm_load_print_meta: n_ff             = 8192
0.00.049.914 I llm_load_print_meta: n_expert         = 0
0.00.049.914 I llm_load_print_meta: n_expert_used    = 0
0.00.049.914 I llm_load_print_meta: causal attn      = 1
0.00.049.914 I llm_load_print_meta: pooling type     = 0
0.00.049.914 I llm_load_print_meta: rope type        = 2
0.00.049.914 I llm_load_print_meta: rope scaling     = linear
0.00.049.915 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.915 I llm_load_print_meta: freq_scale_train = 1
0.00.049.915 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.915 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.916 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.916 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.916 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.916 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.916 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.917 I llm_load_print_meta: model type       = 1.4B
0.00.049.926 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.049.927 I llm_load_print_meta: model params     = 1.41 B
0.00.049.929 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.049.929 I llm_load_print_meta: general.name     = 1.4B
0.00.049.929 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.929 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.930 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.930 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.930 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.049.930 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.930 I llm_load_print_meta: max token length = 1024
0.00.051.933 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.933 I llm_load_tensors: offloading output layer to GPU
0.00.051.934 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.944 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.051.945 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.052.826 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.827 I llama_new_context_with_model: n_ctx         = 2048
0.00.052.827 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.052.827 I llama_new_context_with_model: n_batch       = 2048
0.00.052.827 I llama_new_context_with_model: n_ubatch      = 512
0.00.052.828 I llama_new_context_with_model: flash_attn    = 0
0.00.052.828 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.828 I llama_new_context_with_model: freq_scale    = 1
0.00.052.829 I ggml_metal_init: allocating
0.00.052.832 I ggml_metal_init: found device: Apple M4
0.00.052.834 I ggml_metal_init: picking default device: Apple M4
0.00.053.430 I ggml_metal_init: using embedded metal library
0.00.055.850 I ggml_metal_init: GPU name:   Apple M4
0.00.055.852 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.852 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.853 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.853 I ggml_metal_init: simdgroup reduction   = true
0.00.055.853 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.853 I ggml_metal_init: has bfloat            = true
0.00.055.854 I ggml_metal_init: use bfloat            = true
0.00.055.854 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.855 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.965 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.085.906 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.911 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.931 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.980 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.982 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.982 I llama_new_context_with_model: graph nodes  = 967
0.00.086.983 I llama_new_context_with_model: graph splits = 2
0.00.086.999 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.087.139 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.140 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.628.476 I main: llama threadpool init, n_threads = 4
0.00.628.530 I 
0.00.628.575 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.628.576 I 
0.00.628.828 I sampler seed: 1234
0.00.628.834 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.628.868 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.628.886 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.628.887 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.388.001 I llama_perf_sampler_print:    sampling time =       1.17 ms /    71 runs   (    0.02 ms per token, 60683.76 tokens per second)
0.01.388.001 I llama_perf_context_print:        load time =     619.50 ms
0.01.388.002 I llama_perf_context_print: prompt eval time =      50.21 ms /     7 tokens (    7.17 ms per token,   139.41 tokens per second)
0.01.388.003 I llama_perf_context_print:        eval time =     706.02 ms /    63 runs   (   11.21 ms per token,    89.23 tokens per second)
0.01.388.003 I llama_perf_context_print:       total time =     759.53 ms /    70 tokens
0.01.388.230 I ggml_metal_free: deallocating

real	0m1.405s
user	0m0.111s
sys	0m0.150s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4399 (0ccae21e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.071 I main: llama backend init
0.00.000.073 I main: load the model and apply lora adapter, if any
0.00.009.595 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.331 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.336 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.338 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.338 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.338 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.339 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.343 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.344 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.344 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.345 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.345 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.345 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.346 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.346 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.351 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.351 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.353 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.364 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.450 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.441 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.442 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.442 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.443 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.443 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.443 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.444 I llama_model_loader: - type  f32:  194 tensors
0.00.025.444 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.444 I llama_model_loader: - type q6_K:   37 tensors
0.00.046.710 I llm_load_vocab: special tokens cache size = 25
0.00.052.520 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.523 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.523 I llm_load_print_meta: arch             = gptneox
0.00.052.523 I llm_load_print_meta: vocab type       = BPE
0.00.052.524 I llm_load_print_meta: n_vocab          = 50304
0.00.052.524 I llm_load_print_meta: n_merges         = 50009
0.00.052.524 I llm_load_print_meta: vocab_only       = 0
0.00.052.524 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.524 I llm_load_print_meta: n_embd           = 2048
0.00.052.524 I llm_load_print_meta: n_layer          = 24
0.00.052.527 I llm_load_print_meta: n_head           = 16
0.00.052.540 I llm_load_print_meta: n_head_kv        = 16
0.00.052.540 I llm_load_print_meta: n_rot            = 32
0.00.052.540 I llm_load_print_meta: n_swa            = 0
0.00.052.540 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.540 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.541 I llm_load_print_meta: n_gqa            = 1
0.00.052.542 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.543 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.543 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.544 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.544 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.544 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.545 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.546 I llm_load_print_meta: n_ff             = 8192
0.00.052.546 I llm_load_print_meta: n_expert         = 0
0.00.052.546 I llm_load_print_meta: n_expert_used    = 0
0.00.052.547 I llm_load_print_meta: causal attn      = 1
0.00.052.549 I llm_load_print_meta: pooling type     = 0
0.00.052.549 I llm_load_print_meta: rope type        = 2
0.00.052.549 I llm_load_print_meta: rope scaling     = linear
0.00.052.549 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.550 I llm_load_print_meta: freq_scale_train = 1
0.00.052.550 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.551 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.551 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.551 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.552 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.552 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.552 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.552 I llm_load_print_meta: model type       = 1.4B
0.00.052.562 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.052.562 I llm_load_print_meta: model params     = 1.41 B
0.00.052.564 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.052.564 I llm_load_print_meta: general.name     = 1.4B
0.00.052.564 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.564 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.564 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.565 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.565 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.052.565 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.565 I llm_load_print_meta: max token length = 1024
0.00.054.672 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.673 I llm_load_tensors: offloading output layer to GPU
0.00.054.673 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.684 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.054.685 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.055.606 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.607 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.607 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.608 I llama_new_context_with_model: n_batch       = 2048
0.00.055.608 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.608 I llama_new_context_with_model: flash_attn    = 0
0.00.055.609 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.609 I llama_new_context_with_model: freq_scale    = 1
0.00.055.610 I ggml_metal_init: allocating
0.00.055.613 I ggml_metal_init: found device: Apple M4
0.00.055.615 I ggml_metal_init: picking default device: Apple M4
0.00.056.278 I ggml_metal_init: using embedded metal library
0.00.058.776 I ggml_metal_init: GPU name:   Apple M4
0.00.058.778 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.780 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.781 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.781 I ggml_metal_init: simdgroup reduction   = true
0.00.058.781 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.781 I ggml_metal_init: has bfloat            = true
0.00.058.781 I ggml_metal_init: use bfloat            = true
0.00.058.782 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.783 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.920 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.088.910 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.088.914 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.088.930 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.089.994 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.089.995 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.089.995 I llama_new_context_with_model: graph nodes  = 967
0.00.089.996 I llama_new_context_with_model: graph splits = 2
0.00.090.011 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.090.160 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.090.161 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.705.557 I main: llama threadpool init, n_threads = 4
0.00.705.595 I 
0.00.705.627 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.705.628 I 
0.00.705.857 I sampler seed: 1234
0.00.705.861 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.705.904 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.705.905 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.705.905 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.565.160 I llama_perf_sampler_print:    sampling time =       1.28 ms /    71 runs   (    0.02 ms per token, 55686.27 tokens per second)
0.01.565.160 I llama_perf_context_print:        load time =     695.96 ms
0.01.565.161 I llama_perf_context_print: prompt eval time =      58.23 ms /     7 tokens (    8.32 ms per token,   120.21 tokens per second)
0.01.565.162 I llama_perf_context_print:        eval time =     798.17 ms /    63 runs   (   12.67 ms per token,    78.93 tokens per second)
0.01.565.162 I llama_perf_context_print:       total time =     859.61 ms /    70 tokens
0.01.565.406 I ggml_metal_free: deallocating

real	0m1.584s
user	0m0.111s
sys	0m0.170s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.036 I build: 4399 (0ccae21e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.063 I main: llama backend init
0.00.000.066 I main: load the model and apply lora adapter, if any
0.00.008.467 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.300 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.305 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.307 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.307 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.308 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.308 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.308 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.309 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.310 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.310 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.310 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.311 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.311 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.311 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.317 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.318 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.318 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.218 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.309 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.167 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.169 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.169 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.169 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.170 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.170 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.171 I llama_model_loader: - type  f32:  194 tensors
0.00.024.171 I llama_model_loader: - type q6_K:   98 tensors
0.00.044.945 I llm_load_vocab: special tokens cache size = 25
0.00.050.992 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.994 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.995 I llm_load_print_meta: arch             = gptneox
0.00.050.995 I llm_load_print_meta: vocab type       = BPE
0.00.050.996 I llm_load_print_meta: n_vocab          = 50304
0.00.050.996 I llm_load_print_meta: n_merges         = 50009
0.00.050.996 I llm_load_print_meta: vocab_only       = 0
0.00.050.996 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.996 I llm_load_print_meta: n_embd           = 2048
0.00.050.996 I llm_load_print_meta: n_layer          = 24
0.00.050.999 I llm_load_print_meta: n_head           = 16
0.00.051.013 I llm_load_print_meta: n_head_kv        = 16
0.00.051.014 I llm_load_print_meta: n_rot            = 32
0.00.051.014 I llm_load_print_meta: n_swa            = 0
0.00.051.014 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.015 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.015 I llm_load_print_meta: n_gqa            = 1
0.00.051.016 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.017 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.017 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.017 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.018 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.018 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.019 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.020 I llm_load_print_meta: n_ff             = 8192
0.00.051.020 I llm_load_print_meta: n_expert         = 0
0.00.051.021 I llm_load_print_meta: n_expert_used    = 0
0.00.051.021 I llm_load_print_meta: causal attn      = 1
0.00.051.022 I llm_load_print_meta: pooling type     = 0
0.00.051.023 I llm_load_print_meta: rope type        = 2
0.00.051.023 I llm_load_print_meta: rope scaling     = linear
0.00.051.024 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.024 I llm_load_print_meta: freq_scale_train = 1
0.00.051.024 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.025 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.025 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.025 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.025 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.025 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.026 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.026 I llm_load_print_meta: model type       = 1.4B
0.00.051.036 I llm_load_print_meta: model ftype      = Q6_K
0.00.051.036 I llm_load_print_meta: model params     = 1.41 B
0.00.051.036 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.051.036 I llm_load_print_meta: general.name     = 1.4B
0.00.051.037 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.037 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.037 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.037 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.037 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.039 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.039 I llm_load_print_meta: max token length = 1024
0.00.053.155 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.156 I llm_load_tensors: offloading output layer to GPU
0.00.053.156 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.167 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.053.168 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.054.058 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.059 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.059 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.059 I llama_new_context_with_model: n_batch       = 2048
0.00.054.059 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.059 I llama_new_context_with_model: flash_attn    = 0
0.00.054.060 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.060 I llama_new_context_with_model: freq_scale    = 1
0.00.054.061 I ggml_metal_init: allocating
0.00.054.068 I ggml_metal_init: found device: Apple M4
0.00.054.070 I ggml_metal_init: picking default device: Apple M4
0.00.054.697 I ggml_metal_init: using embedded metal library
0.00.057.079 I ggml_metal_init: GPU name:   Apple M4
0.00.057.081 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.081 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.082 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.082 I ggml_metal_init: simdgroup reduction   = true
0.00.057.083 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.084 I ggml_metal_init: has bfloat            = true
0.00.057.084 I ggml_metal_init: use bfloat            = true
0.00.057.084 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.085 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.051 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.087.088 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.087.097 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.087.119 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.088.135 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.088.136 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.088.137 I llama_new_context_with_model: graph nodes  = 967
0.00.088.137 I llama_new_context_with_model: graph splits = 2
0.00.088.155 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.088.282 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.088.283 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.759.322 I main: llama threadpool init, n_threads = 4
0.00.759.362 I 
0.00.759.396 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.759.398 I 
0.00.759.635 I sampler seed: 1234
0.00.759.640 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.759.692 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.759.694 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.759.694 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.638.978 I llama_perf_sampler_print:    sampling time =       1.17 ms /    71 runs   (    0.02 ms per token, 60839.76 tokens per second)
0.01.638.979 I llama_perf_context_print:        load time =     750.85 ms
0.01.638.981 I llama_perf_context_print: prompt eval time =      54.43 ms /     7 tokens (    7.78 ms per token,   128.62 tokens per second)
0.01.638.981 I llama_perf_context_print:        eval time =     821.98 ms /    63 runs   (   13.05 ms per token,    76.64 tokens per second)
0.01.638.982 I llama_perf_context_print:       total time =     879.66 ms /    70 tokens
0.01.639.220 I ggml_metal_free: deallocating

real	0m1.657s
user	0m0.110s
sys	0m0.169s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.560 I build: 4399 (0ccae21e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.024.012 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.035.973 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.035.979 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.035.981 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.035.981 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.035.982 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.035.988 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.035.988 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.035.990 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.035.990 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.035.991 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.035.991 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.035.992 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.035.992 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.035.993 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.035.996 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.035.997 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.035.997 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.044.663 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.046.715 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.054.195 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.054.197 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.054.198 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.054.199 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.054.199 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.054.200 I llama_model_loader: - type  f32:  194 tensors
0.00.054.200 I llama_model_loader: - type  f16:   98 tensors
0.00.083.805 I llm_load_vocab: special tokens cache size = 25
0.00.090.506 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.090.508 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.090.509 I llm_load_print_meta: arch             = gptneox
0.00.090.509 I llm_load_print_meta: vocab type       = BPE
0.00.090.509 I llm_load_print_meta: n_vocab          = 50304
0.00.090.509 I llm_load_print_meta: n_merges         = 50009
0.00.090.509 I llm_load_print_meta: vocab_only       = 0
0.00.090.510 I llm_load_print_meta: n_ctx_train      = 2048
0.00.090.510 I llm_load_print_meta: n_embd           = 2048
0.00.090.510 I llm_load_print_meta: n_layer          = 24
0.00.090.513 I llm_load_print_meta: n_head           = 16
0.00.090.525 I llm_load_print_meta: n_head_kv        = 16
0.00.090.526 I llm_load_print_meta: n_rot            = 32
0.00.090.526 I llm_load_print_meta: n_swa            = 0
0.00.090.527 I llm_load_print_meta: n_embd_head_k    = 128
0.00.090.527 I llm_load_print_meta: n_embd_head_v    = 128
0.00.090.527 I llm_load_print_meta: n_gqa            = 1
0.00.090.528 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.090.529 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.090.529 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.090.530 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.090.530 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.090.530 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.090.530 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.090.531 I llm_load_print_meta: n_ff             = 8192
0.00.090.531 I llm_load_print_meta: n_expert         = 0
0.00.090.531 I llm_load_print_meta: n_expert_used    = 0
0.00.090.531 I llm_load_print_meta: causal attn      = 1
0.00.090.531 I llm_load_print_meta: pooling type     = 0
0.00.090.531 I llm_load_print_meta: rope type        = 2
0.00.090.531 I llm_load_print_meta: rope scaling     = linear
0.00.090.533 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.090.534 I llm_load_print_meta: freq_scale_train = 1
0.00.090.534 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.090.534 I llm_load_print_meta: rope_finetuned   = unknown
0.00.090.534 I llm_load_print_meta: ssm_d_conv       = 0
0.00.090.534 I llm_load_print_meta: ssm_d_inner      = 0
0.00.090.534 I llm_load_print_meta: ssm_d_state      = 0
0.00.090.534 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.090.535 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.090.535 I llm_load_print_meta: model type       = 1.4B
0.00.090.545 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.090.545 I llm_load_print_meta: model params     = 1.41 B
0.00.090.546 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.090.546 I llm_load_print_meta: general.name     = 1.4B
0.00.090.546 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.090.546 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.090.546 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.090.547 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.090.547 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.090.547 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.090.547 I llm_load_print_meta: max token length = 1024
0.00.093.045 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.093.045 I llm_load_tensors: offloading output layer to GPU
0.00.093.046 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.093.056 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.093.057 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.093.984 I llama_new_context_with_model: n_seq_max     = 1
0.00.093.985 I llama_new_context_with_model: n_ctx         = 128
0.00.093.985 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.093.985 I llama_new_context_with_model: n_batch       = 128
0.00.093.985 I llama_new_context_with_model: n_ubatch      = 128
0.00.093.986 I llama_new_context_with_model: flash_attn    = 0
0.00.093.986 I llama_new_context_with_model: freq_base     = 10000.0
0.00.093.986 I llama_new_context_with_model: freq_scale    = 1
0.00.093.986 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.093.987 I ggml_metal_init: allocating
0.00.093.990 I ggml_metal_init: found device: Apple M4
0.00.093.992 I ggml_metal_init: picking default device: Apple M4
0.00.094.607 I ggml_metal_init: using embedded metal library
0.00.097.182 I ggml_metal_init: GPU name:   Apple M4
0.00.097.184 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.097.184 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.097.185 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.097.185 I ggml_metal_init: simdgroup reduction   = true
0.00.097.185 I ggml_metal_init: simdgroup matrix mul. = true
0.00.097.185 I ggml_metal_init: has bfloat            = true
0.00.097.185 I ggml_metal_init: use bfloat            = true
0.00.097.186 I ggml_metal_init: hasUnifiedMemory      = true
0.00.097.186 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.107.052 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.108.312 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.108.316 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.108.329 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.109.187 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.109.188 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.109.188 I llama_new_context_with_model: graph nodes  = 967
0.00.109.188 I llama_new_context_with_model: graph splits = 2
0.00.109.201 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.109.201 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.447.804 I 
0.01.447.849 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.447.876 I perplexity: tokenizing the input ..
0.01.456.014 I perplexity: tokenization took 8.133 ms
0.01.456.019 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.574.373 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.575.800 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.575.813 I llama_perf_context_print:        load time =    1423.77 ms
0.01.575.815 I llama_perf_context_print: prompt eval time =     118.09 ms /   128 tokens (    0.92 ms per token,  1083.87 tokens per second)
0.01.575.816 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.575.816 I llama_perf_context_print:       total time =     128.02 ms /   129 tokens
0.01.576.124 I ggml_metal_free: deallocating

real	0m1.776s
user	0m0.113s
sys	0m0.242s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.100 I build: 4399 (0ccae21e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.354 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.565 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.015.571 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.578 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.578 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.579 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.579 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.579 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.580 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.580 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.581 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.581 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.581 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.582 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.582 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.584 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.585 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.585 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.477 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.539 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.471 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.472 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.473 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.473 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.473 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.474 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.024.474 I llama_model_loader: - type  f32:  194 tensors
0.00.024.475 I llama_model_loader: - type q8_0:   98 tensors
0.00.046.019 I llm_load_vocab: special tokens cache size = 25
0.00.052.102 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.106 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.107 I llm_load_print_meta: arch             = gptneox
0.00.052.107 I llm_load_print_meta: vocab type       = BPE
0.00.052.107 I llm_load_print_meta: n_vocab          = 50304
0.00.052.107 I llm_load_print_meta: n_merges         = 50009
0.00.052.108 I llm_load_print_meta: vocab_only       = 0
0.00.052.108 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.113 I llm_load_print_meta: n_embd           = 2048
0.00.052.114 I llm_load_print_meta: n_layer          = 24
0.00.052.117 I llm_load_print_meta: n_head           = 16
0.00.052.132 I llm_load_print_meta: n_head_kv        = 16
0.00.052.133 I llm_load_print_meta: n_rot            = 32
0.00.052.133 I llm_load_print_meta: n_swa            = 0
0.00.052.133 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.133 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.134 I llm_load_print_meta: n_gqa            = 1
0.00.052.134 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.135 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.135 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.136 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.136 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.136 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.136 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.137 I llm_load_print_meta: n_ff             = 8192
0.00.052.137 I llm_load_print_meta: n_expert         = 0
0.00.052.137 I llm_load_print_meta: n_expert_used    = 0
0.00.052.138 I llm_load_print_meta: causal attn      = 1
0.00.052.139 I llm_load_print_meta: pooling type     = 0
0.00.052.139 I llm_load_print_meta: rope type        = 2
0.00.052.139 I llm_load_print_meta: rope scaling     = linear
0.00.052.139 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.139 I llm_load_print_meta: freq_scale_train = 1
0.00.052.140 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.140 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.142 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.143 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.143 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.143 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.143 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.143 I llm_load_print_meta: model type       = 1.4B
0.00.052.154 I llm_load_print_meta: model ftype      = Q8_0
0.00.052.154 I llm_load_print_meta: model params     = 1.41 B
0.00.052.155 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.052.155 I llm_load_print_meta: general.name     = 1.4B
0.00.052.155 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.155 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.157 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.157 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.157 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.052.158 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.158 I llm_load_print_meta: max token length = 1024
0.00.054.342 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.343 I llm_load_tensors: offloading output layer to GPU
0.00.054.343 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.354 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.054.355 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.055.262 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.263 I llama_new_context_with_model: n_ctx         = 128
0.00.055.263 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.055.263 I llama_new_context_with_model: n_batch       = 128
0.00.055.263 I llama_new_context_with_model: n_ubatch      = 128
0.00.055.263 I llama_new_context_with_model: flash_attn    = 0
0.00.055.264 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.264 I llama_new_context_with_model: freq_scale    = 1
0.00.055.264 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.055.265 I ggml_metal_init: allocating
0.00.055.269 I ggml_metal_init: found device: Apple M4
0.00.055.271 I ggml_metal_init: picking default device: Apple M4
0.00.055.895 I ggml_metal_init: using embedded metal library
0.00.058.356 I ggml_metal_init: GPU name:   Apple M4
0.00.058.358 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.358 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.358 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.359 I ggml_metal_init: simdgroup reduction   = true
0.00.058.359 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.359 I ggml_metal_init: has bfloat            = true
0.00.058.359 I ggml_metal_init: use bfloat            = true
0.00.058.360 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.360 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.455 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.069.820 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.069.823 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.069.838 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.070.744 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.070.745 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.070.746 I llama_new_context_with_model: graph nodes  = 967
0.00.070.746 I llama_new_context_with_model: graph splits = 2
0.00.070.759 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.070.760 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.945.918 I 
0.00.945.966 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.945.978 I perplexity: tokenizing the input ..
0.00.953.978 I perplexity: tokenization took 7.999 ms
0.00.953.983 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.077.495 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.078.797 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.078.815 I llama_perf_context_print:        load time =     936.56 ms
0.01.078.815 I llama_perf_context_print: prompt eval time =     123.28 ms /   128 tokens (    0.96 ms per token,  1038.32 tokens per second)
0.01.078.816 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.078.816 I llama_perf_context_print:       total time =     132.90 ms /   129 tokens
0.01.079.167 I ggml_metal_free: deallocating

real	0m1.094s
user	0m0.081s
sys	0m0.159s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.105 I build: 4399 (0ccae21e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.913 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.674 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.015.679 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.681 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.682 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.687 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.687 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.688 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.689 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.689 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.691 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.691 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.691 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.691 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.692 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.693 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.694 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.694 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.685 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.802 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.771 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.772 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.773 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.773 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.773 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.774 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.024.774 I llama_model_loader: - type  f32:  194 tensors
0.00.024.775 I llama_model_loader: - type q4_0:   97 tensors
0.00.024.775 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.946 I llm_load_vocab: special tokens cache size = 25
0.00.051.788 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.791 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.791 I llm_load_print_meta: arch             = gptneox
0.00.051.792 I llm_load_print_meta: vocab type       = BPE
0.00.051.792 I llm_load_print_meta: n_vocab          = 50304
0.00.051.792 I llm_load_print_meta: n_merges         = 50009
0.00.051.792 I llm_load_print_meta: vocab_only       = 0
0.00.051.792 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.792 I llm_load_print_meta: n_embd           = 2048
0.00.051.793 I llm_load_print_meta: n_layer          = 24
0.00.051.796 I llm_load_print_meta: n_head           = 16
0.00.051.809 I llm_load_print_meta: n_head_kv        = 16
0.00.051.812 I llm_load_print_meta: n_rot            = 32
0.00.051.812 I llm_load_print_meta: n_swa            = 0
0.00.051.812 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.813 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.813 I llm_load_print_meta: n_gqa            = 1
0.00.051.814 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.814 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.815 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.815 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.816 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.816 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.816 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.816 I llm_load_print_meta: n_ff             = 8192
0.00.051.817 I llm_load_print_meta: n_expert         = 0
0.00.051.817 I llm_load_print_meta: n_expert_used    = 0
0.00.051.817 I llm_load_print_meta: causal attn      = 1
0.00.051.817 I llm_load_print_meta: pooling type     = 0
0.00.051.817 I llm_load_print_meta: rope type        = 2
0.00.051.817 I llm_load_print_meta: rope scaling     = linear
0.00.051.818 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.818 I llm_load_print_meta: freq_scale_train = 1
0.00.051.818 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.819 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.820 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.820 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.820 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.820 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.820 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.821 I llm_load_print_meta: model type       = 1.4B
0.00.051.830 I llm_load_print_meta: model ftype      = Q4_0
0.00.051.831 I llm_load_print_meta: model params     = 1.41 B
0.00.051.831 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.051.831 I llm_load_print_meta: general.name     = 1.4B
0.00.051.832 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.832 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.832 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.832 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.832 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.833 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.833 I llm_load_print_meta: max token length = 1024
0.00.053.837 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.838 I llm_load_tensors: offloading output layer to GPU
0.00.053.838 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.849 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.053.850 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.054.861 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.862 I llama_new_context_with_model: n_ctx         = 128
0.00.054.862 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.862 I llama_new_context_with_model: n_batch       = 128
0.00.054.862 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.862 I llama_new_context_with_model: flash_attn    = 0
0.00.054.863 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.863 I llama_new_context_with_model: freq_scale    = 1
0.00.054.863 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.864 I ggml_metal_init: allocating
0.00.054.870 I ggml_metal_init: found device: Apple M4
0.00.054.873 I ggml_metal_init: picking default device: Apple M4
0.00.055.501 I ggml_metal_init: using embedded metal library
0.00.057.942 I ggml_metal_init: GPU name:   Apple M4
0.00.057.944 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.944 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.945 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.945 I ggml_metal_init: simdgroup reduction   = true
0.00.057.945 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.945 I ggml_metal_init: has bfloat            = true
0.00.057.945 I ggml_metal_init: use bfloat            = true
0.00.057.946 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.947 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.317 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.069.559 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.069.569 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.069.592 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.070.577 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.070.578 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.070.579 I llama_new_context_with_model: graph nodes  = 967
0.00.070.579 I llama_new_context_with_model: graph splits = 2
0.00.070.592 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.070.593 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.615.418 I 
0.00.615.452 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.615.464 I perplexity: tokenizing the input ..
0.00.623.235 I perplexity: tokenization took 7.768 ms
0.00.623.240 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.746.175 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.747.468 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.747.485 I llama_perf_context_print:        load time =     605.50 ms
0.00.747.486 I llama_perf_context_print: prompt eval time =     122.71 ms /   128 tokens (    0.96 ms per token,  1043.13 tokens per second)
0.00.747.487 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.747.488 I llama_perf_context_print:       total time =     132.07 ms /   129 tokens
0.00.747.919 I ggml_metal_free: deallocating

real	0m0.763s
user	0m0.080s
sys	0m0.092s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.092 I build: 4399 (0ccae21e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.872 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.847 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.014.851 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.853 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.853 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.854 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.854 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.854 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.855 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.855 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.856 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.856 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.856 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.857 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.857 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.858 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.859 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.859 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.836 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.902 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.870 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.871 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.872 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.872 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.872 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.873 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.023.873 I llama_model_loader: - type  f32:  194 tensors
0.00.023.874 I llama_model_loader: - type q4_1:   97 tensors
0.00.023.874 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.574 I llm_load_vocab: special tokens cache size = 25
0.00.050.601 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.604 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.604 I llm_load_print_meta: arch             = gptneox
0.00.050.605 I llm_load_print_meta: vocab type       = BPE
0.00.050.605 I llm_load_print_meta: n_vocab          = 50304
0.00.050.605 I llm_load_print_meta: n_merges         = 50009
0.00.050.605 I llm_load_print_meta: vocab_only       = 0
0.00.050.605 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.606 I llm_load_print_meta: n_embd           = 2048
0.00.050.606 I llm_load_print_meta: n_layer          = 24
0.00.050.609 I llm_load_print_meta: n_head           = 16
0.00.050.621 I llm_load_print_meta: n_head_kv        = 16
0.00.050.621 I llm_load_print_meta: n_rot            = 32
0.00.050.622 I llm_load_print_meta: n_swa            = 0
0.00.050.622 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.622 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.623 I llm_load_print_meta: n_gqa            = 1
0.00.050.623 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.624 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.625 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.625 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.625 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.625 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.626 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.626 I llm_load_print_meta: n_ff             = 8192
0.00.050.628 I llm_load_print_meta: n_expert         = 0
0.00.050.628 I llm_load_print_meta: n_expert_used    = 0
0.00.050.628 I llm_load_print_meta: causal attn      = 1
0.00.050.628 I llm_load_print_meta: pooling type     = 0
0.00.050.628 I llm_load_print_meta: rope type        = 2
0.00.050.629 I llm_load_print_meta: rope scaling     = linear
0.00.050.629 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.629 I llm_load_print_meta: freq_scale_train = 1
0.00.050.630 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.630 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.630 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.630 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.630 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.630 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.631 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.631 I llm_load_print_meta: model type       = 1.4B
0.00.050.641 I llm_load_print_meta: model ftype      = Q4_1
0.00.050.641 I llm_load_print_meta: model params     = 1.41 B
0.00.050.641 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.050.642 I llm_load_print_meta: general.name     = 1.4B
0.00.050.643 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.643 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.643 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.643 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.643 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.644 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.644 I llm_load_print_meta: max token length = 1024
0.00.052.665 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.666 I llm_load_tensors: offloading output layer to GPU
0.00.052.666 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.676 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.052.678 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.053.623 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.624 I llama_new_context_with_model: n_ctx         = 128
0.00.053.624 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.624 I llama_new_context_with_model: n_batch       = 128
0.00.053.624 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.624 I llama_new_context_with_model: flash_attn    = 0
0.00.053.625 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.625 I llama_new_context_with_model: freq_scale    = 1
0.00.053.625 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.626 I ggml_metal_init: allocating
0.00.053.629 I ggml_metal_init: found device: Apple M4
0.00.053.631 I ggml_metal_init: picking default device: Apple M4
0.00.054.209 I ggml_metal_init: using embedded metal library
0.00.056.624 I ggml_metal_init: GPU name:   Apple M4
0.00.056.626 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.626 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.627 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.627 I ggml_metal_init: simdgroup reduction   = true
0.00.056.627 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.627 I ggml_metal_init: has bfloat            = true
0.00.056.627 I ggml_metal_init: use bfloat            = true
0.00.056.628 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.628 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.454 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.067.767 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.769 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.783 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.797 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.798 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.799 I llama_new_context_with_model: graph nodes  = 967
0.00.068.799 I llama_new_context_with_model: graph splits = 2
0.00.068.812 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.812 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.656.642 I 
0.00.656.689 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.656.708 I perplexity: tokenizing the input ..
0.00.664.466 I perplexity: tokenization took 7.755 ms
0.00.664.471 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.787.510 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.788.680 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.788.698 I llama_perf_context_print:        load time =     647.76 ms
0.00.788.699 I llama_perf_context_print: prompt eval time =     122.81 ms /   128 tokens (    0.96 ms per token,  1042.23 tokens per second)
0.00.788.700 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.788.701 I llama_perf_context_print:       total time =     132.06 ms /   129 tokens
0.00.789.137 I ggml_metal_free: deallocating

real	0m0.803s
user	0m0.079s
sys	0m0.106s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.097 I build: 4399 (0ccae21e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.890 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.667 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.014.672 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.677 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.678 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.678 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.679 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.679 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.682 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.682 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.682 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.683 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.683 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.684 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.684 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.686 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.687 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.687 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.659 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.697 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.608 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.609 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.609 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.609 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.610 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.610 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.023.611 I llama_model_loader: - type  f32:  194 tensors
0.00.023.611 I llama_model_loader: - type q5_0:   97 tensors
0.00.023.611 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.892 I llm_load_vocab: special tokens cache size = 25
0.00.050.925 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.928 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.928 I llm_load_print_meta: arch             = gptneox
0.00.050.928 I llm_load_print_meta: vocab type       = BPE
0.00.050.929 I llm_load_print_meta: n_vocab          = 50304
0.00.050.929 I llm_load_print_meta: n_merges         = 50009
0.00.050.929 I llm_load_print_meta: vocab_only       = 0
0.00.050.929 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.929 I llm_load_print_meta: n_embd           = 2048
0.00.050.930 I llm_load_print_meta: n_layer          = 24
0.00.050.932 I llm_load_print_meta: n_head           = 16
0.00.050.946 I llm_load_print_meta: n_head_kv        = 16
0.00.050.946 I llm_load_print_meta: n_rot            = 32
0.00.050.946 I llm_load_print_meta: n_swa            = 0
0.00.050.947 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.947 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.948 I llm_load_print_meta: n_gqa            = 1
0.00.050.948 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.949 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.950 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.951 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.951 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.952 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.952 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.952 I llm_load_print_meta: n_ff             = 8192
0.00.050.953 I llm_load_print_meta: n_expert         = 0
0.00.050.953 I llm_load_print_meta: n_expert_used    = 0
0.00.050.953 I llm_load_print_meta: causal attn      = 1
0.00.050.953 I llm_load_print_meta: pooling type     = 0
0.00.050.953 I llm_load_print_meta: rope type        = 2
0.00.050.953 I llm_load_print_meta: rope scaling     = linear
0.00.050.954 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.954 I llm_load_print_meta: freq_scale_train = 1
0.00.050.955 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.955 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.955 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.955 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.955 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.955 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.955 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.956 I llm_load_print_meta: model type       = 1.4B
0.00.050.966 I llm_load_print_meta: model ftype      = Q5_0
0.00.050.966 I llm_load_print_meta: model params     = 1.41 B
0.00.050.968 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.050.968 I llm_load_print_meta: general.name     = 1.4B
0.00.050.968 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.968 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.968 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.969 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.969 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.969 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.969 I llm_load_print_meta: max token length = 1024
0.00.053.028 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.028 I llm_load_tensors: offloading output layer to GPU
0.00.053.029 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.039 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.053.040 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.053.938 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.939 I llama_new_context_with_model: n_ctx         = 128
0.00.053.939 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.939 I llama_new_context_with_model: n_batch       = 128
0.00.053.939 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.939 I llama_new_context_with_model: flash_attn    = 0
0.00.053.940 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.940 I llama_new_context_with_model: freq_scale    = 1
0.00.053.940 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.941 I ggml_metal_init: allocating
0.00.053.944 I ggml_metal_init: found device: Apple M4
0.00.053.946 I ggml_metal_init: picking default device: Apple M4
0.00.054.531 I ggml_metal_init: using embedded metal library
0.00.056.932 I ggml_metal_init: GPU name:   Apple M4
0.00.056.933 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.934 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.934 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.934 I ggml_metal_init: simdgroup reduction   = true
0.00.056.934 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.934 I ggml_metal_init: has bfloat            = true
0.00.056.935 I ggml_metal_init: use bfloat            = true
0.00.056.935 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.936 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.934 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.068.245 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.247 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.264 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.069.207 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.069.208 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.069.208 I llama_new_context_with_model: graph nodes  = 967
0.00.069.208 I llama_new_context_with_model: graph splits = 2
0.00.069.221 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.069.222 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.733.626 I 
0.00.733.675 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.733.688 I perplexity: tokenizing the input ..
0.00.741.891 I perplexity: tokenization took 8.202 ms
0.00.741.895 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.877.093 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.878.269 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.878.289 I llama_perf_context_print:        load time =     724.73 ms
0.00.878.290 I llama_perf_context_print: prompt eval time =     134.97 ms /   128 tokens (    1.05 ms per token,   948.34 tokens per second)
0.00.878.290 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.878.291 I llama_perf_context_print:       total time =     144.67 ms /   129 tokens
0.00.878.832 I ggml_metal_free: deallocating

real	0m0.893s
user	0m0.080s
sys	0m0.125s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.098 I build: 4399 (0ccae21e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.575 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.546 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.015.550 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.556 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.557 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.557 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.557 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.558 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.559 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.559 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.559 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.560 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.560 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.560 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.561 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.564 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.564 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.564 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.544 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.586 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.525 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.526 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.526 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.526 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.527 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.527 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.527 I llama_model_loader: - type  f32:  194 tensors
0.00.024.528 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.528 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.249 I llm_load_vocab: special tokens cache size = 25
0.00.051.147 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.150 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.150 I llm_load_print_meta: arch             = gptneox
0.00.051.150 I llm_load_print_meta: vocab type       = BPE
0.00.051.150 I llm_load_print_meta: n_vocab          = 50304
0.00.051.151 I llm_load_print_meta: n_merges         = 50009
0.00.051.151 I llm_load_print_meta: vocab_only       = 0
0.00.051.151 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.151 I llm_load_print_meta: n_embd           = 2048
0.00.051.151 I llm_load_print_meta: n_layer          = 24
0.00.051.155 I llm_load_print_meta: n_head           = 16
0.00.051.167 I llm_load_print_meta: n_head_kv        = 16
0.00.051.168 I llm_load_print_meta: n_rot            = 32
0.00.051.168 I llm_load_print_meta: n_swa            = 0
0.00.051.168 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.168 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.169 I llm_load_print_meta: n_gqa            = 1
0.00.051.170 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.171 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.171 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.171 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.172 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.172 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.172 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.173 I llm_load_print_meta: n_ff             = 8192
0.00.051.173 I llm_load_print_meta: n_expert         = 0
0.00.051.173 I llm_load_print_meta: n_expert_used    = 0
0.00.051.173 I llm_load_print_meta: causal attn      = 1
0.00.051.173 I llm_load_print_meta: pooling type     = 0
0.00.051.173 I llm_load_print_meta: rope type        = 2
0.00.051.173 I llm_load_print_meta: rope scaling     = linear
0.00.051.174 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.174 I llm_load_print_meta: freq_scale_train = 1
0.00.051.174 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.174 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.175 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.175 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.175 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.177 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.177 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.178 I llm_load_print_meta: model type       = 1.4B
0.00.051.187 I llm_load_print_meta: model ftype      = Q5_1
0.00.051.188 I llm_load_print_meta: model params     = 1.41 B
0.00.051.189 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.051.189 I llm_load_print_meta: general.name     = 1.4B
0.00.051.190 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.190 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.190 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.190 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.190 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.191 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.191 I llm_load_print_meta: max token length = 1024
0.00.053.205 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.206 I llm_load_tensors: offloading output layer to GPU
0.00.053.206 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.216 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.053.217 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.054.118 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.119 I llama_new_context_with_model: n_ctx         = 128
0.00.054.119 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.119 I llama_new_context_with_model: n_batch       = 128
0.00.054.119 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.120 I llama_new_context_with_model: flash_attn    = 0
0.00.054.120 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.120 I llama_new_context_with_model: freq_scale    = 1
0.00.054.121 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.121 I ggml_metal_init: allocating
0.00.054.128 I ggml_metal_init: found device: Apple M4
0.00.054.131 I ggml_metal_init: picking default device: Apple M4
0.00.054.720 I ggml_metal_init: using embedded metal library
0.00.057.087 I ggml_metal_init: GPU name:   Apple M4
0.00.057.088 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.089 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.089 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.089 I ggml_metal_init: simdgroup reduction   = true
0.00.057.089 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.090 I ggml_metal_init: has bfloat            = true
0.00.057.090 I ggml_metal_init: use bfloat            = true
0.00.057.090 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.091 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.884 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.068.462 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.465 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.479 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.069.430 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.069.431 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.069.431 I llama_new_context_with_model: graph nodes  = 967
0.00.069.431 I llama_new_context_with_model: graph splits = 2
0.00.069.444 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.069.445 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.637.785 I 
0.00.637.886 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.637.912 I perplexity: tokenizing the input ..
0.00.646.049 I perplexity: tokenization took 8.136 ms
0.00.646.053 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.781.405 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.782.641 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.782.662 I llama_perf_context_print:        load time =     628.20 ms
0.00.782.667 I llama_perf_context_print: prompt eval time =     135.12 ms /   128 tokens (    1.06 ms per token,   947.27 tokens per second)
0.00.782.668 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.782.668 I llama_perf_context_print:       total time =     144.88 ms /   129 tokens
0.00.783.239 I ggml_metal_free: deallocating

real	0m0.798s
user	0m0.079s
sys	0m0.106s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.100 I build: 4399 (0ccae21e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.854 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.540 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.014.544 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.546 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.547 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.547 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.547 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.548 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.549 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.549 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.549 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.550 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.551 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.553 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.553 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.555 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.555 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.555 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.526 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.674 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.529 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.530 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.531 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.531 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.531 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.532 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.023.532 I llama_model_loader: - type  f32:  194 tensors
0.00.023.533 I llama_model_loader: - type q2_K:   49 tensors
0.00.023.533 I llama_model_loader: - type q3_K:   48 tensors
0.00.023.533 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.280 I llm_load_vocab: special tokens cache size = 25
0.00.050.254 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.257 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.257 I llm_load_print_meta: arch             = gptneox
0.00.050.257 I llm_load_print_meta: vocab type       = BPE
0.00.050.258 I llm_load_print_meta: n_vocab          = 50304
0.00.050.258 I llm_load_print_meta: n_merges         = 50009
0.00.050.258 I llm_load_print_meta: vocab_only       = 0
0.00.050.258 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.258 I llm_load_print_meta: n_embd           = 2048
0.00.050.259 I llm_load_print_meta: n_layer          = 24
0.00.050.261 I llm_load_print_meta: n_head           = 16
0.00.050.273 I llm_load_print_meta: n_head_kv        = 16
0.00.050.273 I llm_load_print_meta: n_rot            = 32
0.00.050.273 I llm_load_print_meta: n_swa            = 0
0.00.050.274 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.274 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.275 I llm_load_print_meta: n_gqa            = 1
0.00.050.275 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.276 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.276 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.277 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.277 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.277 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.278 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.279 I llm_load_print_meta: n_ff             = 8192
0.00.050.279 I llm_load_print_meta: n_expert         = 0
0.00.050.279 I llm_load_print_meta: n_expert_used    = 0
0.00.050.279 I llm_load_print_meta: causal attn      = 1
0.00.050.279 I llm_load_print_meta: pooling type     = 0
0.00.050.279 I llm_load_print_meta: rope type        = 2
0.00.050.279 I llm_load_print_meta: rope scaling     = linear
0.00.050.281 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.281 I llm_load_print_meta: freq_scale_train = 1
0.00.050.281 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.283 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.283 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.283 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.283 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.283 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.283 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.284 I llm_load_print_meta: model type       = 1.4B
0.00.050.294 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.050.294 I llm_load_print_meta: model params     = 1.41 B
0.00.050.294 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.050.294 I llm_load_print_meta: general.name     = 1.4B
0.00.050.295 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.295 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.295 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.295 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.295 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.296 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.296 I llm_load_print_meta: max token length = 1024
0.00.052.102 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.103 I llm_load_tensors: offloading output layer to GPU
0.00.052.103 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.109 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.052.109 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.052.999 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.999 I llama_new_context_with_model: n_ctx         = 128
0.00.053.000 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.000 I llama_new_context_with_model: n_batch       = 128
0.00.053.000 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.000 I llama_new_context_with_model: flash_attn    = 0
0.00.053.001 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.001 I llama_new_context_with_model: freq_scale    = 1
0.00.053.001 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.002 I ggml_metal_init: allocating
0.00.053.005 I ggml_metal_init: found device: Apple M4
0.00.053.007 I ggml_metal_init: picking default device: Apple M4
0.00.053.575 I ggml_metal_init: using embedded metal library
0.00.055.942 I ggml_metal_init: GPU name:   Apple M4
0.00.055.943 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.944 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.944 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.944 I ggml_metal_init: simdgroup reduction   = true
0.00.055.945 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.945 I ggml_metal_init: has bfloat            = true
0.00.055.945 I ggml_metal_init: use bfloat            = true
0.00.055.945 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.946 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.738 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.067.091 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.093 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.109 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.023 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.024 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.024 I llama_new_context_with_model: graph nodes  = 967
0.00.068.025 I llama_new_context_with_model: graph splits = 2
0.00.068.032 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.032 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.381.278 I 
0.00.381.324 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.381.337 I perplexity: tokenizing the input ..
0.00.389.748 I perplexity: tokenization took 8.409 ms
0.00.389.751 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.522.155 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.523.332 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.523.356 I llama_perf_context_print:        load time =     372.42 ms
0.00.523.357 I llama_perf_context_print: prompt eval time =     132.18 ms /   128 tokens (    1.03 ms per token,   968.38 tokens per second)
0.00.523.358 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.523.358 I llama_perf_context_print:       total time =     142.08 ms /   129 tokens
0.00.523.923 I ggml_metal_free: deallocating

real	0m0.537s
user	0m0.080s
sys	0m0.066s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.089 I build: 4399 (0ccae21e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.883 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.558 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.014.563 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.565 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.565 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.565 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.566 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.566 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.567 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.569 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.569 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.570 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.570 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.570 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.571 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.577 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.577 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.578 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.502 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.562 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.522 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.523 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.524 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.524 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.524 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.525 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.023.525 I llama_model_loader: - type  f32:  194 tensors
0.00.023.526 I llama_model_loader: - type q3_K:   25 tensors
0.00.023.526 I llama_model_loader: - type q4_K:   71 tensors
0.00.023.526 I llama_model_loader: - type q5_K:    1 tensors
0.00.023.526 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.755 I llm_load_vocab: special tokens cache size = 25
0.00.050.553 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.556 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.556 I llm_load_print_meta: arch             = gptneox
0.00.050.556 I llm_load_print_meta: vocab type       = BPE
0.00.050.557 I llm_load_print_meta: n_vocab          = 50304
0.00.050.557 I llm_load_print_meta: n_merges         = 50009
0.00.050.557 I llm_load_print_meta: vocab_only       = 0
0.00.050.557 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.557 I llm_load_print_meta: n_embd           = 2048
0.00.050.557 I llm_load_print_meta: n_layer          = 24
0.00.050.560 I llm_load_print_meta: n_head           = 16
0.00.050.573 I llm_load_print_meta: n_head_kv        = 16
0.00.050.573 I llm_load_print_meta: n_rot            = 32
0.00.050.573 I llm_load_print_meta: n_swa            = 0
0.00.050.574 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.574 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.574 I llm_load_print_meta: n_gqa            = 1
0.00.050.575 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.576 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.576 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.577 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.577 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.577 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.577 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.579 I llm_load_print_meta: n_ff             = 8192
0.00.050.579 I llm_load_print_meta: n_expert         = 0
0.00.050.579 I llm_load_print_meta: n_expert_used    = 0
0.00.050.579 I llm_load_print_meta: causal attn      = 1
0.00.050.579 I llm_load_print_meta: pooling type     = 0
0.00.050.581 I llm_load_print_meta: rope type        = 2
0.00.050.581 I llm_load_print_meta: rope scaling     = linear
0.00.050.581 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.582 I llm_load_print_meta: freq_scale_train = 1
0.00.050.582 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.582 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.582 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.582 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.582 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.583 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.583 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.583 I llm_load_print_meta: model type       = 1.4B
0.00.050.593 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.050.593 I llm_load_print_meta: model params     = 1.41 B
0.00.050.594 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.050.595 I llm_load_print_meta: general.name     = 1.4B
0.00.050.595 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.595 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.596 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.596 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.596 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.596 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.596 I llm_load_print_meta: max token length = 1024
0.00.052.588 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.589 I llm_load_tensors: offloading output layer to GPU
0.00.052.589 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.600 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.052.601 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.053.484 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.485 I llama_new_context_with_model: n_ctx         = 128
0.00.053.485 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.485 I llama_new_context_with_model: n_batch       = 128
0.00.053.485 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.486 I llama_new_context_with_model: flash_attn    = 0
0.00.053.486 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.486 I llama_new_context_with_model: freq_scale    = 1
0.00.053.487 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.488 I ggml_metal_init: allocating
0.00.053.491 I ggml_metal_init: found device: Apple M4
0.00.053.493 I ggml_metal_init: picking default device: Apple M4
0.00.054.079 I ggml_metal_init: using embedded metal library
0.00.056.531 I ggml_metal_init: GPU name:   Apple M4
0.00.056.533 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.533 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.533 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.534 I ggml_metal_init: simdgroup reduction   = true
0.00.056.534 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.534 I ggml_metal_init: has bfloat            = true
0.00.056.534 I ggml_metal_init: use bfloat            = true
0.00.056.535 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.535 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.648 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.067.869 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.873 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.888 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.802 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.803 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.803 I llama_new_context_with_model: graph nodes  = 967
0.00.068.803 I llama_new_context_with_model: graph splits = 2
0.00.068.816 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.817 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.490.081 I 
0.00.490.113 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.490.124 I perplexity: tokenizing the input ..
0.00.498.160 I perplexity: tokenization took 8.035 ms
0.00.498.170 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.630.029 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.631.198 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.631.217 I llama_perf_context_print:        load time =     481.19 ms
0.00.631.218 I llama_perf_context_print: prompt eval time =     131.62 ms /   128 tokens (    1.03 ms per token,   972.50 tokens per second)
0.00.631.219 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.631.219 I llama_perf_context_print:       total time =     141.14 ms /   129 tokens
0.00.631.767 I ggml_metal_free: deallocating

real	0m0.646s
user	0m0.080s
sys	0m0.093s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.090 I build: 4399 (0ccae21e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.541 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.225 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.015.229 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.231 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.232 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.232 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.232 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.233 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.234 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.234 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.234 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.235 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.238 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.238 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.238 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.240 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.240 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.240 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.154 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.299 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.265 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.266 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.266 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.266 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.267 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.267 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.267 I llama_model_loader: - type  f32:  194 tensors
0.00.024.268 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.268 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.268 I llama_model_loader: - type q6_K:   13 tensors
0.00.045.052 I llm_load_vocab: special tokens cache size = 25
0.00.051.035 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.037 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.038 I llm_load_print_meta: arch             = gptneox
0.00.051.038 I llm_load_print_meta: vocab type       = BPE
0.00.051.038 I llm_load_print_meta: n_vocab          = 50304
0.00.051.038 I llm_load_print_meta: n_merges         = 50009
0.00.051.039 I llm_load_print_meta: vocab_only       = 0
0.00.051.039 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.039 I llm_load_print_meta: n_embd           = 2048
0.00.051.039 I llm_load_print_meta: n_layer          = 24
0.00.051.042 I llm_load_print_meta: n_head           = 16
0.00.051.054 I llm_load_print_meta: n_head_kv        = 16
0.00.051.054 I llm_load_print_meta: n_rot            = 32
0.00.051.054 I llm_load_print_meta: n_swa            = 0
0.00.051.055 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.055 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.056 I llm_load_print_meta: n_gqa            = 1
0.00.051.056 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.057 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.058 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.058 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.058 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.059 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.059 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.059 I llm_load_print_meta: n_ff             = 8192
0.00.051.059 I llm_load_print_meta: n_expert         = 0
0.00.051.060 I llm_load_print_meta: n_expert_used    = 0
0.00.051.060 I llm_load_print_meta: causal attn      = 1
0.00.051.060 I llm_load_print_meta: pooling type     = 0
0.00.051.060 I llm_load_print_meta: rope type        = 2
0.00.051.060 I llm_load_print_meta: rope scaling     = linear
0.00.051.060 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.061 I llm_load_print_meta: freq_scale_train = 1
0.00.051.061 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.061 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.061 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.061 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.061 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.062 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.062 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.063 I llm_load_print_meta: model type       = 1.4B
0.00.051.073 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.051.073 I llm_load_print_meta: model params     = 1.41 B
0.00.051.074 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.051.074 I llm_load_print_meta: general.name     = 1.4B
0.00.051.074 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.074 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.074 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.074 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.075 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.076 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.076 I llm_load_print_meta: max token length = 1024
0.00.053.067 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.067 I llm_load_tensors: offloading output layer to GPU
0.00.053.068 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.078 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.053.079 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.053.971 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.971 I llama_new_context_with_model: n_ctx         = 128
0.00.053.972 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.972 I llama_new_context_with_model: n_batch       = 128
0.00.053.972 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.972 I llama_new_context_with_model: flash_attn    = 0
0.00.053.972 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.973 I llama_new_context_with_model: freq_scale    = 1
0.00.053.973 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.974 I ggml_metal_init: allocating
0.00.053.977 I ggml_metal_init: found device: Apple M4
0.00.053.979 I ggml_metal_init: picking default device: Apple M4
0.00.054.545 I ggml_metal_init: using embedded metal library
0.00.056.906 I ggml_metal_init: GPU name:   Apple M4
0.00.056.907 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.908 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.908 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.908 I ggml_metal_init: simdgroup reduction   = true
0.00.056.909 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.909 I ggml_metal_init: has bfloat            = true
0.00.056.909 I ggml_metal_init: use bfloat            = true
0.00.056.909 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.910 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.759 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.068.007 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.012 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.028 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.899 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.900 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.900 I llama_new_context_with_model: graph nodes  = 967
0.00.068.901 I llama_new_context_with_model: graph splits = 2
0.00.068.913 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.914 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.557.174 I 
0.00.557.216 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.557.228 I perplexity: tokenizing the input ..
0.00.565.382 I perplexity: tokenization took 8.153 ms
0.00.565.388 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.699.293 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.700.634 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.700.646 I llama_perf_context_print:        load time =     547.63 ms
0.00.700.647 I llama_perf_context_print: prompt eval time =     133.68 ms /   128 tokens (    1.04 ms per token,   957.52 tokens per second)
0.00.700.648 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.700.648 I llama_perf_context_print:       total time =     143.47 ms /   129 tokens
0.00.700.983 I ggml_metal_free: deallocating

real	0m0.715s
user	0m0.079s
sys	0m0.102s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.094 I build: 4399 (0ccae21e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.735 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.677 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.014.681 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.683 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.684 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.684 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.684 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.685 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.685 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.686 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.686 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.686 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.687 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.687 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.687 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.689 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.689 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.689 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.614 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.702 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.625 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.626 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.627 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.627 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.627 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.627 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.023.628 I llama_model_loader: - type  f32:  194 tensors
0.00.023.628 I llama_model_loader: - type q5_K:   61 tensors
0.00.023.629 I llama_model_loader: - type q6_K:   37 tensors
0.00.044.374 I llm_load_vocab: special tokens cache size = 25
0.00.050.191 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.194 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.194 I llm_load_print_meta: arch             = gptneox
0.00.050.194 I llm_load_print_meta: vocab type       = BPE
0.00.050.194 I llm_load_print_meta: n_vocab          = 50304
0.00.050.195 I llm_load_print_meta: n_merges         = 50009
0.00.050.195 I llm_load_print_meta: vocab_only       = 0
0.00.050.195 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.195 I llm_load_print_meta: n_embd           = 2048
0.00.050.195 I llm_load_print_meta: n_layer          = 24
0.00.050.198 I llm_load_print_meta: n_head           = 16
0.00.050.205 I llm_load_print_meta: n_head_kv        = 16
0.00.050.206 I llm_load_print_meta: n_rot            = 32
0.00.050.206 I llm_load_print_meta: n_swa            = 0
0.00.050.206 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.206 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.207 I llm_load_print_meta: n_gqa            = 1
0.00.050.208 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.208 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.209 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.209 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.210 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.210 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.210 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.211 I llm_load_print_meta: n_ff             = 8192
0.00.050.211 I llm_load_print_meta: n_expert         = 0
0.00.050.211 I llm_load_print_meta: n_expert_used    = 0
0.00.050.211 I llm_load_print_meta: causal attn      = 1
0.00.050.211 I llm_load_print_meta: pooling type     = 0
0.00.050.211 I llm_load_print_meta: rope type        = 2
0.00.050.212 I llm_load_print_meta: rope scaling     = linear
0.00.050.212 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.213 I llm_load_print_meta: freq_scale_train = 1
0.00.050.213 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.213 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.213 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.213 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.213 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.213 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.214 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.214 I llm_load_print_meta: model type       = 1.4B
0.00.050.219 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.050.219 I llm_load_print_meta: model params     = 1.41 B
0.00.050.221 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.050.222 I llm_load_print_meta: general.name     = 1.4B
0.00.050.222 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.222 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.222 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.222 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.222 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.224 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.224 I llm_load_print_meta: max token length = 1024
0.00.052.018 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.019 I llm_load_tensors: offloading output layer to GPU
0.00.052.019 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.024 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.052.025 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.052.927 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.928 I llama_new_context_with_model: n_ctx         = 128
0.00.052.928 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.928 I llama_new_context_with_model: n_batch       = 128
0.00.052.928 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.928 I llama_new_context_with_model: flash_attn    = 0
0.00.052.929 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.929 I llama_new_context_with_model: freq_scale    = 1
0.00.052.929 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.930 I ggml_metal_init: allocating
0.00.052.933 I ggml_metal_init: found device: Apple M4
0.00.052.935 I ggml_metal_init: picking default device: Apple M4
0.00.053.512 I ggml_metal_init: using embedded metal library
0.00.055.852 I ggml_metal_init: GPU name:   Apple M4
0.00.055.854 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.854 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.854 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.855 I ggml_metal_init: simdgroup reduction   = true
0.00.055.855 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.855 I ggml_metal_init: has bfloat            = true
0.00.055.855 I ggml_metal_init: use bfloat            = true
0.00.055.856 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.856 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.785 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.067.044 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.049 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.065 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.969 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.970 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.971 I llama_new_context_with_model: graph nodes  = 967
0.00.067.971 I llama_new_context_with_model: graph splits = 2
0.00.067.978 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.979 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.661.488 I 
0.00.661.569 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.661.591 I perplexity: tokenizing the input ..
0.00.669.709 I perplexity: tokenization took 8.116 ms
0.00.669.712 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.810.668 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.811.915 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.811.927 I llama_perf_context_print:        load time =     652.75 ms
0.00.811.928 I llama_perf_context_print: prompt eval time =     140.73 ms /   128 tokens (    1.10 ms per token,   909.54 tokens per second)
0.00.811.929 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.811.929 I llama_perf_context_print:       total time =     150.44 ms /   129 tokens
0.00.812.462 I ggml_metal_free: deallocating

real	0m0.826s
user	0m0.080s
sys	0m0.137s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.089 I build: 4399 (0ccae21e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.792 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.819 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.014.823 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.824 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.829 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.830 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.830 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.831 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.831 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.832 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.832 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.832 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.833 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.835 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.835 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.836 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.837 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.837 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.746 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.741 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.616 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.617 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.617 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.618 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.618 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.618 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.023.619 I llama_model_loader: - type  f32:  194 tensors
0.00.023.619 I llama_model_loader: - type q6_K:   98 tensors
0.00.044.453 I llm_load_vocab: special tokens cache size = 25
0.00.050.476 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.478 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.479 I llm_load_print_meta: arch             = gptneox
0.00.050.479 I llm_load_print_meta: vocab type       = BPE
0.00.050.479 I llm_load_print_meta: n_vocab          = 50304
0.00.050.480 I llm_load_print_meta: n_merges         = 50009
0.00.050.480 I llm_load_print_meta: vocab_only       = 0
0.00.050.480 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.480 I llm_load_print_meta: n_embd           = 2048
0.00.050.480 I llm_load_print_meta: n_layer          = 24
0.00.050.483 I llm_load_print_meta: n_head           = 16
0.00.050.496 I llm_load_print_meta: n_head_kv        = 16
0.00.050.496 I llm_load_print_meta: n_rot            = 32
0.00.050.496 I llm_load_print_meta: n_swa            = 0
0.00.050.496 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.496 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.497 I llm_load_print_meta: n_gqa            = 1
0.00.050.498 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.498 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.499 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.499 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.499 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.500 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.500 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.502 I llm_load_print_meta: n_ff             = 8192
0.00.050.503 I llm_load_print_meta: n_expert         = 0
0.00.050.503 I llm_load_print_meta: n_expert_used    = 0
0.00.050.503 I llm_load_print_meta: causal attn      = 1
0.00.050.503 I llm_load_print_meta: pooling type     = 0
0.00.050.503 I llm_load_print_meta: rope type        = 2
0.00.050.504 I llm_load_print_meta: rope scaling     = linear
0.00.050.505 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.505 I llm_load_print_meta: freq_scale_train = 1
0.00.050.505 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.506 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.506 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.507 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.507 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.507 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.507 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.508 I llm_load_print_meta: model type       = 1.4B
0.00.050.517 I llm_load_print_meta: model ftype      = Q6_K
0.00.050.518 I llm_load_print_meta: model params     = 1.41 B
0.00.050.518 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.050.518 I llm_load_print_meta: general.name     = 1.4B
0.00.050.518 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.518 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.519 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.519 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.519 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.519 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.519 I llm_load_print_meta: max token length = 1024
0.00.052.599 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.600 I llm_load_tensors: offloading output layer to GPU
0.00.052.600 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.611 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.052.612 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.053.531 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.532 I llama_new_context_with_model: n_ctx         = 128
0.00.053.532 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.532 I llama_new_context_with_model: n_batch       = 128
0.00.053.533 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.533 I llama_new_context_with_model: flash_attn    = 0
0.00.053.533 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.533 I llama_new_context_with_model: freq_scale    = 1
0.00.053.534 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.534 I ggml_metal_init: allocating
0.00.053.537 I ggml_metal_init: found device: Apple M4
0.00.053.540 I ggml_metal_init: picking default device: Apple M4
0.00.054.134 I ggml_metal_init: using embedded metal library
0.00.056.563 I ggml_metal_init: GPU name:   Apple M4
0.00.056.565 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.565 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.565 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.566 I ggml_metal_init: simdgroup reduction   = true
0.00.056.566 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.566 I ggml_metal_init: has bfloat            = true
0.00.056.566 I ggml_metal_init: use bfloat            = true
0.00.056.566 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.567 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.398 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.067.670 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.672 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.686 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.700 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.701 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.701 I llama_new_context_with_model: graph nodes  = 967
0.00.068.701 I llama_new_context_with_model: graph splits = 2
0.00.068.714 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.714 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.660.352 I 
0.00.660.420 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.660.440 I perplexity: tokenizing the input ..
0.00.668.551 I perplexity: tokenization took 8.109 ms
0.00.668.554 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.807.646 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.809.043 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.809.062 I llama_perf_context_print:        load time =     651.56 ms
0.00.809.062 I llama_perf_context_print: prompt eval time =     138.86 ms /   128 tokens (    1.08 ms per token,   921.81 tokens per second)
0.00.809.063 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.809.063 I llama_perf_context_print:       total time =     148.71 ms /   129 tokens
0.00.809.442 I ggml_metal_free: deallocating

real	0m0.823s
user	0m0.079s
sys	0m0.123s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.221 I build: 4399 (0ccae21e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.023.381 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.034.745 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.034.761 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.034.764 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.034.778 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.034.779 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.034.780 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.034.780 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.034.782 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.034.783 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.034.783 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.034.784 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.034.785 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.034.785 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.034.786 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.034.791 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.034.792 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.034.792 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.043.227 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.045.431 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.053.375 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.053.378 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.053.378 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.053.379 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.053.379 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.053.380 I llama_model_loader: - type  f32:  194 tensors
0.00.053.381 I llama_model_loader: - type  f16:   98 tensors
0.00.084.784 I llm_load_vocab: special tokens cache size = 25
0.00.091.672 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.091.675 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.091.675 I llm_load_print_meta: arch             = gptneox
0.00.091.675 I llm_load_print_meta: vocab type       = BPE
0.00.091.675 I llm_load_print_meta: n_vocab          = 50304
0.00.091.675 I llm_load_print_meta: n_merges         = 50009
0.00.091.676 I llm_load_print_meta: vocab_only       = 0
0.00.091.676 I llm_load_print_meta: n_ctx_train      = 2048
0.00.091.676 I llm_load_print_meta: n_embd           = 2048
0.00.091.676 I llm_load_print_meta: n_layer          = 24
0.00.091.680 I llm_load_print_meta: n_head           = 16
0.00.091.692 I llm_load_print_meta: n_head_kv        = 16
0.00.091.693 I llm_load_print_meta: n_rot            = 32
0.00.091.693 I llm_load_print_meta: n_swa            = 0
0.00.091.693 I llm_load_print_meta: n_embd_head_k    = 128
0.00.091.693 I llm_load_print_meta: n_embd_head_v    = 128
0.00.091.694 I llm_load_print_meta: n_gqa            = 1
0.00.091.695 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.091.695 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.091.696 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.091.696 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.091.696 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.091.698 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.091.699 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.091.699 I llm_load_print_meta: n_ff             = 8192
0.00.091.699 I llm_load_print_meta: n_expert         = 0
0.00.091.700 I llm_load_print_meta: n_expert_used    = 0
0.00.091.700 I llm_load_print_meta: causal attn      = 1
0.00.091.700 I llm_load_print_meta: pooling type     = 0
0.00.091.700 I llm_load_print_meta: rope type        = 2
0.00.091.700 I llm_load_print_meta: rope scaling     = linear
0.00.091.701 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.091.701 I llm_load_print_meta: freq_scale_train = 1
0.00.091.701 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.091.701 I llm_load_print_meta: rope_finetuned   = unknown
0.00.091.703 I llm_load_print_meta: ssm_d_conv       = 0
0.00.091.703 I llm_load_print_meta: ssm_d_inner      = 0
0.00.091.703 I llm_load_print_meta: ssm_d_state      = 0
0.00.091.704 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.091.704 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.091.704 I llm_load_print_meta: model type       = 1.4B
0.00.091.714 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.091.714 I llm_load_print_meta: model params     = 1.41 B
0.00.091.715 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.091.715 I llm_load_print_meta: general.name     = 1.4B
0.00.091.715 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.091.715 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.091.717 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.091.717 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.091.717 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.091.718 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.091.718 I llm_load_print_meta: max token length = 1024
0.00.094.349 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.094.350 I llm_load_tensors: offloading output layer to GPU
0.00.094.350 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.094.361 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.094.361 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.095.406 I llama_new_context_with_model: n_seq_max     = 1
0.00.095.407 I llama_new_context_with_model: n_ctx         = 128
0.00.095.407 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.095.408 I llama_new_context_with_model: n_batch       = 128
0.00.095.408 I llama_new_context_with_model: n_ubatch      = 128
0.00.095.408 I llama_new_context_with_model: flash_attn    = 0
0.00.095.408 I llama_new_context_with_model: freq_base     = 10000.0
0.00.095.409 I llama_new_context_with_model: freq_scale    = 1
0.00.095.409 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.095.409 I ggml_metal_init: allocating
0.00.095.412 I ggml_metal_init: found device: Apple M4
0.00.095.414 I ggml_metal_init: picking default device: Apple M4
0.00.096.048 I ggml_metal_init: using embedded metal library
0.00.098.718 I ggml_metal_init: GPU name:   Apple M4
0.00.098.720 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.098.721 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.098.721 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.098.721 I ggml_metal_init: simdgroup reduction   = true
0.00.098.721 I ggml_metal_init: simdgroup matrix mul. = true
0.00.098.722 I ggml_metal_init: has bfloat            = true
0.00.098.722 I ggml_metal_init: use bfloat            = true
0.00.098.722 I ggml_metal_init: hasUnifiedMemory      = true
0.00.098.723 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.108.296 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.109.533 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.109.539 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.109.553 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.110.465 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.110.466 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.110.466 I llama_new_context_with_model: graph nodes  = 967
0.00.110.467 I llama_new_context_with_model: graph splits = 2
0.00.110.480 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.110.480 I 
0.00.110.516 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.110.518 I compute_imatrix: tokenizing the input ..
0.00.117.740 I compute_imatrix: tokenization took 7.22 ms
0.00.117.743 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.655.977 I compute_imatrix: 1.54 seconds per pass - ETA 0.02 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.658.371 I llama_perf_context_print:        load time =    1632.59 ms
0.01.658.372 I llama_perf_context_print: prompt eval time =    1537.52 ms /   128 tokens (   12.01 ms per token,    83.25 tokens per second)
0.01.658.373 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.658.373 I llama_perf_context_print:       total time =    1634.98 ms /   129 tokens
0.01.658.927 I ggml_metal_free: deallocating

real	0m1.848s
user	0m0.170s
sys	0m0.249s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4399 (0ccae21e)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 'Ä'
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x146e0a360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x146e0aa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x146e0b020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x146e0b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x146e0bb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x146e0c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x146e0c6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x146e0cc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x146e0d240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x146e0d740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x146e0dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x146e0e140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x146e0ec60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x146e0f410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x146e0fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x146e10340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x146e10a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x146e11180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x146e118a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x146e12070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x146e12790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x146e12eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x146e135d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x146e13e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x146e14590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x146e14850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x146e14e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x146e15ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x146e16010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x146e162d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x146e16770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x146e16a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x146e172c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x146e17800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x146e17ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x146e17f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x146e18400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x146e188a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x146e18d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x146e191e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x146e19680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x146e19b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x146e19fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x146e1a460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x146e1a720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x146e1ad30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x146e1b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x146e1bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x146e1c270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x146e1c880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x146e1ce90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x146e1d4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x146e1dab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x146e1e0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x146e1e8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x146e1ed50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x146e1f1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x146e1f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x146e1fac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x146e202b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x146e20570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x146e20a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x146e20eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x146e21350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x146e217f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x146e21c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x146e22130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x146e225d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x146e22a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x146e22f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x146e233b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x146e23850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x146e23cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x146e24240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x146e24790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x146e24ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x146e25230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x146e25780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x146e25cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x146e26220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x146e26770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x146e26cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x146e27210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x146e27760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x146e27cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x146e28200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x146e28750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x146e28ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x146e291f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x146e29740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x146e29c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x146e2a1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x146e2a730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x146e2ac80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x146e2b1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x146e2b720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x146e2bc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x146e1b950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x146e2c0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x146e2c890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x146e2cde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x146e2d330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x146e2d880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x146e2ddd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x146e2e320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x146e2e870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x146e2edc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x146e2f310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x146e2f860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x146e2fdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x146e30300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x146e30850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x146e30da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x146e31240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x146e316e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x146e31b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x146e32020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x146e324c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x146e32960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x146e32e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x146e332a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x146e33740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x146e33be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x146e34080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x146e34520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x146e349c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x146e34e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x146e35300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x146e357a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x146e35c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x146e360e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x146e36580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x146e36a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x146e36ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x146e37360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x146e37800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x146e37ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x146e38140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x146e385e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x146e38a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x146e38f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x146e393c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x146e39860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x146e39d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x146e3a1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x146e3a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x146e3aae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x146e3af80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x146e3b420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x146e3b8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x146e3bd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x146e3c200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x146e3c6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x146e3cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x146e3cfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x146e3d480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x146e3d920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x146e3ddc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x146e3e260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x146e3e700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x146e3eba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x146e3f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x146e3f4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x146e3f980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x146e3fe20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x146e402c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x146e40760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x146e40c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x146e410a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x146e41540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x146e419e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x146e41e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x146e42320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x146e427c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x146e42c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x146e43100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x146e435a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x146e43a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x146e43ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x146e44380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x146e44820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x146e44cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x146e45160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x146e45600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x146e45aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x146e45f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x146e463e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x146e46880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x146e46d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x146e471c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x146e47660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x146e47b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x146e47fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x146e484f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x146e48a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x146e48f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x146e494e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x146e497a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x146e49db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x146e4a3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x146e4a9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x146e4b1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x146e4b660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x146e4b920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x146e4bf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x146e4c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x146e4cd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x146e4d1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x146e4d670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x146e4db10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x146e4e2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x146e4e810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x146e4ed60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x146e4f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x146e4f800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x146e4fd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x146e502a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x146e507f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x146e50d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x146e51290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x146e517e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x146e51d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x146e52280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x146e527d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x146e52d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x146e53270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x146e537c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x146e53d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x146e54260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x146e547b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x146e54d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x146e55250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x146e557a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x146e55cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x146e56240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x146e56790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x146e56ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x146e57230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x146e57780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x146e57cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x146e58220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x146e58770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x146e58cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x146e59210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x146e59760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x146e59cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x146e5a200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x146e5a750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x146e5aca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x146e5b1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x146e5b740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x146e5bc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x146e5c1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x146e5c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x146e5cc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x146e5d1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x146e5d720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x146e5dc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x146e5e1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x146e5e710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x146e5ec60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x146e5f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x146e5f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x146e5fc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x146e601a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x146e606f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x146e60c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x146e610e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x146e61580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x146e61a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x146e61ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x146e62360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x146e62800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x146e62ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x146e63140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x146e635e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x146e63a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x146e63f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x146e643c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x146e64860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x146e64d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x146e651a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x146e656f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x146e65e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x146e66530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x146e66c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x146e67370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x146e67630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x146e67e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x146e680e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x146e686f0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
0.00.141.203 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.141.206 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x130d04bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x130d05040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x130d054b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x130d05920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x130d05d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x130d06200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x130d06670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x130d06ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x130d06f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x130d073c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x130d07830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x130d07f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x130d08a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x130d091f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x130d09a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x130d0a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x130d0a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x130d0af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x130d0b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x130d0bdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x130d0c4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x130d0cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x130d0d310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x130d0da30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x130d0e150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x130d0e410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x130d0e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x130d0eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x130d0efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x130d0f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x130d0f890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x130d0fdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x130d10230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x130d104f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x130d10960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x130d10dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x130d11240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x130d116b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x130d11b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x130d11f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x130d12400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x130d12870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x130d12ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x130d13150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x130d135c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x130d13a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x130d13ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x130d14310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x130d14780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x130d14bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x130d15060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x130d154d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x130d15940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x130d15db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x130d16220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x130d16690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x130d16c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x130d17100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x130d17570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x130d179e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x130d17e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x130d182c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x130d18730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x130d18ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x130d19010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x130d19480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x130d198f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x130d19d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x130d1a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x130d1a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x130d1aab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x130d1af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x130d1b390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x130d1b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x130d1bc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x130d1c0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x130d1c550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x130d1c9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x130d1ce30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x130d1d2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x130d1d710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x130d1db80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x130d1dff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x130d1e460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x130d1e8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x130d1ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x130d1f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x130d1f620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x130d1fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x130d1ff00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x130d20370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x130d207e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x130d20c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x130d210c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x130d21530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x130d219a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x130d21e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x130d22280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x130d226f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x130d22b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x130d22fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x130d23440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x130d238b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x130d23d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x130d24190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x130d24600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x130d24a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x130d24ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x130d25350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x130d257c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x130d25c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x130d260a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x130d26510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x130d26980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x130d26df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x130d27260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x130d276d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x130d27b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x130d27fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x130d28420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x130d28890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x130d28d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x130d29170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x130d295e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x130d29a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x130d29ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x130d2a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x130d2a7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x130d2ac10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x130d2b080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x130d2b4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x130d2b960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x130d2bdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x130d2c240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x130d2c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x130d2cb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x130d2cf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x130d2d400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x130d2d870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x130d2dce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x130d2e150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x130d2e5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x130d2ea30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x130d2eea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x130d2f310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x130d2f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x130d2fbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x130d30060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x130d304d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x130d30940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x130d30db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x130d31220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x130d31690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x130d31b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x130d31f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x130d323e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x130d32850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x130d32cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x130d33130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x130d335a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x130d33a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x130d33e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x130d342f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x130d34760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x130d34bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x130d35040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x130d35c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x130d35f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x130d361f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x130d36660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x130d36ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x130d36f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x130d373b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x130d37820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x130d37c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x130d38100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x130d38570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x130d389e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x130d38e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x130d392c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x130d39730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x130d39ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x130d3a010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x130d3a480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x130d3a8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x130d3ad60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x130d3b1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x130d3b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x130d3bab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x130d3bf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x130d3c390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x130d3c800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x130d3cc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x130d3d0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x130d3d550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x130d3d9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x130d3de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x130d3e2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x130d3e710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x130d3eb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x130d3eff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x130d3f460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x130d3f9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x130d3fed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x130d40340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x130d407b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x130d40c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x130d41090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x130d415b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x130d41ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x130d42630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x130d428f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x130d42eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x130d43470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x130d43a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x130d43ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x130d445b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x130d44b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x130d45130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x130d456f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x130d45cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x130d46270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x130d46830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x130d46df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x130d473b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x130d47970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x130d47f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x130d484f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x130d48ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x130d49070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x130d49630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x130d49bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x130d4a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x130d4a770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x130d4ad30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x130d4b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x130d4b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x130d4be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x130d4c430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x130d4c9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x130d4cfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x130d4d570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x130d4db30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x130d4e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x130d4e6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x130d4ec70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x130d4f230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x130d4f7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x130d4fdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x130d50370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x130d50930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x130d50ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x130d514b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x130d51a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x130d52030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x130d525f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x130d52bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x130d53170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x130d53730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x130d53cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x130d542b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x130d54870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x130d54e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x130d553f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x130d559b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x130d55f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x130d56530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x130d56af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x130d56ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x130d574f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x130d579f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x130d57ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x130d583f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x130d588f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x130d58df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x130d592f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x130d597f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x130d59cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x130d5a1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x130d5a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x130d5abf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x130d5b0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x130d5b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x130d5c000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x130d5c720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x130d5ce40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x130d5d560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x130d5d820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x130d5e010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x130d5e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x130d5e8e0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x130d5b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x130d4c6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x130d4b5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x130d481f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x130d459b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x130d550f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x130d528b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x130d50630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x130d4e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x130d46530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x130d43cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x130d48d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x130d49eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x130d4f4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x130d4c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x130d53fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x130d47c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x130d511b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x130d4aa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x130d4ccb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x130d47670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x130d556b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x130d44870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x130d43170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x130d453f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x130d55c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x130d4aff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x130d53430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x130d49330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x130d4bb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x130d4fab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x130d470b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x130d50070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x130d51770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x130d45f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x130d54570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x130d51d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x130d4d830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x130d567f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x130d44e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x130d56230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x130d442b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x130d54b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x130d4e970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x130d50bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x130d539f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x130d522f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x130d4a470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x130d41d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x130d04680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x130d5dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x130d0b940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x130d5ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x130d5f000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x130d5f2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x130d5f580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x130d5f840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x130d5fb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x130d5fdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x130d60080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x130d60340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x130d60600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x130d608c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x130d60b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x130d60e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x130d61100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x130d613c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x130d61680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x130d61940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x130d61c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x130d61ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x130d62180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x130d62440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x130d62700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x130d629c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x130d62c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x130d62f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x130d63200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x130d634c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x130d63780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x130d63a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x130d63d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x130d63fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x130d64280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x130d64540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x130d64800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x130d64ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x130d64d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x130d65040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x130d65300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x130d655c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x130d65880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x130d65b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x130d65e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x130d660c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x130d66380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x130d66640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x130d66900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x130d66bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x130d66e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x130d67140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x130d67400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x130d676c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x130d67980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x130d67c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x130d67f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x130d681c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x130d68480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x130d68740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x130d68a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x130d68cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x130d68f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x130d69240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x130d69500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x130d697c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x130d69a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x130d69d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x130d6a000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x130d6a2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x130d6a580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x130d6a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x130d6ab00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x130d6adc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x130d6b080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x130d6b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x130d6b600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x130d6b8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x130d6bb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x130d6be40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x130d6c100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x130d6c3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x130d6c680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x130d6c940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x130d6cc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x130d6cec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x130d6d180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x130d6d440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x130d6d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x130d6d9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x130d6dc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x130d6df40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x130d6e200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x130d6e4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x130d6e780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x130d6ea40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x130d6ed00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x130d6efc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x130d6f280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x130d6f540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x130d6f800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x130d6fac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x130d6fd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x130d70040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x130d70300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x130d705c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x130d70880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x130d70b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x130d70e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x130d710c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x130d71380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x130d71640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x130d71900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x130d71bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x130d71e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x130d72140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x130d72400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x130d726c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x130d72980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x130d72c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x130d72f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x130d731c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x130d73480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x130d73740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x130d73a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x130d73cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x130d73f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x130d74240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x130d74500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x130d747c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x130d74a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x130d74d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x130d75000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x130d752c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x130d75580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x130d75840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x130d75b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x130d75dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x130d76080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x130d76340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x130d76600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x130d768c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x130d76b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x130d76e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x130d77100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x130d773c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x130d77680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x130d77940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x130d77c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x130d77ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x130d78180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x130d78440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x130d78700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x130d789c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x130d78c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x130d78f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x130d79200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x130d794c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x130d79780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x130d79a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x130d79d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x130d7a2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x130d7a590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x130d7a850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x130d7ab10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x130d7add0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x130d7b090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x130d7b350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x130d7b610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x130d7bb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x130d7c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x130d7c600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x130d7cb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x130d7d0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x130d7d5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x130d7db40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x130d7e090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x130d7e5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x130d7eb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x130d7f080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x130d7f5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x130d7fb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x130d80070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x130d805c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x130d80b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x130d81060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x130d815b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x130d81b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x130d82050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x130d825a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x130d82af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x130d83040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x130d83590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x130d83ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x130d84030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x130d84580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x130d84ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x130d85020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x130d85570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x130d85ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x130d86010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x130d86560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x130d86ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x130d87000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x130d87550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x130d87aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x130d87ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x130d88540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x130d88a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x130d88fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x130d89530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x130d89a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x130d89fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x130d8a520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x130d8aa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x130d8afc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x130d8b510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x130d8ba60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x130d8bd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x130d8bfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x130d8c2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x130d8c710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x130d8cb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x130d8cff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x130d8d460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x130d8d8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x130d8dd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x130d8e1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x130d8e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x130d8ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x130d8ef00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x130d8f370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x130d8f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x130d8fc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x130d900c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x130d90db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x130d914d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x130d91bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x130d91eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x130d92320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x130d92920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x130d92f30 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.843s
user	0m0.295s
sys	0m0.327s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4399 (0ccae21e)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 'Ä'
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x15be0e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x15be0eac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x15be0f070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x15be0f620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x15be0fbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x15be10180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x15be10730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x15be10ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x15be11290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x15be11790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x15be11c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x15be12190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x15be12cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x15be13460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x15be13c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x15be14390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x15be14ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x15be151d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x15be158f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x15be160c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x15be167e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x15be16f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x15be17620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x15be17ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x15be185e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x15be188a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x15be18eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x15be19b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x15be1a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x15be1a320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x15be1a7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x15be1aa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x15be1b310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x15be1b850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x15be1bb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x15be1bfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x15be1c450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x15be1c8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x15be1cd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x15be1d230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x15be1d6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x15be1db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x15be1e010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x15be1e4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x15be1e770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x15be1ed80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x15be1f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x15be1fcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x15be202c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x15be208d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x15be20ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x15be214f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x15be21b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x15be22110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x15be22900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x15be22da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x15be23240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x15be23500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x15be23b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x15be24300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x15be245c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x15be24a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x15be24f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x15be253a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x15be25840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x15be25ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x15be26180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x15be26620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x15be26ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x15be26f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x15be27400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x15be278a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x15be27d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x15be28290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x15be287e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x15be28d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x15be29280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x15be297d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x15be29d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x15be2a270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x15be2a7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x15be2ad10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x15be2b260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x15be2b7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x15be2bd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x15be2c250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x15be2c7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x15be2ccf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x15be2d240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x15be2d790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x15be2dce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x15be2e230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x15be2e780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x15be2ecd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x15be2f220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x15be2f770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x15be2fcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x15be1f9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x15be30130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x15be308e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x15be30e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x15be31380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x15be318d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x15be31e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x15be32370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x15be328c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x15be32e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x15be33360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x15be338b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x15be33e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x15be34350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x15be348a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x15be34df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x15be35290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x15be35730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x15be35bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x15be36070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x15be36510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x15be369b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x15be36e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x15be372f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x15be37790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x15be37c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x15be380d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x15be38570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x15be38a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x15be38eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x15be39350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x15be397f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x15be39c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x15be3a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x15be3a5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x15be3aa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x15be3af10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x15be3b3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x15be3b850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x15be3bcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x15be3c190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x15be3c630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x15be3cad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x15be3cf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x15be3d410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x15be3d8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x15be3dd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x15be3e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x15be3e690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x15be3eb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x15be3efd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x15be3f470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x15be3f910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x15be3fdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x15be40250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x15be406f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x15be40b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x15be41030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x15be414d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x15be41970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x15be41e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x15be422b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x15be42750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x15be42bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x15be43090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x15be43530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x15be439d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x15be43e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x15be44310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x15be447b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x15be44c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x15be450f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x15be45590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x15be45a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x15be45ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x15be46370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x15be46810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x15be46cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x15be47150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x15be475f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x15be47a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x15be47f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x15be483d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x15be48870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x15be48d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x15be491b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x15be49650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x15be49af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x15be49f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x15be4a430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x15be4a8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x15be4ad70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x15be4b210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x15be4b6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x15be4bb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x15be4bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x15be4c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x15be4ca90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x15be4cfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x15be4d530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x15be4d7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x15be4de00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x15be4e410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x15be4ea20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x15be4f210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x15be4f6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x15be4f970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x15be4ff80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x15be50590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x15be50d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x15be51220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x15be516c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x15be51b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x15be52310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x15be52860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x15be52db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x15be53300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x15be53850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x15be53da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x15be542f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x15be54840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x15be54d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x15be552e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x15be55830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x15be55d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x15be562d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x15be56820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x15be56d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x15be572c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x15be57810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x15be57d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x15be582b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x15be58800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x15be58d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x15be592a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x15be597f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x15be59d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x15be5a290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x15be5a7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x15be5ad30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x15be5b280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x15be5b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x15be5bd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x15be5c270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x15be5c7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x15be5cd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x15be5d260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x15be5d7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x15be5dd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x15be5e250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x15be5e7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x15be5ecf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x15be5f240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x15be5f790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x15be5fce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x15be60230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x15be60780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x15be60cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x15be61220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x15be61770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x15be61cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x15be62210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x15be62760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x15be62cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x15be63200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x15be63750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x15be63ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x15be641f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x15be64740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x15be64c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x15be65130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x15be655d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x15be65a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x15be65f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x15be663b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x15be66850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x15be66cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x15be67190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x15be67630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x15be67ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x15be67f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x15be68410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x15be688b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x15be68d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x15be691f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x15be69740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x15be69e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x15be6a580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x15be6aca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x15be6b3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x15be6b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x15be6be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x15be6c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x15be6c740 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
0.00.089.954 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.089.958 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x15d004d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x15d0051f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x15d005660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x15d005ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x15d005f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x15d0063b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x15d006820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x15d006c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x15d007100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x15d007570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x15d0079e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x15d0080d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x15d008bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x15d0093a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x15d009bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x15d00a2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x15d00a9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x15d00b110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x15d00b830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x15d00bf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x15d00c680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x15d00cda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x15d00d4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x15d00dbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x15d00e300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x15d00e5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x15d00e880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x15d00ecf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x15d00f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x15d00f5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x15d00fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x15d00ff70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x15d0103e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x15d0106a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x15d010b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x15d010f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x15d0113f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x15d011860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x15d011cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x15d012140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x15d0125b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x15d012a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x15d012e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x15d013300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x15d013770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x15d013be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x15d014050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x15d0144c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x15d014930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x15d014da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x15d015210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x15d015680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x15d015af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x15d015f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x15d0163d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x15d016840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x15d016db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x15d0172b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x15d017720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x15d017b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x15d018000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x15d018470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x15d0188e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x15d018d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x15d0191c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x15d019630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x15bf06fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x15bf07420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x15bf07890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x15bf07d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x15bf08170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x15bf085e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x15bf08a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x15bf08ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x15bf09330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x15bf097a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x15bf09c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x15bf0a080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x15bf0a4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x15bf0a960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x15bf0add0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x15bf0b240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x15bf0b6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x15bf0bb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x15bf0bf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x15bf0c400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x15bf0c870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x15bf0cce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x15bf0d150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x15bf0d5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x15bf0da30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x15bf0dea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x15bf0e310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x15bf0e780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x15bf0ebf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x15bf0f060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x15bf0f4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x15bf0f940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x15bf0fdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x15bf10220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x15bf10690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x15bf10b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x15bf10f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x15bf113e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x15bf11850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x15bf11cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x15bf12130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x15bf125a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x15bf12a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x15bf12e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x15bf132f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x15bf13760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x15bf13bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x15bf14040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x15bf144b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x15bf14920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x15bf14d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x15bf15200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x15bf15670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x15bf15ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x15bf15f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x15bf163c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x15bf16830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x15bf16ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x15bf17110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x15bf17580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x15bf179f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x15bf17e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x15bf182d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x15bf18740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x15bf18bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x15bf19020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x15bf19490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x15bf19900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x15bf19d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x15bf1a1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x15bf1a650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x15bf1aac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x15bf1af30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x15bf1b3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x15bf1b810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x15bf1bc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x15bf1c0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x15bf1c560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x15bf1c9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x15bf1ce40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x15bf1d2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x15bf1d720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x15bf1db90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x15bf1e000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x15bf1e470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x15bf1e8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x15bf1ed50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x15bf1f1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x15bf1f630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x15bf1faa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x15bf1ff10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x15bf20380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x15bf207f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x15bf20c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x15bf210d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x15bf21540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x15bf219b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x15bf21e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x15bf22290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x15bf22700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x15bf23330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x15bf235f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x15bf238b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x15bf23d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x15bf24190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x15bf24600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x15bf24a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x15bf24ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x15bf25350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x15bf257c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x15bf25c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x15bf260a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x15bf26510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x15bf26980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x15bf26df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x15bf27260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x15bf276d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x15bf27b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x15bf27fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x15bf28420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x15bf28890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x15bf28d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x15bf29170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x15bf295e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x15bf29a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x15bf29ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x15bf2a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x15bf2a7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x15bf2ac10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x15bf2b080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x15bf2b4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x15bf2b960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x15bf2bdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x15bf2c240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x15bf2c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x15bf2cb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x15bf2d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x15bf2d590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x15bf2da00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x15bf2de70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x15bf2e2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x15bf2e750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x15bf2ec70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x15bf2f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x15bf2fcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x15bf2ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x15bf30570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x15bf30b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x15bf310f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x15bf316b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x15bf31c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x15bf32230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x15bf327f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x15bf32db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x15bf33370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x15bf33930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x15bf33ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x15bf344b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x15bf34a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x15bf35030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x15bf355f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x15bf35bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x15bf36170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x15bf36730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x15bf36cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x15bf372b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x15bf37870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x15bf37e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x15bf383f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x15bf389b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x15bf38f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x15bf39530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x15bf39af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x15bf3a0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x15bf3a670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x15bf3ac30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x15bf3b1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x15bf3b7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x15bf3bd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x15bf3c330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x15bf3c8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x15bf3ceb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x15bf3d470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x15bf3da30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x15bf3dff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x15bf3e5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x15bf3eb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x15bf3f130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x15bf3f6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x15bf3fcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x15bf40270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x15bf40830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x15bf40df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x15bf413b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x15bf41970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x15bf41f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x15bf424f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x15bf42ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x15bf43070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x15bf43630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x15bf43bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x15bf441b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x15bf446b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x15bf44bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x15bf450b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x15bf455b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x15bf45ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x15bf45fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x15bf464b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x15bf469b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x15bf46eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x15bf473b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x15bf478b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x15bf47db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x15bf482b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x15bf487b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x15bf48cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x15bf496c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x15bf49de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x15bf4a500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x15bf4ac20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x15bf4aee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x15bf4b6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x15bf4b990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x15bf4bfa0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x15d0198f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x15d008390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x15d004880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x15d00baf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x15d019bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x15d01a110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x15d01a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x15d01a690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x15d01a950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x15d01ac10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x15d01aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x15d01b190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x15d01b760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x15d01bd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x15d01c360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x15d01c620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x15d01cb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x15d01d0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x15d01d5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x15d01ddb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x15d01e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x15d01e830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x15d01ed70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x15d01f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x15d01f7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x15d01fd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x15d01fff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x15d0202b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x15d020570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x15d020830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x15d020af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x15d020db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x15d021070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x15d021330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x15d0215f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x15d0218b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x15d021b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x15d021e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x15d0220f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x15d0223b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x15d022670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x15d022930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x15d022bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x15d022eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x15d023170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x15d023430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x15d0236f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x15d0239b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x15d023c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x15d023f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x15d0241f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x15d0244b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x15d024770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x15d024a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x15d024cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x15d024fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x15d025270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x15d025530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x15d0257f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x15d025ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x15d025d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x15d026170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x15d026430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x15d0266f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x15d026b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x15d026fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x15d027440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x15d0278b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x15d027d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x15d028190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x15d028600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x15d028a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x15d028ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x15d029350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x15d0297c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x15d029c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x15d02a0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x15d02a510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x15d02a980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x15d02adf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x15d02b260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x15d02b6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x15d02bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x15d02bfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x15d02c420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x15d02c890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x15d02cd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x15d02d170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x15d02d5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x15d02da50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x15d02dec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x15d02e330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x15d02e7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x15d02ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x15d02f080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x15d02f4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x15d02f960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x15d02fdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x15d030240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x15d0306b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x15d030b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x15d030f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x15d031400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x15d031b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x15d0323a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x15d0328f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x15d032e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x15d033390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x15d0338e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x15d033e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x15d034380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x15d0348d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x15d034e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x15d0352c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x15d035760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x15d035c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x15d0360a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x15d036540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x15d0369e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x15d036e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x15d037320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x15d0377c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x15d037c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x15d038100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x15d0385a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x15d038a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x15d038ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x15d039380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x15d039820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x15d039cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x15d03a160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x15d03a600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x15d03aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x15d03af40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x15d03b3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x15d03b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x15d03bd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x15d03c1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x15d03c660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x15d03cb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x15d03cfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x15d03d440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x15d03d8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x15d03dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x15d03e220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x15d03e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x15d03eb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x15d03f000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x15d03f4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x15d03f940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x15d03fde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x15d040280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x15d040720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x15d040bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x15d041060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x15d041500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x15d0419a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x15d041e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x15d0422e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x15d042780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x15d042c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x15d0430c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x15d043560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x15d043a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x15d043ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x15d044340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x15d0447e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x15d044c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x15d045120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x15d0455c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x15d045a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x15d045f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x15d0463a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x15d046840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x15d046ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x15d047180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x15d047620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x15d047ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x15d047f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x15d048400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x15d0488a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x15d048d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x15d0491e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x15d049680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x15d049b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x15d049fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x15d04a460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x15d04a900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x15d04ada0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x15d04b240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x15d04b6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x15d04bb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x15d04c020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x15d04c570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x15d04cac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x15d04d010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x15d04d560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x15d04d820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x15d04de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x15d04e440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x15d04ea50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x15d04f240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x15d04f6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x15d04f9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x15d04ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x15d0505c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x15d050db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x15d051250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x15d0516f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x15d051b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x15d052340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x15d052890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x15d052de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x15d053330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x15d053880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x15d053dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x15d054320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x15d054870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x15d054dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x15d055310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x15d055860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x15d055db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x15d056300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x15d056850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x15d056da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x15d0572f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x15d057840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x15d057d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x15d0582e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x15d058830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x15d058d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x15d0592d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x15d059820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x15d059d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x15d05a2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x15d05a810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x15d05ad60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x15d05b2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x15d05b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x15d05bd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x15d05c2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x15d05c7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x15d05cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x15d05d290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x15d05d7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x15d05dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x15d05e280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x15d05e7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x15d05ed20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x15d05f270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x15d05f7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x15d05fd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x15d060260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x15d0607b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x15d060d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x15d061250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x15d0617a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x15d061cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x15d062240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x15d062790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x15d062ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x15d063230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x15d063780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x15d063cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x15d064220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x15d064770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x15d064cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x15d065160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x15d065600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x15d065aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x15d065f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x15d0663e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x15d066880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x15d066d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x15d0671c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x15d067660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x15d067b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x15d067fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x15d068440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x15d0688e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x15d068d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x15d069220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x15d069770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x15d069e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x15d06a5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x15d06acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x15d06b3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x15d06b6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x15d06bea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x15d06c160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x15d06c770 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.916s
user	0m0.244s
sys	0m0.136s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
