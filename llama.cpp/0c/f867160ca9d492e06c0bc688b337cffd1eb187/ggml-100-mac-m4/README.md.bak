### ctest_debug

Runs ctest in debug mode
- status: 0
```
+ ctest --output-on-failure -L main -E test-opt
Test project /Users/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-tokenizer-0-bert-bge
 1/29 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.26 sec
      Start  2: test-tokenizer-0-command-r
 2/29 Test  #2: test-tokenizer-0-command-r ........   Passed    1.09 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/29 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.16 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/29 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.44 sec
      Start  5: test-tokenizer-0-falcon
 5/29 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.28 sec
      Start  6: test-tokenizer-0-gpt-2
 6/29 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.22 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/29 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.66 sec
      Start  8: test-tokenizer-0-llama-spm
 8/29 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.08 sec
      Start  9: test-tokenizer-0-mpt
 9/29 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.22 sec
      Start 10: test-tokenizer-0-phi-3
10/29 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.08 sec
      Start 11: test-tokenizer-0-qwen2
11/29 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.60 sec
      Start 12: test-tokenizer-0-refact
12/29 Test #12: test-tokenizer-0-refact ...........   Passed    0.22 sec
      Start 13: test-tokenizer-0-starcoder
13/29 Test #13: test-tokenizer-0-starcoder ........   Passed    0.22 sec
      Start 14: test-sampling
14/29 Test #14: test-sampling .....................   Passed    2.15 sec
      Start 15: test-grammar-parser
15/29 Test #15: test-grammar-parser ...............   Passed    0.18 sec
      Start 16: test-grammar-integration
16/29 Test #16: test-grammar-integration ..........   Passed    0.24 sec
      Start 17: test-llama-grammar
17/29 Test #17: test-llama-grammar ................   Passed    0.19 sec
      Start 18: test-chat
18/29 Test #18: test-chat .........................   Passed   17.11 sec
      Start 19: test-json-schema-to-grammar
19/29 Test #19: test-json-schema-to-grammar .......   Passed    2.29 sec
      Start 20: test-tokenizer-1-llama-spm
20/29 Test #20: test-tokenizer-1-llama-spm ........   Passed    1.09 sec
      Start 21: test-log
21/29 Test #21: test-log ..........................   Passed    0.22 sec
      Start 22: test-arg-parser
22/29 Test #22: test-arg-parser ...................   Passed    0.29 sec
      Start 23: test-chat-template
23/29 Test #23: test-chat-template ................   Passed    2.88 sec
      Start 24: test-gguf
24/29 Test #24: test-gguf .........................   Passed    0.97 sec
      Start 25: test-backend-ops
25/29 Test #25: test-backend-ops ..................   Passed  187.59 sec
      Start 28: test-barrier
26/29 Test #28: test-barrier ......................   Passed    0.91 sec
      Start 29: test-quantize-fns
27/29 Test #29: test-quantize-fns .................   Passed   25.80 sec
      Start 30: test-quantize-perf
28/29 Test #30: test-quantize-perf ................   Passed    0.33 sec
      Start 31: test-rope
29/29 Test #31: test-rope .........................   Passed    0.22 sec

100% tests passed, 0 tests failed out of 29

Label Time Summary:
main    = 248.01 sec*proc (29 tests)

Total Test time (real) = 248.02 sec

real	4m8.051s
user	8m24.531s
sys	0m6.458s
```

### ctest_release

Runs ctest in release mode
- status: 0
```
+ ctest --output-on-failure -L main
Test project /Users/ggml/work/llama.cpp/build-ci-release
      Start  1: test-tokenizer-0-bert-bge
 1/29 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.15 sec
      Start  2: test-tokenizer-0-command-r
 2/29 Test  #2: test-tokenizer-0-command-r ........   Passed    0.22 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/29 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.04 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/29 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.08 sec
      Start  5: test-tokenizer-0-falcon
 5/29 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.06 sec
      Start  6: test-tokenizer-0-gpt-2
 6/29 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.05 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/29 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.13 sec
      Start  8: test-tokenizer-0-llama-spm
 8/29 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.03 sec
      Start  9: test-tokenizer-0-mpt
 9/29 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.05 sec
      Start 10: test-tokenizer-0-phi-3
10/29 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.03 sec
      Start 11: test-tokenizer-0-qwen2
11/29 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.11 sec
      Start 12: test-tokenizer-0-refact
12/29 Test #12: test-tokenizer-0-refact ...........   Passed    0.05 sec
      Start 13: test-tokenizer-0-starcoder
13/29 Test #13: test-tokenizer-0-starcoder ........   Passed    0.04 sec
      Start 14: test-sampling
14/29 Test #14: test-sampling .....................   Passed    0.88 sec
      Start 15: test-grammar-parser
15/29 Test #15: test-grammar-parser ...............   Passed    0.17 sec
      Start 16: test-grammar-integration
16/29 Test #16: test-grammar-integration ..........   Passed    0.18 sec
      Start 17: test-llama-grammar
17/29 Test #17: test-llama-grammar ................   Passed    0.19 sec
      Start 18: test-chat
18/29 Test #18: test-chat .........................   Passed    1.81 sec
      Start 19: test-json-schema-to-grammar
19/29 Test #19: test-json-schema-to-grammar .......   Passed    2.07 sec
      Start 20: test-tokenizer-1-llama-spm
20/29 Test #20: test-tokenizer-1-llama-spm ........   Passed    0.30 sec
      Start 21: test-log
21/29 Test #21: test-log ..........................   Passed    0.17 sec
      Start 22: test-arg-parser
22/29 Test #22: test-arg-parser ...................   Passed    0.22 sec
      Start 23: test-chat-template
23/29 Test #23: test-chat-template ................   Passed    0.45 sec
      Start 24: test-gguf
24/29 Test #24: test-gguf .........................   Passed    0.40 sec
      Start 25: test-backend-ops
25/29 Test #25: test-backend-ops ..................   Passed   29.04 sec
      Start 28: test-barrier
26/29 Test #28: test-barrier ......................   Passed    0.36 sec
      Start 29: test-quantize-fns
27/29 Test #29: test-quantize-fns .................   Passed   14.10 sec
      Start 30: test-quantize-perf
28/29 Test #30: test-quantize-perf ................   Passed    0.21 sec
      Start 31: test-rope
29/29 Test #31: test-rope .........................   Passed    0.20 sec

100% tests passed, 0 tests failed out of 29

Label Time Summary:
main    =  52.81 sec*proc (29 tests)

Total Test time (real) =  52.82 sec

real	0m52.833s
user	1m18.000s
sys	0m5.356s
```
### embd_bge_small

BGE Small (BERT):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.120 I build: 4668 (0cf86716) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.017.980 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.020.977 I llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.020.980 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.020.982 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.020.982 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.020.982 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.020.983 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.020.983 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.020.984 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.020.984 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.020.984 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.020.985 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.020.985 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.020.987 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.020.988 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.020.988 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.020.988 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.020.989 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.020.989 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.020.990 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.023.066 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.023.692 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.692 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.023.693 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.023.693 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.023.693 I llama_model_loader: - kv  22:               tokenizer.ggml.mask_token_id u32              = 103
0.00.023.694 I llama_model_loader: - kv  23:               general.quantization_version u32              = 2
0.00.023.694 I llama_model_loader: - type  f32:  124 tensors
0.00.023.694 I llama_model_loader: - type  f16:   73 tensors
0.00.023.695 I print_info: file format = GGUF V3 (latest)
0.00.023.696 I print_info: file type   = F16
0.00.023.696 I print_info: file size   = 63.84 MiB (16.12 BPW) 
0.00.026.190 I load: special tokens cache size = 5
0.00.027.362 I load: token to piece cache size = 0.2032 MB
0.00.027.365 I print_info: arch             = bert
0.00.027.365 I print_info: vocab_only       = 0
0.00.027.365 I print_info: n_ctx_train      = 512
0.00.027.365 I print_info: n_embd           = 384
0.00.027.365 I print_info: n_layer          = 12
0.00.027.368 I print_info: n_head           = 12
0.00.027.368 I print_info: n_head_kv        = 12
0.00.027.368 I print_info: n_rot            = 32
0.00.027.368 I print_info: n_swa            = 0
0.00.027.369 I print_info: n_embd_head_k    = 32
0.00.027.369 I print_info: n_embd_head_v    = 32
0.00.027.369 I print_info: n_gqa            = 1
0.00.027.370 I print_info: n_embd_k_gqa     = 384
0.00.027.371 I print_info: n_embd_v_gqa     = 384
0.00.027.371 I print_info: f_norm_eps       = 1.0e-12
0.00.027.372 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.027.372 I print_info: f_clamp_kqv      = 0.0e+00
0.00.027.372 I print_info: f_max_alibi_bias = 0.0e+00
0.00.027.372 I print_info: f_logit_scale    = 0.0e+00
0.00.027.373 I print_info: n_ff             = 1536
0.00.027.373 I print_info: n_expert         = 0
0.00.027.373 I print_info: n_expert_used    = 0
0.00.027.373 I print_info: causal attn      = 0
0.00.027.374 I print_info: pooling type     = 2
0.00.027.374 I print_info: rope type        = 2
0.00.027.374 I print_info: rope scaling     = linear
0.00.027.374 I print_info: freq_base_train  = 10000.0
0.00.027.375 I print_info: freq_scale_train = 1
0.00.027.375 I print_info: n_ctx_orig_yarn  = 512
0.00.027.378 I print_info: rope_finetuned   = unknown
0.00.027.378 I print_info: ssm_d_conv       = 0
0.00.027.378 I print_info: ssm_d_inner      = 0
0.00.027.378 I print_info: ssm_d_state      = 0
0.00.027.378 I print_info: ssm_dt_rank      = 0
0.00.027.378 I print_info: ssm_dt_b_c_rms   = 0
0.00.027.378 I print_info: model type       = 33M
0.00.027.379 I print_info: model params     = 33.21 M
0.00.027.379 I print_info: general.name     = Bge Small
0.00.027.381 I print_info: vocab type       = WPM
0.00.027.381 I print_info: n_vocab          = 30522
0.00.027.381 I print_info: n_merges         = 0
0.00.027.381 I print_info: BOS token        = 101 '[CLS]'
0.00.027.381 I print_info: UNK token        = 100 '[UNK]'
0.00.027.382 I print_info: SEP token        = 102 '[SEP]'
0.00.027.382 I print_info: PAD token        = 0 '[PAD]'
0.00.027.382 I print_info: MASK token       = 103 '[MASK]'
0.00.027.382 I print_info: LF token         = 0 '[PAD]'
0.00.027.383 I print_info: max token length = 21
0.00.027.383 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.029.676 I load_tensors: offloading 12 repeating layers to GPU
0.00.029.677 I load_tensors: offloading output layer to GPU
0.00.029.677 I load_tensors: offloaded 13/13 layers to GPU
0.00.029.696 I load_tensors: Metal_Mapped model buffer size =    40.73 MiB
0.00.029.698 I load_tensors:   CPU_Mapped model buffer size =    23.11 MiB
................................................
0.00.029.910 I llama_init_from_model: n_seq_max     = 1
0.00.029.911 I llama_init_from_model: n_ctx         = 512
0.00.029.911 I llama_init_from_model: n_ctx_per_seq = 512
0.00.029.911 I llama_init_from_model: n_batch       = 2048
0.00.029.912 I llama_init_from_model: n_ubatch      = 2048
0.00.029.912 I llama_init_from_model: flash_attn    = 0
0.00.029.912 I llama_init_from_model: freq_base     = 10000.0
0.00.029.912 I llama_init_from_model: freq_scale    = 1
0.00.029.913 I ggml_metal_init: allocating
0.00.029.916 I ggml_metal_init: found device: Apple M4
0.00.029.919 I ggml_metal_init: picking default device: Apple M4
0.00.030.447 I ggml_metal_init: using embedded metal library
0.00.032.984 I ggml_metal_init: GPU name:   Apple M4
0.00.032.986 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.032.986 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.032.987 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.032.987 I ggml_metal_init: simdgroup reduction   = true
0.00.032.987 I ggml_metal_init: simdgroup matrix mul. = true
0.00.032.987 I ggml_metal_init: has residency sets    = true
0.00.032.987 I ggml_metal_init: has bfloat            = true
0.00.032.987 I ggml_metal_init: use bfloat            = true
0.00.032.988 I ggml_metal_init: hasUnifiedMemory      = true
0.00.032.988 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.043.389 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.043.994 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.043.996 I llama_init_from_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.043.998 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.045.042 I llama_init_from_model:      Metal compute buffer size =    16.00 MiB
0.00.045.043 I llama_init_from_model:        CPU compute buffer size =     2.51 MiB
0.00.045.043 I llama_init_from_model: graph nodes  = 429
0.00.045.043 I llama_init_from_model: graph splits = 2
0.00.045.045 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.045.045 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.049.383 I 
0.00.049.415 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.049.952 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.043984 -0.019895  0.007670 -0.000838  0.001372 -0.037036  0.109436  0.042589  0.092046 -0.015918  0.006773 -0.035692 -0.017889  0.015040  0.018126  0.015860 -0.011308  0.010431 -0.085223 -0.008454  0.091368 -0.017067 -0.060328 -0.024484  0.027524  0.076059  0.027984 -0.014564  0.017645 -0.033268 -0.037874 -0.019012  0.068670 -0.009845 -0.025030  0.072346 -0.046570  0.011026 -0.050252  0.047693  0.032387 -0.011761  0.022055  0.049645  0.010465  0.005795 -0.028860  0.008933 -0.018507 -0.051466 -0.046036  0.030497 -0.035418  0.054204 -0.069650  0.044238  0.029786  0.046305  0.073410 -0.042580  0.076109  0.038851 -0.181182  0.082503  0.042250 -0.064549 -0.060097 -0.017853  0.006472  0.005873  0.017196 -0.026615  0.064578  0.112612  0.035159 -0.067416  0.027073 -0.067295 -0.033493 -0.033219  0.033252  0.013523 -0.003346 -0.037473 -0.052055  0.055155 -0.001982 -0.038276  0.064447  0.028824 -0.043334 -0.029236 -0.039462  0.036315  0.008379 -0.015481 -0.036573  0.018130  0.028600  0.342831 -0.044484  0.056114  0.017624 -0.020867 -0.066808  0.000146 -0.037885 -0.030063 -0.008541 -0.021595  0.000548 -0.003219  0.004020  0.018908 -0.008548  0.025835  0.049433  0.000091  0.050920 -0.042479 -0.031884  0.023606  0.030691 -0.023148 -0.046271 -0.079278  0.115198  0.046757  0.027829 -0.040710  0.067777 -0.022955  0.010332 -0.032937 -0.018305  0.043844  0.024268  0.052413  0.007469  0.008905  0.011244 -0.074655 -0.065561 -0.026750 -0.041192 -0.023884  0.026710  0.006912  0.027741  0.052873 -0.036673  0.057707 -0.000170  0.031764 -0.019760 -0.022083  0.041053 -0.058903  0.019619  0.043144  0.043582  0.041593 -0.022527  0.027037 -0.021833  0.005430 -0.041317 -0.001239  0.024452  0.002088  0.044333 -0.022745  0.043662  0.064763  0.055430  0.037060 -0.000919  0.046111  0.045790 -0.008487  0.063057 -0.073235 -0.011935  0.032107  0.023957  0.014725 -0.033688  0.001083 -0.015827 -0.019002  0.047872  0.110842  0.028428  0.031366 -0.013288 -0.057499  0.006644  0.005140 -0.012262 -0.051443 -0.000965 -0.017642 -0.019440 -0.040937  0.009209 -0.057957  0.050961  0.052340 -0.009617 -0.040259 -0.014075 -0.024877 -0.017251  0.006302  0.006588 -0.026919  0.015600  0.030760  0.002578  0.023206 -0.022199 -0.098557 -0.051105 -0.278031 -0.014994 -0.061553 -0.027218  0.017666 -0.010939 -0.017077  0.035055  0.046994 -0.015432  0.015232 -0.025477  0.047844 -0.005949 -0.000749 -0.061008 -0.068946 -0.060377 -0.035942  0.043319 -0.055047  0.015075  0.000537 -0.058175 -0.010439  0.012643  0.151495  0.127111 -0.013622  0.041993 -0.025672  0.014026 -0.001037 -0.150469  0.044857  0.005320 -0.036274 -0.029801 -0.020192 -0.034891  0.010235  0.033548 -0.048178 -0.051795 -0.017463 -0.023492  0.047349  0.052076 -0.016783 -0.055452  0.025836 -0.005707  0.010710  0.038705  0.008191 -0.009764 -0.105779 -0.027430 -0.096101  0.025068 -0.011260  0.092355  0.056101  0.003758  0.027791  0.002092 -0.051085 -0.039890 -0.013542 -0.044956 -0.015335  0.002910 -0.043498 -0.077951  0.065217 -0.006840 -0.001630 -0.014642  0.071560  0.023714 -0.037180  0.009175  0.001562 -0.032268  0.015468  0.037880  0.000337 -0.053207  0.021321 -0.039829  0.000031  0.013400  0.019798 -0.057879  0.006473 -0.049531 -0.267858  0.039156 -0.067972  0.038248 -0.012333  0.041485 -0.016117  0.052383 -0.071363  0.011369  0.024719 -0.007232  0.082087  0.028535 -0.021504  0.040494 -0.004570 -0.074594 -0.014749  0.020037  0.002304  0.023148  0.197212 -0.043222 -0.025983 -0.004959 -0.019278  0.074258  0.001716 -0.031979 -0.036599 -0.045082  0.000547 -0.011560  0.018126 -0.029461 -0.008463  0.006419  0.050799 -0.014959  0.006176  0.026096 -0.030809  0.048055  0.114093 -0.040826 -0.011457  0.005419 -0.003599  0.025154 -0.059142  0.013753 -0.010399  0.038709  0.051449  0.035413  0.035031 -0.017027  0.026360 -0.014490 -0.050032  0.003219  0.054121  0.039723 -0.039127 

0.00.054.272 I llama_perf_context_print:        load time =      31.40 ms
0.00.054.273 I llama_perf_context_print: prompt eval time =       4.20 ms /     9 tokens (    0.47 ms per token,  2143.88 tokens per second)
0.00.054.274 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.054.275 I llama_perf_context_print:       total time =       4.89 ms /    10 tokens
0.00.054.452 I ggml_metal_free: deallocating

real	0m0.227s
user	0m0.036s
sys	0m0.025s
```
- q8_0:
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.046 I build: 4668 (0cf86716) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.868 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.011.310 I llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.011.314 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.011.315 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.011.316 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.011.321 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.011.321 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.011.322 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.011.323 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.011.323 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.011.324 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.011.324 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.011.324 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.011.328 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.011.329 I llama_model_loader: - kv  11:                      bert.attention.causal bool             = false
0.00.011.329 I llama_model_loader: - kv  12:                          bert.pooling_type u32              = 2
0.00.011.331 I llama_model_loader: - kv  13:            tokenizer.ggml.token_type_count u32              = 2
0.00.011.331 I llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = bert
0.00.011.332 I llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.013.466 I llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.014.057 I llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.014.058 I llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.014.058 I llama_model_loader: - kv  19:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.014.058 I llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 0
0.00.014.059 I llama_model_loader: - kv  21:               tokenizer.ggml.mask_token_id u32              = 103
0.00.014.059 I llama_model_loader: - kv  22:               general.quantization_version u32              = 2
0.00.014.059 I llama_model_loader: - kv  23:                          general.file_type u32              = 7
0.00.014.060 I llama_model_loader: - type  f32:  124 tensors
0.00.014.060 I llama_model_loader: - type q8_0:   73 tensors
0.00.014.061 I print_info: file format = GGUF V3 (latest)
0.00.014.061 I print_info: file type   = Q8_0
0.00.014.062 I print_info: file size   = 34.38 MiB (8.68 BPW) 
0.00.016.283 I load: special tokens cache size = 5
0.00.017.463 I load: token to piece cache size = 0.2032 MB
0.00.017.466 I print_info: arch             = bert
0.00.017.467 I print_info: vocab_only       = 0
0.00.017.467 I print_info: n_ctx_train      = 512
0.00.017.467 I print_info: n_embd           = 384
0.00.017.467 I print_info: n_layer          = 12
0.00.017.470 I print_info: n_head           = 12
0.00.017.471 I print_info: n_head_kv        = 12
0.00.017.472 I print_info: n_rot            = 32
0.00.017.472 I print_info: n_swa            = 0
0.00.017.472 I print_info: n_embd_head_k    = 32
0.00.017.472 I print_info: n_embd_head_v    = 32
0.00.017.473 I print_info: n_gqa            = 1
0.00.017.474 I print_info: n_embd_k_gqa     = 384
0.00.017.474 I print_info: n_embd_v_gqa     = 384
0.00.017.475 I print_info: f_norm_eps       = 1.0e-12
0.00.017.475 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.017.475 I print_info: f_clamp_kqv      = 0.0e+00
0.00.017.475 I print_info: f_max_alibi_bias = 0.0e+00
0.00.017.476 I print_info: f_logit_scale    = 0.0e+00
0.00.017.476 I print_info: n_ff             = 1536
0.00.017.476 I print_info: n_expert         = 0
0.00.017.477 I print_info: n_expert_used    = 0
0.00.017.477 I print_info: causal attn      = 0
0.00.017.477 I print_info: pooling type     = 2
0.00.017.477 I print_info: rope type        = 2
0.00.017.477 I print_info: rope scaling     = linear
0.00.017.479 I print_info: freq_base_train  = 10000.0
0.00.017.479 I print_info: freq_scale_train = 1
0.00.017.480 I print_info: n_ctx_orig_yarn  = 512
0.00.017.480 I print_info: rope_finetuned   = unknown
0.00.017.480 I print_info: ssm_d_conv       = 0
0.00.017.480 I print_info: ssm_d_inner      = 0
0.00.017.480 I print_info: ssm_d_state      = 0
0.00.017.481 I print_info: ssm_dt_rank      = 0
0.00.017.481 I print_info: ssm_dt_b_c_rms   = 0
0.00.017.481 I print_info: model type       = 33M
0.00.017.481 I print_info: model params     = 33.21 M
0.00.017.481 I print_info: general.name     = Bge Small
0.00.017.482 I print_info: vocab type       = WPM
0.00.017.482 I print_info: n_vocab          = 30522
0.00.017.482 I print_info: n_merges         = 0
0.00.017.483 I print_info: BOS token        = 101 '[CLS]'
0.00.017.483 I print_info: UNK token        = 100 '[UNK]'
0.00.017.485 I print_info: SEP token        = 102 '[SEP]'
0.00.017.485 I print_info: PAD token        = 0 '[PAD]'
0.00.017.485 I print_info: MASK token       = 103 '[MASK]'
0.00.017.485 I print_info: LF token         = 0 '[PAD]'
0.00.017.485 I print_info: max token length = 21
0.00.017.486 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.019.147 I load_tensors: offloading 12 repeating layers to GPU
0.00.019.148 I load_tensors: offloading output layer to GPU
0.00.019.148 I load_tensors: offloaded 13/13 layers to GPU
0.00.019.154 I load_tensors: Metal_Mapped model buffer size =    21.75 MiB
0.00.019.154 I load_tensors:   CPU_Mapped model buffer size =    12.63 MiB
................................................
0.00.019.335 I llama_init_from_model: n_seq_max     = 1
0.00.019.336 I llama_init_from_model: n_ctx         = 512
0.00.019.336 I llama_init_from_model: n_ctx_per_seq = 512
0.00.019.336 I llama_init_from_model: n_batch       = 2048
0.00.019.337 I llama_init_from_model: n_ubatch      = 2048
0.00.019.337 I llama_init_from_model: flash_attn    = 0
0.00.019.337 I llama_init_from_model: freq_base     = 10000.0
0.00.019.337 I llama_init_from_model: freq_scale    = 1
0.00.019.338 I ggml_metal_init: allocating
0.00.019.342 I ggml_metal_init: found device: Apple M4
0.00.019.345 I ggml_metal_init: picking default device: Apple M4
0.00.019.877 I ggml_metal_init: using embedded metal library
0.00.022.266 I ggml_metal_init: GPU name:   Apple M4
0.00.022.268 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.022.268 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.022.269 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.022.269 I ggml_metal_init: simdgroup reduction   = true
0.00.022.270 I ggml_metal_init: simdgroup matrix mul. = true
0.00.022.270 I ggml_metal_init: has residency sets    = true
0.00.022.270 I ggml_metal_init: has bfloat            = true
0.00.022.270 I ggml_metal_init: use bfloat            = true
0.00.022.271 I ggml_metal_init: hasUnifiedMemory      = true
0.00.022.271 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.032.430 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.033.017 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.033.019 I llama_init_from_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.033.021 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.033.983 I llama_init_from_model:      Metal compute buffer size =    16.00 MiB
0.00.033.984 I llama_init_from_model:        CPU compute buffer size =     2.51 MiB
0.00.033.984 I llama_init_from_model: graph nodes  = 429
0.00.033.984 I llama_init_from_model: graph splits = 2
0.00.033.985 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.033.986 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.038.005 I 
0.00.038.028 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.038.546 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.044994 -0.019858  0.007986 -0.001410  0.001531 -0.036276  0.109404  0.043235  0.092836 -0.015201  0.005971 -0.036330 -0.018159  0.015015  0.017667  0.014510 -0.011792  0.011812 -0.084764 -0.008081  0.091650 -0.017638 -0.061084 -0.025066  0.026879  0.076180  0.028256 -0.014273  0.017420 -0.033553 -0.037520 -0.018280  0.068843 -0.010129 -0.024896  0.071968 -0.046406  0.011471 -0.050333  0.048931  0.032317 -0.012107  0.021812  0.049796  0.010366  0.005264 -0.028665  0.008581 -0.018508 -0.052358 -0.046410  0.029426 -0.034989  0.053289 -0.069849  0.043767  0.029291  0.046759  0.073664 -0.043090  0.075213  0.038641 -0.180496  0.081648  0.043389 -0.065615 -0.059878 -0.017387  0.006913  0.005155  0.017165 -0.026679  0.064347  0.112564  0.034871 -0.067783  0.027124 -0.066989 -0.034296 -0.033344  0.033189  0.014325 -0.004253 -0.036945 -0.052008  0.054151 -0.001715 -0.038112  0.063016  0.028668 -0.041665 -0.028879 -0.039263  0.036879  0.007718 -0.015657 -0.036270  0.018699  0.029792  0.344948 -0.043880  0.056487  0.018022 -0.021448 -0.065281  0.000253 -0.037910 -0.029663 -0.008828 -0.020643  0.001162 -0.003510  0.003091  0.018576 -0.009637  0.025316  0.049298 -0.000937  0.051568 -0.042363 -0.031550  0.023315  0.030044 -0.023267 -0.045483 -0.079644  0.114489  0.047503  0.027208 -0.041477  0.067717 -0.022845  0.010232 -0.033486 -0.017199  0.044319  0.022956  0.052194  0.008122  0.008112  0.009951 -0.074643 -0.065035 -0.025938 -0.040806 -0.024533  0.027409  0.006078  0.027693  0.052243 -0.036587  0.058682  0.001467  0.032191 -0.019964 -0.021840  0.041805 -0.058801  0.019348  0.042626  0.043999  0.040911 -0.021909  0.028025 -0.022801  0.005520 -0.041266 -0.000085  0.024325  0.001532  0.044802 -0.022888  0.043056  0.065030  0.056292  0.037666 -0.001004  0.047521  0.045624 -0.008614  0.062270 -0.073313 -0.011718  0.032708  0.023348  0.014650 -0.033924  0.001158 -0.015872 -0.018974  0.047982  0.110740  0.029177  0.030881 -0.012325 -0.057836  0.006939  0.004378 -0.011864 -0.051754 -0.003018 -0.017766 -0.020051 -0.041018  0.009118 -0.058568  0.050632  0.051795 -0.008968 -0.040668 -0.014765 -0.024320 -0.015980  0.005823  0.007147 -0.027335  0.016275  0.030734  0.002342  0.023167 -0.022169 -0.098059 -0.050901 -0.277240 -0.015100 -0.061566 -0.026219  0.017141 -0.010285 -0.017522  0.034725  0.047897 -0.016594  0.015489 -0.024132  0.048613 -0.004993 -0.000001 -0.060860 -0.068346 -0.059622 -0.036157  0.043605 -0.056353  0.014713  0.000292 -0.058627 -0.010995  0.012274  0.151597  0.127025 -0.012582  0.042765 -0.025227  0.013629 -0.000381 -0.150686  0.043974  0.004714 -0.036955 -0.029586 -0.019853 -0.034063  0.009522  0.034004 -0.048909 -0.051765 -0.017243 -0.025080  0.047588  0.050981 -0.017553 -0.056746  0.024054 -0.006285  0.011003  0.038719  0.008083 -0.008595 -0.106361 -0.027365 -0.096966  0.025294 -0.011096  0.091579  0.056180  0.004494  0.028066  0.001727 -0.051388 -0.039497 -0.013817 -0.045907 -0.015238  0.002920 -0.043938 -0.077716  0.066102 -0.006195 -0.000885 -0.013768  0.071562  0.023826 -0.036363  0.007848  0.001487 -0.032916  0.016336  0.037378  0.000625 -0.052389  0.021549 -0.039695 -0.000026  0.013496  0.020436 -0.057513  0.005900 -0.049906 -0.267546  0.038686 -0.067509  0.037450 -0.011471  0.041547 -0.015904  0.051347 -0.072313  0.012571  0.024808 -0.007420  0.082069  0.028175 -0.021956  0.040736 -0.003945 -0.074192 -0.014923  0.020417  0.002644  0.023207  0.197144 -0.043068 -0.025876 -0.004886 -0.018582  0.073936  0.001341 -0.031663 -0.037140 -0.044262  0.000049 -0.010741  0.018225 -0.028045 -0.008828  0.006047  0.050183 -0.015017  0.007037  0.025894 -0.030628  0.048075  0.112795 -0.039982 -0.011695  0.005039 -0.002800  0.025072 -0.059720  0.014182 -0.010019  0.038100  0.051088  0.034726  0.035947 -0.017035  0.027243 -0.015391 -0.051195  0.003744  0.053928  0.040014 -0.039140 

0.00.042.894 I llama_perf_context_print:        load time =      29.13 ms
0.00.042.895 I llama_perf_context_print: prompt eval time =       4.22 ms /     9 tokens (    0.47 ms per token,  2131.19 tokens per second)
0.00.042.896 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.042.896 I llama_perf_context_print:       total time =       4.89 ms /    10 tokens
0.00.043.111 I ggml_metal_free: deallocating

real	0m0.054s
user	0m0.029s
sys	0m0.015s
```
### rerank_tiny

Rerank Tiny (Jina):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/rerank-tiny/ggml-model-f16.gguf -p 'what is panda?</s></s>hi\nwhat is panda?</s></s>it'\''s a bear\nwhat is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.' -ngl 99 -c 0 --pooling rank --embd-normalize -1 --verbose-prompt
0.00.000.266 I build: 4668 (0cf86716) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.024.360 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.037.104 I llama_model_loader: loaded meta data with 28 key-value pairs and 70 tensors from ../models-mnt/rerank-tiny/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.037.109 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.037.111 I llama_model_loader: - kv   0:                       general.architecture str              = jina-bert-v2
0.00.037.112 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.037.115 I llama_model_loader: - kv   2:                               general.name str              = Jina Bert Implementation
0.00.037.115 I llama_model_loader: - kv   3:                       general.organization str              = Jinaai
0.00.037.116 I llama_model_loader: - kv   4:                         general.size_label str              = 33M
0.00.037.117 I llama_model_loader: - kv   5:                   jina-bert-v2.block_count u32              = 4
0.00.037.118 I llama_model_loader: - kv   6:                jina-bert-v2.context_length u32              = 8192
0.00.037.119 I llama_model_loader: - kv   7:              jina-bert-v2.embedding_length u32              = 384
0.00.037.120 I llama_model_loader: - kv   8:           jina-bert-v2.feed_forward_length u32              = 1536
0.00.037.120 I llama_model_loader: - kv   9:          jina-bert-v2.attention.head_count u32              = 12
0.00.037.124 I llama_model_loader: - kv  10:  jina-bert-v2.attention.layer_norm_epsilon f32              = 0.000000
0.00.037.124 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.037.125 I llama_model_loader: - kv  12:              jina-bert-v2.attention.causal bool             = false
0.00.037.125 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.037.126 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = jina-v1-en
0.00.044.333 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,61056]   = ["<s>", "<pad>", "</s>", "<unk>", "<m...
0.00.046.347 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,61056]   = [3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, ...
0.00.050.918 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,39382]   = ["t h", "i n", "a n", "e r", "th e", ...
0.00.050.920 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.050.920 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
0.00.050.921 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 3
0.00.050.921 I llama_model_loader: - kv  21:          tokenizer.ggml.seperator_token_id u32              = 2
0.00.050.922 I llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 1
0.00.050.922 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 4
0.00.050.922 I llama_model_loader: - kv  24:            tokenizer.ggml.token_type_count u32              = 2
0.00.050.923 I llama_model_loader: - kv  25:               tokenizer.ggml.add_bos_token bool             = true
0.00.050.923 I llama_model_loader: - kv  26:               tokenizer.ggml.add_eos_token bool             = true
0.00.050.923 I llama_model_loader: - kv  27:               general.quantization_version u32              = 2
0.00.050.924 I llama_model_loader: - type  f32:   40 tensors
0.00.050.924 I llama_model_loader: - type  f16:   30 tensors
0.00.050.925 I print_info: file format = GGUF V3 (latest)
0.00.050.926 I print_info: file type   = F16
0.00.050.927 I print_info: file size   = 62.78 MiB (16.01 BPW) 
0.00.055.127 W load: empty token at index 5
0.00.060.135 W load: model vocab missing newline token, using special_pad_id instead
0.00.061.657 W load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
0.00.061.693 I load: special tokens cache size = 5
0.00.323.986 I load: token to piece cache size = 1.5060 MB
0.00.323.992 I print_info: arch             = jina-bert-v2
0.00.323.992 I print_info: vocab_only       = 0
0.00.323.992 I print_info: n_ctx_train      = 8192
0.00.323.993 I print_info: n_embd           = 384
0.00.323.993 I print_info: n_layer          = 4
0.00.323.999 I print_info: n_head           = 12
0.00.324.005 I print_info: n_head_kv        = 12
0.00.324.005 I print_info: n_rot            = 32
0.00.324.005 I print_info: n_swa            = 0
0.00.324.005 I print_info: n_embd_head_k    = 32
0.00.324.006 I print_info: n_embd_head_v    = 32
0.00.324.006 I print_info: n_gqa            = 1
0.00.324.007 I print_info: n_embd_k_gqa     = 384
0.00.324.007 I print_info: n_embd_v_gqa     = 384
0.00.324.010 I print_info: f_norm_eps       = 1.0e-12
0.00.324.011 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.324.011 I print_info: f_clamp_kqv      = 0.0e+00
0.00.324.011 I print_info: f_max_alibi_bias = 8.0e+00
0.00.324.011 I print_info: f_logit_scale    = 0.0e+00
0.00.324.012 I print_info: n_ff             = 1536
0.00.324.012 I print_info: n_expert         = 0
0.00.324.012 I print_info: n_expert_used    = 0
0.00.324.012 I print_info: causal attn      = 0
0.00.324.012 I print_info: pooling type     = -1
0.00.324.012 I print_info: rope type        = -1
0.00.324.013 I print_info: rope scaling     = linear
0.00.324.013 I print_info: freq_base_train  = 10000.0
0.00.324.013 I print_info: freq_scale_train = 1
0.00.324.014 I print_info: n_ctx_orig_yarn  = 8192
0.00.324.014 I print_info: rope_finetuned   = unknown
0.00.324.014 I print_info: ssm_d_conv       = 0
0.00.324.014 I print_info: ssm_d_inner      = 0
0.00.324.014 I print_info: ssm_d_state      = 0
0.00.324.015 I print_info: ssm_dt_rank      = 0
0.00.324.016 I print_info: ssm_dt_b_c_rms   = 0
0.00.324.016 I print_info: model type       = 33M
0.00.324.016 I print_info: model params     = 32.90 M
0.00.324.017 I print_info: general.name     = Jina Bert Implementation
0.00.324.018 I print_info: vocab type       = BPE
0.00.324.018 I print_info: n_vocab          = 61056
0.00.324.018 I print_info: n_merges         = 39382
0.00.324.018 I print_info: BOS token        = 0 '<s>'
0.00.324.018 I print_info: EOS token        = 2 '</s>'
0.00.324.019 I print_info: UNK token        = 3 '<unk>'
0.00.324.019 I print_info: SEP token        = 2 '</s>'
0.00.324.019 I print_info: PAD token        = 1 '<pad>'
0.00.324.019 I print_info: MASK token       = 4 '<mask>'
0.00.324.019 I print_info: EOG token        = 2 '</s>'
0.00.324.019 I print_info: max token length = 45
0.00.324.020 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.326.090 I load_tensors: offloading 4 repeating layers to GPU
0.00.326.091 I load_tensors: offloading output layer to GPU
0.00.326.091 I load_tensors: offloaded 5/5 layers to GPU
0.00.326.115 I load_tensors: Metal_Mapped model buffer size =    18.06 MiB
0.00.326.117 I load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
....................
0.00.326.426 I llama_init_from_model: n_seq_max     = 1
0.00.326.426 I llama_init_from_model: n_ctx         = 8192
0.00.326.426 I llama_init_from_model: n_ctx_per_seq = 8192
0.00.326.427 I llama_init_from_model: n_batch       = 2048
0.00.326.427 I llama_init_from_model: n_ubatch      = 2048
0.00.326.427 I llama_init_from_model: flash_attn    = 0
0.00.326.428 I llama_init_from_model: freq_base     = 10000.0
0.00.326.428 I llama_init_from_model: freq_scale    = 1
0.00.326.428 I ggml_metal_init: allocating
0.00.326.432 I ggml_metal_init: found device: Apple M4
0.00.326.435 I ggml_metal_init: picking default device: Apple M4
0.00.327.282 I ggml_metal_init: using embedded metal library
0.00.330.108 I ggml_metal_init: GPU name:   Apple M4
0.00.330.110 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.330.110 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.330.111 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.330.111 I ggml_metal_init: simdgroup reduction   = true
0.00.330.111 I ggml_metal_init: simdgroup matrix mul. = true
0.00.330.111 I ggml_metal_init: has residency sets    = true
0.00.330.111 I ggml_metal_init: has bfloat            = true
0.00.330.111 I ggml_metal_init: use bfloat            = true
0.00.330.112 I ggml_metal_init: hasUnifiedMemory      = true
0.00.330.112 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.339.462 I llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 4, can_shift = 1
0.00.342.407 I llama_kv_cache_init:      Metal KV buffer size =    48.00 MiB
0.00.342.408 I llama_init_from_model: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB
0.00.342.410 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.348.811 I llama_init_from_model:      Metal compute buffer size =   220.01 MiB
0.00.348.812 I llama_init_from_model:        CPU compute buffer size =    22.02 MiB
0.00.348.812 I llama_init_from_model: graph nodes  = 154
0.00.348.813 I llama_init_from_model: graph splits = 2
0.00.348.814 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 8192
0.00.348.814 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.356.337 I 
0.00.356.373 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.356.480 I main: prompt 0: 'what is panda?</s></s>hi'
0.00.356.481 I main: number of tokens in prompt = 9
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 23233 -> 'hi'
     2 -> '</s>'


0.00.356.484 I main: prompt 1: 'what is panda?</s></s>it's a bear'
0.00.356.484 I main: number of tokens in prompt = 13
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21153 -> 'it'
    37 -> '''
    87 -> 's'
    69 -> 'a'
 25706 -> 'bear'
     2 -> '</s>'


0.00.356.487 I main: prompt 2: 'what is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'
0.00.356.488 I main: number of tokens in prompt = 40
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21215 -> 'he'
 28390 -> 'giant'
 49990 -> 'panda'
    38 -> '('
 21163 -> 'il'
 26237 -> 'uro'
 21223 -> 'po'
 23179 -> 'da'
 36906 -> 'melan'
 26791 -> 'ole'
    89 -> 'u'
 21402 -> 'ca'
 21686 -> '),'
 23314 -> 'sometimes'
 22517 -> 'called'
    69 -> 'a'
 49990 -> 'panda'
 25706 -> 'bear'
 21142 -> 'or'
 22810 -> 'simply'
 49990 -> 'panda'
    42 -> ','
 21152 -> 'is'
    69 -> 'a'
 25706 -> 'bear'
 25677 -> 'species'
 28930 -> 'ende'
 22024 -> 'mic'
 21148 -> 'to'
    76 -> 'h'
 22344 -> 'ina'
    44 -> '.'
     2 -> '</s>'


0.00.356.975 I batch_decode: n_tokens = 62, n_seq = 3

rerank score 0:    0.023
rerank score 1:    0.024
rerank score 2:    0.199

0.00.359.745 I llama_perf_context_print:        load time =     331.97 ms
0.00.359.746 I llama_perf_context_print: prompt eval time =       2.76 ms /    62 tokens (    0.04 ms per token, 22431.26 tokens per second)
0.00.359.746 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.359.747 I llama_perf_context_print:       total time =       3.41 ms /    63 tokens
0.00.359.991 I ggml_metal_free: deallocating

real	0m1.062s
user	0m0.329s
sys	0m0.047s
  - rerank score 0 @ 0.023 OK
  - rerank score 1 @ 0.024 OK
  - rerank score 2 @ 0.199 OK
```
### pythia_1_4b

Pythia 1.4B:
- status: 0
- perplexity:
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
- imatrix:
```
Final estimate: PPL = 10.1498 +/- 3.22650
```
- f16: 
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.282 I build: 4668 (0cf86716) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.442 I main: llama backend init
0.00.000.448 I main: load the model and apply lora adapter, if any
0.00.060.032 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.072.217 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.072.231 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.072.235 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.072.236 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.072.236 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.072.237 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.072.241 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.072.244 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.072.245 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.072.246 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.072.246 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.072.247 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.072.247 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.072.248 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.072.268 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.072.268 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.072.269 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.079.139 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.081.265 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.090.374 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.090.383 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.090.383 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.090.384 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.090.385 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.090.385 I llama_model_loader: - type  f32:  194 tensors
0.00.090.386 I llama_model_loader: - type  f16:   98 tensors
0.00.090.387 I print_info: file format = GGUF V3 (latest)
0.00.090.389 I print_info: file type   = all F32 (guessed)
0.00.090.390 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.106.678 I load: special tokens cache size = 25
0.00.116.239 I load: token to piece cache size = 0.2984 MB
0.00.116.243 I print_info: arch             = gptneox
0.00.116.244 I print_info: vocab_only       = 0
0.00.116.244 I print_info: n_ctx_train      = 2048
0.00.116.244 I print_info: n_embd           = 2048
0.00.116.244 I print_info: n_layer          = 24
0.00.116.249 I print_info: n_head           = 16
0.00.116.250 I print_info: n_head_kv        = 16
0.00.116.250 I print_info: n_rot            = 32
0.00.116.250 I print_info: n_swa            = 0
0.00.116.253 I print_info: n_embd_head_k    = 128
0.00.116.253 I print_info: n_embd_head_v    = 128
0.00.116.254 I print_info: n_gqa            = 1
0.00.116.255 I print_info: n_embd_k_gqa     = 2048
0.00.116.256 I print_info: n_embd_v_gqa     = 2048
0.00.116.257 I print_info: f_norm_eps       = 1.0e-05
0.00.116.257 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.116.258 I print_info: f_clamp_kqv      = 0.0e+00
0.00.116.258 I print_info: f_max_alibi_bias = 0.0e+00
0.00.116.258 I print_info: f_logit_scale    = 0.0e+00
0.00.116.259 I print_info: n_ff             = 8192
0.00.116.260 I print_info: n_expert         = 0
0.00.116.260 I print_info: n_expert_used    = 0
0.00.116.260 I print_info: causal attn      = 1
0.00.116.260 I print_info: pooling type     = 0
0.00.116.260 I print_info: rope type        = 2
0.00.116.262 I print_info: rope scaling     = linear
0.00.116.263 I print_info: freq_base_train  = 10000.0
0.00.116.263 I print_info: freq_scale_train = 1
0.00.116.263 I print_info: n_ctx_orig_yarn  = 2048
0.00.116.264 I print_info: rope_finetuned   = unknown
0.00.116.264 I print_info: ssm_d_conv       = 0
0.00.116.264 I print_info: ssm_d_inner      = 0
0.00.116.264 I print_info: ssm_d_state      = 0
0.00.116.265 I print_info: ssm_dt_rank      = 0
0.00.116.265 I print_info: ssm_dt_b_c_rms   = 0
0.00.116.265 I print_info: model type       = 1.4B
0.00.116.265 I print_info: model params     = 1.41 B
0.00.116.266 I print_info: general.name     = 1.4B
0.00.116.266 I print_info: vocab type       = BPE
0.00.116.266 I print_info: n_vocab          = 50304
0.00.116.267 I print_info: n_merges         = 50009
0.00.116.267 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.116.267 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.116.267 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.116.268 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.116.268 I print_info: LF token         = 187 ''
0.00.116.268 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.116.269 I print_info: max token length = 1024
0.00.116.269 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.169.053 I load_tensors: offloading 24 repeating layers to GPU
0.00.169.057 I load_tensors: offloading output layer to GPU
0.00.169.058 I load_tensors: offloaded 25/25 layers to GPU
0.00.169.086 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.169.088 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.169.678 I llama_init_from_model: n_seq_max     = 1
0.00.169.679 I llama_init_from_model: n_ctx         = 2048
0.00.169.679 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.169.679 I llama_init_from_model: n_batch       = 2048
0.00.169.679 I llama_init_from_model: n_ubatch      = 512
0.00.169.679 I llama_init_from_model: flash_attn    = 0
0.00.169.680 I llama_init_from_model: freq_base     = 10000.0
0.00.169.680 I llama_init_from_model: freq_scale    = 1
0.00.169.681 I ggml_metal_init: allocating
0.00.169.716 I ggml_metal_init: found device: Apple M4
0.00.169.721 I ggml_metal_init: picking default device: Apple M4
0.00.170.354 I ggml_metal_init: using embedded metal library
0.00.200.245 I ggml_metal_init: GPU name:   Apple M4
0.00.200.247 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.200.247 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.200.248 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.200.248 I ggml_metal_init: simdgroup reduction   = true
0.00.200.248 I ggml_metal_init: simdgroup matrix mul. = true
0.00.200.248 I ggml_metal_init: has residency sets    = true
0.00.200.249 I ggml_metal_init: has bfloat            = true
0.00.200.249 I ggml_metal_init: use bfloat            = true
0.00.200.249 I ggml_metal_init: hasUnifiedMemory      = true
0.00.200.250 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.331.483 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.360.116 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.360.123 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.360.145 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.364.053 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.364.055 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.364.055 I llama_init_from_model: graph nodes  = 967
0.00.364.055 I llama_init_from_model: graph splits = 2
0.00.364.060 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.364.195 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.364.195 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.432.593 I main: llama threadpool init, n_threads = 4
0.00.432.636 I 
0.00.432.670 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.432.672 I 
0.00.432.857 I sampler seed: 1234
0.00.432.862 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.432.886 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.432.888 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.432.888 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.268.148 I llama_perf_sampler_print:    sampling time =       1.23 ms /    71 runs   (    0.02 ms per token, 57583.13 tokens per second)
0.02.268.149 I llama_perf_context_print:        load time =     371.71 ms
0.02.268.150 I llama_perf_context_print: prompt eval time =      43.83 ms /     7 tokens (    6.26 ms per token,   159.73 tokens per second)
0.02.268.151 I llama_perf_context_print:        eval time =    1788.47 ms /    63 runs   (   28.39 ms per token,    35.23 tokens per second)
0.02.268.153 I llama_perf_context_print:       total time =    1836.40 ms /    70 tokens
0.02.268.365 I ggml_metal_free: deallocating

real	0m2.596s
user	0m0.135s
sys	0m0.150s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.918 I build: 4668 (0cf86716) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.028.659 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.042.782 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.042.800 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.042.804 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.042.805 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.042.806 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.042.806 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.042.808 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.042.812 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.042.812 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.042.813 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.042.814 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.042.814 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.042.815 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.042.816 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.042.820 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.042.820 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.042.821 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.050.325 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.052.672 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.061.601 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.061.607 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.061.608 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.061.608 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.061.609 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.061.610 I llama_model_loader: - type  f32:  194 tensors
0.00.061.610 I llama_model_loader: - type  f16:   98 tensors
0.00.061.611 I print_info: file format = GGUF V3 (latest)
0.00.061.612 I print_info: file type   = all F32 (guessed)
0.00.061.614 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.077.488 I load: special tokens cache size = 25
0.00.087.036 I load: token to piece cache size = 0.2984 MB
0.00.087.039 I print_info: arch             = gptneox
0.00.087.040 I print_info: vocab_only       = 0
0.00.087.040 I print_info: n_ctx_train      = 2048
0.00.087.040 I print_info: n_embd           = 2048
0.00.087.040 I print_info: n_layer          = 24
0.00.087.044 I print_info: n_head           = 16
0.00.087.045 I print_info: n_head_kv        = 16
0.00.087.045 I print_info: n_rot            = 32
0.00.087.046 I print_info: n_swa            = 0
0.00.087.046 I print_info: n_embd_head_k    = 128
0.00.087.046 I print_info: n_embd_head_v    = 128
0.00.087.050 I print_info: n_gqa            = 1
0.00.087.050 I print_info: n_embd_k_gqa     = 2048
0.00.087.052 I print_info: n_embd_v_gqa     = 2048
0.00.087.053 I print_info: f_norm_eps       = 1.0e-05
0.00.087.053 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.087.054 I print_info: f_clamp_kqv      = 0.0e+00
0.00.087.054 I print_info: f_max_alibi_bias = 0.0e+00
0.00.087.054 I print_info: f_logit_scale    = 0.0e+00
0.00.087.055 I print_info: n_ff             = 8192
0.00.087.055 I print_info: n_expert         = 0
0.00.087.055 I print_info: n_expert_used    = 0
0.00.087.056 I print_info: causal attn      = 1
0.00.087.056 I print_info: pooling type     = 0
0.00.087.056 I print_info: rope type        = 2
0.00.087.056 I print_info: rope scaling     = linear
0.00.087.057 I print_info: freq_base_train  = 10000.0
0.00.087.057 I print_info: freq_scale_train = 1
0.00.087.057 I print_info: n_ctx_orig_yarn  = 2048
0.00.087.057 I print_info: rope_finetuned   = unknown
0.00.087.058 I print_info: ssm_d_conv       = 0
0.00.087.059 I print_info: ssm_d_inner      = 0
0.00.087.059 I print_info: ssm_d_state      = 0
0.00.087.059 I print_info: ssm_dt_rank      = 0
0.00.087.060 I print_info: ssm_dt_b_c_rms   = 0
0.00.087.060 I print_info: model type       = 1.4B
0.00.087.060 I print_info: model params     = 1.41 B
0.00.087.061 I print_info: general.name     = 1.4B
0.00.087.061 I print_info: vocab type       = BPE
0.00.087.061 I print_info: n_vocab          = 50304
0.00.087.061 I print_info: n_merges         = 50009
0.00.087.062 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.087.062 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.087.062 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.087.064 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.087.064 I print_info: LF token         = 187 ''
0.00.087.065 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.087.066 I print_info: max token length = 1024
0.00.087.066 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.479.059 I load_tensors: offloading 24 repeating layers to GPU
0.01.479.065 I load_tensors: offloading output layer to GPU
0.01.479.067 I load_tensors: offloaded 25/25 layers to GPU
0.01.479.091 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.479.095 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.01.480.041 I llama_init_from_model: n_seq_max     = 1
0.01.480.042 I llama_init_from_model: n_ctx         = 128
0.01.480.042 I llama_init_from_model: n_ctx_per_seq = 128
0.01.480.042 I llama_init_from_model: n_batch       = 128
0.01.480.042 I llama_init_from_model: n_ubatch      = 128
0.01.480.043 I llama_init_from_model: flash_attn    = 0
0.01.480.043 I llama_init_from_model: freq_base     = 10000.0
0.01.480.043 I llama_init_from_model: freq_scale    = 1
0.01.480.044 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.480.044 I ggml_metal_init: allocating
0.01.480.108 I ggml_metal_init: found device: Apple M4
0.01.480.113 I ggml_metal_init: picking default device: Apple M4
0.01.481.241 I ggml_metal_init: using embedded metal library
0.01.485.100 I ggml_metal_init: GPU name:   Apple M4
0.01.485.103 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.485.103 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.485.103 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.485.104 I ggml_metal_init: simdgroup reduction   = true
0.01.485.104 I ggml_metal_init: simdgroup matrix mul. = true
0.01.485.104 I ggml_metal_init: has residency sets    = true
0.01.485.104 I ggml_metal_init: has bfloat            = true
0.01.485.104 I ggml_metal_init: use bfloat            = true
0.01.485.105 I ggml_metal_init: hasUnifiedMemory      = true
0.01.485.106 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.495.666 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.497.545 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.497.547 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.497.562 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.499.266 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.499.267 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.499.268 I llama_init_from_model: graph nodes  = 967
0.01.499.268 I llama_init_from_model: graph splits = 2
0.01.499.269 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.499.269 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.534.297 I 
0.01.534.337 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.534.341 I perplexity: tokenizing the input ..
0.01.539.421 I perplexity: tokenization took 5.075 ms
0.01.539.427 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.658.187 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.659.525 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.659.559 I llama_perf_context_print:        load time =    1505.63 ms
0.01.659.561 I llama_perf_context_print: prompt eval time =     118.49 ms /   128 tokens (    0.93 ms per token,  1080.23 tokens per second)
0.01.659.561 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.659.562 I llama_perf_context_print:       total time =     125.26 ms /   129 tokens
0.01.659.927 I ggml_metal_free: deallocating

real	0m1.847s
user	0m0.103s
sys	0m0.290s
```
- q8_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.055 I build: 4668 (0cf86716) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.091 I main: llama backend init
0.00.000.093 I main: load the model and apply lora adapter, if any
0.00.009.963 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.935 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.017.940 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.944 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.944 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.945 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.945 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.945 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.947 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.947 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.948 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.948 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.949 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.949 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.951 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.953 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.953 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.953 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.904 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.928 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.808 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.810 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.810 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.811 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.811 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.812 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.026.813 I llama_model_loader: - type  f32:  194 tensors
0.00.026.813 I llama_model_loader: - type q8_0:   98 tensors
0.00.026.814 I print_info: file format = GGUF V3 (latest)
0.00.026.814 I print_info: file type   = Q8_0
0.00.026.815 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.035.219 I load: special tokens cache size = 25
0.00.041.465 I load: token to piece cache size = 0.2984 MB
0.00.041.470 I print_info: arch             = gptneox
0.00.041.471 I print_info: vocab_only       = 0
0.00.041.471 I print_info: n_ctx_train      = 2048
0.00.041.471 I print_info: n_embd           = 2048
0.00.041.471 I print_info: n_layer          = 24
0.00.041.479 I print_info: n_head           = 16
0.00.041.480 I print_info: n_head_kv        = 16
0.00.041.480 I print_info: n_rot            = 32
0.00.041.481 I print_info: n_swa            = 0
0.00.041.481 I print_info: n_embd_head_k    = 128
0.00.041.481 I print_info: n_embd_head_v    = 128
0.00.041.481 I print_info: n_gqa            = 1
0.00.041.482 I print_info: n_embd_k_gqa     = 2048
0.00.041.483 I print_info: n_embd_v_gqa     = 2048
0.00.041.485 I print_info: f_norm_eps       = 1.0e-05
0.00.041.486 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.486 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.486 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.486 I print_info: f_logit_scale    = 0.0e+00
0.00.041.487 I print_info: n_ff             = 8192
0.00.041.487 I print_info: n_expert         = 0
0.00.041.488 I print_info: n_expert_used    = 0
0.00.041.488 I print_info: causal attn      = 1
0.00.041.488 I print_info: pooling type     = 0
0.00.041.488 I print_info: rope type        = 2
0.00.041.489 I print_info: rope scaling     = linear
0.00.041.490 I print_info: freq_base_train  = 10000.0
0.00.041.490 I print_info: freq_scale_train = 1
0.00.041.491 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.491 I print_info: rope_finetuned   = unknown
0.00.041.491 I print_info: ssm_d_conv       = 0
0.00.041.491 I print_info: ssm_d_inner      = 0
0.00.041.491 I print_info: ssm_d_state      = 0
0.00.041.491 I print_info: ssm_dt_rank      = 0
0.00.041.492 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.492 I print_info: model type       = 1.4B
0.00.041.492 I print_info: model params     = 1.41 B
0.00.041.492 I print_info: general.name     = 1.4B
0.00.041.493 I print_info: vocab type       = BPE
0.00.041.493 I print_info: n_vocab          = 50304
0.00.041.493 I print_info: n_merges         = 50009
0.00.041.494 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.494 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.494 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.494 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.494 I print_info: LF token         = 187 ''
0.00.041.495 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.495 I print_info: max token length = 1024
0.00.041.495 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.941.478 I load_tensors: offloading 24 repeating layers to GPU
0.00.941.482 I load_tensors: offloading output layer to GPU
0.00.941.482 I load_tensors: offloaded 25/25 layers to GPU
0.00.941.503 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.941.506 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.942.345 I llama_init_from_model: n_seq_max     = 1
0.00.942.347 I llama_init_from_model: n_ctx         = 2048
0.00.942.347 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.942.347 I llama_init_from_model: n_batch       = 2048
0.00.942.348 I llama_init_from_model: n_ubatch      = 512
0.00.942.348 I llama_init_from_model: flash_attn    = 0
0.00.942.349 I llama_init_from_model: freq_base     = 10000.0
0.00.942.349 I llama_init_from_model: freq_scale    = 1
0.00.942.350 I ggml_metal_init: allocating
0.00.942.366 I ggml_metal_init: found device: Apple M4
0.00.942.373 I ggml_metal_init: picking default device: Apple M4
0.00.943.711 I ggml_metal_init: using embedded metal library
0.00.949.115 I ggml_metal_init: GPU name:   Apple M4
0.00.949.118 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.949.118 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.949.120 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.949.120 I ggml_metal_init: simdgroup reduction   = true
0.00.949.120 I ggml_metal_init: simdgroup matrix mul. = true
0.00.949.120 I ggml_metal_init: has residency sets    = true
0.00.949.120 I ggml_metal_init: has bfloat            = true
0.00.949.121 I ggml_metal_init: use bfloat            = true
0.00.949.121 I ggml_metal_init: hasUnifiedMemory      = true
0.00.949.122 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.964.377 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.011.744 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.01.011.750 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.01.011.774 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.016.259 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.01.016.261 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.01.016.262 I llama_init_from_model: graph nodes  = 967
0.01.016.262 I llama_init_from_model: graph splits = 2
0.01.016.267 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.01.016.396 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.016.397 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.070.406 I main: llama threadpool init, n_threads = 4
0.01.070.449 I 
0.01.070.470 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.070.470 I 
0.01.070.621 I sampler seed: 1234
0.01.070.626 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.070.666 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.070.668 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.070.669 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.168.681 I llama_perf_sampler_print:    sampling time =       1.46 ms /    71 runs   (    0.02 ms per token, 48497.27 tokens per second)
0.02.168.682 I llama_perf_context_print:        load time =    1059.74 ms
0.02.168.682 I llama_perf_context_print: prompt eval time =      39.90 ms /     7 tokens (    5.70 ms per token,   175.43 tokens per second)
0.02.168.683 I llama_perf_context_print:        eval time =    1055.33 ms /    63 runs   (   16.75 ms per token,    59.70 tokens per second)
0.02.168.684 I llama_perf_context_print:       total time =    1098.98 ms /    70 tokens
0.02.168.941 I ggml_metal_free: deallocating

real	0m2.187s
user	0m0.108s
sys	0m0.248s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.102 I build: 4668 (0cf86716) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.231 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.602 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.016.608 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.615 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.616 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.616 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.616 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.617 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.618 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.618 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.618 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.619 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.619 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.619 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.620 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.622 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.622 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.623 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.464 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.457 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.254 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.256 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.256 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.257 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.257 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.258 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.025.258 I llama_model_loader: - type  f32:  194 tensors
0.00.025.259 I llama_model_loader: - type q8_0:   98 tensors
0.00.025.259 I print_info: file format = GGUF V3 (latest)
0.00.025.260 I print_info: file type   = Q8_0
0.00.025.261 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.033.353 I load: special tokens cache size = 25
0.00.039.603 I load: token to piece cache size = 0.2984 MB
0.00.039.607 I print_info: arch             = gptneox
0.00.039.607 I print_info: vocab_only       = 0
0.00.039.607 I print_info: n_ctx_train      = 2048
0.00.039.608 I print_info: n_embd           = 2048
0.00.039.608 I print_info: n_layer          = 24
0.00.039.612 I print_info: n_head           = 16
0.00.039.613 I print_info: n_head_kv        = 16
0.00.039.613 I print_info: n_rot            = 32
0.00.039.613 I print_info: n_swa            = 0
0.00.039.614 I print_info: n_embd_head_k    = 128
0.00.039.614 I print_info: n_embd_head_v    = 128
0.00.039.614 I print_info: n_gqa            = 1
0.00.039.615 I print_info: n_embd_k_gqa     = 2048
0.00.039.619 I print_info: n_embd_v_gqa     = 2048
0.00.039.620 I print_info: f_norm_eps       = 1.0e-05
0.00.039.620 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.620 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.620 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.621 I print_info: f_logit_scale    = 0.0e+00
0.00.039.621 I print_info: n_ff             = 8192
0.00.039.621 I print_info: n_expert         = 0
0.00.039.621 I print_info: n_expert_used    = 0
0.00.039.621 I print_info: causal attn      = 1
0.00.039.622 I print_info: pooling type     = 0
0.00.039.623 I print_info: rope type        = 2
0.00.039.623 I print_info: rope scaling     = linear
0.00.039.624 I print_info: freq_base_train  = 10000.0
0.00.039.624 I print_info: freq_scale_train = 1
0.00.039.624 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.624 I print_info: rope_finetuned   = unknown
0.00.039.624 I print_info: ssm_d_conv       = 0
0.00.039.625 I print_info: ssm_d_inner      = 0
0.00.039.625 I print_info: ssm_d_state      = 0
0.00.039.625 I print_info: ssm_dt_rank      = 0
0.00.039.625 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.625 I print_info: model type       = 1.4B
0.00.039.626 I print_info: model params     = 1.41 B
0.00.039.626 I print_info: general.name     = 1.4B
0.00.039.626 I print_info: vocab type       = BPE
0.00.039.629 I print_info: n_vocab          = 50304
0.00.039.629 I print_info: n_merges         = 50009
0.00.039.629 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.629 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.630 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.630 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.631 I print_info: LF token         = 187 ''
0.00.039.631 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.631 I print_info: max token length = 1024
0.00.039.631 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.846.893 I load_tensors: offloading 24 repeating layers to GPU
0.00.846.898 I load_tensors: offloading output layer to GPU
0.00.846.898 I load_tensors: offloaded 25/25 layers to GPU
0.00.846.928 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.846.932 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.848.402 I llama_init_from_model: n_seq_max     = 1
0.00.848.404 I llama_init_from_model: n_ctx         = 128
0.00.848.405 I llama_init_from_model: n_ctx_per_seq = 128
0.00.848.405 I llama_init_from_model: n_batch       = 128
0.00.848.405 I llama_init_from_model: n_ubatch      = 128
0.00.848.405 I llama_init_from_model: flash_attn    = 0
0.00.848.406 I llama_init_from_model: freq_base     = 10000.0
0.00.848.407 I llama_init_from_model: freq_scale    = 1
0.00.848.407 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.848.409 I ggml_metal_init: allocating
0.00.848.510 I ggml_metal_init: found device: Apple M4
0.00.848.520 I ggml_metal_init: picking default device: Apple M4
0.00.849.912 I ggml_metal_init: using embedded metal library
0.00.854.907 I ggml_metal_init: GPU name:   Apple M4
0.00.854.909 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.854.910 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.854.911 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.854.913 I ggml_metal_init: simdgroup reduction   = true
0.00.854.913 I ggml_metal_init: simdgroup matrix mul. = true
0.00.854.913 I ggml_metal_init: has residency sets    = true
0.00.854.914 I ggml_metal_init: has bfloat            = true
0.00.854.914 I ggml_metal_init: use bfloat            = true
0.00.854.915 I ggml_metal_init: hasUnifiedMemory      = true
0.00.854.918 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.869.591 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.872.919 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.872.923 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.872.951 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.875.972 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.875.974 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.875.975 I llama_init_from_model: graph nodes  = 967
0.00.875.975 I llama_init_from_model: graph splits = 2
0.00.875.979 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.875.980 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.900.339 I 
0.00.900.393 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.900.400 I perplexity: tokenizing the input ..
0.00.907.135 I perplexity: tokenization took 6.728 ms
0.00.907.146 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.032.793 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.034.269 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.034.299 I llama_perf_context_print:        load time =     891.10 ms
0.01.034.302 I llama_perf_context_print: prompt eval time =     124.76 ms /   128 tokens (    0.97 ms per token,  1025.98 tokens per second)
0.01.034.303 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.034.303 I llama_perf_context_print:       total time =     133.96 ms /   129 tokens
0.01.034.648 I ggml_metal_free: deallocating

real	0m1.049s
user	0m0.075s
sys	0m0.163s
```
- q4_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.063 I build: 4668 (0cf86716) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.102 I main: llama backend init
0.00.000.104 I main: load the model and apply lora adapter, if any
0.00.015.896 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.024.000 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.024.006 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.024.008 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.024.008 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.024.009 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.024.009 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.024.009 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.024.011 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.024.011 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.024.011 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.024.013 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.024.013 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.024.015 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.024.015 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.024.017 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.024.018 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.024.018 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.028.460 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.029.620 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.034.388 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.034.389 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.034.390 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.034.390 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.034.391 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.034.391 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.034.392 I llama_model_loader: - type  f32:  194 tensors
0.00.034.392 I llama_model_loader: - type q4_0:   97 tensors
0.00.034.392 I llama_model_loader: - type q6_K:    1 tensors
0.00.034.393 I print_info: file format = GGUF V3 (latest)
0.00.034.393 I print_info: file type   = Q4_0
0.00.034.395 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.044.895 I load: special tokens cache size = 25
0.00.054.422 I load: token to piece cache size = 0.2984 MB
0.00.054.426 I print_info: arch             = gptneox
0.00.054.427 I print_info: vocab_only       = 0
0.00.054.427 I print_info: n_ctx_train      = 2048
0.00.054.427 I print_info: n_embd           = 2048
0.00.054.427 I print_info: n_layer          = 24
0.00.054.432 I print_info: n_head           = 16
0.00.054.433 I print_info: n_head_kv        = 16
0.00.054.433 I print_info: n_rot            = 32
0.00.054.434 I print_info: n_swa            = 0
0.00.054.434 I print_info: n_embd_head_k    = 128
0.00.054.434 I print_info: n_embd_head_v    = 128
0.00.054.435 I print_info: n_gqa            = 1
0.00.054.436 I print_info: n_embd_k_gqa     = 2048
0.00.054.437 I print_info: n_embd_v_gqa     = 2048
0.00.054.438 I print_info: f_norm_eps       = 1.0e-05
0.00.054.438 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.054.441 I print_info: f_clamp_kqv      = 0.0e+00
0.00.054.441 I print_info: f_max_alibi_bias = 0.0e+00
0.00.054.441 I print_info: f_logit_scale    = 0.0e+00
0.00.054.442 I print_info: n_ff             = 8192
0.00.054.444 I print_info: n_expert         = 0
0.00.054.444 I print_info: n_expert_used    = 0
0.00.054.444 I print_info: causal attn      = 1
0.00.054.444 I print_info: pooling type     = 0
0.00.054.445 I print_info: rope type        = 2
0.00.054.445 I print_info: rope scaling     = linear
0.00.054.445 I print_info: freq_base_train  = 10000.0
0.00.054.446 I print_info: freq_scale_train = 1
0.00.054.446 I print_info: n_ctx_orig_yarn  = 2048
0.00.054.446 I print_info: rope_finetuned   = unknown
0.00.054.446 I print_info: ssm_d_conv       = 0
0.00.054.447 I print_info: ssm_d_inner      = 0
0.00.054.447 I print_info: ssm_d_state      = 0
0.00.054.447 I print_info: ssm_dt_rank      = 0
0.00.054.447 I print_info: ssm_dt_b_c_rms   = 0
0.00.054.452 I print_info: model type       = 1.4B
0.00.054.452 I print_info: model params     = 1.41 B
0.00.054.453 I print_info: general.name     = 1.4B
0.00.054.453 I print_info: vocab type       = BPE
0.00.054.454 I print_info: n_vocab          = 50304
0.00.054.454 I print_info: n_merges         = 50009
0.00.054.454 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.054.454 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.054.455 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.054.455 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.054.455 I print_info: LF token         = 187 ''
0.00.054.455 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.054.456 I print_info: max token length = 1024
0.00.054.456 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.622.348 I load_tensors: offloading 24 repeating layers to GPU
0.00.622.362 I load_tensors: offloading output layer to GPU
0.00.622.364 I load_tensors: offloaded 25/25 layers to GPU
0.00.622.401 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.622.406 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.624.064 I llama_init_from_model: n_seq_max     = 1
0.00.624.067 I llama_init_from_model: n_ctx         = 2048
0.00.624.067 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.624.068 I llama_init_from_model: n_batch       = 2048
0.00.624.068 I llama_init_from_model: n_ubatch      = 512
0.00.624.069 I llama_init_from_model: flash_attn    = 0
0.00.624.072 I llama_init_from_model: freq_base     = 10000.0
0.00.624.072 I llama_init_from_model: freq_scale    = 1
0.00.624.075 I ggml_metal_init: allocating
0.00.624.182 I ggml_metal_init: found device: Apple M4
0.00.624.195 I ggml_metal_init: picking default device: Apple M4
0.00.626.137 I ggml_metal_init: using embedded metal library
0.00.631.637 I ggml_metal_init: GPU name:   Apple M4
0.00.631.654 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.631.655 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.631.656 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.631.656 I ggml_metal_init: simdgroup reduction   = true
0.00.631.657 I ggml_metal_init: simdgroup matrix mul. = true
0.00.631.657 I ggml_metal_init: has residency sets    = true
0.00.631.657 I ggml_metal_init: has bfloat            = true
0.00.631.657 I ggml_metal_init: use bfloat            = true
0.00.631.661 I ggml_metal_init: hasUnifiedMemory      = true
0.00.631.665 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.651.382 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.708.547 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.708.554 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.708.575 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.714.729 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.714.731 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.714.731 I llama_init_from_model: graph nodes  = 967
0.00.714.731 I llama_init_from_model: graph splits = 2
0.00.714.738 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.714.870 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.714.871 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.772.672 I main: llama threadpool init, n_threads = 4
0.00.772.712 I 
0.00.772.733 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.772.734 I 
0.00.772.908 I sampler seed: 1234
0.00.772.913 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.772.924 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.772.925 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.772.925 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.461.351 I llama_perf_sampler_print:    sampling time =       1.36 ms /    71 runs   (    0.02 ms per token, 52282.77 tokens per second)
0.01.461.352 I llama_perf_context_print:        load time =     756.02 ms
0.01.461.352 I llama_perf_context_print: prompt eval time =      45.09 ms /     7 tokens (    6.44 ms per token,   155.25 tokens per second)
0.01.461.354 I llama_perf_context_print:        eval time =     640.46 ms /    63 runs   (   10.17 ms per token,    98.37 tokens per second)
0.01.461.354 I llama_perf_context_print:       total time =     689.43 ms /    70 tokens
0.01.461.550 I ggml_metal_free: deallocating

real	0m1.500s
user	0m0.120s
sys	0m0.215s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.107 I build: 4668 (0cf86716) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.704 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.648 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.016.654 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.657 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.658 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.658 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.658 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.659 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.660 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.660 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.660 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.661 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.661 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.661 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.662 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.665 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.665 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.665 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.524 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.538 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.454 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.455 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.455 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.456 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.456 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.456 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.025.457 I llama_model_loader: - type  f32:  194 tensors
0.00.025.457 I llama_model_loader: - type q4_0:   97 tensors
0.00.025.458 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.458 I print_info: file format = GGUF V3 (latest)
0.00.025.459 I print_info: file type   = Q4_0
0.00.025.460 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.033.442 I load: special tokens cache size = 25
0.00.039.785 I load: token to piece cache size = 0.2984 MB
0.00.039.789 I print_info: arch             = gptneox
0.00.039.789 I print_info: vocab_only       = 0
0.00.039.789 I print_info: n_ctx_train      = 2048
0.00.039.789 I print_info: n_embd           = 2048
0.00.039.790 I print_info: n_layer          = 24
0.00.039.794 I print_info: n_head           = 16
0.00.039.795 I print_info: n_head_kv        = 16
0.00.039.795 I print_info: n_rot            = 32
0.00.039.795 I print_info: n_swa            = 0
0.00.039.796 I print_info: n_embd_head_k    = 128
0.00.039.796 I print_info: n_embd_head_v    = 128
0.00.039.796 I print_info: n_gqa            = 1
0.00.039.797 I print_info: n_embd_k_gqa     = 2048
0.00.039.798 I print_info: n_embd_v_gqa     = 2048
0.00.039.798 I print_info: f_norm_eps       = 1.0e-05
0.00.039.799 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.800 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.800 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.800 I print_info: f_logit_scale    = 0.0e+00
0.00.039.803 I print_info: n_ff             = 8192
0.00.039.803 I print_info: n_expert         = 0
0.00.039.803 I print_info: n_expert_used    = 0
0.00.039.803 I print_info: causal attn      = 1
0.00.039.803 I print_info: pooling type     = 0
0.00.039.804 I print_info: rope type        = 2
0.00.039.804 I print_info: rope scaling     = linear
0.00.039.806 I print_info: freq_base_train  = 10000.0
0.00.039.806 I print_info: freq_scale_train = 1
0.00.039.807 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.807 I print_info: rope_finetuned   = unknown
0.00.039.807 I print_info: ssm_d_conv       = 0
0.00.039.807 I print_info: ssm_d_inner      = 0
0.00.039.807 I print_info: ssm_d_state      = 0
0.00.039.807 I print_info: ssm_dt_rank      = 0
0.00.039.807 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.808 I print_info: model type       = 1.4B
0.00.039.808 I print_info: model params     = 1.41 B
0.00.039.808 I print_info: general.name     = 1.4B
0.00.039.809 I print_info: vocab type       = BPE
0.00.039.809 I print_info: n_vocab          = 50304
0.00.039.809 I print_info: n_merges         = 50009
0.00.039.809 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.809 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.809 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.809 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.810 I print_info: LF token         = 187 ''
0.00.039.810 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.810 I print_info: max token length = 1024
0.00.039.811 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.597.849 I load_tensors: offloading 24 repeating layers to GPU
0.00.597.864 I load_tensors: offloading output layer to GPU
0.00.597.864 I load_tensors: offloaded 25/25 layers to GPU
0.00.597.900 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.597.902 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.599.418 I llama_init_from_model: n_seq_max     = 1
0.00.599.421 I llama_init_from_model: n_ctx         = 128
0.00.599.422 I llama_init_from_model: n_ctx_per_seq = 128
0.00.599.423 I llama_init_from_model: n_batch       = 128
0.00.599.423 I llama_init_from_model: n_ubatch      = 128
0.00.599.424 I llama_init_from_model: flash_attn    = 0
0.00.599.426 I llama_init_from_model: freq_base     = 10000.0
0.00.599.427 I llama_init_from_model: freq_scale    = 1
0.00.599.427 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.599.430 I ggml_metal_init: allocating
0.00.599.517 I ggml_metal_init: found device: Apple M4
0.00.599.531 I ggml_metal_init: picking default device: Apple M4
0.00.601.392 I ggml_metal_init: using embedded metal library
0.00.606.955 I ggml_metal_init: GPU name:   Apple M4
0.00.606.964 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.606.964 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.606.965 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.606.966 I ggml_metal_init: simdgroup reduction   = true
0.00.606.966 I ggml_metal_init: simdgroup matrix mul. = true
0.00.606.967 I ggml_metal_init: has residency sets    = true
0.00.606.967 I ggml_metal_init: has bfloat            = true
0.00.606.967 I ggml_metal_init: use bfloat            = true
0.00.606.969 I ggml_metal_init: hasUnifiedMemory      = true
0.00.606.972 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.626.586 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.630.160 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.630.170 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.630.218 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.633.528 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.633.530 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.633.530 I llama_init_from_model: graph nodes  = 967
0.00.633.530 I llama_init_from_model: graph splits = 2
0.00.633.534 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.633.534 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.661.926 I 
0.00.661.980 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.661.985 I perplexity: tokenizing the input ..
0.00.666.890 I perplexity: tokenization took 4.901 ms
0.00.666.896 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.790.793 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.792.138 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.792.162 I llama_perf_context_print:        load time =     652.21 ms
0.00.792.163 I llama_perf_context_print: prompt eval time =     123.67 ms /   128 tokens (    0.97 ms per token,  1035.02 tokens per second)
0.00.792.164 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.792.164 I llama_perf_context_print:       total time =     130.24 ms /   129 tokens
0.00.792.542 I ggml_metal_free: deallocating

real	0m0.808s
user	0m0.077s
sys	0m0.134s
```
- q4_1:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.056 I build: 4668 (0cf86716) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.090 I main: llama backend init
0.00.000.092 I main: load the model and apply lora adapter, if any
0.00.008.806 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.025.567 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.025.572 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.025.573 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.025.574 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.025.574 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.025.575 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.025.575 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.025.576 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.025.576 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.025.579 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.025.579 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.025.579 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.025.580 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.025.580 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.025.583 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.025.583 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.025.584 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.029.455 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.030.539 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.034.382 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.034.383 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.034.383 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.034.384 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.034.384 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.034.384 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.034.385 I llama_model_loader: - type  f32:  194 tensors
0.00.034.385 I llama_model_loader: - type q4_1:   97 tensors
0.00.034.385 I llama_model_loader: - type q6_K:    1 tensors
0.00.034.386 I print_info: file format = GGUF V3 (latest)
0.00.034.387 I print_info: file type   = Q4_1
0.00.034.387 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.043.327 I load: special tokens cache size = 25
0.00.050.074 I load: token to piece cache size = 0.2984 MB
0.00.050.077 I print_info: arch             = gptneox
0.00.050.078 I print_info: vocab_only       = 0
0.00.050.078 I print_info: n_ctx_train      = 2048
0.00.050.078 I print_info: n_embd           = 2048
0.00.050.078 I print_info: n_layer          = 24
0.00.050.081 I print_info: n_head           = 16
0.00.050.082 I print_info: n_head_kv        = 16
0.00.050.082 I print_info: n_rot            = 32
0.00.050.084 I print_info: n_swa            = 0
0.00.050.085 I print_info: n_embd_head_k    = 128
0.00.050.085 I print_info: n_embd_head_v    = 128
0.00.050.086 I print_info: n_gqa            = 1
0.00.050.086 I print_info: n_embd_k_gqa     = 2048
0.00.050.087 I print_info: n_embd_v_gqa     = 2048
0.00.050.088 I print_info: f_norm_eps       = 1.0e-05
0.00.050.088 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.088 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.088 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.089 I print_info: f_logit_scale    = 0.0e+00
0.00.050.089 I print_info: n_ff             = 8192
0.00.050.089 I print_info: n_expert         = 0
0.00.050.089 I print_info: n_expert_used    = 0
0.00.050.090 I print_info: causal attn      = 1
0.00.050.090 I print_info: pooling type     = 0
0.00.050.092 I print_info: rope type        = 2
0.00.050.093 I print_info: rope scaling     = linear
0.00.050.094 I print_info: freq_base_train  = 10000.0
0.00.050.094 I print_info: freq_scale_train = 1
0.00.050.094 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.099 I print_info: rope_finetuned   = unknown
0.00.050.099 I print_info: ssm_d_conv       = 0
0.00.050.100 I print_info: ssm_d_inner      = 0
0.00.050.100 I print_info: ssm_d_state      = 0
0.00.050.100 I print_info: ssm_dt_rank      = 0
0.00.050.100 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.100 I print_info: model type       = 1.4B
0.00.050.101 I print_info: model params     = 1.41 B
0.00.050.101 I print_info: general.name     = 1.4B
0.00.050.101 I print_info: vocab type       = BPE
0.00.050.101 I print_info: n_vocab          = 50304
0.00.050.102 I print_info: n_merges         = 50009
0.00.050.102 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.103 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.103 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.104 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.104 I print_info: LF token         = 187 ''
0.00.050.104 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.104 I print_info: max token length = 1024
0.00.050.105 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.630.940 I load_tensors: offloading 24 repeating layers to GPU
0.00.630.954 I load_tensors: offloading output layer to GPU
0.00.630.955 I load_tensors: offloaded 25/25 layers to GPU
0.00.630.990 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.631.007 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.632.575 I llama_init_from_model: n_seq_max     = 1
0.00.632.577 I llama_init_from_model: n_ctx         = 2048
0.00.632.578 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.632.579 I llama_init_from_model: n_batch       = 2048
0.00.632.579 I llama_init_from_model: n_ubatch      = 512
0.00.632.580 I llama_init_from_model: flash_attn    = 0
0.00.632.582 I llama_init_from_model: freq_base     = 10000.0
0.00.632.582 I llama_init_from_model: freq_scale    = 1
0.00.632.584 I ggml_metal_init: allocating
0.00.632.659 I ggml_metal_init: found device: Apple M4
0.00.632.672 I ggml_metal_init: picking default device: Apple M4
0.00.634.530 I ggml_metal_init: using embedded metal library
0.00.640.461 I ggml_metal_init: GPU name:   Apple M4
0.00.640.466 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.640.467 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.640.468 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.640.469 I ggml_metal_init: simdgroup reduction   = true
0.00.640.469 I ggml_metal_init: simdgroup matrix mul. = true
0.00.640.469 I ggml_metal_init: has residency sets    = true
0.00.640.470 I ggml_metal_init: has bfloat            = true
0.00.640.470 I ggml_metal_init: use bfloat            = true
0.00.640.471 I ggml_metal_init: hasUnifiedMemory      = true
0.00.640.473 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.659.218 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.711.294 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.711.301 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.711.322 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.715.731 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.715.733 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.715.733 I llama_init_from_model: graph nodes  = 967
0.00.715.733 I llama_init_from_model: graph splits = 2
0.00.715.740 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.715.864 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.715.864 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.769.605 I main: llama threadpool init, n_threads = 4
0.00.769.650 I 
0.00.769.672 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.769.672 I 
0.00.769.844 I sampler seed: 1234
0.00.769.849 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.769.870 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.769.871 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.769.871 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.506.796 I llama_perf_sampler_print:    sampling time =       1.26 ms /    71 runs   (    0.02 ms per token, 56349.21 tokens per second)
0.01.506.797 I llama_perf_context_print:        load time =     760.10 ms
0.01.506.798 I llama_perf_context_print: prompt eval time =      48.96 ms /     7 tokens (    6.99 ms per token,   142.98 tokens per second)
0.01.506.798 I llama_perf_context_print:        eval time =     685.19 ms /    63 runs   (   10.88 ms per token,    91.95 tokens per second)
0.01.506.799 I llama_perf_context_print:       total time =     737.89 ms /    70 tokens
0.01.507.112 I ggml_metal_free: deallocating

real	0m1.523s
user	0m0.110s
sys	0m0.210s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.099 I build: 4668 (0cf86716) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.946 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.447 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.452 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.454 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.461 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.462 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.462 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.462 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.465 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.465 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.465 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.465 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.466 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.466 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.466 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.468 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.468 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.469 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.346 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.373 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.213 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.214 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.214 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.215 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.215 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.215 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.025.216 I llama_model_loader: - type  f32:  194 tensors
0.00.025.216 I llama_model_loader: - type q4_1:   97 tensors
0.00.025.217 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.217 I print_info: file format = GGUF V3 (latest)
0.00.025.218 I print_info: file type   = Q4_1
0.00.025.219 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.033.498 I load: special tokens cache size = 25
0.00.039.624 I load: token to piece cache size = 0.2984 MB
0.00.039.628 I print_info: arch             = gptneox
0.00.039.628 I print_info: vocab_only       = 0
0.00.039.629 I print_info: n_ctx_train      = 2048
0.00.039.629 I print_info: n_embd           = 2048
0.00.039.629 I print_info: n_layer          = 24
0.00.039.633 I print_info: n_head           = 16
0.00.039.636 I print_info: n_head_kv        = 16
0.00.039.638 I print_info: n_rot            = 32
0.00.039.638 I print_info: n_swa            = 0
0.00.039.638 I print_info: n_embd_head_k    = 128
0.00.039.638 I print_info: n_embd_head_v    = 128
0.00.039.639 I print_info: n_gqa            = 1
0.00.039.640 I print_info: n_embd_k_gqa     = 2048
0.00.039.641 I print_info: n_embd_v_gqa     = 2048
0.00.039.642 I print_info: f_norm_eps       = 1.0e-05
0.00.039.643 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.643 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.643 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.643 I print_info: f_logit_scale    = 0.0e+00
0.00.039.644 I print_info: n_ff             = 8192
0.00.039.644 I print_info: n_expert         = 0
0.00.039.644 I print_info: n_expert_used    = 0
0.00.039.646 I print_info: causal attn      = 1
0.00.039.646 I print_info: pooling type     = 0
0.00.039.646 I print_info: rope type        = 2
0.00.039.646 I print_info: rope scaling     = linear
0.00.039.647 I print_info: freq_base_train  = 10000.0
0.00.039.648 I print_info: freq_scale_train = 1
0.00.039.648 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.648 I print_info: rope_finetuned   = unknown
0.00.039.648 I print_info: ssm_d_conv       = 0
0.00.039.648 I print_info: ssm_d_inner      = 0
0.00.039.648 I print_info: ssm_d_state      = 0
0.00.039.648 I print_info: ssm_dt_rank      = 0
0.00.039.649 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.649 I print_info: model type       = 1.4B
0.00.039.649 I print_info: model params     = 1.41 B
0.00.039.649 I print_info: general.name     = 1.4B
0.00.039.650 I print_info: vocab type       = BPE
0.00.039.650 I print_info: n_vocab          = 50304
0.00.039.650 I print_info: n_merges         = 50009
0.00.039.650 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.650 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.651 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.651 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.651 I print_info: LF token         = 187 ''
0.00.039.651 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.657 I print_info: max token length = 1024
0.00.039.659 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.622.987 I load_tensors: offloading 24 repeating layers to GPU
0.00.623.002 I load_tensors: offloading output layer to GPU
0.00.623.002 I load_tensors: offloaded 25/25 layers to GPU
0.00.623.038 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.623.040 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.624.765 I llama_init_from_model: n_seq_max     = 1
0.00.624.767 I llama_init_from_model: n_ctx         = 128
0.00.624.768 I llama_init_from_model: n_ctx_per_seq = 128
0.00.624.768 I llama_init_from_model: n_batch       = 128
0.00.624.768 I llama_init_from_model: n_ubatch      = 128
0.00.624.769 I llama_init_from_model: flash_attn    = 0
0.00.624.771 I llama_init_from_model: freq_base     = 10000.0
0.00.624.771 I llama_init_from_model: freq_scale    = 1
0.00.624.772 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.624.774 I ggml_metal_init: allocating
0.00.624.850 I ggml_metal_init: found device: Apple M4
0.00.624.864 I ggml_metal_init: picking default device: Apple M4
0.00.626.605 I ggml_metal_init: using embedded metal library
0.00.633.309 I ggml_metal_init: GPU name:   Apple M4
0.00.633.314 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.633.315 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.633.315 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.633.316 I ggml_metal_init: simdgroup reduction   = true
0.00.633.317 I ggml_metal_init: simdgroup matrix mul. = true
0.00.633.317 I ggml_metal_init: has residency sets    = true
0.00.633.317 I ggml_metal_init: has bfloat            = true
0.00.633.317 I ggml_metal_init: use bfloat            = true
0.00.633.318 I ggml_metal_init: hasUnifiedMemory      = true
0.00.633.322 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.651.296 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.654.693 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.654.697 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.654.723 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.657.975 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.657.977 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.657.978 I llama_init_from_model: graph nodes  = 967
0.00.657.978 I llama_init_from_model: graph splits = 2
0.00.657.981 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.657.981 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.686.264 I 
0.00.686.347 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.686.355 I perplexity: tokenizing the input ..
0.00.693.389 I perplexity: tokenization took 7.028 ms
0.00.693.396 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.830.119 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.831.464 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.831.491 I llama_perf_context_print:        load time =     677.31 ms
0.00.831.492 I llama_perf_context_print: prompt eval time =     135.84 ms /   128 tokens (    1.06 ms per token,   942.29 tokens per second)
0.00.831.493 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.831.493 I llama_perf_context_print:       total time =     145.23 ms /   129 tokens
0.00.831.866 I ggml_metal_free: deallocating

real	0m0.845s
user	0m0.080s
sys	0m0.140s
```
- q5_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.052 I build: 4668 (0cf86716) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.085 I main: llama backend init
0.00.000.087 I main: load the model and apply lora adapter, if any
0.00.010.200 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.663 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.017.668 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.669 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.670 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.670 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.670 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.671 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.672 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.672 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.673 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.674 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.674 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.674 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.675 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.676 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.676 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.677 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.355 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.404 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.050 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.051 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.052 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.052 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.052 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.053 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.026.053 I llama_model_loader: - type  f32:  194 tensors
0.00.026.054 I llama_model_loader: - type q5_0:   97 tensors
0.00.026.054 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.054 I print_info: file format = GGUF V3 (latest)
0.00.026.055 I print_info: file type   = Q5_0
0.00.026.056 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.033.837 I load: special tokens cache size = 25
0.00.039.970 I load: token to piece cache size = 0.2984 MB
0.00.039.973 I print_info: arch             = gptneox
0.00.039.973 I print_info: vocab_only       = 0
0.00.039.973 I print_info: n_ctx_train      = 2048
0.00.039.973 I print_info: n_embd           = 2048
0.00.039.973 I print_info: n_layer          = 24
0.00.039.976 I print_info: n_head           = 16
0.00.039.977 I print_info: n_head_kv        = 16
0.00.039.977 I print_info: n_rot            = 32
0.00.039.979 I print_info: n_swa            = 0
0.00.039.979 I print_info: n_embd_head_k    = 128
0.00.039.980 I print_info: n_embd_head_v    = 128
0.00.039.980 I print_info: n_gqa            = 1
0.00.039.981 I print_info: n_embd_k_gqa     = 2048
0.00.039.983 I print_info: n_embd_v_gqa     = 2048
0.00.039.984 I print_info: f_norm_eps       = 1.0e-05
0.00.039.984 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.984 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.984 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.984 I print_info: f_logit_scale    = 0.0e+00
0.00.039.985 I print_info: n_ff             = 8192
0.00.039.986 I print_info: n_expert         = 0
0.00.039.986 I print_info: n_expert_used    = 0
0.00.039.987 I print_info: causal attn      = 1
0.00.039.987 I print_info: pooling type     = 0
0.00.039.988 I print_info: rope type        = 2
0.00.039.990 I print_info: rope scaling     = linear
0.00.039.990 I print_info: freq_base_train  = 10000.0
0.00.039.991 I print_info: freq_scale_train = 1
0.00.039.991 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.991 I print_info: rope_finetuned   = unknown
0.00.039.991 I print_info: ssm_d_conv       = 0
0.00.039.991 I print_info: ssm_d_inner      = 0
0.00.039.991 I print_info: ssm_d_state      = 0
0.00.039.991 I print_info: ssm_dt_rank      = 0
0.00.039.993 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.993 I print_info: model type       = 1.4B
0.00.039.994 I print_info: model params     = 1.41 B
0.00.039.994 I print_info: general.name     = 1.4B
0.00.039.994 I print_info: vocab type       = BPE
0.00.039.994 I print_info: n_vocab          = 50304
0.00.039.995 I print_info: n_merges         = 50009
0.00.039.995 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.995 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.995 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.995 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.995 I print_info: LF token         = 187 ''
0.00.039.996 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.996 I print_info: max token length = 1024
0.00.039.996 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.661.101 I load_tensors: offloading 24 repeating layers to GPU
0.00.661.116 I load_tensors: offloading output layer to GPU
0.00.661.117 I load_tensors: offloaded 25/25 layers to GPU
0.00.661.152 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.661.153 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.662.931 I llama_init_from_model: n_seq_max     = 1
0.00.662.935 I llama_init_from_model: n_ctx         = 2048
0.00.662.935 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.662.936 I llama_init_from_model: n_batch       = 2048
0.00.662.936 I llama_init_from_model: n_ubatch      = 512
0.00.662.936 I llama_init_from_model: flash_attn    = 0
0.00.662.938 I llama_init_from_model: freq_base     = 10000.0
0.00.662.939 I llama_init_from_model: freq_scale    = 1
0.00.662.940 I ggml_metal_init: allocating
0.00.662.955 I ggml_metal_init: found device: Apple M4
0.00.662.965 I ggml_metal_init: picking default device: Apple M4
0.00.664.438 I ggml_metal_init: using embedded metal library
0.00.670.689 I ggml_metal_init: GPU name:   Apple M4
0.00.670.692 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.670.693 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.670.694 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.670.695 I ggml_metal_init: simdgroup reduction   = true
0.00.670.695 I ggml_metal_init: simdgroup matrix mul. = true
0.00.670.695 I ggml_metal_init: has residency sets    = true
0.00.670.695 I ggml_metal_init: has bfloat            = true
0.00.670.696 I ggml_metal_init: use bfloat            = true
0.00.670.696 I ggml_metal_init: hasUnifiedMemory      = true
0.00.670.708 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.687.408 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.737.744 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.737.750 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.737.782 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.741.832 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.741.834 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.741.834 I llama_init_from_model: graph nodes  = 967
0.00.741.835 I llama_init_from_model: graph splits = 2
0.00.741.840 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.741.966 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.741.967 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.798.249 I main: llama threadpool init, n_threads = 4
0.00.798.294 I 
0.00.798.316 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.798.319 I 
0.00.798.471 I sampler seed: 1234
0.00.798.475 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.798.495 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.798.496 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.798.496 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.583.638 I llama_perf_sampler_print:    sampling time =       1.30 ms /    71 runs   (    0.02 ms per token, 54447.85 tokens per second)
0.01.583.639 I llama_perf_context_print:        load time =     787.35 ms
0.01.583.639 I llama_perf_context_print: prompt eval time =      42.86 ms /     7 tokens (    6.12 ms per token,   163.31 tokens per second)
0.01.583.640 I llama_perf_context_print:        eval time =     739.40 ms /    63 runs   (   11.74 ms per token,    85.20 tokens per second)
0.01.583.640 I llama_perf_context_print:       total time =     786.08 ms /    70 tokens
0.01.583.903 I ggml_metal_free: deallocating

real	0m1.603s
user	0m0.107s
sys	0m0.219s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.103 I build: 4668 (0cf86716) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.936 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.670 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.016.676 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.677 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.678 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.683 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.684 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.684 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.685 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.685 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.686 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.686 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.686 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.687 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.689 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.692 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.692 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.692 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.576 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.669 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.519 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.521 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.521 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.522 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.522 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.522 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.025.523 I llama_model_loader: - type  f32:  194 tensors
0.00.025.523 I llama_model_loader: - type q5_0:   97 tensors
0.00.025.524 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.524 I print_info: file format = GGUF V3 (latest)
0.00.025.525 I print_info: file type   = Q5_0
0.00.025.527 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.033.926 I load: special tokens cache size = 25
0.00.039.846 I load: token to piece cache size = 0.2984 MB
0.00.039.850 I print_info: arch             = gptneox
0.00.039.850 I print_info: vocab_only       = 0
0.00.039.850 I print_info: n_ctx_train      = 2048
0.00.039.850 I print_info: n_embd           = 2048
0.00.039.850 I print_info: n_layer          = 24
0.00.039.854 I print_info: n_head           = 16
0.00.039.855 I print_info: n_head_kv        = 16
0.00.039.855 I print_info: n_rot            = 32
0.00.039.856 I print_info: n_swa            = 0
0.00.039.856 I print_info: n_embd_head_k    = 128
0.00.039.856 I print_info: n_embd_head_v    = 128
0.00.039.857 I print_info: n_gqa            = 1
0.00.039.857 I print_info: n_embd_k_gqa     = 2048
0.00.039.860 I print_info: n_embd_v_gqa     = 2048
0.00.039.861 I print_info: f_norm_eps       = 1.0e-05
0.00.039.862 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.862 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.863 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.863 I print_info: f_logit_scale    = 0.0e+00
0.00.039.865 I print_info: n_ff             = 8192
0.00.039.865 I print_info: n_expert         = 0
0.00.039.865 I print_info: n_expert_used    = 0
0.00.039.865 I print_info: causal attn      = 1
0.00.039.865 I print_info: pooling type     = 0
0.00.039.865 I print_info: rope type        = 2
0.00.039.866 I print_info: rope scaling     = linear
0.00.039.866 I print_info: freq_base_train  = 10000.0
0.00.039.866 I print_info: freq_scale_train = 1
0.00.039.866 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.871 I print_info: rope_finetuned   = unknown
0.00.039.871 I print_info: ssm_d_conv       = 0
0.00.039.871 I print_info: ssm_d_inner      = 0
0.00.039.871 I print_info: ssm_d_state      = 0
0.00.039.872 I print_info: ssm_dt_rank      = 0
0.00.039.872 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.873 I print_info: model type       = 1.4B
0.00.039.874 I print_info: model params     = 1.41 B
0.00.039.874 I print_info: general.name     = 1.4B
0.00.039.874 I print_info: vocab type       = BPE
0.00.039.874 I print_info: n_vocab          = 50304
0.00.039.875 I print_info: n_merges         = 50009
0.00.039.875 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.875 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.875 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.875 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.875 I print_info: LF token         = 187 ''
0.00.039.876 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.876 I print_info: max token length = 1024
0.00.039.876 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.675.889 I load_tensors: offloading 24 repeating layers to GPU
0.00.675.903 I load_tensors: offloading output layer to GPU
0.00.675.904 I load_tensors: offloaded 25/25 layers to GPU
0.00.675.932 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.675.933 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.677.469 I llama_init_from_model: n_seq_max     = 1
0.00.677.474 I llama_init_from_model: n_ctx         = 128
0.00.677.475 I llama_init_from_model: n_ctx_per_seq = 128
0.00.677.475 I llama_init_from_model: n_batch       = 128
0.00.677.476 I llama_init_from_model: n_ubatch      = 128
0.00.677.476 I llama_init_from_model: flash_attn    = 0
0.00.677.478 I llama_init_from_model: freq_base     = 10000.0
0.00.677.478 I llama_init_from_model: freq_scale    = 1
0.00.677.479 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.677.483 I ggml_metal_init: allocating
0.00.677.554 I ggml_metal_init: found device: Apple M4
0.00.677.566 I ggml_metal_init: picking default device: Apple M4
0.00.679.378 I ggml_metal_init: using embedded metal library
0.00.686.113 I ggml_metal_init: GPU name:   Apple M4
0.00.686.119 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.686.120 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.686.121 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.686.122 I ggml_metal_init: simdgroup reduction   = true
0.00.686.122 I ggml_metal_init: simdgroup matrix mul. = true
0.00.686.122 I ggml_metal_init: has residency sets    = true
0.00.686.122 I ggml_metal_init: has bfloat            = true
0.00.686.123 I ggml_metal_init: use bfloat            = true
0.00.686.123 I ggml_metal_init: hasUnifiedMemory      = true
0.00.686.125 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.704.061 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.707.620 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.707.627 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.707.660 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.711.079 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.711.080 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.711.081 I llama_init_from_model: graph nodes  = 967
0.00.711.081 I llama_init_from_model: graph splits = 2
0.00.711.084 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.711.084 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.743.682 I 
0.00.743.767 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.743.776 I perplexity: tokenizing the input ..
0.00.750.833 I perplexity: tokenization took 7.051 ms
0.00.750.840 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.892.787 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.894.120 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.894.142 I llama_perf_context_print:        load time =     734.74 ms
0.00.894.142 I llama_perf_context_print: prompt eval time =     141.67 ms /   128 tokens (    1.11 ms per token,   903.51 tokens per second)
0.00.894.145 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.894.145 I llama_perf_context_print:       total time =     150.47 ms /   129 tokens
0.00.894.513 I ggml_metal_free: deallocating

real	0m0.908s
user	0m0.079s
sys	0m0.165s
```
- q5_1:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.051 I build: 4668 (0cf86716) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.085 I main: llama backend init
0.00.000.087 I main: load the model and apply lora adapter, if any
0.00.008.875 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.748 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.017.752 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.754 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.755 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.755 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.756 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.756 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.757 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.757 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.758 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.758 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.760 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.762 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.763 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.765 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.766 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.766 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.648 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.701 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.565 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.566 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.566 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.566 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.566 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.567 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.026.567 I llama_model_loader: - type  f32:  194 tensors
0.00.026.567 I llama_model_loader: - type q5_1:   97 tensors
0.00.026.568 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.568 I print_info: file format = GGUF V3 (latest)
0.00.026.569 I print_info: file type   = Q5_1
0.00.026.569 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.034.692 I load: special tokens cache size = 25
0.00.040.812 I load: token to piece cache size = 0.2984 MB
0.00.040.815 I print_info: arch             = gptneox
0.00.040.815 I print_info: vocab_only       = 0
0.00.040.815 I print_info: n_ctx_train      = 2048
0.00.040.816 I print_info: n_embd           = 2048
0.00.040.816 I print_info: n_layer          = 24
0.00.040.818 I print_info: n_head           = 16
0.00.040.819 I print_info: n_head_kv        = 16
0.00.040.819 I print_info: n_rot            = 32
0.00.040.819 I print_info: n_swa            = 0
0.00.040.820 I print_info: n_embd_head_k    = 128
0.00.040.822 I print_info: n_embd_head_v    = 128
0.00.040.823 I print_info: n_gqa            = 1
0.00.040.823 I print_info: n_embd_k_gqa     = 2048
0.00.040.824 I print_info: n_embd_v_gqa     = 2048
0.00.040.829 I print_info: f_norm_eps       = 1.0e-05
0.00.040.829 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.829 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.829 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.831 I print_info: f_logit_scale    = 0.0e+00
0.00.040.832 I print_info: n_ff             = 8192
0.00.040.832 I print_info: n_expert         = 0
0.00.040.832 I print_info: n_expert_used    = 0
0.00.040.832 I print_info: causal attn      = 1
0.00.040.832 I print_info: pooling type     = 0
0.00.040.834 I print_info: rope type        = 2
0.00.040.836 I print_info: rope scaling     = linear
0.00.040.836 I print_info: freq_base_train  = 10000.0
0.00.040.837 I print_info: freq_scale_train = 1
0.00.040.837 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.837 I print_info: rope_finetuned   = unknown
0.00.040.837 I print_info: ssm_d_conv       = 0
0.00.040.837 I print_info: ssm_d_inner      = 0
0.00.040.838 I print_info: ssm_d_state      = 0
0.00.040.838 I print_info: ssm_dt_rank      = 0
0.00.040.838 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.839 I print_info: model type       = 1.4B
0.00.040.839 I print_info: model params     = 1.41 B
0.00.040.839 I print_info: general.name     = 1.4B
0.00.040.841 I print_info: vocab type       = BPE
0.00.040.841 I print_info: n_vocab          = 50304
0.00.040.841 I print_info: n_merges         = 50009
0.00.040.841 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.841 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.841 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.841 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.842 I print_info: LF token         = 187 ''
0.00.040.843 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.843 I print_info: max token length = 1024
0.00.040.843 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.666.487 I load_tensors: offloading 24 repeating layers to GPU
0.00.666.491 I load_tensors: offloading output layer to GPU
0.00.666.492 I load_tensors: offloaded 25/25 layers to GPU
0.00.666.515 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.666.519 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.667.943 I llama_init_from_model: n_seq_max     = 1
0.00.667.945 I llama_init_from_model: n_ctx         = 2048
0.00.667.945 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.667.946 I llama_init_from_model: n_batch       = 2048
0.00.667.947 I llama_init_from_model: n_ubatch      = 512
0.00.667.947 I llama_init_from_model: flash_attn    = 0
0.00.667.948 I llama_init_from_model: freq_base     = 10000.0
0.00.667.949 I llama_init_from_model: freq_scale    = 1
0.00.667.950 I ggml_metal_init: allocating
0.00.667.961 I ggml_metal_init: found device: Apple M4
0.00.667.969 I ggml_metal_init: picking default device: Apple M4
0.00.669.473 I ggml_metal_init: using embedded metal library
0.00.675.590 I ggml_metal_init: GPU name:   Apple M4
0.00.675.593 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.675.594 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.675.595 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.675.595 I ggml_metal_init: simdgroup reduction   = true
0.00.675.596 I ggml_metal_init: simdgroup matrix mul. = true
0.00.675.596 I ggml_metal_init: has residency sets    = true
0.00.675.596 I ggml_metal_init: has bfloat            = true
0.00.675.596 I ggml_metal_init: use bfloat            = true
0.00.675.597 I ggml_metal_init: hasUnifiedMemory      = true
0.00.675.601 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.692.889 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.746.409 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.746.415 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.746.440 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.750.708 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.750.710 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.750.711 I llama_init_from_model: graph nodes  = 967
0.00.750.711 I llama_init_from_model: graph splits = 2
0.00.750.716 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.750.840 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.750.841 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.810.847 I main: llama threadpool init, n_threads = 4
0.00.810.893 I 
0.00.810.918 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.810.918 I 
0.00.811.091 I sampler seed: 1234
0.00.811.096 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.811.116 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.811.116 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.811.116 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.651.654 I llama_perf_sampler_print:    sampling time =       1.29 ms /    71 runs   (    0.02 ms per token, 54911.06 tokens per second)
0.01.651.654 I llama_perf_context_print:        load time =     801.28 ms
0.01.651.655 I llama_perf_context_print: prompt eval time =      48.65 ms /     7 tokens (    6.95 ms per token,   143.88 tokens per second)
0.01.651.656 I llama_perf_context_print:        eval time =     789.06 ms /    63 runs   (   12.52 ms per token,    79.84 tokens per second)
0.01.651.656 I llama_perf_context_print:       total time =     841.49 ms /    70 tokens
0.01.651.905 I ggml_metal_free: deallocating

real	0m1.668s
user	0m0.109s
sys	0m0.200s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.106 I build: 4668 (0cf86716) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.955 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.398 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.017.403 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.405 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.410 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.410 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.410 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.411 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.412 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.412 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.412 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.414 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.414 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.414 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.415 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.416 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.417 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.417 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.161 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.163 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.852 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.854 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.854 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.854 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.855 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.855 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.855 I llama_model_loader: - type  f32:  194 tensors
0.00.025.856 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.856 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.856 I print_info: file format = GGUF V3 (latest)
0.00.025.857 I print_info: file type   = Q5_1
0.00.025.858 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.033.908 I load: special tokens cache size = 25
0.00.040.015 I load: token to piece cache size = 0.2984 MB
0.00.040.018 I print_info: arch             = gptneox
0.00.040.019 I print_info: vocab_only       = 0
0.00.040.019 I print_info: n_ctx_train      = 2048
0.00.040.019 I print_info: n_embd           = 2048
0.00.040.019 I print_info: n_layer          = 24
0.00.040.022 I print_info: n_head           = 16
0.00.040.023 I print_info: n_head_kv        = 16
0.00.040.023 I print_info: n_rot            = 32
0.00.040.023 I print_info: n_swa            = 0
0.00.040.024 I print_info: n_embd_head_k    = 128
0.00.040.024 I print_info: n_embd_head_v    = 128
0.00.040.024 I print_info: n_gqa            = 1
0.00.040.025 I print_info: n_embd_k_gqa     = 2048
0.00.040.026 I print_info: n_embd_v_gqa     = 2048
0.00.040.026 I print_info: f_norm_eps       = 1.0e-05
0.00.040.027 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.027 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.027 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.028 I print_info: f_logit_scale    = 0.0e+00
0.00.040.028 I print_info: n_ff             = 8192
0.00.040.028 I print_info: n_expert         = 0
0.00.040.028 I print_info: n_expert_used    = 0
0.00.040.029 I print_info: causal attn      = 1
0.00.040.029 I print_info: pooling type     = 0
0.00.040.029 I print_info: rope type        = 2
0.00.040.029 I print_info: rope scaling     = linear
0.00.040.030 I print_info: freq_base_train  = 10000.0
0.00.040.030 I print_info: freq_scale_train = 1
0.00.040.030 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.030 I print_info: rope_finetuned   = unknown
0.00.040.030 I print_info: ssm_d_conv       = 0
0.00.040.030 I print_info: ssm_d_inner      = 0
0.00.040.031 I print_info: ssm_d_state      = 0
0.00.040.031 I print_info: ssm_dt_rank      = 0
0.00.040.031 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.031 I print_info: model type       = 1.4B
0.00.040.031 I print_info: model params     = 1.41 B
0.00.040.032 I print_info: general.name     = 1.4B
0.00.040.032 I print_info: vocab type       = BPE
0.00.040.032 I print_info: n_vocab          = 50304
0.00.040.032 I print_info: n_merges         = 50009
0.00.040.035 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.035 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.035 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.035 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.036 I print_info: LF token         = 187 ''
0.00.040.036 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.036 I print_info: max token length = 1024
0.00.040.037 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.662.576 I load_tensors: offloading 24 repeating layers to GPU
0.00.662.591 I load_tensors: offloading output layer to GPU
0.00.662.592 I load_tensors: offloaded 25/25 layers to GPU
0.00.662.626 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.662.638 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.664.278 I llama_init_from_model: n_seq_max     = 1
0.00.664.281 I llama_init_from_model: n_ctx         = 128
0.00.664.281 I llama_init_from_model: n_ctx_per_seq = 128
0.00.664.282 I llama_init_from_model: n_batch       = 128
0.00.664.282 I llama_init_from_model: n_ubatch      = 128
0.00.664.283 I llama_init_from_model: flash_attn    = 0
0.00.664.285 I llama_init_from_model: freq_base     = 10000.0
0.00.664.285 I llama_init_from_model: freq_scale    = 1
0.00.664.286 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.664.294 I ggml_metal_init: allocating
0.00.664.369 I ggml_metal_init: found device: Apple M4
0.00.664.383 I ggml_metal_init: picking default device: Apple M4
0.00.666.039 I ggml_metal_init: using embedded metal library
0.00.672.458 I ggml_metal_init: GPU name:   Apple M4
0.00.672.461 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.672.462 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.672.462 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.672.463 I ggml_metal_init: simdgroup reduction   = true
0.00.672.463 I ggml_metal_init: simdgroup matrix mul. = true
0.00.672.463 I ggml_metal_init: has residency sets    = true
0.00.672.464 I ggml_metal_init: has bfloat            = true
0.00.672.464 I ggml_metal_init: use bfloat            = true
0.00.672.465 I ggml_metal_init: hasUnifiedMemory      = true
0.00.672.466 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.689.280 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.692.925 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.692.929 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.692.955 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.696.222 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.696.224 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.696.224 I llama_init_from_model: graph nodes  = 967
0.00.696.225 I llama_init_from_model: graph splits = 2
0.00.696.228 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.696.230 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.722.821 I 
0.00.722.903 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.722.914 I perplexity: tokenizing the input ..
0.00.730.314 I perplexity: tokenization took 7.393 ms
0.00.730.323 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.866.870 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.868.249 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.868.270 I llama_perf_context_print:        load time =     712.86 ms
0.00.868.271 I llama_perf_context_print: prompt eval time =     135.59 ms /   128 tokens (    1.06 ms per token,   944.04 tokens per second)
0.00.868.272 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.868.272 I llama_perf_context_print:       total time =     145.45 ms /   129 tokens
0.00.868.609 I ggml_metal_free: deallocating

real	0m0.885s
user	0m0.079s
sys	0m0.131s
```
- q2_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.052 I build: 4668 (0cf86716) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.085 I main: llama backend init
0.00.000.087 I main: load the model and apply lora adapter, if any
0.00.009.956 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.507 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.512 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.517 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.518 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.518 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.519 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.519 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.520 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.520 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.521 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.521 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.521 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.522 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.522 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.524 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.524 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.524 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.317 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.355 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.164 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.165 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.166 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.166 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.166 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.167 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.167 I llama_model_loader: - type  f32:  194 tensors
0.00.025.167 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.168 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.168 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.168 I print_info: file format = GGUF V3 (latest)
0.00.025.169 I print_info: file type   = Q2_K - Medium
0.00.025.170 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.032.932 I load: special tokens cache size = 25
0.00.038.863 I load: token to piece cache size = 0.2984 MB
0.00.038.865 I print_info: arch             = gptneox
0.00.038.866 I print_info: vocab_only       = 0
0.00.038.866 I print_info: n_ctx_train      = 2048
0.00.038.866 I print_info: n_embd           = 2048
0.00.038.866 I print_info: n_layer          = 24
0.00.038.869 I print_info: n_head           = 16
0.00.038.870 I print_info: n_head_kv        = 16
0.00.038.872 I print_info: n_rot            = 32
0.00.038.872 I print_info: n_swa            = 0
0.00.038.872 I print_info: n_embd_head_k    = 128
0.00.038.873 I print_info: n_embd_head_v    = 128
0.00.038.873 I print_info: n_gqa            = 1
0.00.038.874 I print_info: n_embd_k_gqa     = 2048
0.00.038.875 I print_info: n_embd_v_gqa     = 2048
0.00.038.875 I print_info: f_norm_eps       = 1.0e-05
0.00.038.875 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.876 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.876 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.876 I print_info: f_logit_scale    = 0.0e+00
0.00.038.876 I print_info: n_ff             = 8192
0.00.038.877 I print_info: n_expert         = 0
0.00.038.877 I print_info: n_expert_used    = 0
0.00.038.877 I print_info: causal attn      = 1
0.00.038.877 I print_info: pooling type     = 0
0.00.038.877 I print_info: rope type        = 2
0.00.038.877 I print_info: rope scaling     = linear
0.00.038.878 I print_info: freq_base_train  = 10000.0
0.00.038.878 I print_info: freq_scale_train = 1
0.00.038.878 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.878 I print_info: rope_finetuned   = unknown
0.00.038.879 I print_info: ssm_d_conv       = 0
0.00.038.879 I print_info: ssm_d_inner      = 0
0.00.038.879 I print_info: ssm_d_state      = 0
0.00.038.880 I print_info: ssm_dt_rank      = 0
0.00.038.880 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.882 I print_info: model type       = 1.4B
0.00.038.882 I print_info: model params     = 1.41 B
0.00.038.882 I print_info: general.name     = 1.4B
0.00.038.883 I print_info: vocab type       = BPE
0.00.038.883 I print_info: n_vocab          = 50304
0.00.038.883 I print_info: n_merges         = 50009
0.00.038.883 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.883 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.884 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.884 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.884 I print_info: LF token         = 187 ''
0.00.038.884 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.884 I print_info: max token length = 1024
0.00.038.885 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.363.741 I load_tensors: offloading 24 repeating layers to GPU
0.00.363.755 I load_tensors: offloading output layer to GPU
0.00.363.756 I load_tensors: offloaded 25/25 layers to GPU
0.00.363.789 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.363.791 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.365.376 I llama_init_from_model: n_seq_max     = 1
0.00.365.381 I llama_init_from_model: n_ctx         = 2048
0.00.365.382 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.365.382 I llama_init_from_model: n_batch       = 2048
0.00.365.383 I llama_init_from_model: n_ubatch      = 512
0.00.365.383 I llama_init_from_model: flash_attn    = 0
0.00.365.385 I llama_init_from_model: freq_base     = 10000.0
0.00.365.385 I llama_init_from_model: freq_scale    = 1
0.00.365.387 I ggml_metal_init: allocating
0.00.365.466 I ggml_metal_init: found device: Apple M4
0.00.365.479 I ggml_metal_init: picking default device: Apple M4
0.00.367.286 I ggml_metal_init: using embedded metal library
0.00.372.837 I ggml_metal_init: GPU name:   Apple M4
0.00.372.853 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.372.853 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.372.854 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.372.855 I ggml_metal_init: simdgroup reduction   = true
0.00.372.855 I ggml_metal_init: simdgroup matrix mul. = true
0.00.372.855 I ggml_metal_init: has residency sets    = true
0.00.372.856 I ggml_metal_init: has bfloat            = true
0.00.372.856 I ggml_metal_init: use bfloat            = true
0.00.372.860 I ggml_metal_init: hasUnifiedMemory      = true
0.00.372.870 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.393.670 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.450.420 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.450.428 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.450.457 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.455.396 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.455.398 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.455.399 I llama_init_from_model: graph nodes  = 967
0.00.455.399 I llama_init_from_model: graph splits = 2
0.00.455.404 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.455.531 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.455.532 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.513.656 I main: llama threadpool init, n_threads = 4
0.00.513.696 I 
0.00.513.720 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.513.721 I 
0.00.513.894 I sampler seed: 1234
0.00.513.899 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.513.909 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.513.910 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.513.910 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.185.794 I llama_perf_sampler_print:    sampling time =       1.30 ms /    71 runs   (    0.02 ms per token, 54406.13 tokens per second)
0.01.185.794 I llama_perf_context_print:        load time =     503.00 ms
0.01.185.795 I llama_perf_context_print: prompt eval time =      35.82 ms /     7 tokens (    5.12 ms per token,   195.43 tokens per second)
0.01.185.796 I llama_perf_context_print:        eval time =     633.19 ms /    63 runs   (   10.05 ms per token,    99.50 tokens per second)
0.01.185.796 I llama_perf_context_print:       total time =     672.83 ms /    70 tokens
0.01.185.991 I ggml_metal_free: deallocating

real	0m1.204s
user	0m0.111s
sys	0m0.172s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.104 I build: 4668 (0cf86716) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.901 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.202 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.209 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.216 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.216 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.217 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.217 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.217 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.218 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.219 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.219 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.219 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.220 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.220 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.220 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.222 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.222 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.222 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.008 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.028 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.875 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.876 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.877 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.877 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.877 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.878 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.878 I llama_model_loader: - type  f32:  194 tensors
0.00.024.879 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.879 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.879 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.880 I print_info: file format = GGUF V3 (latest)
0.00.024.881 I print_info: file type   = Q2_K - Medium
0.00.024.882 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.032.811 I load: special tokens cache size = 25
0.00.038.996 I load: token to piece cache size = 0.2984 MB
0.00.038.999 I print_info: arch             = gptneox
0.00.038.999 I print_info: vocab_only       = 0
0.00.039.000 I print_info: n_ctx_train      = 2048
0.00.039.000 I print_info: n_embd           = 2048
0.00.039.000 I print_info: n_layer          = 24
0.00.039.004 I print_info: n_head           = 16
0.00.039.005 I print_info: n_head_kv        = 16
0.00.039.006 I print_info: n_rot            = 32
0.00.039.006 I print_info: n_swa            = 0
0.00.039.006 I print_info: n_embd_head_k    = 128
0.00.039.006 I print_info: n_embd_head_v    = 128
0.00.039.007 I print_info: n_gqa            = 1
0.00.039.007 I print_info: n_embd_k_gqa     = 2048
0.00.039.008 I print_info: n_embd_v_gqa     = 2048
0.00.039.009 I print_info: f_norm_eps       = 1.0e-05
0.00.039.009 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.009 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.010 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.010 I print_info: f_logit_scale    = 0.0e+00
0.00.039.011 I print_info: n_ff             = 8192
0.00.039.011 I print_info: n_expert         = 0
0.00.039.011 I print_info: n_expert_used    = 0
0.00.039.011 I print_info: causal attn      = 1
0.00.039.011 I print_info: pooling type     = 0
0.00.039.011 I print_info: rope type        = 2
0.00.039.014 I print_info: rope scaling     = linear
0.00.039.014 I print_info: freq_base_train  = 10000.0
0.00.039.015 I print_info: freq_scale_train = 1
0.00.039.015 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.015 I print_info: rope_finetuned   = unknown
0.00.039.015 I print_info: ssm_d_conv       = 0
0.00.039.015 I print_info: ssm_d_inner      = 0
0.00.039.015 I print_info: ssm_d_state      = 0
0.00.039.015 I print_info: ssm_dt_rank      = 0
0.00.039.016 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.016 I print_info: model type       = 1.4B
0.00.039.016 I print_info: model params     = 1.41 B
0.00.039.016 I print_info: general.name     = 1.4B
0.00.039.017 I print_info: vocab type       = BPE
0.00.039.017 I print_info: n_vocab          = 50304
0.00.039.018 I print_info: n_merges         = 50009
0.00.039.021 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.022 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.022 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.022 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.022 I print_info: LF token         = 187 ''
0.00.039.022 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.023 I print_info: max token length = 1024
0.00.039.023 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.359.479 I load_tensors: offloading 24 repeating layers to GPU
0.00.359.492 I load_tensors: offloading output layer to GPU
0.00.359.493 I load_tensors: offloaded 25/25 layers to GPU
0.00.359.524 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.359.525 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.361.193 I llama_init_from_model: n_seq_max     = 1
0.00.361.198 I llama_init_from_model: n_ctx         = 128
0.00.361.198 I llama_init_from_model: n_ctx_per_seq = 128
0.00.361.199 I llama_init_from_model: n_batch       = 128
0.00.361.199 I llama_init_from_model: n_ubatch      = 128
0.00.361.200 I llama_init_from_model: flash_attn    = 0
0.00.361.202 I llama_init_from_model: freq_base     = 10000.0
0.00.361.202 I llama_init_from_model: freq_scale    = 1
0.00.361.203 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.361.205 I ggml_metal_init: allocating
0.00.361.266 I ggml_metal_init: found device: Apple M4
0.00.361.280 I ggml_metal_init: picking default device: Apple M4
0.00.363.060 I ggml_metal_init: using embedded metal library
0.00.368.454 I ggml_metal_init: GPU name:   Apple M4
0.00.368.465 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.368.466 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.368.467 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.368.468 I ggml_metal_init: simdgroup reduction   = true
0.00.368.468 I ggml_metal_init: simdgroup matrix mul. = true
0.00.368.468 I ggml_metal_init: has residency sets    = true
0.00.368.468 I ggml_metal_init: has bfloat            = true
0.00.368.469 I ggml_metal_init: use bfloat            = true
0.00.368.470 I ggml_metal_init: hasUnifiedMemory      = true
0.00.368.474 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.389.511 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.393.198 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.393.205 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.393.231 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.396.647 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.396.649 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.396.649 I llama_init_from_model: graph nodes  = 967
0.00.396.650 I llama_init_from_model: graph splits = 2
0.00.396.653 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.396.653 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.423.355 I 
0.00.423.437 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.423.444 I perplexity: tokenizing the input ..
0.00.430.049 I perplexity: tokenization took 6.599 ms
0.00.430.055 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.562.919 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.564.219 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.564.250 I llama_perf_context_print:        load time =     414.44 ms
0.00.564.251 I llama_perf_context_print: prompt eval time =     132.46 ms /   128 tokens (    1.03 ms per token,   966.37 tokens per second)
0.00.564.252 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.564.252 I llama_perf_context_print:       total time =     140.90 ms /   129 tokens
0.00.564.617 I ggml_metal_free: deallocating

real	0m0.578s
user	0m0.080s
sys	0m0.094s
```
- q3_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.054 I build: 4668 (0cf86716) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.089 I main: llama backend init
0.00.000.091 I main: load the model and apply lora adapter, if any
0.00.008.733 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.305 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.015.310 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.312 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.312 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.313 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.313 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.313 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.314 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.314 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.315 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.315 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.315 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.316 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.316 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.318 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.318 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.318 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.022 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.069 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.797 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.798 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.798 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.798 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.799 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.799 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.023.799 I llama_model_loader: - type  f32:  194 tensors
0.00.023.800 I llama_model_loader: - type q3_K:   25 tensors
0.00.023.800 I llama_model_loader: - type q4_K:   71 tensors
0.00.023.800 I llama_model_loader: - type q5_K:    1 tensors
0.00.023.801 I llama_model_loader: - type q6_K:    1 tensors
0.00.023.801 I print_info: file format = GGUF V3 (latest)
0.00.023.802 I print_info: file type   = Q3_K - Medium
0.00.023.803 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.031.893 I load: special tokens cache size = 25
0.00.038.016 I load: token to piece cache size = 0.2984 MB
0.00.038.018 I print_info: arch             = gptneox
0.00.038.019 I print_info: vocab_only       = 0
0.00.038.019 I print_info: n_ctx_train      = 2048
0.00.038.019 I print_info: n_embd           = 2048
0.00.038.019 I print_info: n_layer          = 24
0.00.038.022 I print_info: n_head           = 16
0.00.038.023 I print_info: n_head_kv        = 16
0.00.038.023 I print_info: n_rot            = 32
0.00.038.023 I print_info: n_swa            = 0
0.00.038.023 I print_info: n_embd_head_k    = 128
0.00.038.026 I print_info: n_embd_head_v    = 128
0.00.038.026 I print_info: n_gqa            = 1
0.00.038.027 I print_info: n_embd_k_gqa     = 2048
0.00.038.028 I print_info: n_embd_v_gqa     = 2048
0.00.038.028 I print_info: f_norm_eps       = 1.0e-05
0.00.038.029 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.029 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.029 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.030 I print_info: f_logit_scale    = 0.0e+00
0.00.038.030 I print_info: n_ff             = 8192
0.00.038.030 I print_info: n_expert         = 0
0.00.038.031 I print_info: n_expert_used    = 0
0.00.038.031 I print_info: causal attn      = 1
0.00.038.031 I print_info: pooling type     = 0
0.00.038.031 I print_info: rope type        = 2
0.00.038.031 I print_info: rope scaling     = linear
0.00.038.032 I print_info: freq_base_train  = 10000.0
0.00.038.032 I print_info: freq_scale_train = 1
0.00.038.032 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.032 I print_info: rope_finetuned   = unknown
0.00.038.032 I print_info: ssm_d_conv       = 0
0.00.038.032 I print_info: ssm_d_inner      = 0
0.00.038.033 I print_info: ssm_d_state      = 0
0.00.038.033 I print_info: ssm_dt_rank      = 0
0.00.038.034 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.034 I print_info: model type       = 1.4B
0.00.038.034 I print_info: model params     = 1.41 B
0.00.038.035 I print_info: general.name     = 1.4B
0.00.038.035 I print_info: vocab type       = BPE
0.00.038.040 I print_info: n_vocab          = 50304
0.00.038.041 I print_info: n_merges         = 50009
0.00.038.042 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.042 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.042 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.042 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.042 I print_info: LF token         = 187 ''
0.00.038.042 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.043 I print_info: max token length = 1024
0.00.038.044 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.457.897 I load_tensors: offloading 24 repeating layers to GPU
0.00.457.912 I load_tensors: offloading output layer to GPU
0.00.457.913 I load_tensors: offloaded 25/25 layers to GPU
0.00.457.942 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.457.944 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.459.370 I llama_init_from_model: n_seq_max     = 1
0.00.459.373 I llama_init_from_model: n_ctx         = 2048
0.00.459.374 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.459.375 I llama_init_from_model: n_batch       = 2048
0.00.459.375 I llama_init_from_model: n_ubatch      = 512
0.00.459.375 I llama_init_from_model: flash_attn    = 0
0.00.459.377 I llama_init_from_model: freq_base     = 10000.0
0.00.459.378 I llama_init_from_model: freq_scale    = 1
0.00.459.380 I ggml_metal_init: allocating
0.00.459.458 I ggml_metal_init: found device: Apple M4
0.00.459.471 I ggml_metal_init: picking default device: Apple M4
0.00.461.283 I ggml_metal_init: using embedded metal library
0.00.467.310 I ggml_metal_init: GPU name:   Apple M4
0.00.467.323 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.467.324 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.467.325 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.467.325 I ggml_metal_init: simdgroup reduction   = true
0.00.467.326 I ggml_metal_init: simdgroup matrix mul. = true
0.00.467.326 I ggml_metal_init: has residency sets    = true
0.00.467.326 I ggml_metal_init: has bfloat            = true
0.00.467.326 I ggml_metal_init: use bfloat            = true
0.00.467.342 I ggml_metal_init: hasUnifiedMemory      = true
0.00.467.347 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.488.268 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.545.907 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.545.914 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.545.936 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.550.627 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.550.629 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.550.629 I llama_init_from_model: graph nodes  = 967
0.00.550.630 I llama_init_from_model: graph splits = 2
0.00.550.635 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.550.759 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.550.760 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.608.349 I main: llama threadpool init, n_threads = 4
0.00.608.393 I 
0.00.608.415 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.608.416 I 
0.00.608.592 I sampler seed: 1234
0.00.608.597 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.608.608 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.608.608 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.608.608 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.354.279 I llama_perf_sampler_print:    sampling time =       1.31 ms /    71 runs   (    0.02 ms per token, 54033.49 tokens per second)
0.01.354.279 I llama_perf_context_print:        load time =     598.93 ms
0.01.354.280 I llama_perf_context_print: prompt eval time =      50.12 ms /     7 tokens (    7.16 ms per token,   139.65 tokens per second)
0.01.354.281 I llama_perf_context_print:        eval time =     692.65 ms /    63 runs   (   10.99 ms per token,    90.95 tokens per second)
0.01.354.282 I llama_perf_context_print:       total time =     746.62 ms /    70 tokens
0.01.354.470 I ggml_metal_free: deallocating

real	0m1.370s
user	0m0.110s
sys	0m0.198s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.089 I build: 4668 (0cf86716) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.756 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.324 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.330 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.332 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.333 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.333 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.333 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.334 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.334 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.335 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.335 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.336 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.336 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.336 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.337 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.339 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.339 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.339 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.051 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.091 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.876 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.877 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.878 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.878 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.878 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.879 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.879 I llama_model_loader: - type  f32:  194 tensors
0.00.024.880 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.880 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.880 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.880 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.883 I print_info: file format = GGUF V3 (latest)
0.00.024.884 I print_info: file type   = Q3_K - Medium
0.00.024.885 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.032.845 I load: special tokens cache size = 25
0.00.038.948 I load: token to piece cache size = 0.2984 MB
0.00.038.951 I print_info: arch             = gptneox
0.00.038.951 I print_info: vocab_only       = 0
0.00.038.952 I print_info: n_ctx_train      = 2048
0.00.038.952 I print_info: n_embd           = 2048
0.00.038.952 I print_info: n_layer          = 24
0.00.038.956 I print_info: n_head           = 16
0.00.038.957 I print_info: n_head_kv        = 16
0.00.038.957 I print_info: n_rot            = 32
0.00.038.957 I print_info: n_swa            = 0
0.00.038.958 I print_info: n_embd_head_k    = 128
0.00.038.958 I print_info: n_embd_head_v    = 128
0.00.038.959 I print_info: n_gqa            = 1
0.00.038.960 I print_info: n_embd_k_gqa     = 2048
0.00.038.960 I print_info: n_embd_v_gqa     = 2048
0.00.038.961 I print_info: f_norm_eps       = 1.0e-05
0.00.038.961 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.962 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.962 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.962 I print_info: f_logit_scale    = 0.0e+00
0.00.038.963 I print_info: n_ff             = 8192
0.00.038.963 I print_info: n_expert         = 0
0.00.038.963 I print_info: n_expert_used    = 0
0.00.038.963 I print_info: causal attn      = 1
0.00.038.963 I print_info: pooling type     = 0
0.00.038.963 I print_info: rope type        = 2
0.00.038.964 I print_info: rope scaling     = linear
0.00.038.965 I print_info: freq_base_train  = 10000.0
0.00.038.965 I print_info: freq_scale_train = 1
0.00.038.965 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.965 I print_info: rope_finetuned   = unknown
0.00.038.965 I print_info: ssm_d_conv       = 0
0.00.038.966 I print_info: ssm_d_inner      = 0
0.00.038.966 I print_info: ssm_d_state      = 0
0.00.038.968 I print_info: ssm_dt_rank      = 0
0.00.038.968 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.968 I print_info: model type       = 1.4B
0.00.038.968 I print_info: model params     = 1.41 B
0.00.038.968 I print_info: general.name     = 1.4B
0.00.038.969 I print_info: vocab type       = BPE
0.00.038.969 I print_info: n_vocab          = 50304
0.00.038.969 I print_info: n_merges         = 50009
0.00.038.970 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.970 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.970 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.970 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.970 I print_info: LF token         = 187 ''
0.00.038.971 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.971 I print_info: max token length = 1024
0.00.038.975 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.462.766 I load_tensors: offloading 24 repeating layers to GPU
0.00.462.775 I load_tensors: offloading output layer to GPU
0.00.462.776 I load_tensors: offloaded 25/25 layers to GPU
0.00.462.799 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.462.800 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.464.259 I llama_init_from_model: n_seq_max     = 1
0.00.464.263 I llama_init_from_model: n_ctx         = 128
0.00.464.264 I llama_init_from_model: n_ctx_per_seq = 128
0.00.464.264 I llama_init_from_model: n_batch       = 128
0.00.464.264 I llama_init_from_model: n_ubatch      = 128
0.00.464.265 I llama_init_from_model: flash_attn    = 0
0.00.464.267 I llama_init_from_model: freq_base     = 10000.0
0.00.464.267 I llama_init_from_model: freq_scale    = 1
0.00.464.268 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.464.278 I ggml_metal_init: allocating
0.00.464.352 I ggml_metal_init: found device: Apple M4
0.00.464.365 I ggml_metal_init: picking default device: Apple M4
0.00.466.014 I ggml_metal_init: using embedded metal library
0.00.471.625 I ggml_metal_init: GPU name:   Apple M4
0.00.471.630 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.471.631 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.471.633 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.471.637 I ggml_metal_init: simdgroup reduction   = true
0.00.471.638 I ggml_metal_init: simdgroup matrix mul. = true
0.00.471.638 I ggml_metal_init: has residency sets    = true
0.00.471.638 I ggml_metal_init: has bfloat            = true
0.00.471.638 I ggml_metal_init: use bfloat            = true
0.00.471.639 I ggml_metal_init: hasUnifiedMemory      = true
0.00.471.644 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.490.931 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.494.675 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.494.681 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.494.715 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.498.221 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.498.225 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.498.225 I llama_init_from_model: graph nodes  = 967
0.00.498.225 I llama_init_from_model: graph splits = 2
0.00.498.229 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.498.229 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.525.757 I 
0.00.525.825 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.525.832 I perplexity: tokenizing the input ..
0.00.531.724 I perplexity: tokenization took 5.888 ms
0.00.531.731 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.663.048 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.664.475 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.664.502 I llama_perf_context_print:        load time =     516.99 ms
0.00.664.503 I llama_perf_context_print: prompt eval time =     131.09 ms /   128 tokens (    1.02 ms per token,   976.42 tokens per second)
0.00.664.504 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.664.504 I llama_perf_context_print:       total time =     138.75 ms /   129 tokens
0.00.664.903 I ggml_metal_free: deallocating

real	0m0.678s
user	0m0.076s
sys	0m0.127s
```
- q4_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.051 I build: 4668 (0cf86716) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.083 I main: llama backend init
0.00.000.085 I main: load the model and apply lora adapter, if any
0.00.011.010 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.372 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.017.377 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.379 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.379 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.380 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.380 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.380 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.381 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.382 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.382 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.382 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.383 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.383 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.383 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.385 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.386 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.386 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.121 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.103 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.842 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.843 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.844 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.844 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.844 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.845 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.845 I llama_model_loader: - type  f32:  194 tensors
0.00.025.845 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.845 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.846 I llama_model_loader: - type q6_K:   13 tensors
0.00.025.846 I print_info: file format = GGUF V3 (latest)
0.00.025.846 I print_info: file type   = Q4_K - Medium
0.00.025.847 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.033.591 I load: special tokens cache size = 25
0.00.039.674 I load: token to piece cache size = 0.2984 MB
0.00.039.676 I print_info: arch             = gptneox
0.00.039.677 I print_info: vocab_only       = 0
0.00.039.677 I print_info: n_ctx_train      = 2048
0.00.039.677 I print_info: n_embd           = 2048
0.00.039.677 I print_info: n_layer          = 24
0.00.039.680 I print_info: n_head           = 16
0.00.039.681 I print_info: n_head_kv        = 16
0.00.039.681 I print_info: n_rot            = 32
0.00.039.681 I print_info: n_swa            = 0
0.00.039.681 I print_info: n_embd_head_k    = 128
0.00.039.681 I print_info: n_embd_head_v    = 128
0.00.039.684 I print_info: n_gqa            = 1
0.00.039.685 I print_info: n_embd_k_gqa     = 2048
0.00.039.685 I print_info: n_embd_v_gqa     = 2048
0.00.039.690 I print_info: f_norm_eps       = 1.0e-05
0.00.039.691 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.691 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.691 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.691 I print_info: f_logit_scale    = 0.0e+00
0.00.039.692 I print_info: n_ff             = 8192
0.00.039.692 I print_info: n_expert         = 0
0.00.039.692 I print_info: n_expert_used    = 0
0.00.039.692 I print_info: causal attn      = 1
0.00.039.693 I print_info: pooling type     = 0
0.00.039.693 I print_info: rope type        = 2
0.00.039.693 I print_info: rope scaling     = linear
0.00.039.695 I print_info: freq_base_train  = 10000.0
0.00.039.695 I print_info: freq_scale_train = 1
0.00.039.695 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.696 I print_info: rope_finetuned   = unknown
0.00.039.696 I print_info: ssm_d_conv       = 0
0.00.039.696 I print_info: ssm_d_inner      = 0
0.00.039.696 I print_info: ssm_d_state      = 0
0.00.039.696 I print_info: ssm_dt_rank      = 0
0.00.039.696 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.698 I print_info: model type       = 1.4B
0.00.039.698 I print_info: model params     = 1.41 B
0.00.039.698 I print_info: general.name     = 1.4B
0.00.039.698 I print_info: vocab type       = BPE
0.00.039.699 I print_info: n_vocab          = 50304
0.00.039.699 I print_info: n_merges         = 50009
0.00.039.699 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.699 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.699 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.700 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.703 I print_info: LF token         = 187 ''
0.00.039.703 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.703 I print_info: max token length = 1024
0.00.039.704 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.515.801 I load_tensors: offloading 24 repeating layers to GPU
0.00.515.812 I load_tensors: offloading output layer to GPU
0.00.515.813 I load_tensors: offloaded 25/25 layers to GPU
0.00.515.846 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.515.848 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.517.440 I llama_init_from_model: n_seq_max     = 1
0.00.517.446 I llama_init_from_model: n_ctx         = 2048
0.00.517.446 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.517.447 I llama_init_from_model: n_batch       = 2048
0.00.517.447 I llama_init_from_model: n_ubatch      = 512
0.00.517.448 I llama_init_from_model: flash_attn    = 0
0.00.517.449 I llama_init_from_model: freq_base     = 10000.0
0.00.517.449 I llama_init_from_model: freq_scale    = 1
0.00.517.451 I ggml_metal_init: allocating
0.00.517.532 I ggml_metal_init: found device: Apple M4
0.00.517.547 I ggml_metal_init: picking default device: Apple M4
0.00.519.349 I ggml_metal_init: using embedded metal library
0.00.526.256 I ggml_metal_init: GPU name:   Apple M4
0.00.526.264 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.526.265 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.526.266 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.526.267 I ggml_metal_init: simdgroup reduction   = true
0.00.526.267 I ggml_metal_init: simdgroup matrix mul. = true
0.00.526.268 I ggml_metal_init: has residency sets    = true
0.00.526.268 I ggml_metal_init: has bfloat            = true
0.00.526.268 I ggml_metal_init: use bfloat            = true
0.00.526.269 I ggml_metal_init: hasUnifiedMemory      = true
0.00.526.276 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.544.297 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.596.465 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.596.471 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.596.495 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.601.667 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.601.669 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.601.670 I llama_init_from_model: graph nodes  = 967
0.00.601.670 I llama_init_from_model: graph splits = 2
0.00.601.675 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.601.788 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.601.788 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.659.386 I main: llama threadpool init, n_threads = 4
0.00.659.431 I 
0.00.659.452 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.659.453 I 
0.00.659.630 I sampler seed: 1234
0.00.659.635 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.659.676 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.659.679 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.659.679 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.408.353 I llama_perf_sampler_print:    sampling time =       1.48 ms /    71 runs   (    0.02 ms per token, 47940.58 tokens per second)
0.01.408.354 I llama_perf_context_print:        load time =     647.67 ms
0.01.408.356 I llama_perf_context_print: prompt eval time =      47.22 ms /     7 tokens (    6.75 ms per token,   148.23 tokens per second)
0.01.408.357 I llama_perf_context_print:        eval time =     698.63 ms /    63 runs   (   11.09 ms per token,    90.18 tokens per second)
0.01.408.357 I llama_perf_context_print:       total time =     749.67 ms /    70 tokens
0.01.408.647 I ggml_metal_free: deallocating

real	0m1.424s
user	0m0.109s
sys	0m0.194s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.101 I build: 4668 (0cf86716) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.885 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.780 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.787 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.789 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.789 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.790 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.790 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.790 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.791 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.792 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.792 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.793 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.793 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.793 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.794 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.796 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.796 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.796 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.633 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.706 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.630 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.632 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.632 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.633 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.633 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.633 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.634 I llama_model_loader: - type  f32:  194 tensors
0.00.025.634 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.635 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.635 I llama_model_loader: - type q6_K:   13 tensors
0.00.025.635 I print_info: file format = GGUF V3 (latest)
0.00.025.638 I print_info: file type   = Q4_K - Medium
0.00.025.639 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.034.067 I load: special tokens cache size = 25
0.00.039.999 I load: token to piece cache size = 0.2984 MB
0.00.040.003 I print_info: arch             = gptneox
0.00.040.003 I print_info: vocab_only       = 0
0.00.040.003 I print_info: n_ctx_train      = 2048
0.00.040.004 I print_info: n_embd           = 2048
0.00.040.004 I print_info: n_layer          = 24
0.00.040.008 I print_info: n_head           = 16
0.00.040.009 I print_info: n_head_kv        = 16
0.00.040.009 I print_info: n_rot            = 32
0.00.040.011 I print_info: n_swa            = 0
0.00.040.011 I print_info: n_embd_head_k    = 128
0.00.040.012 I print_info: n_embd_head_v    = 128
0.00.040.012 I print_info: n_gqa            = 1
0.00.040.013 I print_info: n_embd_k_gqa     = 2048
0.00.040.014 I print_info: n_embd_v_gqa     = 2048
0.00.040.015 I print_info: f_norm_eps       = 1.0e-05
0.00.040.015 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.015 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.015 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.016 I print_info: f_logit_scale    = 0.0e+00
0.00.040.016 I print_info: n_ff             = 8192
0.00.040.016 I print_info: n_expert         = 0
0.00.040.019 I print_info: n_expert_used    = 0
0.00.040.019 I print_info: causal attn      = 1
0.00.040.019 I print_info: pooling type     = 0
0.00.040.019 I print_info: rope type        = 2
0.00.040.020 I print_info: rope scaling     = linear
0.00.040.020 I print_info: freq_base_train  = 10000.0
0.00.040.020 I print_info: freq_scale_train = 1
0.00.040.021 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.021 I print_info: rope_finetuned   = unknown
0.00.040.021 I print_info: ssm_d_conv       = 0
0.00.040.021 I print_info: ssm_d_inner      = 0
0.00.040.021 I print_info: ssm_d_state      = 0
0.00.040.021 I print_info: ssm_dt_rank      = 0
0.00.040.022 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.022 I print_info: model type       = 1.4B
0.00.040.022 I print_info: model params     = 1.41 B
0.00.040.022 I print_info: general.name     = 1.4B
0.00.040.023 I print_info: vocab type       = BPE
0.00.040.023 I print_info: n_vocab          = 50304
0.00.040.023 I print_info: n_merges         = 50009
0.00.040.023 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.024 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.024 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.028 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.028 I print_info: LF token         = 187 ''
0.00.040.028 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.029 I print_info: max token length = 1024
0.00.040.029 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.521.246 I load_tensors: offloading 24 repeating layers to GPU
0.00.521.262 I load_tensors: offloading output layer to GPU
0.00.521.263 I load_tensors: offloaded 25/25 layers to GPU
0.00.521.297 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.521.298 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.522.954 I llama_init_from_model: n_seq_max     = 1
0.00.522.956 I llama_init_from_model: n_ctx         = 128
0.00.522.957 I llama_init_from_model: n_ctx_per_seq = 128
0.00.522.957 I llama_init_from_model: n_batch       = 128
0.00.522.958 I llama_init_from_model: n_ubatch      = 128
0.00.522.958 I llama_init_from_model: flash_attn    = 0
0.00.522.961 I llama_init_from_model: freq_base     = 10000.0
0.00.522.962 I llama_init_from_model: freq_scale    = 1
0.00.522.962 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.522.964 I ggml_metal_init: allocating
0.00.523.064 I ggml_metal_init: found device: Apple M4
0.00.523.078 I ggml_metal_init: picking default device: Apple M4
0.00.524.934 I ggml_metal_init: using embedded metal library
0.00.531.683 I ggml_metal_init: GPU name:   Apple M4
0.00.531.688 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.531.689 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.531.690 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.531.690 I ggml_metal_init: simdgroup reduction   = true
0.00.531.691 I ggml_metal_init: simdgroup matrix mul. = true
0.00.531.691 I ggml_metal_init: has residency sets    = true
0.00.531.691 I ggml_metal_init: has bfloat            = true
0.00.531.691 I ggml_metal_init: use bfloat            = true
0.00.531.692 I ggml_metal_init: hasUnifiedMemory      = true
0.00.531.694 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.550.067 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.553.575 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.553.578 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.553.610 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.556.749 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.556.750 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.556.751 I llama_init_from_model: graph nodes  = 967
0.00.556.751 I llama_init_from_model: graph splits = 2
0.00.556.755 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.556.755 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.587.833 I 
0.00.587.919 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.587.928 I perplexity: tokenizing the input ..
0.00.595.015 I perplexity: tokenization took 7.08 ms
0.00.595.022 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.739.103 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.740.461 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.740.480 I llama_perf_context_print:        load time =     577.94 ms
0.00.740.481 I llama_perf_context_print: prompt eval time =     143.13 ms /   128 tokens (    1.12 ms per token,   894.27 tokens per second)
0.00.740.481 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.740.482 I llama_perf_context_print:       total time =     152.65 ms /   129 tokens
0.00.740.847 I ggml_metal_free: deallocating

real	0m0.756s
user	0m0.080s
sys	0m0.126s
```
- q5_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.058 I build: 4668 (0cf86716) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.104 I main: llama backend init
0.00.000.106 I main: load the model and apply lora adapter, if any
0.00.009.603 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.313 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.017.319 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.325 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.326 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.326 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.327 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.327 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.328 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.328 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.328 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.329 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.329 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.329 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.330 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.331 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.331 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.331 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.361 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.480 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.302 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.303 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.304 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.304 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.305 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.305 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.026.305 I llama_model_loader: - type  f32:  194 tensors
0.00.026.306 I llama_model_loader: - type q5_K:   61 tensors
0.00.026.306 I llama_model_loader: - type q6_K:   37 tensors
0.00.026.307 I print_info: file format = GGUF V3 (latest)
0.00.026.307 I print_info: file type   = Q5_K - Medium
0.00.026.308 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.034.304 I load: special tokens cache size = 25
0.00.040.586 I load: token to piece cache size = 0.2984 MB
0.00.040.598 I print_info: arch             = gptneox
0.00.040.598 I print_info: vocab_only       = 0
0.00.040.599 I print_info: n_ctx_train      = 2048
0.00.040.599 I print_info: n_embd           = 2048
0.00.040.599 I print_info: n_layer          = 24
0.00.040.603 I print_info: n_head           = 16
0.00.040.604 I print_info: n_head_kv        = 16
0.00.040.604 I print_info: n_rot            = 32
0.00.040.604 I print_info: n_swa            = 0
0.00.040.605 I print_info: n_embd_head_k    = 128
0.00.040.605 I print_info: n_embd_head_v    = 128
0.00.040.606 I print_info: n_gqa            = 1
0.00.040.606 I print_info: n_embd_k_gqa     = 2048
0.00.040.607 I print_info: n_embd_v_gqa     = 2048
0.00.040.608 I print_info: f_norm_eps       = 1.0e-05
0.00.040.608 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.608 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.608 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.609 I print_info: f_logit_scale    = 0.0e+00
0.00.040.609 I print_info: n_ff             = 8192
0.00.040.609 I print_info: n_expert         = 0
0.00.040.610 I print_info: n_expert_used    = 0
0.00.040.610 I print_info: causal attn      = 1
0.00.040.610 I print_info: pooling type     = 0
0.00.040.610 I print_info: rope type        = 2
0.00.040.610 I print_info: rope scaling     = linear
0.00.040.611 I print_info: freq_base_train  = 10000.0
0.00.040.611 I print_info: freq_scale_train = 1
0.00.040.613 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.613 I print_info: rope_finetuned   = unknown
0.00.040.613 I print_info: ssm_d_conv       = 0
0.00.040.613 I print_info: ssm_d_inner      = 0
0.00.040.613 I print_info: ssm_d_state      = 0
0.00.040.613 I print_info: ssm_dt_rank      = 0
0.00.040.613 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.613 I print_info: model type       = 1.4B
0.00.040.614 I print_info: model params     = 1.41 B
0.00.040.614 I print_info: general.name     = 1.4B
0.00.040.614 I print_info: vocab type       = BPE
0.00.040.615 I print_info: n_vocab          = 50304
0.00.040.615 I print_info: n_merges         = 50009
0.00.040.615 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.615 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.616 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.616 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.616 I print_info: LF token         = 187 ''
0.00.040.616 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.616 I print_info: max token length = 1024
0.00.040.617 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.609.051 I load_tensors: offloading 24 repeating layers to GPU
0.00.609.063 I load_tensors: offloading output layer to GPU
0.00.609.064 I load_tensors: offloaded 25/25 layers to GPU
0.00.609.097 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.609.098 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.610.204 I llama_init_from_model: n_seq_max     = 1
0.00.610.206 I llama_init_from_model: n_ctx         = 2048
0.00.610.207 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.610.207 I llama_init_from_model: n_batch       = 2048
0.00.610.208 I llama_init_from_model: n_ubatch      = 512
0.00.610.208 I llama_init_from_model: flash_attn    = 0
0.00.610.209 I llama_init_from_model: freq_base     = 10000.0
0.00.610.210 I llama_init_from_model: freq_scale    = 1
0.00.610.212 I ggml_metal_init: allocating
0.00.610.251 I ggml_metal_init: found device: Apple M4
0.00.610.261 I ggml_metal_init: picking default device: Apple M4
0.00.611.772 I ggml_metal_init: using embedded metal library
0.00.618.111 I ggml_metal_init: GPU name:   Apple M4
0.00.618.115 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.618.116 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.618.117 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.618.118 I ggml_metal_init: simdgroup reduction   = true
0.00.618.118 I ggml_metal_init: simdgroup matrix mul. = true
0.00.618.118 I ggml_metal_init: has residency sets    = true
0.00.618.119 I ggml_metal_init: has bfloat            = true
0.00.618.119 I ggml_metal_init: use bfloat            = true
0.00.618.120 I ggml_metal_init: hasUnifiedMemory      = true
0.00.618.121 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.635.787 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.684.261 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.684.268 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.684.292 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.688.442 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.688.444 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.688.444 I llama_init_from_model: graph nodes  = 967
0.00.688.445 I llama_init_from_model: graph splits = 2
0.00.688.449 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.688.580 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.688.581 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.752.988 I main: llama threadpool init, n_threads = 4
0.00.753.033 I 
0.00.753.055 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.753.055 I 
0.00.753.236 I sampler seed: 1234
0.00.753.240 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.753.260 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.753.260 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.753.260 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.601.277 I llama_perf_sampler_print:    sampling time =       1.40 ms /    71 runs   (    0.02 ms per token, 50605.84 tokens per second)
0.01.601.279 I llama_perf_context_print:        load time =     742.66 ms
0.01.601.280 I llama_perf_context_print: prompt eval time =      51.38 ms /     7 tokens (    7.34 ms per token,   136.25 tokens per second)
0.01.601.280 I llama_perf_context_print:        eval time =     794.16 ms /    63 runs   (   12.61 ms per token,    79.33 tokens per second)
0.01.601.281 I llama_perf_context_print:       total time =     849.02 ms /    70 tokens
0.01.601.501 I ggml_metal_free: deallocating

real	0m1.619s
user	0m0.110s
sys	0m0.208s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.098 I build: 4668 (0cf86716) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.846 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.924 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.930 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.931 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.939 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.939 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.939 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.940 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.942 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.942 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.942 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.943 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.943 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.943 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.944 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.945 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.945 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.946 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.697 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.715 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.419 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.421 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.421 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.421 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.421 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.422 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.422 I llama_model_loader: - type  f32:  194 tensors
0.00.024.423 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.423 I llama_model_loader: - type q6_K:   37 tensors
0.00.024.424 I print_info: file format = GGUF V3 (latest)
0.00.024.424 I print_info: file type   = Q5_K - Medium
0.00.024.425 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.032.438 I load: special tokens cache size = 25
0.00.038.476 I load: token to piece cache size = 0.2984 MB
0.00.038.479 I print_info: arch             = gptneox
0.00.038.480 I print_info: vocab_only       = 0
0.00.038.480 I print_info: n_ctx_train      = 2048
0.00.038.480 I print_info: n_embd           = 2048
0.00.038.480 I print_info: n_layer          = 24
0.00.038.483 I print_info: n_head           = 16
0.00.038.484 I print_info: n_head_kv        = 16
0.00.038.484 I print_info: n_rot            = 32
0.00.038.486 I print_info: n_swa            = 0
0.00.038.486 I print_info: n_embd_head_k    = 128
0.00.038.486 I print_info: n_embd_head_v    = 128
0.00.038.487 I print_info: n_gqa            = 1
0.00.038.488 I print_info: n_embd_k_gqa     = 2048
0.00.038.489 I print_info: n_embd_v_gqa     = 2048
0.00.038.489 I print_info: f_norm_eps       = 1.0e-05
0.00.038.489 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.490 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.490 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.490 I print_info: f_logit_scale    = 0.0e+00
0.00.038.491 I print_info: n_ff             = 8192
0.00.038.491 I print_info: n_expert         = 0
0.00.038.491 I print_info: n_expert_used    = 0
0.00.038.491 I print_info: causal attn      = 1
0.00.038.491 I print_info: pooling type     = 0
0.00.038.492 I print_info: rope type        = 2
0.00.038.496 I print_info: rope scaling     = linear
0.00.038.496 I print_info: freq_base_train  = 10000.0
0.00.038.497 I print_info: freq_scale_train = 1
0.00.038.497 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.497 I print_info: rope_finetuned   = unknown
0.00.038.497 I print_info: ssm_d_conv       = 0
0.00.038.497 I print_info: ssm_d_inner      = 0
0.00.038.498 I print_info: ssm_d_state      = 0
0.00.038.498 I print_info: ssm_dt_rank      = 0
0.00.038.498 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.499 I print_info: model type       = 1.4B
0.00.038.500 I print_info: model params     = 1.41 B
0.00.038.500 I print_info: general.name     = 1.4B
0.00.038.501 I print_info: vocab type       = BPE
0.00.038.501 I print_info: n_vocab          = 50304
0.00.038.501 I print_info: n_merges         = 50009
0.00.038.501 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.501 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.501 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.502 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.502 I print_info: LF token         = 187 ''
0.00.038.502 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.502 I print_info: max token length = 1024
0.00.038.503 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.606.667 I load_tensors: offloading 24 repeating layers to GPU
0.00.606.684 I load_tensors: offloading output layer to GPU
0.00.606.685 I load_tensors: offloaded 25/25 layers to GPU
0.00.606.717 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.606.718 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.608.451 I llama_init_from_model: n_seq_max     = 1
0.00.608.455 I llama_init_from_model: n_ctx         = 128
0.00.608.455 I llama_init_from_model: n_ctx_per_seq = 128
0.00.608.456 I llama_init_from_model: n_batch       = 128
0.00.608.456 I llama_init_from_model: n_ubatch      = 128
0.00.608.457 I llama_init_from_model: flash_attn    = 0
0.00.608.460 I llama_init_from_model: freq_base     = 10000.0
0.00.608.460 I llama_init_from_model: freq_scale    = 1
0.00.608.461 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.608.463 I ggml_metal_init: allocating
0.00.608.541 I ggml_metal_init: found device: Apple M4
0.00.608.555 I ggml_metal_init: picking default device: Apple M4
0.00.610.418 I ggml_metal_init: using embedded metal library
0.00.616.938 I ggml_metal_init: GPU name:   Apple M4
0.00.616.941 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.616.942 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.616.943 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.616.947 I ggml_metal_init: simdgroup reduction   = true
0.00.616.947 I ggml_metal_init: simdgroup matrix mul. = true
0.00.616.947 I ggml_metal_init: has residency sets    = true
0.00.616.948 I ggml_metal_init: has bfloat            = true
0.00.616.948 I ggml_metal_init: use bfloat            = true
0.00.616.949 I ggml_metal_init: hasUnifiedMemory      = true
0.00.616.954 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.633.820 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.637.368 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.637.372 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.637.399 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.640.471 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.640.472 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.640.473 I llama_init_from_model: graph nodes  = 967
0.00.640.473 I llama_init_from_model: graph splits = 2
0.00.640.477 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.640.477 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.673.731 I 
0.00.673.796 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.673.803 I perplexity: tokenizing the input ..
0.00.678.900 I perplexity: tokenization took 5.093 ms
0.00.678.908 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.819.239 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.820.661 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.820.681 I llama_perf_context_print:        load time =     664.88 ms
0.00.820.682 I llama_perf_context_print: prompt eval time =     139.78 ms /   128 tokens (    1.09 ms per token,   915.69 tokens per second)
0.00.820.683 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.820.684 I llama_perf_context_print:       total time =     146.95 ms /   129 tokens
0.00.821.035 I ggml_metal_free: deallocating

real	0m0.835s
user	0m0.076s
sys	0m0.144s
```
- q6_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.058 I build: 4668 (0cf86716) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.092 I main: llama backend init
0.00.000.095 I main: load the model and apply lora adapter, if any
0.00.009.859 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.710 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.017.717 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.719 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.719 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.724 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.725 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.725 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.727 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.727 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.728 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.728 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.729 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.729 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.729 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.732 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.733 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.733 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.459 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.494 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.380 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.382 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.382 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.382 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.382 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.383 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.026.383 I llama_model_loader: - type  f32:  194 tensors
0.00.026.384 I llama_model_loader: - type q6_K:   98 tensors
0.00.026.384 I print_info: file format = GGUF V3 (latest)
0.00.026.385 I print_info: file type   = Q6_K
0.00.026.386 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.034.406 I load: special tokens cache size = 25
0.00.040.629 I load: token to piece cache size = 0.2984 MB
0.00.040.633 I print_info: arch             = gptneox
0.00.040.633 I print_info: vocab_only       = 0
0.00.040.634 I print_info: n_ctx_train      = 2048
0.00.040.634 I print_info: n_embd           = 2048
0.00.040.634 I print_info: n_layer          = 24
0.00.040.638 I print_info: n_head           = 16
0.00.040.639 I print_info: n_head_kv        = 16
0.00.040.640 I print_info: n_rot            = 32
0.00.040.640 I print_info: n_swa            = 0
0.00.040.640 I print_info: n_embd_head_k    = 128
0.00.040.640 I print_info: n_embd_head_v    = 128
0.00.040.641 I print_info: n_gqa            = 1
0.00.040.641 I print_info: n_embd_k_gqa     = 2048
0.00.040.645 I print_info: n_embd_v_gqa     = 2048
0.00.040.646 I print_info: f_norm_eps       = 1.0e-05
0.00.040.646 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.646 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.646 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.648 I print_info: f_logit_scale    = 0.0e+00
0.00.040.650 I print_info: n_ff             = 8192
0.00.040.650 I print_info: n_expert         = 0
0.00.040.650 I print_info: n_expert_used    = 0
0.00.040.650 I print_info: causal attn      = 1
0.00.040.650 I print_info: pooling type     = 0
0.00.040.650 I print_info: rope type        = 2
0.00.040.650 I print_info: rope scaling     = linear
0.00.040.651 I print_info: freq_base_train  = 10000.0
0.00.040.651 I print_info: freq_scale_train = 1
0.00.040.651 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.651 I print_info: rope_finetuned   = unknown
0.00.040.652 I print_info: ssm_d_conv       = 0
0.00.040.652 I print_info: ssm_d_inner      = 0
0.00.040.652 I print_info: ssm_d_state      = 0
0.00.040.652 I print_info: ssm_dt_rank      = 0
0.00.040.652 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.652 I print_info: model type       = 1.4B
0.00.040.653 I print_info: model params     = 1.41 B
0.00.040.653 I print_info: general.name     = 1.4B
0.00.040.653 I print_info: vocab type       = BPE
0.00.040.653 I print_info: n_vocab          = 50304
0.00.040.654 I print_info: n_merges         = 50009
0.00.040.654 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.654 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.654 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.658 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.658 I print_info: LF token         = 187 ''
0.00.040.658 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.658 I print_info: max token length = 1024
0.00.040.659 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.642.972 I load_tensors: offloading 24 repeating layers to GPU
0.00.642.978 I load_tensors: offloading output layer to GPU
0.00.642.978 I load_tensors: offloaded 25/25 layers to GPU
0.00.642.998 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.642.999 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.643.883 I llama_init_from_model: n_seq_max     = 1
0.00.643.888 I llama_init_from_model: n_ctx         = 2048
0.00.643.888 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.643.889 I llama_init_from_model: n_batch       = 2048
0.00.643.889 I llama_init_from_model: n_ubatch      = 512
0.00.643.889 I llama_init_from_model: flash_attn    = 0
0.00.643.890 I llama_init_from_model: freq_base     = 10000.0
0.00.643.890 I llama_init_from_model: freq_scale    = 1
0.00.643.892 I ggml_metal_init: allocating
0.00.643.925 I ggml_metal_init: found device: Apple M4
0.00.643.936 I ggml_metal_init: picking default device: Apple M4
0.00.644.999 I ggml_metal_init: using embedded metal library
0.00.649.301 I ggml_metal_init: GPU name:   Apple M4
0.00.649.306 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.649.307 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.649.307 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.649.308 I ggml_metal_init: simdgroup reduction   = true
0.00.649.308 I ggml_metal_init: simdgroup matrix mul. = true
0.00.649.308 I ggml_metal_init: has residency sets    = true
0.00.649.309 I ggml_metal_init: has bfloat            = true
0.00.649.309 I ggml_metal_init: use bfloat            = true
0.00.649.310 I ggml_metal_init: hasUnifiedMemory      = true
0.00.649.320 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.661.930 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.693.819 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.693.827 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.693.850 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.698.070 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.698.072 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.698.072 I llama_init_from_model: graph nodes  = 967
0.00.698.073 I llama_init_from_model: graph splits = 2
0.00.698.078 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.698.195 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.698.195 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.766.975 I main: llama threadpool init, n_threads = 4
0.00.767.014 I 
0.00.767.032 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.767.033 I 
0.00.767.198 I sampler seed: 1234
0.00.767.202 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.767.212 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.767.213 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.767.213 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.

He added: Ive

0.01.637.072 I llama_perf_sampler_print:    sampling time =       1.41 ms /    71 runs   (    0.02 ms per token, 50497.87 tokens per second)
0.01.637.073 I llama_perf_context_print:        load time =     756.38 ms
0.01.637.073 I llama_perf_context_print: prompt eval time =      54.29 ms /     7 tokens (    7.76 ms per token,   128.94 tokens per second)
0.01.637.074 I llama_perf_context_print:        eval time =     812.92 ms /    63 runs   (   12.90 ms per token,    77.50 tokens per second)
0.01.637.074 I llama_perf_context_print:       total time =     870.82 ms /    70 tokens
0.01.637.325 I ggml_metal_free: deallocating

real	0m1.655s
user	0m0.103s
sys	0m0.167s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.103 I build: 4668 (0cf86716) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.843 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.860 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.865 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.869 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.869 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.870 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.870 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.870 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.871 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.872 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.874 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.874 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.875 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.875 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.875 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.877 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.877 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.878 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.732 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.791 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.683 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.685 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.685 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.685 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.686 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.686 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.687 I llama_model_loader: - type  f32:  194 tensors
0.00.024.687 I llama_model_loader: - type q6_K:   98 tensors
0.00.024.688 I print_info: file format = GGUF V3 (latest)
0.00.024.688 I print_info: file type   = Q6_K
0.00.024.689 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.033.126 I load: special tokens cache size = 25
0.00.038.995 I load: token to piece cache size = 0.2984 MB
0.00.038.998 I print_info: arch             = gptneox
0.00.038.999 I print_info: vocab_only       = 0
0.00.038.999 I print_info: n_ctx_train      = 2048
0.00.038.999 I print_info: n_embd           = 2048
0.00.038.999 I print_info: n_layer          = 24
0.00.039.004 I print_info: n_head           = 16
0.00.039.004 I print_info: n_head_kv        = 16
0.00.039.005 I print_info: n_rot            = 32
0.00.039.005 I print_info: n_swa            = 0
0.00.039.005 I print_info: n_embd_head_k    = 128
0.00.039.008 I print_info: n_embd_head_v    = 128
0.00.039.009 I print_info: n_gqa            = 1
0.00.039.009 I print_info: n_embd_k_gqa     = 2048
0.00.039.010 I print_info: n_embd_v_gqa     = 2048
0.00.039.011 I print_info: f_norm_eps       = 1.0e-05
0.00.039.011 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.011 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.012 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.012 I print_info: f_logit_scale    = 0.0e+00
0.00.039.012 I print_info: n_ff             = 8192
0.00.039.013 I print_info: n_expert         = 0
0.00.039.013 I print_info: n_expert_used    = 0
0.00.039.013 I print_info: causal attn      = 1
0.00.039.013 I print_info: pooling type     = 0
0.00.039.013 I print_info: rope type        = 2
0.00.039.015 I print_info: rope scaling     = linear
0.00.039.015 I print_info: freq_base_train  = 10000.0
0.00.039.015 I print_info: freq_scale_train = 1
0.00.039.016 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.016 I print_info: rope_finetuned   = unknown
0.00.039.016 I print_info: ssm_d_conv       = 0
0.00.039.016 I print_info: ssm_d_inner      = 0
0.00.039.016 I print_info: ssm_d_state      = 0
0.00.039.016 I print_info: ssm_dt_rank      = 0
0.00.039.016 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.017 I print_info: model type       = 1.4B
0.00.039.017 I print_info: model params     = 1.41 B
0.00.039.017 I print_info: general.name     = 1.4B
0.00.039.018 I print_info: vocab type       = BPE
0.00.039.018 I print_info: n_vocab          = 50304
0.00.039.018 I print_info: n_merges         = 50009
0.00.039.018 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.019 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.019 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.020 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.023 I print_info: LF token         = 187 ''
0.00.039.023 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.024 I print_info: max token length = 1024
0.00.039.024 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.621.626 I load_tensors: offloading 24 repeating layers to GPU
0.00.621.638 I load_tensors: offloading output layer to GPU
0.00.621.639 I load_tensors: offloaded 25/25 layers to GPU
0.00.621.670 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.621.671 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.623.280 I llama_init_from_model: n_seq_max     = 1
0.00.623.283 I llama_init_from_model: n_ctx         = 128
0.00.623.284 I llama_init_from_model: n_ctx_per_seq = 128
0.00.623.284 I llama_init_from_model: n_batch       = 128
0.00.623.285 I llama_init_from_model: n_ubatch      = 128
0.00.623.285 I llama_init_from_model: flash_attn    = 0
0.00.623.288 I llama_init_from_model: freq_base     = 10000.0
0.00.623.288 I llama_init_from_model: freq_scale    = 1
0.00.623.289 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.623.294 I ggml_metal_init: allocating
0.00.623.358 I ggml_metal_init: found device: Apple M4
0.00.623.371 I ggml_metal_init: picking default device: Apple M4
0.00.624.851 I ggml_metal_init: using embedded metal library
0.00.631.348 I ggml_metal_init: GPU name:   Apple M4
0.00.631.352 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.631.352 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.631.354 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.631.357 I ggml_metal_init: simdgroup reduction   = true
0.00.631.358 I ggml_metal_init: simdgroup matrix mul. = true
0.00.631.358 I ggml_metal_init: has residency sets    = true
0.00.631.358 I ggml_metal_init: has bfloat            = true
0.00.631.358 I ggml_metal_init: use bfloat            = true
0.00.631.359 I ggml_metal_init: hasUnifiedMemory      = true
0.00.631.364 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.648.692 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.652.326 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.652.332 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.652.365 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.655.672 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.655.674 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.655.675 I llama_init_from_model: graph nodes  = 967
0.00.655.675 I llama_init_from_model: graph splits = 2
0.00.655.679 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.655.680 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.691.514 I 
0.00.691.591 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.691.598 I perplexity: tokenizing the input ..
0.00.698.246 I perplexity: tokenization took 6.644 ms
0.00.698.252 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.838.096 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.839.450 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.839.479 I llama_perf_context_print:        load time =     682.66 ms
0.00.839.482 I llama_perf_context_print: prompt eval time =     138.97 ms /   128 tokens (    1.09 ms per token,   921.05 tokens per second)
0.00.839.482 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.839.483 I llama_perf_context_print:       total time =     147.97 ms /   129 tokens
0.00.839.877 I ggml_metal_free: deallocating

real	0m0.854s
user	0m0.079s
sys	0m0.141s
```
- save-load-state: 
```
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4668 (0cf86716)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 ''
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14d305860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14d305ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14d306340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14d309030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14d3094a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14d309910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14d309ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14d30a470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14d30aa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14d30af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14d30b420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14d30b920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14d30c440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14d30cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14d30d400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14d30db20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14d30e240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14d30e960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14d30f080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14d30f850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14d30ff70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14d310690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14d310db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14d311650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14d311d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14d312030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14d312640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14d3132b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14d3137f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14d313ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14d313f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14d314210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14d314aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14d314fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14d3152a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14d315740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14d315be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14d316080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14d316520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14d3169c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14d316e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14d317300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14d3177a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14d317c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14d317f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14d318510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14d318b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14d319440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14d319a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14d31a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14d31a670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14d31ac80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14d31b290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14d31b8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14d31c090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14d31c530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14d31c9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14d31cc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14d31d2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14d31da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14d31dd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14d31e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14d31e690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14d31eb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14d31efd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14d31f470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14d31f910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14d31fdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14d320250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14d3206f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14d320b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14d321030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14d3214d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14d321a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14d321f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14d3224c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14d322a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14d322f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14d3234b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14d323a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14d323f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x14d3244a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x14d3249f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x14d324f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x14d325490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x14d3259e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x14d325f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x14d326480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x14d3269d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x14d326f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x14d327470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x14d3279c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x14d327f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x14d328460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x14d3289b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x14d328f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x14d329450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x14d319130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x14d3298c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x14d32a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x14d32a5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x14d32ab10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x14d32b060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x14d32b5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x14d32bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x14d32c050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x14d32c5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x14d32caf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x14d32d040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x14d32d590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x14d32dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x14d32e030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x14d32e580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14d32ea20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14d32eec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14d32f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14d32f800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14d32fca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14d330140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14d3305e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14d330a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14d330f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14d3313c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14d331860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14d331d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14d3321a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14d332640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14d332ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14d332f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14d333420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14d3338c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14d333d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14d334200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14d3346a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14d334b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14d334fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14d335480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14d335920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14d335dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14d336260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14d336700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14d336ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14d337040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14d3374e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14d337980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14d337e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14d3382c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14d338760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14d338c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14d3390a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14d339540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14d3399e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14d339e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14d33a320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14d33a7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14d33ac60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14d33b100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14d33b5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14d33ba40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14d33bee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14d33c380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14d33c820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14d33ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14d33d160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14d33d600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14d33daa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14d33df40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14d33e3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14d33e880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14d33ed20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14d33f1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14d33f660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14d33fb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14d33ffa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14d340440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14d3408e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14d340d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14d341220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14d3416c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14d341b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14d342000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14d3424a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14d342940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14d342de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14d343280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14d343720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14d343bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14d344060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14d344500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14d3449a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14d344e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14d3452e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14d345780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14d345cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14d346220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14d346770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14d346cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14d346f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14d347590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14d204230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14d2046a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14d204b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14d204f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14d2053f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14d205860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x14d205cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14d206140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14d2066d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14d206b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14d206fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14d207b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14d207dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14d208470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14d208a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14d208fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14d209590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14d209b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14d20a0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14d20a6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14d20ac50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14d20b200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14d20b7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14d20bd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14d20c310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14d20c8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14d20ce70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14d20d420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14d20d9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14d20df80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14d20e530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14d20eae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14d20f090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14d20f640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14d20fbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14d2101a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14d210750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14d210d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14d2112b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14d211860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14d211e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14d2123c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14d212970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14d212f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14d2134d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14d213a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14d214030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14d2145e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14d214b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14d215140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14d2156f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14d215ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14d216250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14d216800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14d216db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14d217360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14d217910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14d217ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14d218470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14d218a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14d218fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14d219580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14d219b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14d21a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14d21a690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14d21ac40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14d21b1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14d21b7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x14d21bd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x14d21c250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14d21c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14d21cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14d21d150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14d21d650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14d21db50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14d21e050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14d21e550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14d21ea50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14d21ef50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14d21f450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14d21f950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14d21fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14d220350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14d220850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14d221260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14d221980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14d2220a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14d2227c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14d222a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x14d223270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14d223530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14d223b40 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.715.387 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.715.391 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14d211b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14d210a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14d20d6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14d20af10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14d21a3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14d217bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14d2159b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14d213790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14d20ba70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14d2092a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14d20e240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14d20f350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14d2148a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14d211570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14d219290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14d20d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14d216510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14d20feb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14d2120d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14d20dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14d218180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14d2131e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14d20a960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14d210460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14d218730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14d20e7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14d210fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14d20c5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14d20b4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14d217070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14d21ba60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14d21b4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14d212680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14d213d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14d218ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14d20f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14d223e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14d224510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14d2249b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14d224e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14d2252f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14d225790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14d225c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14d2260d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14d226390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14d2269a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14d226fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14d2275c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14d227bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14d2281e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14d2287f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14d228e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14d229410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14d229a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14d22a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14d22a6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14d22ab50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14d22ae10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14d22b420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14d22bc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14d22c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14d22c550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14d22c9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14d22ce90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14d22d330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14d22d7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14d22dc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14d22e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14d22e5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14d22ea50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14d22eef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14d22f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14d22f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14d22fd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14d2302d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14d230820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14d230d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14d2312c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14d231810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14d231d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14d2322b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x14d232800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x14d232d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x14d2332a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x14d2337f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x14d233d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x14d234290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x14d2347e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x14d234d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x14d235280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x14d2357d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x14d235d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x14d236270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x14d2367c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x14d236d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x14d237260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x14d2377b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x14d237d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x14d238250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x14d2387a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x14d238cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x14d239240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x14d239790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x14d239ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x14d23a230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x14d23a780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x14d23acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x14d23b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x14d23b770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x14d23bcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x14d23c210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x14d23c760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x14d23ccb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14d23d150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14d23d5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14d23da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14d23df30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14d23e3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14d23e870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14d23ed10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14d23f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14d23f650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14d23faf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14d23ff90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14d240430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14d2408d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14d240d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14d241210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14d2416b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14d241b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14d241ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14d242490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14d242930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14d242dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14d243270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14d243710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14d243bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14d244050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14d2444f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14d244990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14d244e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14d2452d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14d245770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14d245c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14d2460b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14d246550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14d2469f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14d246e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14d247330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14d2477d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14d247c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14d248110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14d2485b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14d248a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14d248ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14d249390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14d249830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14d249cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14d24a170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14d24a610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14d24aab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14d24af50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14d24b3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14d24b890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14d24bd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14d24c1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14d24c670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14d24cb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14d24cfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14d24d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14d24d8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14d24dd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14d24e230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14d24e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14d24eb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14d24f010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14d24f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14d24f950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14d24fdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14d250290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14d250730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14d250bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14d251070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14d251510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14d2519b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14d251e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14d2522f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14d252790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14d252c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14d2530d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14d253570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14d253a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14d253eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14d254400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14d254950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14d254ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14d2553f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14d2556b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14d255cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14d2562d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14d2568e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14d2570d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14d257570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14d257830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14d257e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x14d258450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14d258c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14d2590e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14d259580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14d259a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14d25a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14d25a720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14d25ac70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14d25b1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14d25b710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14d25bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14d25c1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14d25c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14d25cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14d25d1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14d25d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14d25dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14d25e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14d25e6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14d25ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14d25f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14d25f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14d25fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14d260170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14d2606c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14d260c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14d261160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14d2616b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14d261c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14d262150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14d2626a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14d262bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14d263140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14d263690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14d263be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14d264130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14d264680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14d264bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14d265120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14d265670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14d265bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14d266110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14d266660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14d266bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14d267100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14d267650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14d267ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14d2680f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14d268640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14d268b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14d2690e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14d269630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14d269b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14d26a0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14d26a620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14d26ab70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14d26b0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14d26b610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14d26bb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14d26c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14d26c600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14d26cb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x14d26cff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x14d26d490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14d26d930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14d26ddd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14d26e270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14d26e710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14d26ebb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14d26f050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14d26f4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14d26f990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14d26fe30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14d2702d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14d270770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14d270c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14d2710b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14d271600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14d271d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14d272440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14d272b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14d273280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14d273540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x14d273d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14d273ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14d274600 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13a808f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13a806e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13a809520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13a809990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13a809e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13a80a3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13a80a960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13a80af10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13a80b4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13a80b9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13a80bec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13a80c3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13a80cee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13a80d690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13a80dea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13a80e5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13a80ece0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13a80f400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13a80fb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13a8102f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13a810a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13a811130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13a811850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13a811f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13a812690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13a812950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13a812f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13a813570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13a813b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13a814370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13a814810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13a814ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13a815360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13a8158a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13a815b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13a816000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13a8164a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13a816940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13a816de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13a817280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13a817720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13a817bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13a818060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13a818500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13a8187c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13a818dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13a8193e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13a8199f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13a81a000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13a81a610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13a81ac20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13a81b230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13a81b840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13a81be50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13a81c640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13a81cae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13a81cf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13a81d240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13a81d850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13a81e040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13a81e4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13a81e980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13a81ee20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13a81f2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13a81f760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13a81fc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13a8200a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13a820540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13a8209e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13a820e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13a821320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13a8217c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13a821c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13a8221b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13a822700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13a822c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13a8231a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13a8236f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13a823c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13a824190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13a8246e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13a824c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13a825180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13a8256d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13a825c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13a826170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13a8266c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13a826c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13a827160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13a8276b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13a827c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13a828150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13a8286a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13a828bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13a829140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13a829690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13a829be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13a82a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13a82a680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13a82abd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13a82b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13a82b670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13a82bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13a82c110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13a82c660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13a82cbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13a82d100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13a82d650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13a82dba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13a82e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13a82e640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13a82eb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13a82f0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13a82f580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13a82fa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13a82fec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13a830360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13a830800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13a830ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13a831140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13a8315e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13a831a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13a831f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13a8323c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13a832860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13a832d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13a8331a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13a833640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13a833ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13a833f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13a834420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13a8348c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13a834d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13a835200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13a8356a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13a835b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13a835fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13a836480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13a836920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13a836dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13a837260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13a837700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13a837ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13a838040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13a8384e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13a838980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13a838e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13a8392c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13a839760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13a839c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13a83a0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13a83a540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13a83a9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13a83ae80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13a83b320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13a83b7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13a83bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13a83c100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13a83c5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13a83ca40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13a83cee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13a83d380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13a83d820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13a83dcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13a83e160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13a83e600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13a83eaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13a83ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13a83f3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13a83f880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13a83fd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13a8401c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13a840660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13a840b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13a840fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13a841440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13a8418e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13a841d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13a842220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13a8426c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13a842b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13a843000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13a8434a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13a843940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13a843de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13a844280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13a844720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13a844bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13a845060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13a845500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13a8459a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13a845e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13a8462e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13a846830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13a846d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13a8472d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13a847820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13a847ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13a8480f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13a848700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13a848d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13a849500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13a8499a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13a849c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14d347240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x14d31af40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14d31a930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14d31cf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14d3122f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14d318de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14d319d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14d3187d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14d3181c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14d31b550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14d31a320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14d3112f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14d30bbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14d304e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14d31bb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14d31d690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14d329b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14d3144d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14d314790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14d312900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14d312bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14d312e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14d3479f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14d347cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14d347f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14d348230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14d3484f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14d3487b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14d348a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14d348d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14d348ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14d3492b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14d349570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14d349830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14d349af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14d349db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14d34a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14d34a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14d34a5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14d34a8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14d34ab70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14d34ae30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14d34b0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14d34b3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14d34b670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14d34b930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14d34bbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14d34beb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14d34c170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14d34c430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14d34c6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14d34c9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14d34cc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14d34cf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14d34d1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14d34d4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14d34d770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14d34da30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14d34dcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14d34dfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14d34e270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14d34e530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14d34e7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x14d34eab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x14d34ed70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14d34f030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14d34f2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14d34f5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14d34f870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14d34fb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14d34fdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14d3500b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14d350370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14d350630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14d3508f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14d350bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14d350e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14d351130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14d3513f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14d3516b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14d351970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14d351c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14d351ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14d3521b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x14d352470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14d352730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14d3529f0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.776s
user	0m0.272s
sys	0m0.329s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4668 (0cf86716)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 ''
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x1250044c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x125004b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x125004fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x125005410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x125005880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x125005cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x125006160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x1250065d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12370b0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12370b560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12370b9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12370bfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12370cb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12370d2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12370dac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12370e1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12370e900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12370f020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12370f740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12370ff10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x123710630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x123710d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x123711470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x123711d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x123712430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x1237126f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x123712b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x123713430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x1237136f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x123713b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x123714080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x123714590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x123714a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x123714e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x123715130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x1237158b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x123715d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x123716250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x123716720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x123716bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x1237170c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x123717590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x123717a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x123717f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x123718400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x123718870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x123718ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x123719150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x1237198e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x123719d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12371a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12371a630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12371aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12371af10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12371b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12371ba70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12371bf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12371c1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12371c7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12371cfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12371d290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12371d730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12371dbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12371e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12371e510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12371e9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12371ee50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12371f2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12371f790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12371fc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x1237200d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x123720570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x123720a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x123720f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x1237214b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x123721a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x123721f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x1237224a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x1237229f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x123722f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x123723490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x1237239e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x123723f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x123724480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x1237249d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x123724f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x123725470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x1237259c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x123725f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x123726460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x1237269b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x123726f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x123727450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x1237279a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x123727ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x123728440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x123728990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x123719410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x123728e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x1237295b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x123729b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12372a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12372a5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12372aaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12372b040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12372b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12372bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12372c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12372c580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12372cad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12372d020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12372d570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12372dac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12372df60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12372e400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12372e8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12372ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12372f1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12372f680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12372fb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12372ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x123730460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x123730900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x123731010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x1237312d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x1237317d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x123731cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x1237321d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x1237326d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x123732bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x1237330d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x1237335d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x123733ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x123733fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x1237344d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x1237349d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x123734ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x1237353d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x1237358d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x123735dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x1237362d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x1237367d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x123736cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x1237371d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x1237376d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x123737bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x1237380d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x1237385d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x123738ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x123738fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x1237394d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x1237399d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x123739ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12373a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12373a8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12373add0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12373b2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12373b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12373bcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12373c1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12373c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12373cbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12373d0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12373d5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12373dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12373dfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12373e4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12373e9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12373eed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12373f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12373f8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12373fdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x1237402d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x1237407d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x123740cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x1237411d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x1237416d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x123741bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x1237420d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x1237425d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x123742ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x123742fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x1237434d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x1237439d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x123743ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x1237443d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x1237448d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x123744dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x1237452d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x1237457d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x123745cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x1237461d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x1237466d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x123746bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x123747180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x123747730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x123747ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x123748290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x1237488a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x123748eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x1237494c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x123749cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12374a150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12374a410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12374aa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12374b030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12374b820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12374bcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12374c160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12374c600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12374cdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12374d300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12374d850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12374dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12374e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12374e840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12374ed90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12374f2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12374f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12374fd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x1237502d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x123750820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x123750d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x1237512c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x123751810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x123751d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x1237522b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x123752800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x123752d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x1237532a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x1237537f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x123753d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x123754290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x1237547e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x123754d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x123755280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x1237557d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x123755d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x123756270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x1237567c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x123756d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x123757260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x1237577b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x123757d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x123758250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x1237587a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x123758cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x123759240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x123759790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x123759ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12375a230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12375a780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12375acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12375b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12375b770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12375bcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12375c210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12375c760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12375ccb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12375d200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12375d750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12375dca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12375e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12375e740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12375ec90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12375f1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12375f730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12375fbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x123760070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x123760510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x1237609b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x123760e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x1237612f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x123761790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x123761c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1237620d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x123762570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x123762a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x123762eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x123763350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x1237637f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x123763c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x1237641e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x123764900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x123765020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x123765740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x123765e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x123766120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x123766910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x123766bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x1237671e0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.099.785 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.099.789 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x123746e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x123748b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12374a6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x123766e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x123748550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x123749170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12374ace0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x1237119b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12371b1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12371cbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x1237290c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x1237663e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x123749780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x123713130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x1237679b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x123767c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x123767f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x1237681f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x1237684b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x123768770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x123768a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x123768cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x123768fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x123769270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x123769530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x1237697f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x123769ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x123769d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12376a030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12376a2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12376a5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12376a870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12376ab30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12376adf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12376b0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12376b370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12376b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12376b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12376bbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12376be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12376c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12376c3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12376c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12376c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12376cc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12376cef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12376d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12376d470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12376d730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12376d9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12376dcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12376df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12376e230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12376e4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12376e7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12376ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12376ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12376eff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12376f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12376f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12376f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12376faf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12376fdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x123770070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x123770330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x1237705f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x1237708b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x123770b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x123770e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x1237710f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x1237713b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x123771670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x123771930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x123771bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x123771eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x123772170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x123772430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x1237726f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x1237729b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x123772c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x123772f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x1237731f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x1237734b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x123773770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x123773a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x123773cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x123773fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x123774270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x123774530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x1237747f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x123774ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x123774d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x123775030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x1237752f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x1237755b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x123775870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x123775b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x123775df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x1237760b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x123776370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x123776630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x1237768f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x123776bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x123776e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x123777130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x1237773f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x1237776b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x123777970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x123777c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x123777ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x1237781b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x123778470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x123778730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x1237789f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x123778cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x123778f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x123779230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x1237794f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x1237797b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x123779a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x123779d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x123779ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12377a2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12377a570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12377a830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12377aaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12377adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12377b070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12377b330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12377b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12377b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12377bb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12377be30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12377c0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12377c3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12377c670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12377c930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12377cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12377ceb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12377d170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12377d430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12377d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12377d9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12377dc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12377df30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12377e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12377e4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12377e770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12377ea30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12377ecf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12377efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12377f270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12377f530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12377f7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12377fab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12377fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x123780030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x1237802f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x1237805b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x123780870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x123780b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x123780df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x1237810b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x123781370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x123781630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x1237818f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x123781bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x123781e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x123782130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x1237823f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x1237826b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x123782970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x123782c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x123782ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x1237831b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x123783470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x123783730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x1237839f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x123783cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x123783f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x123784230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x1237844f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x1237847b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x123784a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x123784d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x123784ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x1237852b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x123785570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x123785830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x123785af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x123785db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x123786070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x123786330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x1237865f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x1237868b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x123786b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x123786e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x1237870f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x1237873b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x123787670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x123787930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x123787bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x123787eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x123788170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x123788430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x1237886f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x1237889b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x123788c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x123788f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x123789330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x123789ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x123789da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12378a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12378a4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12378a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12378adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12378b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12378b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12378bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12378bf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12378c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12378c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12378ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12378d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12378d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12378da10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12378de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12378e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12378e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12378ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12378f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12378f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12378f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12378fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x123790200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x123790670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x123790ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x123790f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x1237913c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x123791830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x123791ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x123792110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x123792580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x1237929f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x123792e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x112704080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x1127044f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x112704960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x1237932d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x123793740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x123793bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x123794020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x123794490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x123794900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x123794d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x1237951e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x123795650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x123795ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x123795f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x1237963a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x123796810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x123796c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x1237970f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x123797560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x1237979d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x123797e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x1237982b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x123798720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x123798b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x123799000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x123799470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x1237998e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x123799d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12379a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12379a630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12379aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12379af10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12379b380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12379b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12379bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12379c0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12379c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12379c9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12379ce20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12379db20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12379e240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12379e960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12379ec20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12379f410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12379f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12379fce0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x125006d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x125007000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x125007700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x1250079c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x125007c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x125007f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x125008490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x1250089e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x125008ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x125008f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x125009220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x1250098e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12500a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12500abb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12500b3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12500bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12500c200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12500c920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12500d040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12500d810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12500df30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12500e650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12500ed70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12500f490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12500fbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12500fe70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x125010130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x1250105a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x125010a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x125010e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x1250112f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x125011820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x125011c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x125011f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1250123c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x125012830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x125012ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x125013110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x125013580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x1250139f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x125013e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x1250142d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x125014740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x125014bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x125015020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x125015490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x125015900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x125015d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x1250161e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x125016650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x125016ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x125016f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x1250173a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x125017810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x125017c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x1250180f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x125018660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x125018b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x125018fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x125019440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x1250198b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x125019d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12501a190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12501a600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12501aa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12501aee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12501b350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12501b7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12501bc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12501c0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12501c510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12501c980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12501cdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12501d260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12501d6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12501db40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12501dfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12501e420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12501e890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12501ed00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12501f170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12501f5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12501fa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12501fec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x125020330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x1250207a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x125020c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x125021080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x1250214f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x125021960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x125021dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x125022240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x1250226b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x125022b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x125022f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x125023400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x125023870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x125023ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x125024150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x1250245c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x125024a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x125024ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x125025310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x125025ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x125025e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x1250262d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x125026740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x125026bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x125027020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x125027490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x125027900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x125027d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x1250281e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x125028650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x125028ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x125028f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x1250293a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x125029810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x125029c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12502a0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12502a560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12502a9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12502ae40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12502b2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12502b720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12502bb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12502c000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12502c470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12502c8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12502cd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12502d1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12502d630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12502daa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12502df10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12502e380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12502e7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12502ec60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12502f0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12502f540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12502f9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12502fe20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x125030290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x125030700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x125030b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x125030fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x125031450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x1250318c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x125031d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x1250321a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x125032610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x125032a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x125032ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x125033360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x1250337d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x125033c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x1250340b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x125034520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x125034990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x125034e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x125035270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x1250356e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x125035b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x125035fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x125036430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x1250368a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x125036d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x125037180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x1250375f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x125037a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x125037ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x125038340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x1250387b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x125038c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x125039090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x125039500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x125039970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x125039de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12503a250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12503a6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12503ab30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12503afa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12503b410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12503b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12503bcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12503c160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12503c5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12503ca40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12503ceb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12503d320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12503d790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12503dc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12503e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12503e4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12503e950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12503edc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12503f230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12503f6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12503fb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12503ff80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x1250403f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x125040860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x125040cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x125041140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x1250415b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x125041a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x125041e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x125042300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x125042770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x125042be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x125043050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x125043bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x125043e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x125044150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x1250445c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x125044a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x125044ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x125045310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x125045780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x125045bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x125046060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x1250464d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x125046940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x125046db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x125047220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x125047690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x125047b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x125047f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x1250483e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x125048850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x125048cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x125049130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x1250495a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x125049a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x125049e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12504a2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12504a760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12504abd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12504b040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12504b4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12504b920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12504bd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12504c200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12504c670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12504cae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12504cf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12504d3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12504d830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12504dca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12504e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12504e580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12504e9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12504ee60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12504f2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12504f740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12504fbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x125050020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x125050490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x125050900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x125050d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x1250511e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x125051650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x125051ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x125051f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x1250523a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x125052810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x125052c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x1250530f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x125053560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x1250539d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x125053e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x1250542b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x125054720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x125054b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x125055000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x125055470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1250558e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x125055d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x1250561c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x125056630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x125056aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x125056f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x125057380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x1250577f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x125058260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x125058980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x1250590a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1250597c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x125059a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x125059ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12505a4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12505ab00 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.960s
user	0m0.233s
sys	0m0.192s
```
### ctest_with_model_debug

Runs ctest with model files in debug mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-debug
    Start 26: test-model-load-cancel
1/2 Test #26: test-model-load-cancel ...........   Passed    0.43 sec
    Start 27: test-autorelease
2/2 Test #27: test-autorelease .................   Passed    1.26 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   1.69 sec*proc (2 tests)

Total Test time (real) =   1.71 sec
        1.73 real         0.51 user         0.21 sys
```
### ctest_with_model_release

Runs ctest with model files in release mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-release
    Start 26: test-model-load-cancel
1/2 Test #26: test-model-load-cancel ...........   Passed    0.23 sec
    Start 27: test-autorelease
2/2 Test #27: test-autorelease .................   Passed    0.30 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   0.53 sec*proc (2 tests)

Total Test time (real) =   0.54 sec
        0.55 real         0.13 user         0.08 sys
```
