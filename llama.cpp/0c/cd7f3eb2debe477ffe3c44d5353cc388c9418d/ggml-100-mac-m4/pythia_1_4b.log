Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:301 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- ARM -mcpu not found, -mcpu=native will be used
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod - Success
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sve
-- Performing Test GGML_MACHINE_SUPPORTS_sve - Failed
-- ARM feature DOTPROD enabled
-- ARM feature MATMUL_INT8 enabled
-- ARM feature FMA enabled
-- ARM feature FP16_VECTOR_ARITHMETIC enabled
-- Adding CPU backend variant ggml-cpu: -mcpu=native+dotprod+i8mm+nosve 
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (2.3s)
-- Generating done (0.2s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m2.514s
user	0m0.860s
sys	0m1.208s
++ nproc
+ make -j10
[  1%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  1%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  1%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  1%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  2%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  2%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  2%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  3%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  4%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  5%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o
[  5%] Built target sha1
[  5%] Built target sha256
[  5%] Built target build_info
[  5%] Built target xxhash
[  6%] Linking CXX shared library libggml-base.dylib
[  6%] Built target ggml-base
[  6%] Generate assembly for embedded Metal library
Embedding Metal library
[  6%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[  6%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[  6%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[  6%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[  8%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 11%] Linking CXX shared library libggml-blas.dylib
[ 12%] Linking CXX shared library libggml-cpu.dylib
[ 13%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 13%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 13%] Built target ggml-blas
[ 13%] Built target ggml-cpu
[ 14%] Linking C shared library libggml-metal.dylib
[ 14%] Built target ggml-metal
[ 14%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 15%] Linking CXX shared library libggml.dylib
[ 15%] Built target ggml
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o
[ 15%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o
[ 15%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o
[ 19%] Linking CXX executable ../../bin/llama-gguf-hash
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o
[ 21%] Linking CXX executable ../../bin/llama-gguf
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o
[ 25%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 25%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 25%] Built target llama-gguf
[ 26%] Linking CXX shared library libllama.dylib
[ 26%] Built target llama-gguf-hash
[ 26%] Built target llama
[ 26%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 28%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 29%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 29%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 30%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 30%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 30%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 30%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 30%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 31%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 31%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 31%] Linking CXX executable ../../bin/llama-quantize-stats
[ 32%] Linking C executable ../bin/test-c
[ 33%] Linking CXX executable ../../bin/llama-simple
[ 34%] Linking CXX executable ../../bin/llama-simple-chat
[ 34%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 35%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 35%] Built target llava
[ 36%] Linking CXX static library libcommon.a
[ 37%] Linking CXX static library libllava_static.a
[ 37%] Linking CXX shared library libllava_shared.dylib
[ 37%] Built target test-c
[ 37%] Built target llama-quantize-stats
[ 37%] Built target llama-simple
[ 37%] Built target llama-simple-chat
[ 37%] Built target common
[ 37%] Built target llava_static
[ 38%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 40%] Built target llava_shared
[ 41%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 44%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 44%] Linking CXX executable ../bin/test-tokenizer-0
[ 44%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 44%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 44%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 45%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 45%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 45%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 46%] Linking CXX executable ../bin/test-sampling
[ 46%] Linking CXX executable ../bin/test-llama-grammar
[ 47%] Linking CXX executable ../bin/test-log
[ 48%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 49%] Linking CXX executable ../bin/test-grammar-integration
[ 50%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 50%] Linking CXX executable ../bin/test-grammar-parser
[ 50%] Built target test-tokenizer-1-bpe
[ 50%] Linking CXX executable ../bin/test-arg-parser
[ 50%] Built target test-tokenizer-0
[ 50%] Built target test-tokenizer-1-spm
[ 50%] Built target test-llama-grammar
[ 50%] Built target test-log
[ 50%] Built target test-json-schema-to-grammar
[ 50%] Built target test-sampling
[ 50%] Built target test-grammar-integration
[ 50%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 50%] Built target test-grammar-parser
[ 50%] Building CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o
[ 55%] Linking CXX executable ../bin/test-chat-template
[ 55%] Built target test-arg-parser
[ 55%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 57%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 57%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 57%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 57%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 57%] Linking CXX executable ../bin/test-gguf
[ 58%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 58%] Linking CXX executable ../bin/test-backend-ops
[ 59%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 59%] Linking CXX executable ../bin/test-model-load-cancel
[ 59%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 60%] Linking CXX executable ../bin/test-barrier
[ 61%] Linking CXX executable ../bin/test-autorelease
[ 62%] Linking CXX executable ../bin/test-quantize-fns
[ 62%] Built target test-chat-template
[ 62%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 62%] Linking CXX executable ../bin/test-quantize-perf
[ 63%] Linking CXX executable ../../bin/llama-batched-bench
[ 63%] Built target test-gguf
[ 64%] Linking CXX executable ../bin/test-rope
[ 64%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 64%] Built target test-backend-ops
[ 64%] Built target test-autorelease
[ 64%] Built target test-barrier
[ 64%] Built target test-model-load-cancel
[ 64%] Built target test-quantize-fns
[ 64%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 64%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 64%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 64%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 65%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 66%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 66%] Built target llama-batched-bench
[ 67%] Linking CXX executable ../../bin/llama-batched
[ 67%] Built target test-rope
[ 68%] Linking CXX executable ../../bin/llama-embedding
[ 68%] Built target test-quantize-perf
[ 68%] Linking CXX executable ../../bin/llama-gguf-split
[ 69%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 70%] Linking CXX executable ../../bin/llama-eval-callback
[ 70%] Linking CXX executable ../../bin/llama-imatrix
[ 70%] Linking CXX executable ../../bin/llama-gritlm
[ 71%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 71%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 72%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 72%] Linking CXX executable ../../bin/llama-infill
[ 72%] Built target llama-embedding
[ 73%] Linking CXX executable ../../bin/llama-bench
[ 73%] Built target llama-batched
[ 73%] Linking CXX executable ../../bin/llama-lookahead
[ 73%] Built target llama-gguf-split
[ 73%] Built target llama-eval-callback
[ 73%] Built target llama-imatrix
[ 73%] Built target llama-gritlm
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 75%] Built target llama-gbnf-validator
[ 75%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 75%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 75%] Built target llama-infill
[ 76%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 77%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 78%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 78%] Linking CXX executable ../../bin/llama-lookup
[ 78%] Built target llama-lookahead
[ 78%] Linking CXX executable ../../bin/llama-lookup-create
[ 79%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 79%] Built target llama-bench
[ 80%] Linking CXX executable ../../bin/llama-cli
[ 81%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 81%] Linking CXX executable ../../bin/llama-parallel
[ 82%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 82%] Linking CXX executable ../../bin/llama-lookup-stats
[ 82%] Linking CXX executable ../../bin/llama-lookup-merge
[ 82%] Linking CXX executable ../../bin/llama-passkey
[ 83%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 83%] Linking CXX executable ../../bin/llama-perplexity
[ 83%] Built target llama-lookup-create
[ 83%] Built target llama-lookup
[ 83%] Linking CXX executable ../../bin/llama-quantize
[ 83%] Linking CXX executable ../../bin/llama-retrieval
[ 83%] Built target llama-cli
[ 84%] Generating loading.html.hpp
[ 84%] Built target llama-lookup-merge
[ 84%] Built target llama-lookup-stats
[ 84%] Built target llama-parallel
[ 84%] Built target llama-passkey
[ 85%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 85%] Generating index.html.gz.hpp
[ 86%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 86%] Built target llama-quantize
[ 86%] Built target llama-retrieval
[ 86%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 86%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 86%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 86%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 86%] Linking CXX executable ../../bin/llama-save-load-state
[ 86%] Built target llama-perplexity
[ 86%] Linking CXX executable ../../bin/llama-run
[ 87%] Linking CXX executable ../../bin/llama-speculative-simple
[ 87%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 88%] Linking CXX executable ../../bin/llama-speculative
[ 88%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 89%] Linking CXX executable ../../bin/llama-tokenize
[ 90%] Linking CXX executable ../../bin/llama-tts
[ 90%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 91%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 92%] Linking CXX executable ../../bin/llama-gen-docs
[ 92%] Built target llama-run
[ 93%] Linking CXX executable ../../bin/llama-cvector-generator
[ 93%] Built target llama-save-load-state
[ 93%] Built target llama-speculative
[ 93%] Built target llama-speculative-simple
[ 93%] Built target llama-tokenize
[ 93%] Built target llama-tts
[ 93%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 95%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 95%] Built target llama-gen-docs
[ 95%] Built target llama-convert-llama2c-to-ggml
[ 95%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 96%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 97%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 97%] Built target llama-cvector-generator
[ 98%] Linking CXX executable ../../bin/llama-export-lora
[ 98%] Linking CXX executable ../../bin/llama-llava-cli
[ 98%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 98%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 99%] Linking CXX executable ../../bin/llama-vdot
[ 99%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Built target llama-minicpmv-cli
[ 99%] Built target llama-export-lora
[ 99%] Built target llama-llava-cli
[ 99%] Built target llama-q8dot
[ 99%] Built target llama-vdot
[ 99%] Built target llama-qwen2vl-cli
[ 99%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m3.028s
user	0m6.024s
sys	0m9.603s

main: quantize time =  3614.38 ms
main:    total time =  3614.38 ms

main: quantize time =  2317.47 ms
main:    total time =  2317.47 ms

main: quantize time =  1459.97 ms
main:    total time =  1459.97 ms

main: quantize time =  1613.54 ms
main:    total time =  1613.54 ms

main: quantize time =  1718.47 ms
main:    total time =  1718.47 ms

main: quantize time =  4929.30 ms
main:    total time =  4929.30 ms

main: quantize time =  5755.89 ms
main:    total time =  5755.89 ms

main: quantize time =  7268.34 ms
main:    total time =  7268.34 ms

main: quantize time =  5979.14 ms
main:    total time =  5979.14 ms

main: quantize time =  4491.51 ms
main:    total time =  4491.51 ms
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.202 I build: 4486 (0ccd7f3e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.317 I main: llama backend init
0.00.000.333 I main: load the model and apply lora adapter, if any
0.00.048.772 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.061.441 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.061.458 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.061.478 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.061.479 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.061.480 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.061.481 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.061.481 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.061.484 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.061.485 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.061.486 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.061.486 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.061.487 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.061.488 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.061.489 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.061.493 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.061.494 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.061.494 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.068.573 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.070.816 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.079.332 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.079.341 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.079.342 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.079.343 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.079.343 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.079.344 I llama_model_loader: - type  f32:  194 tensors
0.00.079.345 I llama_model_loader: - type  f16:   98 tensors
0.00.079.347 I print_info: file format = GGUF V3 (latest)
0.00.079.348 I print_info: file type   = all F32 (guessed)
0.00.079.350 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.113.484 I load: special tokens cache size = 25
0.00.121.023 I load: token to piece cache size = 0.2984 MB
0.00.121.027 I print_info: arch             = gptneox
0.00.121.028 I print_info: vocab_only       = 0
0.00.121.028 I print_info: n_ctx_train      = 2048
0.00.121.028 I print_info: n_embd           = 2048
0.00.121.028 I print_info: n_layer          = 24
0.00.121.032 I print_info: n_head           = 16
0.00.121.033 I print_info: n_head_kv        = 16
0.00.121.033 I print_info: n_rot            = 32
0.00.121.033 I print_info: n_swa            = 0
0.00.121.033 I print_info: n_embd_head_k    = 128
0.00.121.033 I print_info: n_embd_head_v    = 128
0.00.121.034 I print_info: n_gqa            = 1
0.00.121.035 I print_info: n_embd_k_gqa     = 2048
0.00.121.035 I print_info: n_embd_v_gqa     = 2048
0.00.121.036 I print_info: f_norm_eps       = 1.0e-05
0.00.121.036 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.121.037 I print_info: f_clamp_kqv      = 0.0e+00
0.00.121.037 I print_info: f_max_alibi_bias = 0.0e+00
0.00.121.037 I print_info: f_logit_scale    = 0.0e+00
0.00.121.037 I print_info: n_ff             = 8192
0.00.121.038 I print_info: n_expert         = 0
0.00.121.038 I print_info: n_expert_used    = 0
0.00.121.038 I print_info: causal attn      = 1
0.00.121.038 I print_info: pooling type     = 0
0.00.121.038 I print_info: rope type        = 2
0.00.121.039 I print_info: rope scaling     = linear
0.00.121.039 I print_info: freq_base_train  = 10000.0
0.00.121.039 I print_info: freq_scale_train = 1
0.00.121.040 I print_info: n_ctx_orig_yarn  = 2048
0.00.121.040 I print_info: rope_finetuned   = unknown
0.00.121.040 I print_info: ssm_d_conv       = 0
0.00.121.040 I print_info: ssm_d_inner      = 0
0.00.121.040 I print_info: ssm_d_state      = 0
0.00.121.041 I print_info: ssm_dt_rank      = 0
0.00.121.041 I print_info: ssm_dt_b_c_rms   = 0
0.00.121.041 I print_info: model type       = 1.4B
0.00.121.041 I print_info: model params     = 1.41 B
0.00.121.041 I print_info: general.name     = 1.4B
0.00.121.044 I print_info: vocab type       = BPE
0.00.121.044 I print_info: n_vocab          = 50304
0.00.121.044 I print_info: n_merges         = 50009
0.00.121.045 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.121.045 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.121.045 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.121.045 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.121.045 I print_info: LF token         = 128 'Ä'
0.00.121.046 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.121.046 I print_info: max token length = 1024
0.00.123.765 I load_tensors: offloading 24 repeating layers to GPU
0.00.123.765 I load_tensors: offloading output layer to GPU
0.00.123.765 I load_tensors: offloaded 25/25 layers to GPU
0.00.123.784 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.123.785 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.124.119 I llama_init_from_model: n_seq_max     = 1
0.00.124.120 I llama_init_from_model: n_ctx         = 2048
0.00.124.120 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.124.120 I llama_init_from_model: n_batch       = 2048
0.00.124.120 I llama_init_from_model: n_ubatch      = 512
0.00.124.121 I llama_init_from_model: flash_attn    = 0
0.00.124.121 I llama_init_from_model: freq_base     = 10000.0
0.00.124.121 I llama_init_from_model: freq_scale    = 1
0.00.124.122 I ggml_metal_init: allocating
0.00.124.125 I ggml_metal_init: found device: Apple M4
0.00.124.127 I ggml_metal_init: picking default device: Apple M4
0.00.124.860 I ggml_metal_init: using embedded metal library
0.00.134.436 I ggml_metal_init: GPU name:   Apple M4
0.00.134.438 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.134.438 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.134.438 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.134.439 I ggml_metal_init: simdgroup reduction   = true
0.00.134.439 I ggml_metal_init: simdgroup matrix mul. = true
0.00.134.439 I ggml_metal_init: has bfloat            = true
0.00.134.439 I ggml_metal_init: use bfloat            = true
0.00.134.439 I ggml_metal_init: hasUnifiedMemory      = true
0.00.134.440 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.158.618 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.179.114 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.179.120 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.179.144 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.180.092 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.180.094 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.180.094 I llama_init_from_model: graph nodes  = 967
0.00.180.095 I llama_init_from_model: graph splits = 2
0.00.180.099 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.180.224 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.180.225 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.261.942 I main: llama threadpool init, n_threads = 4
0.00.261.987 I 
0.00.262.010 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.262.012 I 
0.00.262.079 I sampler seed: 1234
0.00.262.085 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.262.110 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.262.112 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.262.112 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.101.100 I llama_perf_sampler_print:    sampling time =       1.20 ms /    71 runs   (    0.02 ms per token, 59216.01 tokens per second)
0.02.101.101 I llama_perf_context_print:        load time =     213.16 ms
0.02.101.102 I llama_perf_context_print: prompt eval time =      43.54 ms /     7 tokens (    6.22 ms per token,   160.76 tokens per second)
0.02.101.103 I llama_perf_context_print:        eval time =    1792.60 ms /    63 runs   (   28.45 ms per token,    35.14 tokens per second)
0.02.101.104 I llama_perf_context_print:       total time =    1839.16 ms /    70 tokens
0.02.101.311 I ggml_metal_free: deallocating

real	0m2.423s
user	0m0.148s
sys	0m0.103s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.054 I build: 4486 (0ccd7f3e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.084 I main: llama backend init
0.00.000.087 I main: load the model and apply lora adapter, if any
0.00.009.930 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.026.086 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.026.092 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.026.095 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.026.095 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.026.096 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.026.096 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.026.096 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.026.097 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.026.098 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.026.098 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.026.098 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.026.099 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.026.099 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.026.100 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.026.102 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.026.102 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.026.102 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.029.848 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.030.907 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.034.736 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.034.737 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.034.738 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.034.738 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.034.738 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.034.739 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.034.739 I llama_model_loader: - type  f32:  194 tensors
0.00.034.739 I llama_model_loader: - type q8_0:   98 tensors
0.00.034.740 I print_info: file format = GGUF V3 (latest)
0.00.034.741 I print_info: file type   = Q8_0
0.00.034.742 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.056.359 I load: special tokens cache size = 25
0.00.062.586 I load: token to piece cache size = 0.2984 MB
0.00.062.590 I print_info: arch             = gptneox
0.00.062.590 I print_info: vocab_only       = 0
0.00.062.590 I print_info: n_ctx_train      = 2048
0.00.062.593 I print_info: n_embd           = 2048
0.00.062.593 I print_info: n_layer          = 24
0.00.062.598 I print_info: n_head           = 16
0.00.062.599 I print_info: n_head_kv        = 16
0.00.062.599 I print_info: n_rot            = 32
0.00.062.599 I print_info: n_swa            = 0
0.00.062.600 I print_info: n_embd_head_k    = 128
0.00.062.600 I print_info: n_embd_head_v    = 128
0.00.062.600 I print_info: n_gqa            = 1
0.00.062.601 I print_info: n_embd_k_gqa     = 2048
0.00.062.602 I print_info: n_embd_v_gqa     = 2048
0.00.062.603 I print_info: f_norm_eps       = 1.0e-05
0.00.062.603 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.062.603 I print_info: f_clamp_kqv      = 0.0e+00
0.00.062.603 I print_info: f_max_alibi_bias = 0.0e+00
0.00.062.604 I print_info: f_logit_scale    = 0.0e+00
0.00.062.605 I print_info: n_ff             = 8192
0.00.062.605 I print_info: n_expert         = 0
0.00.062.605 I print_info: n_expert_used    = 0
0.00.062.605 I print_info: causal attn      = 1
0.00.062.605 I print_info: pooling type     = 0
0.00.062.605 I print_info: rope type        = 2
0.00.062.606 I print_info: rope scaling     = linear
0.00.062.606 I print_info: freq_base_train  = 10000.0
0.00.062.607 I print_info: freq_scale_train = 1
0.00.062.609 I print_info: n_ctx_orig_yarn  = 2048
0.00.062.610 I print_info: rope_finetuned   = unknown
0.00.062.610 I print_info: ssm_d_conv       = 0
0.00.062.610 I print_info: ssm_d_inner      = 0
0.00.062.610 I print_info: ssm_d_state      = 0
0.00.062.610 I print_info: ssm_dt_rank      = 0
0.00.062.610 I print_info: ssm_dt_b_c_rms   = 0
0.00.062.610 I print_info: model type       = 1.4B
0.00.062.611 I print_info: model params     = 1.41 B
0.00.062.611 I print_info: general.name     = 1.4B
0.00.062.611 I print_info: vocab type       = BPE
0.00.062.612 I print_info: n_vocab          = 50304
0.00.062.612 I print_info: n_merges         = 50009
0.00.062.613 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.062.613 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.062.613 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.062.613 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.062.614 I print_info: LF token         = 128 'Ä'
0.00.062.614 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.062.614 I print_info: max token length = 1024
0.00.065.059 I load_tensors: offloading 24 repeating layers to GPU
0.00.065.060 I load_tensors: offloading output layer to GPU
0.00.065.060 I load_tensors: offloaded 25/25 layers to GPU
0.00.065.071 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.065.072 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.00.065.411 I llama_init_from_model: n_seq_max     = 1
0.00.065.411 I llama_init_from_model: n_ctx         = 2048
0.00.065.411 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.065.412 I llama_init_from_model: n_batch       = 2048
0.00.065.412 I llama_init_from_model: n_ubatch      = 512
0.00.065.412 I llama_init_from_model: flash_attn    = 0
0.00.065.413 I llama_init_from_model: freq_base     = 10000.0
0.00.065.413 I llama_init_from_model: freq_scale    = 1
0.00.065.413 I ggml_metal_init: allocating
0.00.065.417 I ggml_metal_init: found device: Apple M4
0.00.065.420 I ggml_metal_init: picking default device: Apple M4
0.00.066.192 I ggml_metal_init: using embedded metal library
0.00.068.951 I ggml_metal_init: GPU name:   Apple M4
0.00.068.953 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.068.953 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.068.954 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.068.954 I ggml_metal_init: simdgroup reduction   = true
0.00.068.954 I ggml_metal_init: simdgroup matrix mul. = true
0.00.068.954 I ggml_metal_init: has bfloat            = true
0.00.068.955 I ggml_metal_init: use bfloat            = true
0.00.068.955 I ggml_metal_init: hasUnifiedMemory      = true
0.00.068.956 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.079.329 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.104.495 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.104.508 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.104.534 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.105.789 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.105.792 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.105.792 I llama_init_from_model: graph nodes  = 967
0.00.105.793 I llama_init_from_model: graph splits = 2
0.00.105.798 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.105.920 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.105.921 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.270.251 I main: llama threadpool init, n_threads = 4
0.01.270.287 I 
0.01.270.307 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.270.309 I 
0.01.270.525 I sampler seed: 1234
0.01.270.529 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.270.548 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.270.548 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.270.548 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.358.933 I llama_perf_sampler_print:    sampling time =       1.54 ms /    71 runs   (    0.02 ms per token, 46254.07 tokens per second)
0.02.358.934 I llama_perf_context_print:        load time =    1260.32 ms
0.02.358.934 I llama_perf_context_print: prompt eval time =      42.96 ms /     7 tokens (    6.14 ms per token,   162.95 tokens per second)
0.02.358.935 I llama_perf_context_print:        eval time =    1042.83 ms /    63 runs   (   16.55 ms per token,    60.41 tokens per second)
0.02.358.935 I llama_perf_context_print:       total time =    1088.69 ms /    70 tokens
0.02.359.175 I ggml_metal_free: deallocating

real	0m2.377s
user	0m0.114s
sys	0m0.206s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.055 I build: 4486 (0ccd7f3e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.088 I main: llama backend init
0.00.000.090 I main: load the model and apply lora adapter, if any
0.00.010.616 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.018.559 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.018.564 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.571 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.571 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.572 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.572 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.572 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.573 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.573 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.574 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.574 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.574 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.575 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.575 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.577 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.578 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.578 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.357 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.403 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.124 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.126 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.126 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.126 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.127 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.127 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.027.127 I llama_model_loader: - type  f32:  194 tensors
0.00.027.128 I llama_model_loader: - type q4_0:   97 tensors
0.00.027.128 I llama_model_loader: - type q6_K:    1 tensors
0.00.027.129 I print_info: file format = GGUF V3 (latest)
0.00.027.129 I print_info: file type   = Q4_0
0.00.027.130 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.046.501 I load: special tokens cache size = 25
0.00.052.439 I load: token to piece cache size = 0.2984 MB
0.00.052.442 I print_info: arch             = gptneox
0.00.052.442 I print_info: vocab_only       = 0
0.00.052.442 I print_info: n_ctx_train      = 2048
0.00.052.443 I print_info: n_embd           = 2048
0.00.052.443 I print_info: n_layer          = 24
0.00.052.445 I print_info: n_head           = 16
0.00.052.446 I print_info: n_head_kv        = 16
0.00.052.446 I print_info: n_rot            = 32
0.00.052.446 I print_info: n_swa            = 0
0.00.052.447 I print_info: n_embd_head_k    = 128
0.00.052.448 I print_info: n_embd_head_v    = 128
0.00.052.448 I print_info: n_gqa            = 1
0.00.052.449 I print_info: n_embd_k_gqa     = 2048
0.00.052.451 I print_info: n_embd_v_gqa     = 2048
0.00.052.452 I print_info: f_norm_eps       = 1.0e-05
0.00.052.452 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.052.453 I print_info: f_clamp_kqv      = 0.0e+00
0.00.052.453 I print_info: f_max_alibi_bias = 0.0e+00
0.00.052.453 I print_info: f_logit_scale    = 0.0e+00
0.00.052.454 I print_info: n_ff             = 8192
0.00.052.454 I print_info: n_expert         = 0
0.00.052.454 I print_info: n_expert_used    = 0
0.00.052.454 I print_info: causal attn      = 1
0.00.052.454 I print_info: pooling type     = 0
0.00.052.455 I print_info: rope type        = 2
0.00.052.455 I print_info: rope scaling     = linear
0.00.052.455 I print_info: freq_base_train  = 10000.0
0.00.052.456 I print_info: freq_scale_train = 1
0.00.052.457 I print_info: n_ctx_orig_yarn  = 2048
0.00.052.458 I print_info: rope_finetuned   = unknown
0.00.052.458 I print_info: ssm_d_conv       = 0
0.00.052.458 I print_info: ssm_d_inner      = 0
0.00.052.458 I print_info: ssm_d_state      = 0
0.00.052.458 I print_info: ssm_dt_rank      = 0
0.00.052.458 I print_info: ssm_dt_b_c_rms   = 0
0.00.052.459 I print_info: model type       = 1.4B
0.00.052.459 I print_info: model params     = 1.41 B
0.00.052.459 I print_info: general.name     = 1.4B
0.00.052.460 I print_info: vocab type       = BPE
0.00.052.460 I print_info: n_vocab          = 50304
0.00.052.460 I print_info: n_merges         = 50009
0.00.052.460 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.052.460 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.052.461 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.052.461 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.052.465 I print_info: LF token         = 128 'Ä'
0.00.052.465 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.052.465 I print_info: max token length = 1024
0.00.054.355 I load_tensors: offloading 24 repeating layers to GPU
0.00.054.356 I load_tensors: offloading output layer to GPU
0.00.054.356 I load_tensors: offloaded 25/25 layers to GPU
0.00.054.362 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.054.363 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.054.724 I llama_init_from_model: n_seq_max     = 1
0.00.054.724 I llama_init_from_model: n_ctx         = 2048
0.00.054.725 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.054.725 I llama_init_from_model: n_batch       = 2048
0.00.054.725 I llama_init_from_model: n_ubatch      = 512
0.00.054.725 I llama_init_from_model: flash_attn    = 0
0.00.054.726 I llama_init_from_model: freq_base     = 10000.0
0.00.054.726 I llama_init_from_model: freq_scale    = 1
0.00.054.726 I ggml_metal_init: allocating
0.00.054.730 I ggml_metal_init: found device: Apple M4
0.00.054.732 I ggml_metal_init: picking default device: Apple M4
0.00.055.325 I ggml_metal_init: using embedded metal library
0.00.057.647 I ggml_metal_init: GPU name:   Apple M4
0.00.057.649 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.649 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.650 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.650 I ggml_metal_init: simdgroup reduction   = true
0.00.057.650 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.650 I ggml_metal_init: has bfloat            = true
0.00.057.650 I ggml_metal_init: use bfloat            = true
0.00.057.651 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.658 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.150 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.089.453 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.089.462 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.089.488 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.090.491 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.090.492 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.090.493 I llama_init_from_model: graph nodes  = 967
0.00.090.493 I llama_init_from_model: graph splits = 2
0.00.090.499 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.090.629 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.090.630 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.664.224 I main: llama threadpool init, n_threads = 4
0.00.664.261 I 
0.00.664.279 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.664.279 I 
0.00.664.501 I sampler seed: 1234
0.00.664.505 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.664.534 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.664.535 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.664.535 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.334.655 I llama_perf_sampler_print:    sampling time =       1.16 ms /    71 runs   (    0.02 ms per token, 61049.01 tokens per second)
0.01.334.656 I llama_perf_context_print:        load time =     653.60 ms
0.01.334.657 I llama_perf_context_print: prompt eval time =      39.73 ms /     7 tokens (    5.68 ms per token,   176.17 tokens per second)
0.01.334.659 I llama_perf_context_print:        eval time =     627.44 ms /    63 runs   (    9.96 ms per token,   100.41 tokens per second)
0.01.334.660 I llama_perf_context_print:       total time =     670.43 ms /    70 tokens
0.01.334.889 I ggml_metal_free: deallocating

real	0m1.353s
user	0m0.111s
sys	0m0.142s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4486 (0ccd7f3e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.077 I main: llama backend init
0.00.000.079 I main: load the model and apply lora adapter, if any
0.00.009.622 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.084 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.017.089 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.091 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.092 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.092 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.092 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.093 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.097 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.097 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.097 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.098 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.098 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.099 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.101 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.103 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.104 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.104 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.890 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.892 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.664 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.665 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.665 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.665 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.666 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.666 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.025.666 I llama_model_loader: - type  f32:  194 tensors
0.00.025.667 I llama_model_loader: - type q4_1:   97 tensors
0.00.025.667 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.668 I print_info: file format = GGUF V3 (latest)
0.00.025.668 I print_info: file type   = Q4_1
0.00.025.669 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.045.057 I load: special tokens cache size = 25
0.00.050.980 I load: token to piece cache size = 0.2984 MB
0.00.050.984 I print_info: arch             = gptneox
0.00.050.984 I print_info: vocab_only       = 0
0.00.050.984 I print_info: n_ctx_train      = 2048
0.00.050.984 I print_info: n_embd           = 2048
0.00.050.985 I print_info: n_layer          = 24
0.00.050.988 I print_info: n_head           = 16
0.00.050.989 I print_info: n_head_kv        = 16
0.00.050.989 I print_info: n_rot            = 32
0.00.050.989 I print_info: n_swa            = 0
0.00.050.990 I print_info: n_embd_head_k    = 128
0.00.050.990 I print_info: n_embd_head_v    = 128
0.00.050.990 I print_info: n_gqa            = 1
0.00.050.991 I print_info: n_embd_k_gqa     = 2048
0.00.050.992 I print_info: n_embd_v_gqa     = 2048
0.00.050.993 I print_info: f_norm_eps       = 1.0e-05
0.00.050.993 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.993 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.993 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.993 I print_info: f_logit_scale    = 0.0e+00
0.00.050.994 I print_info: n_ff             = 8192
0.00.050.994 I print_info: n_expert         = 0
0.00.050.994 I print_info: n_expert_used    = 0
0.00.050.994 I print_info: causal attn      = 1
0.00.050.995 I print_info: pooling type     = 0
0.00.050.996 I print_info: rope type        = 2
0.00.050.998 I print_info: rope scaling     = linear
0.00.050.998 I print_info: freq_base_train  = 10000.0
0.00.050.999 I print_info: freq_scale_train = 1
0.00.050.999 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.999 I print_info: rope_finetuned   = unknown
0.00.050.999 I print_info: ssm_d_conv       = 0
0.00.051.000 I print_info: ssm_d_inner      = 0
0.00.051.000 I print_info: ssm_d_state      = 0
0.00.051.000 I print_info: ssm_dt_rank      = 0
0.00.051.000 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.000 I print_info: model type       = 1.4B
0.00.051.001 I print_info: model params     = 1.41 B
0.00.051.001 I print_info: general.name     = 1.4B
0.00.051.001 I print_info: vocab type       = BPE
0.00.051.002 I print_info: n_vocab          = 50304
0.00.051.002 I print_info: n_merges         = 50009
0.00.051.003 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.004 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.004 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.004 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.004 I print_info: LF token         = 128 'Ä'
0.00.051.005 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.005 I print_info: max token length = 1024
0.00.053.010 I load_tensors: offloading 24 repeating layers to GPU
0.00.053.011 I load_tensors: offloading output layer to GPU
0.00.053.011 I load_tensors: offloaded 25/25 layers to GPU
0.00.053.022 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.053.023 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.053.348 I llama_init_from_model: n_seq_max     = 1
0.00.053.349 I llama_init_from_model: n_ctx         = 2048
0.00.053.349 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.053.350 I llama_init_from_model: n_batch       = 2048
0.00.053.350 I llama_init_from_model: n_ubatch      = 512
0.00.053.350 I llama_init_from_model: flash_attn    = 0
0.00.053.350 I llama_init_from_model: freq_base     = 10000.0
0.00.053.350 I llama_init_from_model: freq_scale    = 1
0.00.053.351 I ggml_metal_init: allocating
0.00.053.354 I ggml_metal_init: found device: Apple M4
0.00.053.356 I ggml_metal_init: picking default device: Apple M4
0.00.053.962 I ggml_metal_init: using embedded metal library
0.00.056.335 I ggml_metal_init: GPU name:   Apple M4
0.00.056.336 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.337 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.337 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.337 I ggml_metal_init: simdgroup reduction   = true
0.00.056.338 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.338 I ggml_metal_init: has bfloat            = true
0.00.056.338 I ggml_metal_init: use bfloat            = true
0.00.056.338 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.339 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.687 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.087.921 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.087.927 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.087.946 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.089.096 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.089.097 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.089.098 I llama_init_from_model: graph nodes  = 967
0.00.089.098 I llama_init_from_model: graph splits = 2
0.00.089.101 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.089.228 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.089.229 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.677.304 I main: llama threadpool init, n_threads = 4
0.00.677.345 I 
0.00.677.366 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.677.366 I 
0.00.677.595 I sampler seed: 1234
0.00.677.600 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.677.621 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.677.621 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.677.621 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.402.409 I llama_perf_sampler_print:    sampling time =       1.19 ms /    71 runs   (    0.02 ms per token, 59915.61 tokens per second)
0.01.402.410 I llama_perf_context_print:        load time =     667.68 ms
0.01.402.411 I llama_perf_context_print: prompt eval time =      43.58 ms /     7 tokens (    6.23 ms per token,   160.64 tokens per second)
0.01.402.412 I llama_perf_context_print:        eval time =     678.61 ms /    63 runs   (   10.77 ms per token,    92.84 tokens per second)
0.01.402.412 I llama_perf_context_print:       total time =     725.11 ms /    70 tokens
0.01.402.651 I ggml_metal_free: deallocating

real	0m1.422s
user	0m0.109s
sys	0m0.144s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.056 I build: 4486 (0ccd7f3e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.088 I main: llama backend init
0.00.000.090 I main: load the model and apply lora adapter, if any
0.00.011.276 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.018.653 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.018.658 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.660 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.660 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.661 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.661 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.661 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.663 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.664 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.664 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.664 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.666 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.666 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.667 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.670 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.670 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.671 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.433 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.420 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.129 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.130 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.130 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.130 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.130 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.131 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.027.131 I llama_model_loader: - type  f32:  194 tensors
0.00.027.131 I llama_model_loader: - type q5_0:   97 tensors
0.00.027.132 I llama_model_loader: - type q6_K:    1 tensors
0.00.027.132 I print_info: file format = GGUF V3 (latest)
0.00.027.132 I print_info: file type   = Q5_0
0.00.027.133 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.045.830 I load: special tokens cache size = 25
0.00.051.822 I load: token to piece cache size = 0.2984 MB
0.00.051.824 I print_info: arch             = gptneox
0.00.051.825 I print_info: vocab_only       = 0
0.00.051.825 I print_info: n_ctx_train      = 2048
0.00.051.825 I print_info: n_embd           = 2048
0.00.051.825 I print_info: n_layer          = 24
0.00.051.828 I print_info: n_head           = 16
0.00.051.829 I print_info: n_head_kv        = 16
0.00.051.829 I print_info: n_rot            = 32
0.00.051.830 I print_info: n_swa            = 0
0.00.051.830 I print_info: n_embd_head_k    = 128
0.00.051.830 I print_info: n_embd_head_v    = 128
0.00.051.831 I print_info: n_gqa            = 1
0.00.051.834 I print_info: n_embd_k_gqa     = 2048
0.00.051.834 I print_info: n_embd_v_gqa     = 2048
0.00.051.835 I print_info: f_norm_eps       = 1.0e-05
0.00.051.835 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.835 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.836 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.836 I print_info: f_logit_scale    = 0.0e+00
0.00.051.836 I print_info: n_ff             = 8192
0.00.051.837 I print_info: n_expert         = 0
0.00.051.837 I print_info: n_expert_used    = 0
0.00.051.837 I print_info: causal attn      = 1
0.00.051.837 I print_info: pooling type     = 0
0.00.051.839 I print_info: rope type        = 2
0.00.051.840 I print_info: rope scaling     = linear
0.00.051.841 I print_info: freq_base_train  = 10000.0
0.00.051.841 I print_info: freq_scale_train = 1
0.00.051.841 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.841 I print_info: rope_finetuned   = unknown
0.00.051.841 I print_info: ssm_d_conv       = 0
0.00.051.842 I print_info: ssm_d_inner      = 0
0.00.051.842 I print_info: ssm_d_state      = 0
0.00.051.842 I print_info: ssm_dt_rank      = 0
0.00.051.842 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.842 I print_info: model type       = 1.4B
0.00.051.843 I print_info: model params     = 1.41 B
0.00.051.843 I print_info: general.name     = 1.4B
0.00.051.843 I print_info: vocab type       = BPE
0.00.051.843 I print_info: n_vocab          = 50304
0.00.051.845 I print_info: n_merges         = 50009
0.00.051.845 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.845 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.846 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.846 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.846 I print_info: LF token         = 128 'Ä'
0.00.051.850 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.850 I print_info: max token length = 1024
0.00.053.864 I load_tensors: offloading 24 repeating layers to GPU
0.00.053.864 I load_tensors: offloading output layer to GPU
0.00.053.864 I load_tensors: offloaded 25/25 layers to GPU
0.00.053.875 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.053.876 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.054.168 I llama_init_from_model: n_seq_max     = 1
0.00.054.169 I llama_init_from_model: n_ctx         = 2048
0.00.054.169 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.054.169 I llama_init_from_model: n_batch       = 2048
0.00.054.169 I llama_init_from_model: n_ubatch      = 512
0.00.054.169 I llama_init_from_model: flash_attn    = 0
0.00.054.170 I llama_init_from_model: freq_base     = 10000.0
0.00.054.170 I llama_init_from_model: freq_scale    = 1
0.00.054.170 I ggml_metal_init: allocating
0.00.054.173 I ggml_metal_init: found device: Apple M4
0.00.054.174 I ggml_metal_init: picking default device: Apple M4
0.00.054.781 I ggml_metal_init: using embedded metal library
0.00.057.148 I ggml_metal_init: GPU name:   Apple M4
0.00.057.150 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.150 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.150 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.151 I ggml_metal_init: simdgroup reduction   = true
0.00.057.151 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.151 I ggml_metal_init: has bfloat            = true
0.00.057.151 I ggml_metal_init: use bfloat            = true
0.00.057.151 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.152 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.442 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.087.005 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.087.018 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.087.042 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.088.133 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.088.135 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.088.135 I llama_init_from_model: graph nodes  = 967
0.00.088.136 I llama_init_from_model: graph splits = 2
0.00.088.138 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.088.256 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.088.256 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.739.097 I main: llama threadpool init, n_threads = 4
0.00.739.137 I 
0.00.739.163 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.739.163 I 
0.00.739.387 I sampler seed: 1234
0.00.739.392 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.739.424 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.739.428 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.739.428 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.533.662 I llama_perf_sampler_print:    sampling time =       1.25 ms /    71 runs   (    0.02 ms per token, 56800.00 tokens per second)
0.01.533.662 I llama_perf_context_print:        load time =     727.82 ms
0.01.533.663 I llama_perf_context_print: prompt eval time =      43.13 ms /     7 tokens (    6.16 ms per token,   162.29 tokens per second)
0.01.533.664 I llama_perf_context_print:        eval time =     748.08 ms /    63 runs   (   11.87 ms per token,    84.22 tokens per second)
0.01.533.664 I llama_perf_context_print:       total time =     794.57 ms /    70 tokens
0.01.533.912 I ggml_metal_free: deallocating

real	0m1.553s
user	0m0.109s
sys	0m0.148s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.056 I build: 4486 (0ccd7f3e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.087 I main: llama backend init
0.00.000.089 I main: load the model and apply lora adapter, if any
0.00.013.435 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.020.529 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.020.535 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.020.538 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.020.538 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.020.539 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.020.544 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.020.545 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.020.547 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.020.548 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.020.548 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.020.548 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.020.549 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.020.549 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.020.549 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.020.551 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.020.551 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.020.553 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.025.011 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.026.095 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.030.067 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.030.068 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.030.068 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.030.069 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.030.069 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.030.069 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.030.070 I llama_model_loader: - type  f32:  194 tensors
0.00.030.070 I llama_model_loader: - type q5_1:   97 tensors
0.00.030.070 I llama_model_loader: - type q6_K:    1 tensors
0.00.030.071 I print_info: file format = GGUF V3 (latest)
0.00.030.072 I print_info: file type   = Q5_1
0.00.030.073 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.051.268 I load: special tokens cache size = 25
0.00.058.257 I load: token to piece cache size = 0.2984 MB
0.00.058.260 I print_info: arch             = gptneox
0.00.058.260 I print_info: vocab_only       = 0
0.00.058.261 I print_info: n_ctx_train      = 2048
0.00.058.261 I print_info: n_embd           = 2048
0.00.058.261 I print_info: n_layer          = 24
0.00.058.264 I print_info: n_head           = 16
0.00.058.265 I print_info: n_head_kv        = 16
0.00.058.265 I print_info: n_rot            = 32
0.00.058.265 I print_info: n_swa            = 0
0.00.058.265 I print_info: n_embd_head_k    = 128
0.00.058.265 I print_info: n_embd_head_v    = 128
0.00.058.266 I print_info: n_gqa            = 1
0.00.058.267 I print_info: n_embd_k_gqa     = 2048
0.00.058.267 I print_info: n_embd_v_gqa     = 2048
0.00.058.268 I print_info: f_norm_eps       = 1.0e-05
0.00.058.268 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.058.269 I print_info: f_clamp_kqv      = 0.0e+00
0.00.058.270 I print_info: f_max_alibi_bias = 0.0e+00
0.00.058.272 I print_info: f_logit_scale    = 0.0e+00
0.00.058.272 I print_info: n_ff             = 8192
0.00.058.272 I print_info: n_expert         = 0
0.00.058.273 I print_info: n_expert_used    = 0
0.00.058.273 I print_info: causal attn      = 1
0.00.058.273 I print_info: pooling type     = 0
0.00.058.273 I print_info: rope type        = 2
0.00.058.273 I print_info: rope scaling     = linear
0.00.058.274 I print_info: freq_base_train  = 10000.0
0.00.058.274 I print_info: freq_scale_train = 1
0.00.058.274 I print_info: n_ctx_orig_yarn  = 2048
0.00.058.274 I print_info: rope_finetuned   = unknown
0.00.058.275 I print_info: ssm_d_conv       = 0
0.00.058.275 I print_info: ssm_d_inner      = 0
0.00.058.275 I print_info: ssm_d_state      = 0
0.00.058.275 I print_info: ssm_dt_rank      = 0
0.00.058.275 I print_info: ssm_dt_b_c_rms   = 0
0.00.058.277 I print_info: model type       = 1.4B
0.00.058.277 I print_info: model params     = 1.41 B
0.00.058.277 I print_info: general.name     = 1.4B
0.00.058.278 I print_info: vocab type       = BPE
0.00.058.278 I print_info: n_vocab          = 50304
0.00.058.278 I print_info: n_merges         = 50009
0.00.058.279 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.058.279 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.058.279 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.058.279 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.058.279 I print_info: LF token         = 128 'Ä'
0.00.058.279 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.058.280 I print_info: max token length = 1024
0.00.060.538 I load_tensors: offloading 24 repeating layers to GPU
0.00.060.538 I load_tensors: offloading output layer to GPU
0.00.060.538 I load_tensors: offloaded 25/25 layers to GPU
0.00.060.549 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.060.550 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.060.840 I llama_init_from_model: n_seq_max     = 1
0.00.060.841 I llama_init_from_model: n_ctx         = 2048
0.00.060.841 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.060.842 I llama_init_from_model: n_batch       = 2048
0.00.060.842 I llama_init_from_model: n_ubatch      = 512
0.00.060.842 I llama_init_from_model: flash_attn    = 0
0.00.060.842 I llama_init_from_model: freq_base     = 10000.0
0.00.060.843 I llama_init_from_model: freq_scale    = 1
0.00.060.843 I ggml_metal_init: allocating
0.00.060.846 I ggml_metal_init: found device: Apple M4
0.00.060.848 I ggml_metal_init: picking default device: Apple M4
0.00.061.548 I ggml_metal_init: using embedded metal library
0.00.064.440 I ggml_metal_init: GPU name:   Apple M4
0.00.064.442 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.064.442 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.064.443 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.064.443 I ggml_metal_init: simdgroup reduction   = true
0.00.064.443 I ggml_metal_init: simdgroup matrix mul. = true
0.00.064.443 I ggml_metal_init: has bfloat            = true
0.00.064.444 I ggml_metal_init: use bfloat            = true
0.00.064.444 I ggml_metal_init: hasUnifiedMemory      = true
0.00.064.445 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.074.651 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.095.289 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.095.296 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.095.314 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.096.288 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.096.290 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.096.290 I llama_init_from_model: graph nodes  = 967
0.00.096.290 I llama_init_from_model: graph splits = 2
0.00.096.295 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.096.428 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.096.429 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.721.236 I main: llama threadpool init, n_threads = 4
0.00.721.302 I 
0.00.721.323 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.721.324 I 
0.00.721.575 I sampler seed: 1234
0.00.721.580 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.721.601 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.721.601 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.721.601 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.569.451 I llama_perf_sampler_print:    sampling time =       1.31 ms /    71 runs   (    0.02 ms per token, 54198.47 tokens per second)
0.01.569.452 I llama_perf_context_print:        load time =     707.80 ms
0.01.569.452 I llama_perf_context_print: prompt eval time =      49.90 ms /     7 tokens (    7.13 ms per token,   140.27 tokens per second)
0.01.569.454 I llama_perf_context_print:        eval time =     795.20 ms /    63 runs   (   12.62 ms per token,    79.23 tokens per second)
0.01.569.454 I llama_perf_context_print:       total time =     848.22 ms /    70 tokens
0.01.569.689 I ggml_metal_free: deallocating

real	0m1.608s
user	0m0.115s
sys	0m0.171s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4486 (0ccd7f3e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.078 I main: llama backend init
0.00.000.080 I main: load the model and apply lora adapter, if any
0.00.010.812 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.340 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.017.345 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.346 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.347 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.347 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.347 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.348 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.348 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.349 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.349 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.350 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.350 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.350 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.351 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.353 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.353 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.353 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.076 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.049 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.771 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.772 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.772 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.772 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.773 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.773 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.774 I llama_model_loader: - type  f32:  194 tensors
0.00.025.774 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.774 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.774 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.775 I print_info: file format = GGUF V3 (latest)
0.00.025.775 I print_info: file type   = Q2_K - Medium
0.00.025.776 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.044.350 I load: special tokens cache size = 25
0.00.050.349 I load: token to piece cache size = 0.2984 MB
0.00.050.351 I print_info: arch             = gptneox
0.00.050.352 I print_info: vocab_only       = 0
0.00.050.352 I print_info: n_ctx_train      = 2048
0.00.050.352 I print_info: n_embd           = 2048
0.00.050.352 I print_info: n_layer          = 24
0.00.050.355 I print_info: n_head           = 16
0.00.050.356 I print_info: n_head_kv        = 16
0.00.050.356 I print_info: n_rot            = 32
0.00.050.356 I print_info: n_swa            = 0
0.00.050.356 I print_info: n_embd_head_k    = 128
0.00.050.359 I print_info: n_embd_head_v    = 128
0.00.050.359 I print_info: n_gqa            = 1
0.00.050.360 I print_info: n_embd_k_gqa     = 2048
0.00.050.361 I print_info: n_embd_v_gqa     = 2048
0.00.050.361 I print_info: f_norm_eps       = 1.0e-05
0.00.050.362 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.362 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.362 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.362 I print_info: f_logit_scale    = 0.0e+00
0.00.050.363 I print_info: n_ff             = 8192
0.00.050.363 I print_info: n_expert         = 0
0.00.050.363 I print_info: n_expert_used    = 0
0.00.050.363 I print_info: causal attn      = 1
0.00.050.363 I print_info: pooling type     = 0
0.00.050.363 I print_info: rope type        = 2
0.00.050.364 I print_info: rope scaling     = linear
0.00.050.364 I print_info: freq_base_train  = 10000.0
0.00.050.370 I print_info: freq_scale_train = 1
0.00.050.372 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.372 I print_info: rope_finetuned   = unknown
0.00.050.372 I print_info: ssm_d_conv       = 0
0.00.050.372 I print_info: ssm_d_inner      = 0
0.00.050.373 I print_info: ssm_d_state      = 0
0.00.050.373 I print_info: ssm_dt_rank      = 0
0.00.050.373 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.373 I print_info: model type       = 1.4B
0.00.050.374 I print_info: model params     = 1.41 B
0.00.050.374 I print_info: general.name     = 1.4B
0.00.050.374 I print_info: vocab type       = BPE
0.00.050.375 I print_info: n_vocab          = 50304
0.00.050.375 I print_info: n_merges         = 50009
0.00.050.375 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.375 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.375 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.381 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.383 I print_info: LF token         = 128 'Ä'
0.00.050.383 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.383 I print_info: max token length = 1024
0.00.052.208 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.208 I load_tensors: offloading output layer to GPU
0.00.052.209 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.219 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.052.221 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.052.502 I llama_init_from_model: n_seq_max     = 1
0.00.052.503 I llama_init_from_model: n_ctx         = 2048
0.00.052.504 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.052.504 I llama_init_from_model: n_batch       = 2048
0.00.052.504 I llama_init_from_model: n_ubatch      = 512
0.00.052.504 I llama_init_from_model: flash_attn    = 0
0.00.052.504 I llama_init_from_model: freq_base     = 10000.0
0.00.052.505 I llama_init_from_model: freq_scale    = 1
0.00.052.505 I ggml_metal_init: allocating
0.00.052.507 I ggml_metal_init: found device: Apple M4
0.00.052.509 I ggml_metal_init: picking default device: Apple M4
0.00.053.150 I ggml_metal_init: using embedded metal library
0.00.055.491 I ggml_metal_init: GPU name:   Apple M4
0.00.055.492 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.493 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.493 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.493 I ggml_metal_init: simdgroup reduction   = true
0.00.055.493 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.494 I ggml_metal_init: has bfloat            = true
0.00.055.494 I ggml_metal_init: use bfloat            = true
0.00.055.494 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.495 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.862 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.084.041 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.046 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.067 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.085.003 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.085.005 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.085.005 I llama_init_from_model: graph nodes  = 967
0.00.085.005 I llama_init_from_model: graph splits = 2
0.00.085.009 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.085.137 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.085.138 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.435.917 I main: llama threadpool init, n_threads = 4
0.00.435.962 I 
0.00.435.986 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.435.987 I 
0.00.436.205 I sampler seed: 1234
0.00.436.211 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.436.247 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.436.251 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.436.251 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.116.141 I llama_perf_sampler_print:    sampling time =       1.19 ms /    71 runs   (    0.02 ms per token, 59613.77 tokens per second)
0.01.116.142 I llama_perf_context_print:        load time =     425.10 ms
0.01.116.143 I llama_perf_context_print: prompt eval time =      35.76 ms /     7 tokens (    5.11 ms per token,   195.74 tokens per second)
0.01.116.143 I llama_perf_context_print:        eval time =     641.14 ms /    63 runs   (   10.18 ms per token,    98.26 tokens per second)
0.01.116.147 I llama_perf_context_print:       total time =     680.23 ms /    70 tokens
0.01.116.376 I ggml_metal_free: deallocating

real	0m1.136s
user	0m0.107s
sys	0m0.107s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.049 I build: 4486 (0ccd7f3e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.076 I main: llama backend init
0.00.000.078 I main: load the model and apply lora adapter, if any
0.00.009.514 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.958 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.964 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.967 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.968 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.968 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.968 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.969 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.972 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.972 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.972 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.973 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.973 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.977 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.977 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.980 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.982 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.983 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.702 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.686 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.360 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.361 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.361 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.362 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.362 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.362 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.363 I llama_model_loader: - type  f32:  194 tensors
0.00.025.363 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.363 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.363 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.363 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.364 I print_info: file format = GGUF V3 (latest)
0.00.025.364 I print_info: file type   = Q3_K - Medium
0.00.025.365 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.043.895 I load: special tokens cache size = 25
0.00.049.914 I load: token to piece cache size = 0.2984 MB
0.00.049.917 I print_info: arch             = gptneox
0.00.049.917 I print_info: vocab_only       = 0
0.00.049.917 I print_info: n_ctx_train      = 2048
0.00.049.917 I print_info: n_embd           = 2048
0.00.049.917 I print_info: n_layer          = 24
0.00.049.920 I print_info: n_head           = 16
0.00.049.921 I print_info: n_head_kv        = 16
0.00.049.921 I print_info: n_rot            = 32
0.00.049.921 I print_info: n_swa            = 0
0.00.049.921 I print_info: n_embd_head_k    = 128
0.00.049.921 I print_info: n_embd_head_v    = 128
0.00.049.922 I print_info: n_gqa            = 1
0.00.049.923 I print_info: n_embd_k_gqa     = 2048
0.00.049.923 I print_info: n_embd_v_gqa     = 2048
0.00.049.924 I print_info: f_norm_eps       = 1.0e-05
0.00.049.924 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.924 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.925 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.925 I print_info: f_logit_scale    = 0.0e+00
0.00.049.926 I print_info: n_ff             = 8192
0.00.049.926 I print_info: n_expert         = 0
0.00.049.926 I print_info: n_expert_used    = 0
0.00.049.928 I print_info: causal attn      = 1
0.00.049.929 I print_info: pooling type     = 0
0.00.049.929 I print_info: rope type        = 2
0.00.049.929 I print_info: rope scaling     = linear
0.00.049.929 I print_info: freq_base_train  = 10000.0
0.00.049.930 I print_info: freq_scale_train = 1
0.00.049.930 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.930 I print_info: rope_finetuned   = unknown
0.00.049.930 I print_info: ssm_d_conv       = 0
0.00.049.930 I print_info: ssm_d_inner      = 0
0.00.049.930 I print_info: ssm_d_state      = 0
0.00.049.930 I print_info: ssm_dt_rank      = 0
0.00.049.931 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.931 I print_info: model type       = 1.4B
0.00.049.931 I print_info: model params     = 1.41 B
0.00.049.931 I print_info: general.name     = 1.4B
0.00.049.932 I print_info: vocab type       = BPE
0.00.049.932 I print_info: n_vocab          = 50304
0.00.049.932 I print_info: n_merges         = 50009
0.00.049.933 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.934 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.934 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.934 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.934 I print_info: LF token         = 128 'Ä'
0.00.049.934 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.934 I print_info: max token length = 1024
0.00.051.841 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.841 I load_tensors: offloading output layer to GPU
0.00.051.841 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.851 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.051.853 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.052.137 I llama_init_from_model: n_seq_max     = 1
0.00.052.137 I llama_init_from_model: n_ctx         = 2048
0.00.052.137 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.052.137 I llama_init_from_model: n_batch       = 2048
0.00.052.138 I llama_init_from_model: n_ubatch      = 512
0.00.052.138 I llama_init_from_model: flash_attn    = 0
0.00.052.138 I llama_init_from_model: freq_base     = 10000.0
0.00.052.138 I llama_init_from_model: freq_scale    = 1
0.00.052.139 I ggml_metal_init: allocating
0.00.052.142 I ggml_metal_init: found device: Apple M4
0.00.052.144 I ggml_metal_init: picking default device: Apple M4
0.00.052.741 I ggml_metal_init: using embedded metal library
0.00.055.056 I ggml_metal_init: GPU name:   Apple M4
0.00.055.058 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.058 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.059 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.059 I ggml_metal_init: simdgroup reduction   = true
0.00.055.059 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.059 I ggml_metal_init: has bfloat            = true
0.00.055.059 I ggml_metal_init: use bfloat            = true
0.00.055.060 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.060 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.526 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.085.333 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.339 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.359 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.086.439 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.086.440 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.086.440 I llama_init_from_model: graph nodes  = 967
0.00.086.440 I llama_init_from_model: graph splits = 2
0.00.086.443 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.086.572 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.086.572 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.530.144 I main: llama threadpool init, n_threads = 4
0.00.530.187 I 
0.00.530.207 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.530.207 I 
0.00.530.442 I sampler seed: 1234
0.00.530.446 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.530.474 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.530.475 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.530.475 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.300.692 I llama_perf_sampler_print:    sampling time =       1.23 ms /    71 runs   (    0.02 ms per token, 57536.47 tokens per second)
0.01.300.693 I llama_perf_context_print:        load time =     520.62 ms
0.01.300.694 I llama_perf_context_print: prompt eval time =      43.58 ms /     7 tokens (    6.23 ms per token,   160.64 tokens per second)
0.01.300.695 I llama_perf_context_print:        eval time =     723.78 ms /    63 runs   (   11.49 ms per token,    87.04 tokens per second)
0.01.300.695 I llama_perf_context_print:       total time =     770.55 ms /    70 tokens
0.01.300.960 I ggml_metal_free: deallocating

real	0m1.316s
user	0m0.108s
sys	0m0.123s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.055 I build: 4486 (0ccd7f3e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.085 I main: llama backend init
0.00.000.088 I main: load the model and apply lora adapter, if any
0.00.011.855 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.019.582 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.019.589 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.595 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.596 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.596 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.597 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.597 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.598 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.598 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.599 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.599 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.599 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.600 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.600 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.602 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.603 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.603 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.310 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.337 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.028.129 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.028.130 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.028.131 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.028.131 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.028.131 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.028.132 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.028.132 I llama_model_loader: - type  f32:  194 tensors
0.00.028.133 I llama_model_loader: - type q4_K:   61 tensors
0.00.028.133 I llama_model_loader: - type q5_K:   24 tensors
0.00.028.133 I llama_model_loader: - type q6_K:   13 tensors
0.00.028.134 I print_info: file format = GGUF V3 (latest)
0.00.028.135 I print_info: file type   = Q4_K - Medium
0.00.028.136 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.047.830 I load: special tokens cache size = 25
0.00.053.884 I load: token to piece cache size = 0.2984 MB
0.00.053.890 I print_info: arch             = gptneox
0.00.053.890 I print_info: vocab_only       = 0
0.00.053.890 I print_info: n_ctx_train      = 2048
0.00.053.890 I print_info: n_embd           = 2048
0.00.053.892 I print_info: n_layer          = 24
0.00.053.897 I print_info: n_head           = 16
0.00.053.898 I print_info: n_head_kv        = 16
0.00.053.898 I print_info: n_rot            = 32
0.00.053.898 I print_info: n_swa            = 0
0.00.053.899 I print_info: n_embd_head_k    = 128
0.00.053.899 I print_info: n_embd_head_v    = 128
0.00.053.901 I print_info: n_gqa            = 1
0.00.053.901 I print_info: n_embd_k_gqa     = 2048
0.00.053.902 I print_info: n_embd_v_gqa     = 2048
0.00.053.903 I print_info: f_norm_eps       = 1.0e-05
0.00.053.903 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.053.903 I print_info: f_clamp_kqv      = 0.0e+00
0.00.053.903 I print_info: f_max_alibi_bias = 0.0e+00
0.00.053.904 I print_info: f_logit_scale    = 0.0e+00
0.00.053.904 I print_info: n_ff             = 8192
0.00.053.904 I print_info: n_expert         = 0
0.00.053.904 I print_info: n_expert_used    = 0
0.00.053.905 I print_info: causal attn      = 1
0.00.053.905 I print_info: pooling type     = 0
0.00.053.906 I print_info: rope type        = 2
0.00.053.906 I print_info: rope scaling     = linear
0.00.053.907 I print_info: freq_base_train  = 10000.0
0.00.053.907 I print_info: freq_scale_train = 1
0.00.053.907 I print_info: n_ctx_orig_yarn  = 2048
0.00.053.907 I print_info: rope_finetuned   = unknown
0.00.053.908 I print_info: ssm_d_conv       = 0
0.00.053.908 I print_info: ssm_d_inner      = 0
0.00.053.908 I print_info: ssm_d_state      = 0
0.00.053.908 I print_info: ssm_dt_rank      = 0
0.00.053.909 I print_info: ssm_dt_b_c_rms   = 0
0.00.053.909 I print_info: model type       = 1.4B
0.00.053.910 I print_info: model params     = 1.41 B
0.00.053.911 I print_info: general.name     = 1.4B
0.00.053.912 I print_info: vocab type       = BPE
0.00.053.912 I print_info: n_vocab          = 50304
0.00.053.912 I print_info: n_merges         = 50009
0.00.053.912 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.053.912 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.053.912 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.053.913 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.053.914 I print_info: LF token         = 128 'Ä'
0.00.053.914 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.053.915 I print_info: max token length = 1024
0.00.055.744 I load_tensors: offloading 24 repeating layers to GPU
0.00.055.744 I load_tensors: offloading output layer to GPU
0.00.055.744 I load_tensors: offloaded 25/25 layers to GPU
0.00.055.755 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.055.756 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.056.083 I llama_init_from_model: n_seq_max     = 1
0.00.056.084 I llama_init_from_model: n_ctx         = 2048
0.00.056.084 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.056.084 I llama_init_from_model: n_batch       = 2048
0.00.056.084 I llama_init_from_model: n_ubatch      = 512
0.00.056.085 I llama_init_from_model: flash_attn    = 0
0.00.056.085 I llama_init_from_model: freq_base     = 10000.0
0.00.056.085 I llama_init_from_model: freq_scale    = 1
0.00.056.086 I ggml_metal_init: allocating
0.00.056.089 I ggml_metal_init: found device: Apple M4
0.00.056.092 I ggml_metal_init: picking default device: Apple M4
0.00.056.767 I ggml_metal_init: using embedded metal library
0.00.059.821 I ggml_metal_init: GPU name:   Apple M4
0.00.059.823 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.059.823 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.059.824 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.059.824 I ggml_metal_init: simdgroup reduction   = true
0.00.059.824 I ggml_metal_init: simdgroup matrix mul. = true
0.00.059.824 I ggml_metal_init: has bfloat            = true
0.00.059.824 I ggml_metal_init: use bfloat            = true
0.00.059.825 I ggml_metal_init: hasUnifiedMemory      = true
0.00.059.826 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.069.213 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.091.253 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.091.258 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.091.279 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.092.217 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.092.218 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.092.219 I llama_init_from_model: graph nodes  = 967
0.00.092.219 I llama_init_from_model: graph splits = 2
0.00.092.222 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.092.342 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.092.343 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.614.143 I main: llama threadpool init, n_threads = 4
0.00.614.191 I 
0.00.614.224 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.614.226 I 
0.00.614.384 I sampler seed: 1234
0.00.614.388 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.614.442 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.614.446 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.614.447 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.404.846 I llama_perf_sampler_print:    sampling time =       1.27 ms /    71 runs   (    0.02 ms per token, 55905.51 tokens per second)
0.01.404.847 I llama_perf_context_print:        load time =     602.28 ms
0.01.404.848 I llama_perf_context_print: prompt eval time =      47.23 ms /     7 tokens (    6.75 ms per token,   148.22 tokens per second)
0.01.404.848 I llama_perf_context_print:        eval time =     740.08 ms /    63 runs   (   11.75 ms per token,    85.13 tokens per second)
0.01.404.848 I llama_perf_context_print:       total time =     790.71 ms /    70 tokens
0.01.405.039 I ggml_metal_free: deallocating

real	0m1.424s
user	0m0.111s
sys	0m0.138s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4486 (0ccd7f3e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.078 I main: llama backend init
0.00.000.080 I main: load the model and apply lora adapter, if any
0.00.013.897 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.021.354 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.021.359 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.021.361 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.021.361 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.021.362 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.021.362 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.021.362 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.021.364 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.021.364 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.021.365 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.021.365 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.021.366 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.021.366 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.021.368 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.021.370 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.021.370 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.021.371 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.025.174 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.026.175 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.029.934 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.029.935 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.029.936 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.029.936 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.029.936 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.029.937 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.029.937 I llama_model_loader: - type  f32:  194 tensors
0.00.029.937 I llama_model_loader: - type q5_K:   61 tensors
0.00.029.938 I llama_model_loader: - type q6_K:   37 tensors
0.00.029.938 I print_info: file format = GGUF V3 (latest)
0.00.029.939 I print_info: file type   = Q5_K - Medium
0.00.029.940 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.049.291 I load: special tokens cache size = 25
0.00.055.332 I load: token to piece cache size = 0.2984 MB
0.00.055.335 I print_info: arch             = gptneox
0.00.055.335 I print_info: vocab_only       = 0
0.00.055.336 I print_info: n_ctx_train      = 2048
0.00.055.336 I print_info: n_embd           = 2048
0.00.055.336 I print_info: n_layer          = 24
0.00.055.339 I print_info: n_head           = 16
0.00.055.340 I print_info: n_head_kv        = 16
0.00.055.342 I print_info: n_rot            = 32
0.00.055.342 I print_info: n_swa            = 0
0.00.055.342 I print_info: n_embd_head_k    = 128
0.00.055.343 I print_info: n_embd_head_v    = 128
0.00.055.343 I print_info: n_gqa            = 1
0.00.055.344 I print_info: n_embd_k_gqa     = 2048
0.00.055.345 I print_info: n_embd_v_gqa     = 2048
0.00.055.350 I print_info: f_norm_eps       = 1.0e-05
0.00.055.350 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.055.350 I print_info: f_clamp_kqv      = 0.0e+00
0.00.055.351 I print_info: f_max_alibi_bias = 0.0e+00
0.00.055.351 I print_info: f_logit_scale    = 0.0e+00
0.00.055.352 I print_info: n_ff             = 8192
0.00.055.352 I print_info: n_expert         = 0
0.00.055.352 I print_info: n_expert_used    = 0
0.00.055.352 I print_info: causal attn      = 1
0.00.055.352 I print_info: pooling type     = 0
0.00.055.357 I print_info: rope type        = 2
0.00.055.358 I print_info: rope scaling     = linear
0.00.055.359 I print_info: freq_base_train  = 10000.0
0.00.055.359 I print_info: freq_scale_train = 1
0.00.055.359 I print_info: n_ctx_orig_yarn  = 2048
0.00.055.360 I print_info: rope_finetuned   = unknown
0.00.055.360 I print_info: ssm_d_conv       = 0
0.00.055.360 I print_info: ssm_d_inner      = 0
0.00.055.360 I print_info: ssm_d_state      = 0
0.00.055.360 I print_info: ssm_dt_rank      = 0
0.00.055.360 I print_info: ssm_dt_b_c_rms   = 0
0.00.055.360 I print_info: model type       = 1.4B
0.00.055.361 I print_info: model params     = 1.41 B
0.00.055.361 I print_info: general.name     = 1.4B
0.00.055.361 I print_info: vocab type       = BPE
0.00.055.362 I print_info: n_vocab          = 50304
0.00.055.362 I print_info: n_merges         = 50009
0.00.055.362 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.055.362 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.055.362 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.055.363 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.055.363 I print_info: LF token         = 128 'Ä'
0.00.055.366 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.055.366 I print_info: max token length = 1024
0.00.057.050 I load_tensors: offloading 24 repeating layers to GPU
0.00.057.050 I load_tensors: offloading output layer to GPU
0.00.057.051 I load_tensors: offloaded 25/25 layers to GPU
0.00.057.061 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.057.062 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.057.355 I llama_init_from_model: n_seq_max     = 1
0.00.057.356 I llama_init_from_model: n_ctx         = 2048
0.00.057.356 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.057.356 I llama_init_from_model: n_batch       = 2048
0.00.057.357 I llama_init_from_model: n_ubatch      = 512
0.00.057.357 I llama_init_from_model: flash_attn    = 0
0.00.057.357 I llama_init_from_model: freq_base     = 10000.0
0.00.057.357 I llama_init_from_model: freq_scale    = 1
0.00.057.358 I ggml_metal_init: allocating
0.00.057.361 I ggml_metal_init: found device: Apple M4
0.00.057.363 I ggml_metal_init: picking default device: Apple M4
0.00.057.950 I ggml_metal_init: using embedded metal library
0.00.060.290 I ggml_metal_init: GPU name:   Apple M4
0.00.060.291 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.060.291 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.060.292 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.060.292 I ggml_metal_init: simdgroup reduction   = true
0.00.060.292 I ggml_metal_init: simdgroup matrix mul. = true
0.00.060.292 I ggml_metal_init: has bfloat            = true
0.00.060.292 I ggml_metal_init: use bfloat            = true
0.00.060.293 I ggml_metal_init: hasUnifiedMemory      = true
0.00.060.293 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.070.466 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.090.078 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.090.083 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.090.102 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.091.257 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.091.258 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.091.258 I llama_init_from_model: graph nodes  = 967
0.00.091.259 I llama_init_from_model: graph splits = 2
0.00.091.262 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.091.410 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.091.410 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.693.529 I main: llama threadpool init, n_threads = 4
0.00.693.576 I 
0.00.693.603 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.693.603 I 
0.00.693.761 I sampler seed: 1234
0.00.693.766 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.693.787 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.693.788 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.693.788 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.574.803 I llama_perf_sampler_print:    sampling time =       1.16 ms /    71 runs   (    0.02 ms per token, 61471.86 tokens per second)
0.01.574.803 I llama_perf_context_print:        load time =     679.63 ms
0.01.574.804 I llama_perf_context_print: prompt eval time =      51.67 ms /     7 tokens (    7.38 ms per token,   135.48 tokens per second)
0.01.574.805 I llama_perf_context_print:        eval time =     826.39 ms /    63 runs   (   13.12 ms per token,    76.24 tokens per second)
0.01.574.806 I llama_perf_context_print:       total time =     881.28 ms /    70 tokens
0.01.575.033 I ggml_metal_free: deallocating

real	0m1.593s
user	0m0.110s
sys	0m0.150s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4486 (0ccd7f3e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.074 I main: llama backend init
0.00.000.076 I main: load the model and apply lora adapter, if any
0.00.009.729 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.953 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.017.957 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.959 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.963 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.963 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.965 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.966 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.967 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.970 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.971 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.971 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.971 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.972 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.972 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.975 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.975 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.975 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.714 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.740 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.441 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.442 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.442 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.442 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.443 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.443 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.026.443 I llama_model_loader: - type  f32:  194 tensors
0.00.026.444 I llama_model_loader: - type q6_K:   98 tensors
0.00.026.444 I print_info: file format = GGUF V3 (latest)
0.00.026.444 I print_info: file type   = Q6_K
0.00.026.445 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.045.061 I load: special tokens cache size = 25
0.00.050.790 I load: token to piece cache size = 0.2984 MB
0.00.050.793 I print_info: arch             = gptneox
0.00.050.793 I print_info: vocab_only       = 0
0.00.050.794 I print_info: n_ctx_train      = 2048
0.00.050.794 I print_info: n_embd           = 2048
0.00.050.794 I print_info: n_layer          = 24
0.00.050.797 I print_info: n_head           = 16
0.00.050.798 I print_info: n_head_kv        = 16
0.00.050.798 I print_info: n_rot            = 32
0.00.050.798 I print_info: n_swa            = 0
0.00.050.799 I print_info: n_embd_head_k    = 128
0.00.050.799 I print_info: n_embd_head_v    = 128
0.00.050.799 I print_info: n_gqa            = 1
0.00.050.800 I print_info: n_embd_k_gqa     = 2048
0.00.050.801 I print_info: n_embd_v_gqa     = 2048
0.00.050.802 I print_info: f_norm_eps       = 1.0e-05
0.00.050.802 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.802 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.802 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.802 I print_info: f_logit_scale    = 0.0e+00
0.00.050.803 I print_info: n_ff             = 8192
0.00.050.803 I print_info: n_expert         = 0
0.00.050.803 I print_info: n_expert_used    = 0
0.00.050.804 I print_info: causal attn      = 1
0.00.050.804 I print_info: pooling type     = 0
0.00.050.804 I print_info: rope type        = 2
0.00.050.805 I print_info: rope scaling     = linear
0.00.050.805 I print_info: freq_base_train  = 10000.0
0.00.050.806 I print_info: freq_scale_train = 1
0.00.050.806 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.806 I print_info: rope_finetuned   = unknown
0.00.050.806 I print_info: ssm_d_conv       = 0
0.00.050.806 I print_info: ssm_d_inner      = 0
0.00.050.807 I print_info: ssm_d_state      = 0
0.00.050.807 I print_info: ssm_dt_rank      = 0
0.00.050.807 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.807 I print_info: model type       = 1.4B
0.00.050.808 I print_info: model params     = 1.41 B
0.00.050.808 I print_info: general.name     = 1.4B
0.00.050.808 I print_info: vocab type       = BPE
0.00.050.808 I print_info: n_vocab          = 50304
0.00.050.808 I print_info: n_merges         = 50009
0.00.050.809 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.809 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.809 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.809 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.810 I print_info: LF token         = 128 'Ä'
0.00.050.810 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.810 I print_info: max token length = 1024
0.00.052.433 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.433 I load_tensors: offloading output layer to GPU
0.00.052.433 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.443 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.052.445 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.052.732 I llama_init_from_model: n_seq_max     = 1
0.00.052.733 I llama_init_from_model: n_ctx         = 2048
0.00.052.733 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.052.733 I llama_init_from_model: n_batch       = 2048
0.00.052.733 I llama_init_from_model: n_ubatch      = 512
0.00.052.734 I llama_init_from_model: flash_attn    = 0
0.00.052.734 I llama_init_from_model: freq_base     = 10000.0
0.00.052.734 I llama_init_from_model: freq_scale    = 1
0.00.052.735 I ggml_metal_init: allocating
0.00.052.738 I ggml_metal_init: found device: Apple M4
0.00.052.740 I ggml_metal_init: picking default device: Apple M4
0.00.053.336 I ggml_metal_init: using embedded metal library
0.00.055.655 I ggml_metal_init: GPU name:   Apple M4
0.00.055.656 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.656 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.657 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.657 I ggml_metal_init: simdgroup reduction   = true
0.00.055.657 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.657 I ggml_metal_init: has bfloat            = true
0.00.055.657 I ggml_metal_init: use bfloat            = true
0.00.055.658 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.658 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.496 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.089.841 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.089.852 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.089.882 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.090.964 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.090.965 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.090.966 I llama_init_from_model: graph nodes  = 967
0.00.090.966 I llama_init_from_model: graph splits = 2
0.00.090.971 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.091.104 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.091.105 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.775.572 I main: llama threadpool init, n_threads = 4
0.00.775.608 I 
0.00.775.627 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.775.627 I 
0.00.775.855 I sampler seed: 1234
0.00.775.860 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.775.914 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.775.915 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.775.915 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.649.491 I llama_perf_sampler_print:    sampling time =       1.24 ms /    71 runs   (    0.02 ms per token, 57119.87 tokens per second)
0.01.649.492 I llama_perf_context_print:        load time =     765.84 ms
0.01.649.493 I llama_perf_context_print: prompt eval time =      54.48 ms /     7 tokens (    7.78 ms per token,   128.50 tokens per second)
0.01.649.494 I llama_perf_context_print:        eval time =     816.07 ms /    63 runs   (   12.95 ms per token,    77.20 tokens per second)
0.01.649.494 I llama_perf_context_print:       total time =     873.92 ms /    70 tokens
0.01.649.741 I ggml_metal_free: deallocating

real	0m1.669s
user	0m0.109s
sys	0m0.182s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.527 I build: 4486 (0ccd7f3e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.024.311 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.040.820 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.040.825 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.040.827 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.040.828 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.040.828 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.040.829 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.040.832 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.040.833 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.040.833 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.040.834 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.040.836 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.040.836 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.040.836 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.040.843 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.040.846 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.040.847 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.040.850 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.048.691 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.050.705 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.057.554 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.057.556 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.057.556 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.057.557 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.057.557 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.057.558 I llama_model_loader: - type  f32:  194 tensors
0.00.057.558 I llama_model_loader: - type  f16:   98 tensors
0.00.057.559 I print_info: file format = GGUF V3 (latest)
0.00.057.560 I print_info: file type   = all F32 (guessed)
0.00.057.561 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.084.262 I load: special tokens cache size = 25
0.00.091.093 I load: token to piece cache size = 0.2984 MB
0.00.091.096 I print_info: arch             = gptneox
0.00.091.097 I print_info: vocab_only       = 0
0.00.091.097 I print_info: n_ctx_train      = 2048
0.00.091.097 I print_info: n_embd           = 2048
0.00.091.097 I print_info: n_layer          = 24
0.00.091.101 I print_info: n_head           = 16
0.00.091.102 I print_info: n_head_kv        = 16
0.00.091.102 I print_info: n_rot            = 32
0.00.091.102 I print_info: n_swa            = 0
0.00.091.102 I print_info: n_embd_head_k    = 128
0.00.091.103 I print_info: n_embd_head_v    = 128
0.00.091.103 I print_info: n_gqa            = 1
0.00.091.104 I print_info: n_embd_k_gqa     = 2048
0.00.091.105 I print_info: n_embd_v_gqa     = 2048
0.00.091.105 I print_info: f_norm_eps       = 1.0e-05
0.00.091.108 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.091.108 I print_info: f_clamp_kqv      = 0.0e+00
0.00.091.108 I print_info: f_max_alibi_bias = 0.0e+00
0.00.091.108 I print_info: f_logit_scale    = 0.0e+00
0.00.091.109 I print_info: n_ff             = 8192
0.00.091.109 I print_info: n_expert         = 0
0.00.091.109 I print_info: n_expert_used    = 0
0.00.091.109 I print_info: causal attn      = 1
0.00.091.109 I print_info: pooling type     = 0
0.00.091.110 I print_info: rope type        = 2
0.00.091.110 I print_info: rope scaling     = linear
0.00.091.110 I print_info: freq_base_train  = 10000.0
0.00.091.110 I print_info: freq_scale_train = 1
0.00.091.111 I print_info: n_ctx_orig_yarn  = 2048
0.00.091.111 I print_info: rope_finetuned   = unknown
0.00.091.111 I print_info: ssm_d_conv       = 0
0.00.091.111 I print_info: ssm_d_inner      = 0
0.00.091.111 I print_info: ssm_d_state      = 0
0.00.091.111 I print_info: ssm_dt_rank      = 0
0.00.091.113 I print_info: ssm_dt_b_c_rms   = 0
0.00.091.113 I print_info: model type       = 1.4B
0.00.091.114 I print_info: model params     = 1.41 B
0.00.091.114 I print_info: general.name     = 1.4B
0.00.091.114 I print_info: vocab type       = BPE
0.00.091.115 I print_info: n_vocab          = 50304
0.00.091.115 I print_info: n_merges         = 50009
0.00.091.115 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.091.115 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.091.115 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.091.117 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.091.117 I print_info: LF token         = 128 'Ä'
0.00.091.117 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.091.117 I print_info: max token length = 1024
0.00.093.775 I load_tensors: offloading 24 repeating layers to GPU
0.00.093.775 I load_tensors: offloading output layer to GPU
0.00.093.775 I load_tensors: offloaded 25/25 layers to GPU
0.00.093.786 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.093.787 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.094.097 I llama_init_from_model: n_seq_max     = 1
0.00.094.098 I llama_init_from_model: n_ctx         = 128
0.00.094.098 I llama_init_from_model: n_ctx_per_seq = 128
0.00.094.098 I llama_init_from_model: n_batch       = 128
0.00.094.099 I llama_init_from_model: n_ubatch      = 128
0.00.094.099 I llama_init_from_model: flash_attn    = 0
0.00.094.099 I llama_init_from_model: freq_base     = 10000.0
0.00.094.099 I llama_init_from_model: freq_scale    = 1
0.00.094.100 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.094.100 I ggml_metal_init: allocating
0.00.094.103 I ggml_metal_init: found device: Apple M4
0.00.094.105 I ggml_metal_init: picking default device: Apple M4
0.00.094.765 I ggml_metal_init: using embedded metal library
0.00.097.352 I ggml_metal_init: GPU name:   Apple M4
0.00.097.354 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.097.354 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.097.354 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.097.355 I ggml_metal_init: simdgroup reduction   = true
0.00.097.355 I ggml_metal_init: simdgroup matrix mul. = true
0.00.097.355 I ggml_metal_init: has bfloat            = true
0.00.097.355 I ggml_metal_init: use bfloat            = true
0.00.097.356 I ggml_metal_init: hasUnifiedMemory      = true
0.00.097.356 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.106.785 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.108.148 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.108.153 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.108.170 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.109.087 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.109.088 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.109.089 I llama_init_from_model: graph nodes  = 967
0.00.109.089 I llama_init_from_model: graph splits = 2
0.00.109.090 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.109.090 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.038.411 I 
0.01.038.437 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.038.440 I perplexity: tokenizing the input ..
0.01.051.419 I perplexity: tokenization took 12.975 ms
0.01.051.425 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.173.191 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.175.066 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.175.178 I llama_perf_context_print:        load time =    1014.09 ms
0.01.175.180 I llama_perf_context_print: prompt eval time =     120.89 ms /   128 tokens (    0.94 ms per token,  1058.84 tokens per second)
0.01.175.181 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.175.189 I llama_perf_context_print:       total time =     136.77 ms /   129 tokens
0.01.175.980 I ggml_metal_free: deallocating

real	0m1.367s
user	0m0.125s
sys	0m0.203s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.127 I build: 4486 (0ccd7f3e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.012.168 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.022.984 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.022.991 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.022.994 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.022.994 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.022.995 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.022.995 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.022.996 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.022.997 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.022.997 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.022.998 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.022.998 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.022.999 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.023.001 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.023.001 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.023.003 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.023.004 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.023.004 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.028.647 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.029.961 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.034.874 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.034.876 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.034.876 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.034.877 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.034.877 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.034.878 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.034.878 I llama_model_loader: - type  f32:  194 tensors
0.00.034.879 I llama_model_loader: - type q8_0:   98 tensors
0.00.034.880 I print_info: file format = GGUF V3 (latest)
0.00.034.880 I print_info: file type   = Q8_0
0.00.034.882 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.058.652 I load: special tokens cache size = 25
0.00.064.583 I load: token to piece cache size = 0.2984 MB
0.00.064.586 I print_info: arch             = gptneox
0.00.064.586 I print_info: vocab_only       = 0
0.00.064.587 I print_info: n_ctx_train      = 2048
0.00.064.587 I print_info: n_embd           = 2048
0.00.064.587 I print_info: n_layer          = 24
0.00.064.591 I print_info: n_head           = 16
0.00.064.592 I print_info: n_head_kv        = 16
0.00.064.593 I print_info: n_rot            = 32
0.00.064.593 I print_info: n_swa            = 0
0.00.064.593 I print_info: n_embd_head_k    = 128
0.00.064.593 I print_info: n_embd_head_v    = 128
0.00.064.595 I print_info: n_gqa            = 1
0.00.064.596 I print_info: n_embd_k_gqa     = 2048
0.00.064.597 I print_info: n_embd_v_gqa     = 2048
0.00.064.598 I print_info: f_norm_eps       = 1.0e-05
0.00.064.598 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.064.599 I print_info: f_clamp_kqv      = 0.0e+00
0.00.064.599 I print_info: f_max_alibi_bias = 0.0e+00
0.00.064.599 I print_info: f_logit_scale    = 0.0e+00
0.00.064.600 I print_info: n_ff             = 8192
0.00.064.600 I print_info: n_expert         = 0
0.00.064.600 I print_info: n_expert_used    = 0
0.00.064.600 I print_info: causal attn      = 1
0.00.064.600 I print_info: pooling type     = 0
0.00.064.600 I print_info: rope type        = 2
0.00.064.601 I print_info: rope scaling     = linear
0.00.064.601 I print_info: freq_base_train  = 10000.0
0.00.064.602 I print_info: freq_scale_train = 1
0.00.064.602 I print_info: n_ctx_orig_yarn  = 2048
0.00.064.602 I print_info: rope_finetuned   = unknown
0.00.064.602 I print_info: ssm_d_conv       = 0
0.00.064.602 I print_info: ssm_d_inner      = 0
0.00.064.603 I print_info: ssm_d_state      = 0
0.00.064.603 I print_info: ssm_dt_rank      = 0
0.00.064.603 I print_info: ssm_dt_b_c_rms   = 0
0.00.064.603 I print_info: model type       = 1.4B
0.00.064.603 I print_info: model params     = 1.41 B
0.00.064.604 I print_info: general.name     = 1.4B
0.00.064.604 I print_info: vocab type       = BPE
0.00.064.604 I print_info: n_vocab          = 50304
0.00.064.605 I print_info: n_merges         = 50009
0.00.064.605 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.064.605 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.064.605 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.064.605 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.064.606 I print_info: LF token         = 128 'Ä'
0.00.064.606 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.064.606 I print_info: max token length = 1024
0.00.066.955 I load_tensors: offloading 24 repeating layers to GPU
0.00.066.955 I load_tensors: offloading output layer to GPU
0.00.066.955 I load_tensors: offloaded 25/25 layers to GPU
0.00.066.967 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.066.968 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.00.067.281 I llama_init_from_model: n_seq_max     = 1
0.00.067.282 I llama_init_from_model: n_ctx         = 128
0.00.067.282 I llama_init_from_model: n_ctx_per_seq = 128
0.00.067.282 I llama_init_from_model: n_batch       = 128
0.00.067.283 I llama_init_from_model: n_ubatch      = 128
0.00.067.283 I llama_init_from_model: flash_attn    = 0
0.00.067.283 I llama_init_from_model: freq_base     = 10000.0
0.00.067.283 I llama_init_from_model: freq_scale    = 1
0.00.067.284 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.067.284 I ggml_metal_init: allocating
0.00.067.288 I ggml_metal_init: found device: Apple M4
0.00.067.290 I ggml_metal_init: picking default device: Apple M4
0.00.067.981 I ggml_metal_init: using embedded metal library
0.00.070.697 I ggml_metal_init: GPU name:   Apple M4
0.00.070.698 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.070.699 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.070.699 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.070.699 I ggml_metal_init: simdgroup reduction   = true
0.00.070.699 I ggml_metal_init: simdgroup matrix mul. = true
0.00.070.700 I ggml_metal_init: has bfloat            = true
0.00.070.700 I ggml_metal_init: use bfloat            = true
0.00.070.700 I ggml_metal_init: hasUnifiedMemory      = true
0.00.070.701 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.081.086 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.082.597 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.082.600 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.082.618 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.083.625 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.083.626 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.083.627 I llama_init_from_model: graph nodes  = 967
0.00.083.627 I llama_init_from_model: graph splits = 2
0.00.083.629 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.083.629 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.916.579 I 
0.00.916.607 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.916.610 I perplexity: tokenizing the input ..
0.00.924.882 I perplexity: tokenization took 8.27 ms
0.00.924.886 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.049.494 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.050.661 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.050.689 I llama_perf_context_print:        load time =     904.41 ms
0.01.050.690 I llama_perf_context_print: prompt eval time =     124.35 ms /   128 tokens (    0.97 ms per token,  1029.34 tokens per second)
0.01.050.690 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.050.691 I llama_perf_context_print:       total time =     134.11 ms /   129 tokens
0.01.051.194 I ggml_metal_free: deallocating

real	0m1.069s
user	0m0.093s
sys	0m0.153s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.083 I build: 4486 (0ccd7f3e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.785 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.715 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.016.719 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.721 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.722 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.722 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.722 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.723 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.724 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.724 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.724 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.725 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.725 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.725 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.726 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.727 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.728 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.728 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.469 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.461 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.122 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.123 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.123 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.124 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.124 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.124 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.025.125 I llama_model_loader: - type  f32:  194 tensors
0.00.025.125 I llama_model_loader: - type q4_0:   97 tensors
0.00.025.125 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.126 I print_info: file format = GGUF V3 (latest)
0.00.025.126 I print_info: file type   = Q4_0
0.00.025.127 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.044.276 I load: special tokens cache size = 25
0.00.050.228 I load: token to piece cache size = 0.2984 MB
0.00.050.231 I print_info: arch             = gptneox
0.00.050.232 I print_info: vocab_only       = 0
0.00.050.232 I print_info: n_ctx_train      = 2048
0.00.050.232 I print_info: n_embd           = 2048
0.00.050.232 I print_info: n_layer          = 24
0.00.050.235 I print_info: n_head           = 16
0.00.050.236 I print_info: n_head_kv        = 16
0.00.050.236 I print_info: n_rot            = 32
0.00.050.236 I print_info: n_swa            = 0
0.00.050.237 I print_info: n_embd_head_k    = 128
0.00.050.237 I print_info: n_embd_head_v    = 128
0.00.050.238 I print_info: n_gqa            = 1
0.00.050.238 I print_info: n_embd_k_gqa     = 2048
0.00.050.241 I print_info: n_embd_v_gqa     = 2048
0.00.050.241 I print_info: f_norm_eps       = 1.0e-05
0.00.050.242 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.243 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.243 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.244 I print_info: f_logit_scale    = 0.0e+00
0.00.050.244 I print_info: n_ff             = 8192
0.00.050.244 I print_info: n_expert         = 0
0.00.050.245 I print_info: n_expert_used    = 0
0.00.050.245 I print_info: causal attn      = 1
0.00.050.245 I print_info: pooling type     = 0
0.00.050.245 I print_info: rope type        = 2
0.00.050.245 I print_info: rope scaling     = linear
0.00.050.246 I print_info: freq_base_train  = 10000.0
0.00.050.246 I print_info: freq_scale_train = 1
0.00.050.246 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.246 I print_info: rope_finetuned   = unknown
0.00.050.247 I print_info: ssm_d_conv       = 0
0.00.050.247 I print_info: ssm_d_inner      = 0
0.00.050.247 I print_info: ssm_d_state      = 0
0.00.050.247 I print_info: ssm_dt_rank      = 0
0.00.050.247 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.248 I print_info: model type       = 1.4B
0.00.050.248 I print_info: model params     = 1.41 B
0.00.050.248 I print_info: general.name     = 1.4B
0.00.050.249 I print_info: vocab type       = BPE
0.00.050.249 I print_info: n_vocab          = 50304
0.00.050.249 I print_info: n_merges         = 50009
0.00.050.250 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.251 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.251 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.251 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.252 I print_info: LF token         = 128 'Ä'
0.00.050.252 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.252 I print_info: max token length = 1024
0.00.052.183 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.183 I load_tensors: offloading output layer to GPU
0.00.052.184 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.194 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.052.196 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.052.484 I llama_init_from_model: n_seq_max     = 1
0.00.052.485 I llama_init_from_model: n_ctx         = 128
0.00.052.485 I llama_init_from_model: n_ctx_per_seq = 128
0.00.052.485 I llama_init_from_model: n_batch       = 128
0.00.052.485 I llama_init_from_model: n_ubatch      = 128
0.00.052.486 I llama_init_from_model: flash_attn    = 0
0.00.052.486 I llama_init_from_model: freq_base     = 10000.0
0.00.052.486 I llama_init_from_model: freq_scale    = 1
0.00.052.486 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.487 I ggml_metal_init: allocating
0.00.052.490 I ggml_metal_init: found device: Apple M4
0.00.052.492 I ggml_metal_init: picking default device: Apple M4
0.00.053.078 I ggml_metal_init: using embedded metal library
0.00.055.409 I ggml_metal_init: GPU name:   Apple M4
0.00.055.410 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.411 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.411 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.411 I ggml_metal_init: simdgroup reduction   = true
0.00.055.411 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.411 I ggml_metal_init: has bfloat            = true
0.00.055.412 I ggml_metal_init: use bfloat            = true
0.00.055.412 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.413 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.363 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.653 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.657 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.674 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.067.522 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.067.523 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.067.524 I llama_init_from_model: graph nodes  = 967
0.00.067.524 I llama_init_from_model: graph splits = 2
0.00.067.525 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.525 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.598.213 I 
0.00.598.243 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.598.248 I perplexity: tokenizing the input ..
0.00.606.236 I perplexity: tokenization took 7.985 ms
0.00.606.239 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.728.503 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.729.704 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.729.727 I llama_perf_context_print:        load time =     588.42 ms
0.00.729.728 I llama_perf_context_print: prompt eval time =     122.04 ms /   128 tokens (    0.95 ms per token,  1048.87 tokens per second)
0.00.729.729 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.729.729 I llama_perf_context_print:       total time =     131.52 ms /   129 tokens
0.00.730.107 I ggml_metal_free: deallocating

real	0m0.745s
user	0m0.077s
sys	0m0.091s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.082 I build: 4486 (0ccd7f3e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.866 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.784 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.015.790 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.791 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.792 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.792 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.792 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.793 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.793 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.794 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.794 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.795 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.795 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.795 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.796 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.799 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.799 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.800 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.519 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.548 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.227 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.229 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.229 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.229 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.230 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.230 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.024.231 I llama_model_loader: - type  f32:  194 tensors
0.00.024.231 I llama_model_loader: - type q4_1:   97 tensors
0.00.024.231 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.232 I print_info: file format = GGUF V3 (latest)
0.00.024.232 I print_info: file type   = Q4_1
0.00.024.233 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.043.532 I load: special tokens cache size = 25
0.00.049.508 I load: token to piece cache size = 0.2984 MB
0.00.049.511 I print_info: arch             = gptneox
0.00.049.511 I print_info: vocab_only       = 0
0.00.049.512 I print_info: n_ctx_train      = 2048
0.00.049.512 I print_info: n_embd           = 2048
0.00.049.512 I print_info: n_layer          = 24
0.00.049.515 I print_info: n_head           = 16
0.00.049.516 I print_info: n_head_kv        = 16
0.00.049.516 I print_info: n_rot            = 32
0.00.049.516 I print_info: n_swa            = 0
0.00.049.516 I print_info: n_embd_head_k    = 128
0.00.049.517 I print_info: n_embd_head_v    = 128
0.00.049.521 I print_info: n_gqa            = 1
0.00.049.521 I print_info: n_embd_k_gqa     = 2048
0.00.049.522 I print_info: n_embd_v_gqa     = 2048
0.00.049.523 I print_info: f_norm_eps       = 1.0e-05
0.00.049.523 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.523 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.525 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.525 I print_info: f_logit_scale    = 0.0e+00
0.00.049.525 I print_info: n_ff             = 8192
0.00.049.526 I print_info: n_expert         = 0
0.00.049.526 I print_info: n_expert_used    = 0
0.00.049.526 I print_info: causal attn      = 1
0.00.049.526 I print_info: pooling type     = 0
0.00.049.526 I print_info: rope type        = 2
0.00.049.526 I print_info: rope scaling     = linear
0.00.049.527 I print_info: freq_base_train  = 10000.0
0.00.049.527 I print_info: freq_scale_train = 1
0.00.049.527 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.527 I print_info: rope_finetuned   = unknown
0.00.049.533 I print_info: ssm_d_conv       = 0
0.00.049.534 I print_info: ssm_d_inner      = 0
0.00.049.535 I print_info: ssm_d_state      = 0
0.00.049.535 I print_info: ssm_dt_rank      = 0
0.00.049.535 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.535 I print_info: model type       = 1.4B
0.00.049.537 I print_info: model params     = 1.41 B
0.00.049.537 I print_info: general.name     = 1.4B
0.00.049.537 I print_info: vocab type       = BPE
0.00.049.538 I print_info: n_vocab          = 50304
0.00.049.538 I print_info: n_merges         = 50009
0.00.049.538 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.538 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.539 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.539 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.539 I print_info: LF token         = 128 'Ä'
0.00.049.539 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.539 I print_info: max token length = 1024
0.00.051.274 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.274 I load_tensors: offloading output layer to GPU
0.00.051.274 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.280 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.051.280 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.051.546 I llama_init_from_model: n_seq_max     = 1
0.00.051.547 I llama_init_from_model: n_ctx         = 128
0.00.051.547 I llama_init_from_model: n_ctx_per_seq = 128
0.00.051.547 I llama_init_from_model: n_batch       = 128
0.00.051.547 I llama_init_from_model: n_ubatch      = 128
0.00.051.547 I llama_init_from_model: flash_attn    = 0
0.00.051.548 I llama_init_from_model: freq_base     = 10000.0
0.00.051.548 I llama_init_from_model: freq_scale    = 1
0.00.051.548 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.549 I ggml_metal_init: allocating
0.00.051.552 I ggml_metal_init: found device: Apple M4
0.00.051.553 I ggml_metal_init: picking default device: Apple M4
0.00.052.119 I ggml_metal_init: using embedded metal library
0.00.054.461 I ggml_metal_init: GPU name:   Apple M4
0.00.054.462 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.463 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.463 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.463 I ggml_metal_init: simdgroup reduction   = true
0.00.054.464 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.464 I ggml_metal_init: has bfloat            = true
0.00.054.464 I ggml_metal_init: use bfloat            = true
0.00.054.464 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.465 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.701 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.064.898 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.064.900 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.064.914 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.065.699 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.065.700 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.065.700 I llama_init_from_model: graph nodes  = 967
0.00.065.700 I llama_init_from_model: graph splits = 2
0.00.065.701 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.065.701 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.623.234 I 
0.00.623.280 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.623.292 I perplexity: tokenizing the input ..
0.00.631.030 I perplexity: tokenization took 7.736 ms
0.00.631.037 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.753.986 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.755.161 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.755.185 I llama_perf_context_print:        load time =     614.36 ms
0.00.755.186 I llama_perf_context_print: prompt eval time =     122.72 ms /   128 tokens (    0.96 ms per token,  1043.00 tokens per second)
0.00.755.186 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.755.187 I llama_perf_context_print:       total time =     131.95 ms /   129 tokens
0.00.755.615 I ggml_metal_free: deallocating

real	0m0.769s
user	0m0.077s
sys	0m0.104s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.086 I build: 4486 (0ccd7f3e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.872 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.484 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.017.489 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.490 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.491 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.491 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.491 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.492 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.493 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.493 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.493 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.494 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.494 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.495 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.495 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.498 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.498 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.498 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.345 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.384 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.254 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.255 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.255 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.255 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.256 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.256 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.026.256 I llama_model_loader: - type  f32:  194 tensors
0.00.026.256 I llama_model_loader: - type q5_0:   97 tensors
0.00.026.257 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.257 I print_info: file format = GGUF V3 (latest)
0.00.026.257 I print_info: file type   = Q5_0
0.00.026.258 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.044.686 I load: special tokens cache size = 25
0.00.050.579 I load: token to piece cache size = 0.2984 MB
0.00.050.583 I print_info: arch             = gptneox
0.00.050.583 I print_info: vocab_only       = 0
0.00.050.583 I print_info: n_ctx_train      = 2048
0.00.050.583 I print_info: n_embd           = 2048
0.00.050.584 I print_info: n_layer          = 24
0.00.050.586 I print_info: n_head           = 16
0.00.050.587 I print_info: n_head_kv        = 16
0.00.050.590 I print_info: n_rot            = 32
0.00.050.590 I print_info: n_swa            = 0
0.00.050.590 I print_info: n_embd_head_k    = 128
0.00.050.590 I print_info: n_embd_head_v    = 128
0.00.050.591 I print_info: n_gqa            = 1
0.00.050.592 I print_info: n_embd_k_gqa     = 2048
0.00.050.592 I print_info: n_embd_v_gqa     = 2048
0.00.050.593 I print_info: f_norm_eps       = 1.0e-05
0.00.050.593 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.593 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.594 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.594 I print_info: f_logit_scale    = 0.0e+00
0.00.050.594 I print_info: n_ff             = 8192
0.00.050.594 I print_info: n_expert         = 0
0.00.050.595 I print_info: n_expert_used    = 0
0.00.050.595 I print_info: causal attn      = 1
0.00.050.595 I print_info: pooling type     = 0
0.00.050.595 I print_info: rope type        = 2
0.00.050.599 I print_info: rope scaling     = linear
0.00.050.600 I print_info: freq_base_train  = 10000.0
0.00.050.601 I print_info: freq_scale_train = 1
0.00.050.602 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.602 I print_info: rope_finetuned   = unknown
0.00.050.602 I print_info: ssm_d_conv       = 0
0.00.050.602 I print_info: ssm_d_inner      = 0
0.00.050.602 I print_info: ssm_d_state      = 0
0.00.050.602 I print_info: ssm_dt_rank      = 0
0.00.050.603 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.603 I print_info: model type       = 1.4B
0.00.050.603 I print_info: model params     = 1.41 B
0.00.050.603 I print_info: general.name     = 1.4B
0.00.050.604 I print_info: vocab type       = BPE
0.00.050.604 I print_info: n_vocab          = 50304
0.00.050.604 I print_info: n_merges         = 50009
0.00.050.604 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.607 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.607 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.607 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.608 I print_info: LF token         = 128 'Ä'
0.00.050.609 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.609 I print_info: max token length = 1024
0.00.052.533 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.533 I load_tensors: offloading output layer to GPU
0.00.052.533 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.544 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.052.545 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.052.815 I llama_init_from_model: n_seq_max     = 1
0.00.052.816 I llama_init_from_model: n_ctx         = 128
0.00.052.816 I llama_init_from_model: n_ctx_per_seq = 128
0.00.052.816 I llama_init_from_model: n_batch       = 128
0.00.052.816 I llama_init_from_model: n_ubatch      = 128
0.00.052.817 I llama_init_from_model: flash_attn    = 0
0.00.052.817 I llama_init_from_model: freq_base     = 10000.0
0.00.052.817 I llama_init_from_model: freq_scale    = 1
0.00.052.818 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.818 I ggml_metal_init: allocating
0.00.052.820 I ggml_metal_init: found device: Apple M4
0.00.052.822 I ggml_metal_init: picking default device: Apple M4
0.00.053.379 I ggml_metal_init: using embedded metal library
0.00.055.722 I ggml_metal_init: GPU name:   Apple M4
0.00.055.724 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.724 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.724 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.725 I ggml_metal_init: simdgroup reduction   = true
0.00.055.725 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.725 I ggml_metal_init: has bfloat            = true
0.00.055.725 I ggml_metal_init: use bfloat            = true
0.00.055.725 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.726 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.262 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.065.511 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.515 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.533 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.066.450 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.066.451 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.066.452 I llama_init_from_model: graph nodes  = 967
0.00.066.452 I llama_init_from_model: graph splits = 2
0.00.066.453 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.453 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.680.108 I 
0.00.680.143 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.680.149 I perplexity: tokenizing the input ..
0.00.688.103 I perplexity: tokenization took 7.952 ms
0.00.688.106 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.823.253 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.824.490 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.824.520 I llama_perf_context_print:        load time =     669.23 ms
0.00.824.521 I llama_perf_context_print: prompt eval time =     134.92 ms /   128 tokens (    1.05 ms per token,   948.74 tokens per second)
0.00.824.521 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.824.522 I llama_perf_context_print:       total time =     144.41 ms /   129 tokens
0.00.825.012 I ggml_metal_free: deallocating

real	0m0.841s
user	0m0.075s
sys	0m0.102s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.083 I build: 4486 (0ccd7f3e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.826 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.391 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.015.395 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.397 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.398 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.398 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.398 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.398 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.400 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.400 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.400 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.401 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.401 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.401 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.402 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.405 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.405 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.405 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.044 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.002 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.591 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.592 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.593 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.593 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.593 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.593 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.023.594 I llama_model_loader: - type  f32:  194 tensors
0.00.023.594 I llama_model_loader: - type q5_1:   97 tensors
0.00.023.594 I llama_model_loader: - type q6_K:    1 tensors
0.00.023.595 I print_info: file format = GGUF V3 (latest)
0.00.023.595 I print_info: file type   = Q5_1
0.00.023.596 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.042.042 I load: special tokens cache size = 25
0.00.047.752 I load: token to piece cache size = 0.2984 MB
0.00.047.756 I print_info: arch             = gptneox
0.00.047.756 I print_info: vocab_only       = 0
0.00.047.756 I print_info: n_ctx_train      = 2048
0.00.047.756 I print_info: n_embd           = 2048
0.00.047.756 I print_info: n_layer          = 24
0.00.047.759 I print_info: n_head           = 16
0.00.047.760 I print_info: n_head_kv        = 16
0.00.047.760 I print_info: n_rot            = 32
0.00.047.760 I print_info: n_swa            = 0
0.00.047.760 I print_info: n_embd_head_k    = 128
0.00.047.761 I print_info: n_embd_head_v    = 128
0.00.047.761 I print_info: n_gqa            = 1
0.00.047.762 I print_info: n_embd_k_gqa     = 2048
0.00.047.763 I print_info: n_embd_v_gqa     = 2048
0.00.047.763 I print_info: f_norm_eps       = 1.0e-05
0.00.047.764 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.047.764 I print_info: f_clamp_kqv      = 0.0e+00
0.00.047.764 I print_info: f_max_alibi_bias = 0.0e+00
0.00.047.764 I print_info: f_logit_scale    = 0.0e+00
0.00.047.765 I print_info: n_ff             = 8192
0.00.047.765 I print_info: n_expert         = 0
0.00.047.765 I print_info: n_expert_used    = 0
0.00.047.765 I print_info: causal attn      = 1
0.00.047.765 I print_info: pooling type     = 0
0.00.047.765 I print_info: rope type        = 2
0.00.047.766 I print_info: rope scaling     = linear
0.00.047.766 I print_info: freq_base_train  = 10000.0
0.00.047.766 I print_info: freq_scale_train = 1
0.00.047.766 I print_info: n_ctx_orig_yarn  = 2048
0.00.047.767 I print_info: rope_finetuned   = unknown
0.00.047.767 I print_info: ssm_d_conv       = 0
0.00.047.767 I print_info: ssm_d_inner      = 0
0.00.047.767 I print_info: ssm_d_state      = 0
0.00.047.767 I print_info: ssm_dt_rank      = 0
0.00.047.767 I print_info: ssm_dt_b_c_rms   = 0
0.00.047.768 I print_info: model type       = 1.4B
0.00.047.768 I print_info: model params     = 1.41 B
0.00.047.768 I print_info: general.name     = 1.4B
0.00.047.769 I print_info: vocab type       = BPE
0.00.047.769 I print_info: n_vocab          = 50304
0.00.047.769 I print_info: n_merges         = 50009
0.00.047.770 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.047.770 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.047.770 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.047.770 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.047.771 I print_info: LF token         = 128 'Ä'
0.00.047.771 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.047.771 I print_info: max token length = 1024
0.00.049.795 I load_tensors: offloading 24 repeating layers to GPU
0.00.049.795 I load_tensors: offloading output layer to GPU
0.00.049.795 I load_tensors: offloaded 25/25 layers to GPU
0.00.049.806 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.049.807 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.050.101 I llama_init_from_model: n_seq_max     = 1
0.00.050.102 I llama_init_from_model: n_ctx         = 128
0.00.050.102 I llama_init_from_model: n_ctx_per_seq = 128
0.00.050.102 I llama_init_from_model: n_batch       = 128
0.00.050.102 I llama_init_from_model: n_ubatch      = 128
0.00.050.102 I llama_init_from_model: flash_attn    = 0
0.00.050.103 I llama_init_from_model: freq_base     = 10000.0
0.00.050.103 I llama_init_from_model: freq_scale    = 1
0.00.050.103 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.050.104 I ggml_metal_init: allocating
0.00.050.106 I ggml_metal_init: found device: Apple M4
0.00.050.108 I ggml_metal_init: picking default device: Apple M4
0.00.050.681 I ggml_metal_init: using embedded metal library
0.00.053.067 I ggml_metal_init: GPU name:   Apple M4
0.00.053.069 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.053.069 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.053.069 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.053.070 I ggml_metal_init: simdgroup reduction   = true
0.00.053.070 I ggml_metal_init: simdgroup matrix mul. = true
0.00.053.070 I ggml_metal_init: has bfloat            = true
0.00.053.070 I ggml_metal_init: use bfloat            = true
0.00.053.070 I ggml_metal_init: hasUnifiedMemory      = true
0.00.053.071 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.062.510 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.063.787 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.063.789 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.063.805 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.064.767 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.064.768 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.064.768 I llama_init_from_model: graph nodes  = 967
0.00.064.768 I llama_init_from_model: graph splits = 2
0.00.064.770 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.064.770 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.582.015 I 
0.00.582.053 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.582.060 I perplexity: tokenizing the input ..
0.00.590.038 I perplexity: tokenization took 7.976 ms
0.00.590.047 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.724.810 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.725.945 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.725.971 I llama_perf_context_print:        load time =     573.18 ms
0.00.725.973 I llama_perf_context_print: prompt eval time =     134.53 ms /   128 tokens (    1.05 ms per token,   951.46 tokens per second)
0.00.725.974 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.725.975 I llama_perf_context_print:       total time =     143.96 ms /   129 tokens
0.00.726.353 I ggml_metal_free: deallocating

real	0m0.740s
user	0m0.075s
sys	0m0.107s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.084 I build: 4486 (0ccd7f3e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.395 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.041 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.047 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.052 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.053 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.054 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.055 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.055 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.056 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.056 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.057 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.061 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.061 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.062 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.062 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.066 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.066 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.066 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.809 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.777 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.470 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.471 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.472 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.472 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.472 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.473 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.473 I llama_model_loader: - type  f32:  194 tensors
0.00.024.474 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.474 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.474 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.475 I print_info: file format = GGUF V3 (latest)
0.00.024.475 I print_info: file type   = Q2_K - Medium
0.00.024.476 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.043.770 I load: special tokens cache size = 25
0.00.049.828 I load: token to piece cache size = 0.2984 MB
0.00.049.831 I print_info: arch             = gptneox
0.00.049.831 I print_info: vocab_only       = 0
0.00.049.831 I print_info: n_ctx_train      = 2048
0.00.049.832 I print_info: n_embd           = 2048
0.00.049.832 I print_info: n_layer          = 24
0.00.049.835 I print_info: n_head           = 16
0.00.049.836 I print_info: n_head_kv        = 16
0.00.049.836 I print_info: n_rot            = 32
0.00.049.836 I print_info: n_swa            = 0
0.00.049.836 I print_info: n_embd_head_k    = 128
0.00.049.836 I print_info: n_embd_head_v    = 128
0.00.049.837 I print_info: n_gqa            = 1
0.00.049.838 I print_info: n_embd_k_gqa     = 2048
0.00.049.839 I print_info: n_embd_v_gqa     = 2048
0.00.049.839 I print_info: f_norm_eps       = 1.0e-05
0.00.049.839 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.840 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.840 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.840 I print_info: f_logit_scale    = 0.0e+00
0.00.049.841 I print_info: n_ff             = 8192
0.00.049.841 I print_info: n_expert         = 0
0.00.049.841 I print_info: n_expert_used    = 0
0.00.049.841 I print_info: causal attn      = 1
0.00.049.841 I print_info: pooling type     = 0
0.00.049.841 I print_info: rope type        = 2
0.00.049.849 I print_info: rope scaling     = linear
0.00.049.852 I print_info: freq_base_train  = 10000.0
0.00.049.853 I print_info: freq_scale_train = 1
0.00.049.853 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.853 I print_info: rope_finetuned   = unknown
0.00.049.853 I print_info: ssm_d_conv       = 0
0.00.049.853 I print_info: ssm_d_inner      = 0
0.00.049.853 I print_info: ssm_d_state      = 0
0.00.049.854 I print_info: ssm_dt_rank      = 0
0.00.049.854 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.854 I print_info: model type       = 1.4B
0.00.049.855 I print_info: model params     = 1.41 B
0.00.049.855 I print_info: general.name     = 1.4B
0.00.049.855 I print_info: vocab type       = BPE
0.00.049.856 I print_info: n_vocab          = 50304
0.00.049.856 I print_info: n_merges         = 50009
0.00.049.857 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.857 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.857 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.857 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.857 I print_info: LF token         = 128 'Ä'
0.00.049.858 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.858 I print_info: max token length = 1024
0.00.051.686 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.687 I load_tensors: offloading output layer to GPU
0.00.051.687 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.697 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.051.699 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.052.017 I llama_init_from_model: n_seq_max     = 1
0.00.052.018 I llama_init_from_model: n_ctx         = 128
0.00.052.018 I llama_init_from_model: n_ctx_per_seq = 128
0.00.052.018 I llama_init_from_model: n_batch       = 128
0.00.052.018 I llama_init_from_model: n_ubatch      = 128
0.00.052.019 I llama_init_from_model: flash_attn    = 0
0.00.052.019 I llama_init_from_model: freq_base     = 10000.0
0.00.052.019 I llama_init_from_model: freq_scale    = 1
0.00.052.019 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.020 I ggml_metal_init: allocating
0.00.052.023 I ggml_metal_init: found device: Apple M4
0.00.052.024 I ggml_metal_init: picking default device: Apple M4
0.00.052.573 I ggml_metal_init: using embedded metal library
0.00.054.865 I ggml_metal_init: GPU name:   Apple M4
0.00.054.866 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.866 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.867 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.867 I ggml_metal_init: simdgroup reduction   = true
0.00.054.867 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.867 I ggml_metal_init: has bfloat            = true
0.00.054.867 I ggml_metal_init: use bfloat            = true
0.00.054.868 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.868 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.294 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.065.685 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.694 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.716 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.066.567 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.066.568 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.066.569 I llama_init_from_model: graph nodes  = 967
0.00.066.569 I llama_init_from_model: graph splits = 2
0.00.066.570 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.570 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.366.693 I 
0.00.366.726 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.366.729 I perplexity: tokenizing the input ..
0.00.374.270 I perplexity: tokenization took 7.539 ms
0.00.374.275 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.506.132 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.507.396 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.507.423 I llama_perf_context_print:        load time =     357.29 ms
0.00.507.424 I llama_perf_context_print: prompt eval time =     131.62 ms /   128 tokens (    1.03 ms per token,   972.49 tokens per second)
0.00.507.424 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.507.425 I llama_perf_context_print:       total time =     140.73 ms /   129 tokens
0.00.507.893 I ggml_metal_free: deallocating

real	0m0.524s
user	0m0.077s
sys	0m0.054s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.085 I build: 4486 (0ccd7f3e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.565 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.653 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.015.658 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.660 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.660 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.661 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.661 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.661 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.662 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.662 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.663 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.663 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.663 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.664 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.664 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.666 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.666 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.666 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.327 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.341 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.984 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.985 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.985 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.985 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.986 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.986 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.023.986 I llama_model_loader: - type  f32:  194 tensors
0.00.023.987 I llama_model_loader: - type q3_K:   25 tensors
0.00.023.987 I llama_model_loader: - type q4_K:   71 tensors
0.00.023.987 I llama_model_loader: - type q5_K:    1 tensors
0.00.023.987 I llama_model_loader: - type q6_K:    1 tensors
0.00.023.988 I print_info: file format = GGUF V3 (latest)
0.00.023.988 I print_info: file type   = Q3_K - Medium
0.00.023.989 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.042.543 I load: special tokens cache size = 25
0.00.048.287 I load: token to piece cache size = 0.2984 MB
0.00.048.291 I print_info: arch             = gptneox
0.00.048.291 I print_info: vocab_only       = 0
0.00.048.291 I print_info: n_ctx_train      = 2048
0.00.048.291 I print_info: n_embd           = 2048
0.00.048.291 I print_info: n_layer          = 24
0.00.048.294 I print_info: n_head           = 16
0.00.048.295 I print_info: n_head_kv        = 16
0.00.048.295 I print_info: n_rot            = 32
0.00.048.296 I print_info: n_swa            = 0
0.00.048.296 I print_info: n_embd_head_k    = 128
0.00.048.296 I print_info: n_embd_head_v    = 128
0.00.048.297 I print_info: n_gqa            = 1
0.00.048.297 I print_info: n_embd_k_gqa     = 2048
0.00.048.298 I print_info: n_embd_v_gqa     = 2048
0.00.048.299 I print_info: f_norm_eps       = 1.0e-05
0.00.048.299 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.048.299 I print_info: f_clamp_kqv      = 0.0e+00
0.00.048.299 I print_info: f_max_alibi_bias = 0.0e+00
0.00.048.299 I print_info: f_logit_scale    = 0.0e+00
0.00.048.302 I print_info: n_ff             = 8192
0.00.048.303 I print_info: n_expert         = 0
0.00.048.303 I print_info: n_expert_used    = 0
0.00.048.303 I print_info: causal attn      = 1
0.00.048.303 I print_info: pooling type     = 0
0.00.048.303 I print_info: rope type        = 2
0.00.048.303 I print_info: rope scaling     = linear
0.00.048.305 I print_info: freq_base_train  = 10000.0
0.00.048.306 I print_info: freq_scale_train = 1
0.00.048.306 I print_info: n_ctx_orig_yarn  = 2048
0.00.048.306 I print_info: rope_finetuned   = unknown
0.00.048.306 I print_info: ssm_d_conv       = 0
0.00.048.306 I print_info: ssm_d_inner      = 0
0.00.048.307 I print_info: ssm_d_state      = 0
0.00.048.307 I print_info: ssm_dt_rank      = 0
0.00.048.307 I print_info: ssm_dt_b_c_rms   = 0
0.00.048.307 I print_info: model type       = 1.4B
0.00.048.307 I print_info: model params     = 1.41 B
0.00.048.308 I print_info: general.name     = 1.4B
0.00.048.308 I print_info: vocab type       = BPE
0.00.048.308 I print_info: n_vocab          = 50304
0.00.048.312 I print_info: n_merges         = 50009
0.00.048.312 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.048.313 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.048.313 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.048.313 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.048.315 I print_info: LF token         = 128 'Ä'
0.00.048.315 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.048.315 I print_info: max token length = 1024
0.00.050.234 I load_tensors: offloading 24 repeating layers to GPU
0.00.050.234 I load_tensors: offloading output layer to GPU
0.00.050.235 I load_tensors: offloaded 25/25 layers to GPU
0.00.050.245 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.050.247 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.050.536 I llama_init_from_model: n_seq_max     = 1
0.00.050.537 I llama_init_from_model: n_ctx         = 128
0.00.050.537 I llama_init_from_model: n_ctx_per_seq = 128
0.00.050.537 I llama_init_from_model: n_batch       = 128
0.00.050.537 I llama_init_from_model: n_ubatch      = 128
0.00.050.538 I llama_init_from_model: flash_attn    = 0
0.00.050.538 I llama_init_from_model: freq_base     = 10000.0
0.00.050.538 I llama_init_from_model: freq_scale    = 1
0.00.050.538 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.050.539 I ggml_metal_init: allocating
0.00.050.541 I ggml_metal_init: found device: Apple M4
0.00.050.543 I ggml_metal_init: picking default device: Apple M4
0.00.051.112 I ggml_metal_init: using embedded metal library
0.00.053.484 I ggml_metal_init: GPU name:   Apple M4
0.00.053.486 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.053.486 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.053.486 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.053.487 I ggml_metal_init: simdgroup reduction   = true
0.00.053.487 I ggml_metal_init: simdgroup matrix mul. = true
0.00.053.487 I ggml_metal_init: has bfloat            = true
0.00.053.487 I ggml_metal_init: use bfloat            = true
0.00.053.488 I ggml_metal_init: hasUnifiedMemory      = true
0.00.053.489 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.809 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.065.046 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.051 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.069 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.065.914 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.065.916 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.065.916 I llama_init_from_model: graph nodes  = 967
0.00.065.916 I llama_init_from_model: graph splits = 2
0.00.065.918 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.065.918 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.478.836 I 
0.00.478.876 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.478.895 I perplexity: tokenizing the input ..
0.00.487.019 I perplexity: tokenization took 8.122 ms
0.00.487.022 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.618.162 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.619.558 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.619.580 I llama_perf_context_print:        load time =     470.26 ms
0.00.619.580 I llama_perf_context_print: prompt eval time =     130.90 ms /   128 tokens (    1.02 ms per token,   977.87 tokens per second)
0.00.619.581 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.619.583 I llama_perf_context_print:       total time =     140.75 ms /   129 tokens
0.00.619.951 I ggml_metal_free: deallocating

real	0m0.633s
user	0m0.077s
sys	0m0.081s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.098 I build: 4486 (0ccd7f3e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.766 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.827 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.017.833 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.835 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.836 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.836 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.836 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.837 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.838 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.838 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.839 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.839 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.841 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.841 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.842 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.844 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.844 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.844 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.659 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.751 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.761 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.762 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.763 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.763 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.763 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.764 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.026.764 I llama_model_loader: - type  f32:  194 tensors
0.00.026.764 I llama_model_loader: - type q4_K:   61 tensors
0.00.026.765 I llama_model_loader: - type q5_K:   24 tensors
0.00.026.765 I llama_model_loader: - type q6_K:   13 tensors
0.00.026.765 I print_info: file format = GGUF V3 (latest)
0.00.026.766 I print_info: file type   = Q4_K - Medium
0.00.026.767 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.046.794 I load: special tokens cache size = 25
0.00.053.053 I load: token to piece cache size = 0.2984 MB
0.00.053.058 I print_info: arch             = gptneox
0.00.053.058 I print_info: vocab_only       = 0
0.00.053.058 I print_info: n_ctx_train      = 2048
0.00.053.058 I print_info: n_embd           = 2048
0.00.053.058 I print_info: n_layer          = 24
0.00.053.063 I print_info: n_head           = 16
0.00.053.064 I print_info: n_head_kv        = 16
0.00.053.064 I print_info: n_rot            = 32
0.00.053.064 I print_info: n_swa            = 0
0.00.053.064 I print_info: n_embd_head_k    = 128
0.00.053.064 I print_info: n_embd_head_v    = 128
0.00.053.065 I print_info: n_gqa            = 1
0.00.053.065 I print_info: n_embd_k_gqa     = 2048
0.00.053.066 I print_info: n_embd_v_gqa     = 2048
0.00.053.067 I print_info: f_norm_eps       = 1.0e-05
0.00.053.067 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.053.067 I print_info: f_clamp_kqv      = 0.0e+00
0.00.053.067 I print_info: f_max_alibi_bias = 0.0e+00
0.00.053.067 I print_info: f_logit_scale    = 0.0e+00
0.00.053.068 I print_info: n_ff             = 8192
0.00.053.068 I print_info: n_expert         = 0
0.00.053.068 I print_info: n_expert_used    = 0
0.00.053.068 I print_info: causal attn      = 1
0.00.053.069 I print_info: pooling type     = 0
0.00.053.069 I print_info: rope type        = 2
0.00.053.070 I print_info: rope scaling     = linear
0.00.053.070 I print_info: freq_base_train  = 10000.0
0.00.053.071 I print_info: freq_scale_train = 1
0.00.053.071 I print_info: n_ctx_orig_yarn  = 2048
0.00.053.071 I print_info: rope_finetuned   = unknown
0.00.053.071 I print_info: ssm_d_conv       = 0
0.00.053.071 I print_info: ssm_d_inner      = 0
0.00.053.071 I print_info: ssm_d_state      = 0
0.00.053.073 I print_info: ssm_dt_rank      = 0
0.00.053.073 I print_info: ssm_dt_b_c_rms   = 0
0.00.053.074 I print_info: model type       = 1.4B
0.00.053.074 I print_info: model params     = 1.41 B
0.00.053.074 I print_info: general.name     = 1.4B
0.00.053.075 I print_info: vocab type       = BPE
0.00.053.075 I print_info: n_vocab          = 50304
0.00.053.076 I print_info: n_merges         = 50009
0.00.053.077 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.053.077 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.053.077 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.053.077 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.053.077 I print_info: LF token         = 128 'Ä'
0.00.053.078 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.053.078 I print_info: max token length = 1024
0.00.055.038 I load_tensors: offloading 24 repeating layers to GPU
0.00.055.039 I load_tensors: offloading output layer to GPU
0.00.055.039 I load_tensors: offloaded 25/25 layers to GPU
0.00.055.050 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.055.051 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.055.375 I llama_init_from_model: n_seq_max     = 1
0.00.055.376 I llama_init_from_model: n_ctx         = 128
0.00.055.376 I llama_init_from_model: n_ctx_per_seq = 128
0.00.055.376 I llama_init_from_model: n_batch       = 128
0.00.055.376 I llama_init_from_model: n_ubatch      = 128
0.00.055.376 I llama_init_from_model: flash_attn    = 0
0.00.055.377 I llama_init_from_model: freq_base     = 10000.0
0.00.055.377 I llama_init_from_model: freq_scale    = 1
0.00.055.377 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.055.378 I ggml_metal_init: allocating
0.00.055.381 I ggml_metal_init: found device: Apple M4
0.00.055.383 I ggml_metal_init: picking default device: Apple M4
0.00.056.010 I ggml_metal_init: using embedded metal library
0.00.059.005 I ggml_metal_init: GPU name:   Apple M4
0.00.059.007 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.059.008 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.059.008 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.059.009 I ggml_metal_init: simdgroup reduction   = true
0.00.059.010 I ggml_metal_init: simdgroup matrix mul. = true
0.00.059.010 I ggml_metal_init: has bfloat            = true
0.00.059.010 I ggml_metal_init: use bfloat            = true
0.00.059.010 I ggml_metal_init: hasUnifiedMemory      = true
0.00.059.011 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.243 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.069.572 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.069.580 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.069.600 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.070.521 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.070.522 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.070.522 I llama_init_from_model: graph nodes  = 967
0.00.070.522 I llama_init_from_model: graph splits = 2
0.00.070.523 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.070.524 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.572.887 I 
0.00.572.919 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.572.923 I perplexity: tokenizing the input ..
0.00.580.255 I perplexity: tokenization took 7.33 ms
0.00.580.257 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.714.145 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.715.671 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.715.694 I llama_perf_context_print:        load time =     562.11 ms
0.00.715.697 I llama_perf_context_print: prompt eval time =     133.66 ms /   128 tokens (    1.04 ms per token,   957.64 tokens per second)
0.00.715.697 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.715.699 I llama_perf_context_print:       total time =     142.81 ms /   129 tokens
0.00.716.104 I ggml_metal_free: deallocating

real	0m0.735s
user	0m0.079s
sys	0m0.100s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.099 I build: 4486 (0ccd7f3e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.200 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.040 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.046 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.048 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.048 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.049 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.049 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.049 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.050 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.051 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.051 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.051 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.052 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.052 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.053 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.054 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.055 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.055 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.806 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.835 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.636 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.637 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.637 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.638 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.638 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.638 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.639 I llama_model_loader: - type  f32:  194 tensors
0.00.024.639 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.640 I llama_model_loader: - type q6_K:   37 tensors
0.00.024.640 I print_info: file format = GGUF V3 (latest)
0.00.024.641 I print_info: file type   = Q5_K - Medium
0.00.024.642 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.043.619 I load: special tokens cache size = 25
0.00.049.711 I load: token to piece cache size = 0.2984 MB
0.00.049.716 I print_info: arch             = gptneox
0.00.049.716 I print_info: vocab_only       = 0
0.00.049.717 I print_info: n_ctx_train      = 2048
0.00.049.717 I print_info: n_embd           = 2048
0.00.049.717 I print_info: n_layer          = 24
0.00.049.721 I print_info: n_head           = 16
0.00.049.722 I print_info: n_head_kv        = 16
0.00.049.722 I print_info: n_rot            = 32
0.00.049.723 I print_info: n_swa            = 0
0.00.049.723 I print_info: n_embd_head_k    = 128
0.00.049.723 I print_info: n_embd_head_v    = 128
0.00.049.724 I print_info: n_gqa            = 1
0.00.049.724 I print_info: n_embd_k_gqa     = 2048
0.00.049.725 I print_info: n_embd_v_gqa     = 2048
0.00.049.726 I print_info: f_norm_eps       = 1.0e-05
0.00.049.726 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.726 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.726 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.726 I print_info: f_logit_scale    = 0.0e+00
0.00.049.727 I print_info: n_ff             = 8192
0.00.049.727 I print_info: n_expert         = 0
0.00.049.727 I print_info: n_expert_used    = 0
0.00.049.727 I print_info: causal attn      = 1
0.00.049.728 I print_info: pooling type     = 0
0.00.049.728 I print_info: rope type        = 2
0.00.049.728 I print_info: rope scaling     = linear
0.00.049.728 I print_info: freq_base_train  = 10000.0
0.00.049.729 I print_info: freq_scale_train = 1
0.00.049.729 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.732 I print_info: rope_finetuned   = unknown
0.00.049.732 I print_info: ssm_d_conv       = 0
0.00.049.732 I print_info: ssm_d_inner      = 0
0.00.049.732 I print_info: ssm_d_state      = 0
0.00.049.732 I print_info: ssm_dt_rank      = 0
0.00.049.732 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.733 I print_info: model type       = 1.4B
0.00.049.733 I print_info: model params     = 1.41 B
0.00.049.733 I print_info: general.name     = 1.4B
0.00.049.734 I print_info: vocab type       = BPE
0.00.049.736 I print_info: n_vocab          = 50304
0.00.049.736 I print_info: n_merges         = 50009
0.00.049.736 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.736 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.736 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.737 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.737 I print_info: LF token         = 128 'Ä'
0.00.049.737 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.737 I print_info: max token length = 1024
0.00.051.663 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.664 I load_tensors: offloading output layer to GPU
0.00.051.664 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.675 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.051.677 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.052.059 I llama_init_from_model: n_seq_max     = 1
0.00.052.060 I llama_init_from_model: n_ctx         = 128
0.00.052.060 I llama_init_from_model: n_ctx_per_seq = 128
0.00.052.060 I llama_init_from_model: n_batch       = 128
0.00.052.060 I llama_init_from_model: n_ubatch      = 128
0.00.052.060 I llama_init_from_model: flash_attn    = 0
0.00.052.061 I llama_init_from_model: freq_base     = 10000.0
0.00.052.061 I llama_init_from_model: freq_scale    = 1
0.00.052.062 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.062 I ggml_metal_init: allocating
0.00.052.066 I ggml_metal_init: found device: Apple M4
0.00.052.068 I ggml_metal_init: picking default device: Apple M4
0.00.052.674 I ggml_metal_init: using embedded metal library
0.00.055.156 I ggml_metal_init: GPU name:   Apple M4
0.00.055.158 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.158 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.158 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.159 I ggml_metal_init: simdgroup reduction   = true
0.00.055.159 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.159 I ggml_metal_init: has bfloat            = true
0.00.055.159 I ggml_metal_init: use bfloat            = true
0.00.055.160 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.160 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.849 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.213 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.220 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.238 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.067.153 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.067.155 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.067.155 I llama_init_from_model: graph nodes  = 967
0.00.067.155 I llama_init_from_model: graph splits = 2
0.00.067.157 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.157 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.576.717 I 
0.00.576.746 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.576.749 I perplexity: tokenizing the input ..
0.00.584.953 I perplexity: tokenization took 8.203 ms
0.00.584.960 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.724.857 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.726.297 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.726.319 I llama_perf_context_print:        load time =     567.51 ms
0.00.726.319 I llama_perf_context_print: prompt eval time =     139.65 ms /   128 tokens (    1.09 ms per token,   916.57 tokens per second)
0.00.726.322 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.726.322 I llama_perf_context_print:       total time =     149.61 ms /   129 tokens
0.00.726.653 I ggml_metal_free: deallocating

real	0m0.742s
user	0m0.078s
sys	0m0.101s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.100 I build: 4486 (0ccd7f3e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.505 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.336 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.017.341 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.345 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.346 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.346 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.347 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.347 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.348 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.348 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.349 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.349 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.351 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.351 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.351 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.353 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.353 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.354 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.185 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.174 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.968 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.970 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.970 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.970 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.971 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.971 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.972 I llama_model_loader: - type  f32:  194 tensors
0.00.025.972 I llama_model_loader: - type q6_K:   98 tensors
0.00.025.973 I print_info: file format = GGUF V3 (latest)
0.00.025.973 I print_info: file type   = Q6_K
0.00.025.974 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.045.991 I load: special tokens cache size = 25
0.00.052.298 I load: token to piece cache size = 0.2984 MB
0.00.052.302 I print_info: arch             = gptneox
0.00.052.302 I print_info: vocab_only       = 0
0.00.052.303 I print_info: n_ctx_train      = 2048
0.00.052.303 I print_info: n_embd           = 2048
0.00.052.303 I print_info: n_layer          = 24
0.00.052.307 I print_info: n_head           = 16
0.00.052.308 I print_info: n_head_kv        = 16
0.00.052.308 I print_info: n_rot            = 32
0.00.052.308 I print_info: n_swa            = 0
0.00.052.308 I print_info: n_embd_head_k    = 128
0.00.052.308 I print_info: n_embd_head_v    = 128
0.00.052.309 I print_info: n_gqa            = 1
0.00.052.309 I print_info: n_embd_k_gqa     = 2048
0.00.052.310 I print_info: n_embd_v_gqa     = 2048
0.00.052.310 I print_info: f_norm_eps       = 1.0e-05
0.00.052.313 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.052.313 I print_info: f_clamp_kqv      = 0.0e+00
0.00.052.313 I print_info: f_max_alibi_bias = 0.0e+00
0.00.052.314 I print_info: f_logit_scale    = 0.0e+00
0.00.052.314 I print_info: n_ff             = 8192
0.00.052.314 I print_info: n_expert         = 0
0.00.052.315 I print_info: n_expert_used    = 0
0.00.052.315 I print_info: causal attn      = 1
0.00.052.315 I print_info: pooling type     = 0
0.00.052.315 I print_info: rope type        = 2
0.00.052.315 I print_info: rope scaling     = linear
0.00.052.316 I print_info: freq_base_train  = 10000.0
0.00.052.338 I print_info: freq_scale_train = 1
0.00.052.340 I print_info: n_ctx_orig_yarn  = 2048
0.00.052.340 I print_info: rope_finetuned   = unknown
0.00.052.340 I print_info: ssm_d_conv       = 0
0.00.052.340 I print_info: ssm_d_inner      = 0
0.00.052.343 I print_info: ssm_d_state      = 0
0.00.052.343 I print_info: ssm_dt_rank      = 0
0.00.052.343 I print_info: ssm_dt_b_c_rms   = 0
0.00.052.343 I print_info: model type       = 1.4B
0.00.052.344 I print_info: model params     = 1.41 B
0.00.052.344 I print_info: general.name     = 1.4B
0.00.052.345 I print_info: vocab type       = BPE
0.00.052.345 I print_info: n_vocab          = 50304
0.00.052.345 I print_info: n_merges         = 50009
0.00.052.346 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.052.346 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.052.346 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.052.346 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.052.347 I print_info: LF token         = 128 'Ä'
0.00.052.349 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.052.349 I print_info: max token length = 1024
0.00.054.277 I load_tensors: offloading 24 repeating layers to GPU
0.00.054.277 I load_tensors: offloading output layer to GPU
0.00.054.277 I load_tensors: offloaded 25/25 layers to GPU
0.00.054.288 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.054.290 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.054.596 I llama_init_from_model: n_seq_max     = 1
0.00.054.597 I llama_init_from_model: n_ctx         = 128
0.00.054.597 I llama_init_from_model: n_ctx_per_seq = 128
0.00.054.597 I llama_init_from_model: n_batch       = 128
0.00.054.597 I llama_init_from_model: n_ubatch      = 128
0.00.054.598 I llama_init_from_model: flash_attn    = 0
0.00.054.598 I llama_init_from_model: freq_base     = 10000.0
0.00.054.598 I llama_init_from_model: freq_scale    = 1
0.00.054.599 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.599 I ggml_metal_init: allocating
0.00.054.604 I ggml_metal_init: found device: Apple M4
0.00.054.606 I ggml_metal_init: picking default device: Apple M4
0.00.055.224 I ggml_metal_init: using embedded metal library
0.00.057.638 I ggml_metal_init: GPU name:   Apple M4
0.00.057.640 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.641 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.641 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.641 I ggml_metal_init: simdgroup reduction   = true
0.00.057.641 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.642 I ggml_metal_init: has bfloat            = true
0.00.057.642 I ggml_metal_init: use bfloat            = true
0.00.057.642 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.643 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.379 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.068.648 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.654 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.672 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.069.552 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.069.553 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.069.553 I llama_init_from_model: graph nodes  = 967
0.00.069.553 I llama_init_from_model: graph splits = 2
0.00.069.555 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.069.555 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.255.334 I 
0.00.255.369 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.255.374 I perplexity: tokenizing the input ..
0.00.262.669 I perplexity: tokenization took 7.293 ms
0.00.262.673 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.402.839 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.404.044 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.404.066 I llama_perf_context_print:        load time =     244.82 ms
0.00.404.070 I llama_perf_context_print: prompt eval time =     139.94 ms /   128 tokens (    1.09 ms per token,   914.68 tokens per second)
0.00.404.070 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.404.071 I llama_perf_context_print:       total time =     148.73 ms /   129 tokens
0.00.404.578 I ggml_metal_free: deallocating

real	0m0.420s
user	0m0.079s
sys	0m0.045s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.366 I build: 4486 (0ccd7f3e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.022.161 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.037.710 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.037.717 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.037.719 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.037.720 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.037.721 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.037.722 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.037.722 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.037.724 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.037.724 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.037.725 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.037.727 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.037.727 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.037.728 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.037.729 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.037.731 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.037.731 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.037.732 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.045.161 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.047.038 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.053.419 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.053.420 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.053.421 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.053.421 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.053.422 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.053.422 I llama_model_loader: - type  f32:  194 tensors
0.00.053.423 I llama_model_loader: - type  f16:   98 tensors
0.00.053.423 I print_info: file format = GGUF V3 (latest)
0.00.053.424 I print_info: file type   = all F32 (guessed)
0.00.053.425 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.079.300 I load: special tokens cache size = 25
0.00.085.843 I load: token to piece cache size = 0.2984 MB
0.00.085.846 I print_info: arch             = gptneox
0.00.085.846 I print_info: vocab_only       = 0
0.00.085.846 I print_info: n_ctx_train      = 2048
0.00.085.847 I print_info: n_embd           = 2048
0.00.085.847 I print_info: n_layer          = 24
0.00.085.849 I print_info: n_head           = 16
0.00.085.850 I print_info: n_head_kv        = 16
0.00.085.850 I print_info: n_rot            = 32
0.00.085.850 I print_info: n_swa            = 0
0.00.085.850 I print_info: n_embd_head_k    = 128
0.00.085.851 I print_info: n_embd_head_v    = 128
0.00.085.851 I print_info: n_gqa            = 1
0.00.085.852 I print_info: n_embd_k_gqa     = 2048
0.00.085.852 I print_info: n_embd_v_gqa     = 2048
0.00.085.853 I print_info: f_norm_eps       = 1.0e-05
0.00.085.853 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.085.853 I print_info: f_clamp_kqv      = 0.0e+00
0.00.085.855 I print_info: f_max_alibi_bias = 0.0e+00
0.00.085.855 I print_info: f_logit_scale    = 0.0e+00
0.00.085.856 I print_info: n_ff             = 8192
0.00.085.856 I print_info: n_expert         = 0
0.00.085.856 I print_info: n_expert_used    = 0
0.00.085.856 I print_info: causal attn      = 1
0.00.085.857 I print_info: pooling type     = 0
0.00.085.857 I print_info: rope type        = 2
0.00.085.858 I print_info: rope scaling     = linear
0.00.085.858 I print_info: freq_base_train  = 10000.0
0.00.085.858 I print_info: freq_scale_train = 1
0.00.085.858 I print_info: n_ctx_orig_yarn  = 2048
0.00.085.858 I print_info: rope_finetuned   = unknown
0.00.085.859 I print_info: ssm_d_conv       = 0
0.00.085.859 I print_info: ssm_d_inner      = 0
0.00.085.859 I print_info: ssm_d_state      = 0
0.00.085.859 I print_info: ssm_dt_rank      = 0
0.00.085.859 I print_info: ssm_dt_b_c_rms   = 0
0.00.085.859 I print_info: model type       = 1.4B
0.00.085.860 I print_info: model params     = 1.41 B
0.00.085.860 I print_info: general.name     = 1.4B
0.00.085.860 I print_info: vocab type       = BPE
0.00.085.860 I print_info: n_vocab          = 50304
0.00.085.860 I print_info: n_merges         = 50009
0.00.085.860 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.085.861 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.085.861 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.085.861 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.085.861 I print_info: LF token         = 128 'Ä'
0.00.085.861 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.085.862 I print_info: max token length = 1024
0.00.087.812 I load_tensors: offloading 24 repeating layers to GPU
0.00.087.812 I load_tensors: offloading output layer to GPU
0.00.087.812 I load_tensors: offloaded 25/25 layers to GPU
0.00.087.817 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.087.818 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.088.083 I llama_init_from_model: n_seq_max     = 1
0.00.088.084 I llama_init_from_model: n_ctx         = 128
0.00.088.084 I llama_init_from_model: n_ctx_per_seq = 128
0.00.088.084 I llama_init_from_model: n_batch       = 128
0.00.088.084 I llama_init_from_model: n_ubatch      = 128
0.00.088.084 I llama_init_from_model: flash_attn    = 0
0.00.088.085 I llama_init_from_model: freq_base     = 10000.0
0.00.088.085 I llama_init_from_model: freq_scale    = 1
0.00.088.086 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.088.086 I ggml_metal_init: allocating
0.00.088.089 I ggml_metal_init: found device: Apple M4
0.00.088.091 I ggml_metal_init: picking default device: Apple M4
0.00.088.677 I ggml_metal_init: using embedded metal library
0.00.091.190 I ggml_metal_init: GPU name:   Apple M4
0.00.091.191 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.091.191 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.091.192 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.091.192 I ggml_metal_init: simdgroup reduction   = true
0.00.091.192 I ggml_metal_init: simdgroup matrix mul. = true
0.00.091.192 I ggml_metal_init: has bfloat            = true
0.00.091.192 I ggml_metal_init: use bfloat            = true
0.00.091.193 I ggml_metal_init: hasUnifiedMemory      = true
0.00.091.194 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.100.339 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.101.670 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.101.673 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.101.690 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.102.596 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.102.597 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.102.597 I llama_init_from_model: graph nodes  = 967
0.00.102.598 I llama_init_from_model: graph splits = 2
0.00.102.599 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.102.599 I 
0.00.102.618 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.102.619 I compute_imatrix: tokenizing the input ..
0.00.109.313 I compute_imatrix: tokenization took 6.693 ms
0.00.109.315 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.709.549 I compute_imatrix: 1.60 seconds per pass - ETA 0.02 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.712.024 I llama_perf_context_print:        load time =    1687.39 ms
0.01.712.024 I llama_perf_context_print: prompt eval time =    1599.62 ms /   128 tokens (   12.50 ms per token,    80.02 tokens per second)
0.01.712.025 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.712.026 I llama_perf_context_print:       total time =    1689.86 ms /   129 tokens
0.01.712.521 I ggml_metal_free: deallocating

real	0m1.897s
user	0m0.165s
sys	0m0.236s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4486 (0ccd7f3e)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 'Ä'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x107e0a3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x107e0aab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x107e0b060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x107e0b610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x107e0bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x107e0c170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x107e0c720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x107e0ccd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x107e0d280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x107e0d780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x107e0dc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x107e0e180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x107e0eca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x107e0f450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x107e0fc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x107e10380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x107e10aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x107e111c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x107e118e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x107e120b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x107e127d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x107e12ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x107e13610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x107e13eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x107e145d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x107e14890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x107e14ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x107e15b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x107e16050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x107e16310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x107e167b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x107e16a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x107e17300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x107e17840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x107e17b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x107e17fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x107e18440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x107e188e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x107e18d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x107e19220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x107e196c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x107e19b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x107e1a000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x107e1a4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x107e1a760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x107e1ad70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x107e1b380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x107e1bca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x107e1c2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x107e1c8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x107e1ced0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x107e1d4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x107e1daf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x107e1e100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x107e1e8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x107e1ed90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x107e1f230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x107e1f4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x107e1fb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x107e202f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x107e205b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x107e20a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x107e20ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x107e21390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x107e21830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x107e21cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x107e22170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x107e22610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x107e22ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x107e22f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x107e233f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x107e23890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x107e23d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x107e24280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x107e247d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x107e24d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x107e25270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x107e257c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x107e25d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x107e26260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x107e267b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x107e26d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x107e27250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x107e277a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x107e27cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x107e28240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x107e28790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x107e28ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x107e29230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x107e29780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x107e29cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x107e2a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x107e2a770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x107e2acc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x107e2b210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x107e2b760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x107e2bcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x107e1b990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x107e2c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x107e2c8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x107e2ce20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x107e2d370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x107e2d8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x107e2de10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x107e2e360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x107e2e8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x107e2ee00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x107e2f350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x107e2f8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x107e2fdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x107e30340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x107e30890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x107e30de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x107e31280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x107e31720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x107e31bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x107e32060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x107e32500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x107e329a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x107e32e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x107e332e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x107e33780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x107e33c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x107e340c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x107e34560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x107e34a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x107e34ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x107e35340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x107e357e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x107e35c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x107e36120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x107e365c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x107e36a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x107e36f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x107e373a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x107e37840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x107e37ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x107e38180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x107e38620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x107e38ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x107e38f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x107e39400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x107e398a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x107e39d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x107e3a1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x107e3a680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x107e3ab20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x107e3afc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x107e3b460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x107e3b900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x107e3bda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x107e3c240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x107e3c6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x107e3cb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x107e3d020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x107e3d4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x107e3d960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x107e3de00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x107e3e2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x107e3e740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x107e3ebe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x107e3f080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x107e3f520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x107e3f9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x107e3fe60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x107e40300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x107e407a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x107e40c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x107e410e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x107e41580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x107e41a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x107e41ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x107e42360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x107e42800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x107e42ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x107e43140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x107e435e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x107e43a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x107e43f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x107e443c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x107e44860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x107e44d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x107e451a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x107e45640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x107e45ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x107e45f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x107e46420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x107e468c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x107e46d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x107e47200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x107e476a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x107e47b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x107e47fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x107e48530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x107e48a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x107e48fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x107e49520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x107e497e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x107e49df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x107e4a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x107e4aa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x107e4b200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x107e4b6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x107e4b960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x107e4bf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x107e4c580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x107e4cd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x107e4d210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x107e4d6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x107e4db50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x107e4e300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x107e4e850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x107e4eda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x107e4f2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x107e4f840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x107e4fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x107e502e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x107e50830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x107e50d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x107e512d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x107e51820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x107e51d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x107e522c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x107e52810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x107e52d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x107e532b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x107e53800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x107e53d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x107e542a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x107e547f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x107e54d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x107e55290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x107e557e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x107e55d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x107e56280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x107e567d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x107e56d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x107e57270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x107e577c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x107e57d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x107e58260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x107e587b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x107e58d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x107e59250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x107e597a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x107e59cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x107e5a240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x107e5a790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x107e5ace0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x107e5b230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x107e5b780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x107e5bcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x107e5c220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x107e5c770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x107e5ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x107e5d210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x107e5d760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x107e5dcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x107e5e200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x107e5e750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x107e5eca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x107e5f1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x107e5f740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x107e5fc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x107e601e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x107e60730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x107e60c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x107e61120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x107e615c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x107e61a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x107e61f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x107e623a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x107e62840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x107e62ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x107e63180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x107e63620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x107e63ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x107e63f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x107e64400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x107e648a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x107e64d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x107e651e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x107e65730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x107e65e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x107e66570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x107e66c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x107e673b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x107e67670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x107e67e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x107e68120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x107e68730 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.183.203 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.183.208 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x107e683e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x107e4a0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x107e49aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x107e4a6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x107e1d7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x107e1d190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x107e1f7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x107e4c230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x107e14b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x107e1b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x107e1bf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x107e1c570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x107e1aa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x107e1cb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x107e13b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x107e099d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x107e1e3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x107e1fdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x107e2c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x107e67930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x107e16d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x107e16ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x107e4c840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x107e4acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x107e15160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x107e15420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x107e156e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x107e68b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x107e68e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x107e69110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x107e693d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x107e69690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x107e69950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x107e69c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x107e69ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x107e6a190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x107e6a450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x107e6a710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x107e6a9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x107e6ac90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x107e6af50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x107e6b210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x107e6b4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x107e6b790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x107e6ba50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x107e6bd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x107e6bfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x107e6c290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x107e6c550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x107e6c810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x107e6cad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x107e6cd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x107e6d050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x107e6d310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x107e6d5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x107e6d890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x107e6db50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x107e6de10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x107e6e0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x107e6e390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x107e6e650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x107e6e910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x107e6ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x107e6ee90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x107e6f150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x107e6f410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x107e6f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x107e6f990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x107e6fc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x107e6ff10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x107e701d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x107e70490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x107e70750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x107e70a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x107e70cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x107e70f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x107e71250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x107e71510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x107e717d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x107e71a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x107e71d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x107e72010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x107e722d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x107e72590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x107e72850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x107e72b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x107e72dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x107e73090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x107e73350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x107e73610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x107e738d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x107e73b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x107e73e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x107e74110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x107e743d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x107e74690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x107e74950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x107e74c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x107e74ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x107e75190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x107e75450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x107e75710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x107e759d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x107e75c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x107e75f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x107e76210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x107e764d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x107e76790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x107e76a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x107e76d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x107e76fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x107e77290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x107e77550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x107e77810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x107e77ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x107e77d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x107e78050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x107e78310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x107e785d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x107e78890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x107e78b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x107e78e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x107e790d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x107e79390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x107e79650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x107e79910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x107e79bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x107e79e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x107e7a150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x107e7a410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x107e7a6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x107e7a990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x107e7ac50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x107e7af10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x107e7b1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x107e7b490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x107e7b750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x107e7ba10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x107e7bcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x107e7bf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x107e7c250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x107e7c510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x107e7c7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x107e7ca90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x107e7cd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x107e7d010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x107e7d2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x107e7d590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x107e7d850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x107e7db10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x107e7ddd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x107e7e090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x107e7e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x107e7e610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x107e7e8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x107e7eb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x107e7ee50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x107e7f110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x107e7f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x107e7f690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x107e7f950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x107e7fc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x107e7fed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x107e80190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x107e80450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x107e80710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x107e809d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x107e80c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x107e80f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x107e81210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x107e814d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x107e81790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x107e81a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x107e81d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x107e81fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x107e82290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x107e82550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x107e82810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x107e82ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x107e82d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x107e83050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x107e83310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x107e835d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x107e83890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x107e83b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x107e83e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x107e840d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x107e84390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x107e84650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x107e84910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x107e84bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x107e84e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x107e85150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x107e85410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x107e856d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x107e85990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x107e85c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x107e85f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x107e861d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x107e86490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x107e86750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x107e86a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x107e86cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x107e86f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x107e87250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x107e87510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x107e877d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x107e87a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x107e87d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x107e88010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x107e885e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x107e888a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x107e88b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x107e88e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x107e890e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x107e893a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x107e89660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x107e89920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x107e89be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x107e89ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x107e8a160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x107e8a420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x107e8a6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x107e8a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x107e8ac60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x107e8af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x107e8b1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x107e8b4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x107e8b760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x107e8ba20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x107e8bce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x107e8bfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x107e8c260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x107e8c520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x107e8c7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x107e8caa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x107e8cd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x107e8d020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x107e8d570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x107e8dac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x107e8e010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x107e8e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x107e8eab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x107e8f000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x107e8f550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x107e8faa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x107e8fff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x107e90540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x107e90a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x107e90fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x107e91530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x107e91a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x107e91fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x107e92520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x107e92a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x107e92fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x107e93510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x107e93a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x107e93fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x107e94500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x107e94a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x107e94fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x107e954f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x107e95a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x107e95f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x107e964e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x107e96a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x107e96cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x107e96fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x107e974b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x107e979b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x107e97eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x107e983b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x107e988b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x107e98db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x107e992b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x107e997b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x107e99cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x107e9a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x107e9a6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x107e9abb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x107e9b0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x107e9b5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x107e9bfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x107e9c6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x107e9ce00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x107e9d520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x107e9d7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x107e9dfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x107e9e290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x107e9e8a0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x107e9e550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x107e4bc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x107e9daa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x107e9ed00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x107e9efc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x107e9f280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x107e9f540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x107e9f800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x107e9fac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x107e9fd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x107ea0040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x107ea0300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x107ea08d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x107ea0ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x107ea14d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x107ea1790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x107ea1a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x107ea1d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x107ea1fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x107ea2290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x107ea2550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x107ea2810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x107ea2ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x107ea2d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x107ea3050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x107ea3310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x107ea35d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x107ea3890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x107ea3b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x107ea3e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x107ea40d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x107ea4390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x107ea4650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x107ea4910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x107ea4bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x107ea4e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x107ea5150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x107ea5410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x107ea56d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x107ea5990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x107ea5c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x107ea5f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x107ea61d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x107ea6490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x107ea6750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x107ea6a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x107ea6cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x107ea6f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x107ea7250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x107ea7510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x107ea77d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x107ea7a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x107ea7d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x107ea8010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x107ea82d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x107ea8590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x107ea8850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x107ea8b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x107ea8dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x107ea9090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x107ea9350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x107ea9610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x107ea98d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x107ea9b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x107ea9e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x107eaa110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x107eaa3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x107eaa690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x107eaa950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x107eaac10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x107eaaed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x107eab190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x107eab450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x107eab710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x107eab9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x107eabc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x107eabf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x107eac210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x107eac4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x107eac790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x107eaca50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x107eacd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x107eacfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x107ead290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x107ead550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x107ead810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x107eadad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x107eadd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x107eae050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x107eae310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x107eae5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x107eae890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x107eaeb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x107eaee10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x107eaf0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x107eaf390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x107eaf650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x107eaf910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x107eafbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x107eafe90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x107eb0150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x107eb0410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x107eb06d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x107eb0990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x107eb0c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x107eb0f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x107eb11d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x107eb1490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x107eb1750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x107eb1a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x107eb1cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x107eb1f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x107eb2250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x107eb2510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x107eb27d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x107eb2a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x107eb2d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x107eb3010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x107eb32d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x107eb3590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x107eb3850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x107eb3b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x107eb3dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x107eb4090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x107eb4350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x107eb4610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x107eb48d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x107eb4b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x107eb4e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x107eb5110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x107eb53d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x107eb5690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x107eb5950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x107eb5c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x107eb5ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x107eb6190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x107eb6450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x107eb6710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x107eb69d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x107eb6c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x107eb6f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x107eb7210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x107eb74d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x107eb7790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x107eb7a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x107eb7d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x107eb7fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x107eb8290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x107eb8550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x107eb8810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x107eb8ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x107eb8d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x107eb9050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x107eb9310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x107eb95d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x107eb9890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x107eb9b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x107eb9e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x107eba0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x107eba390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x107eba650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x107eba910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x107ebabd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x107ebae90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x107ebb150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x107ebb410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x107ebb6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x107ebb990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x107ebbc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x107ebbf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x107ebc1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x107ebc490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x107ebc750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x107ebca10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x107ebccd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x107ebcf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x107ebd250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x107ebd510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x107ebd7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x107ebda90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x107ebdd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x107ebe010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x107ebe2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x107ebe590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x107ebe850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x107ebeb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x107ebedd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x107ebf090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x107ebf350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x107ebf610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x107ebf8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x107ebfb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x107ebfe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x107ec0110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x107ec03d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x107ec0690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x107ec0950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x107ec0c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x107ec0ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x107ec1190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x107ec1450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x107ec1710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x107ec19d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x107ec1c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x107ec1f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x107ec2210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x107ec24d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x107ec2790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x107ec2a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x107ec2d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x107ec32e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x107ec35a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x107ec3860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x107ec3b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x107ec3de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x107ec40a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x107ec4360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x107ec4620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x107ec48e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x107ec4ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x107ec4e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x107ec5120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x107ec53e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x107ec56a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x107ec5960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x107ec5c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x107ec5ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x107ec61a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x107ec6460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x107ec6720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x107ec69e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x107ec6ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x107ec6f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x107ec7220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x107ec74e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x107ec77a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x107ec7a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x107ec7d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x107ec7fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x107ec82a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x107ec8560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x107ec8820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x107ec8ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x107ec8da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x107ec9060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x107ec9320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x107ec95e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x107ec98a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x107ec9b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x107ec9e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x107eca0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x107eca3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x107eca660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x107eca920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x107ecabe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x107ecaea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x107ecb160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x107ecb420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x107ecb6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x107ecb9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x107ecbc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x107ecbf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x107ecc1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x107ecc4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x107ecc760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x107ecca20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x107eccce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x107eccfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x107ecd260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x107ecd520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x107ecd7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x107ecdaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x107ecdd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x107ece160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x107ece420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x107ece6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x107eceb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x107ecefc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x107ecf430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x107ecf8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x107ecfd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x107ed0180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x107ed05f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x107ed1110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x107ed1830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x107ed1f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x107ed2670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x107ed2930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x107ed2bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x107ed3120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x107ed3590 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m2.040s
user	0m0.304s
sys	0m0.335s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4486 (0ccd7f3e)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 'Ä'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12280ac40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12280b350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12280b900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12280beb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12280c460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12280ca10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12280cfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12280d570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12280db20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12280e020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12280e520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12280ea20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12280f540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12280fcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x122810500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x122810c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x122811340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x122811a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x122812180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x122812950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x122813070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x122813790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x122813eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x122814750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x122814e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x122815130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x122815740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x1228163b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x1228168f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x122816bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x122817050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x122817310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x122817ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x1228180e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1228183a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x122818840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x122818ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x122819180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x122819620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x122819ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x122819f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12281a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12281a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12281ad40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12281b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12281b610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12281bc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12281c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12281cb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12281d160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12281d770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12281dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12281e390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12281e9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12281f190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12281f630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12281fad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12281fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x1228203a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x122820b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x122820e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x1228212f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x122821790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x122821c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x1228220d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x122822570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x122822a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x122822eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x122823350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x1228237f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x122823c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x122824130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x1228245d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x122824b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x122825070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x1228255c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x122825b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x122826060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x1228265b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x122826b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x122827050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x1228275a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x122827af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x122828040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x122828590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x122828ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x122829030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x122829580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x122829ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12282a020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12282a570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12282aac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12282b010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12282b560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12282bab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12282c000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12282c550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12281c230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12282c9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12282d170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12282d6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12282dc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12282e160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12282e6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12282ec00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12282f150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12282f6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12282fbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x122830140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x122830690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x122830be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x122831130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x122831680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x122831b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x122831fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x122832460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x122832900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x122832da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x122833240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x1228336e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x122833b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x122834020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x1228344c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x122834960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x122834e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x1228352a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x122835740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x122835be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x122836080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x122836520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x1228369c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x122836e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x122837300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x1228377a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x122837c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x1228380e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x122838580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x122838a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x122838ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x122839360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x122839800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x122839ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12283a140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12283a5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12283aa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12283af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12283b3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12283b860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12283bd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12283c1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12283c640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12283cae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12283cf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12283d420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12283d8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12283dd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12283e200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12283e6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12283eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12283efe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12283f480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12283f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12283fdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x122840260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x122840700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x122840ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x122841040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x1228414e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x122841980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x122841e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x1228422c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x122842760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x122842c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x1228430a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x122843540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x1228439e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x122843e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x122844320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x1228447c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x122844c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x122845100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x1228455a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x122845a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x122845ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x122846380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x122846820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x122846cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x122847160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x122847600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x122847aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x122847f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x1228483e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x122848880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x122848dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x122849320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x122849870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x122849dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12284a080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12284a690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12284aca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12284b2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12284baa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12284bf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12284c200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12284c810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12284ce20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12284d610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12284dab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12284df50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12284e3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12284eba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12284f0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12284f640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12284fb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x1228500e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x122850630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x122850b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x1228510d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x122851620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x122851b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x1228520c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x122852610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x122852b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x1228530b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x122853600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x122853b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x1228540a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x1228545f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x122854b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x122855090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x1228555e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x122855b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x122856080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x1228565d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x122856b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x122857070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x1228575c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x122857b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x122858060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x1228585b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x122858b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x122859050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x1228595a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x122859af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12285a040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12285a590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12285aae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12285b030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12285b580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12285bad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12285c020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12285c570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12285cac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12285d010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12285d560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12285dab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12285e000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12285e550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12285eaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12285eff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12285f540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12285fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12285ffe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x122860530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x122860a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x122860fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x122861520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x1228619c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x122861e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x122862300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x1228627a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x122862c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x1228630e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x122863580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x122863a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x122863ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x122864360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x122864800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x122864ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x122865140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x1228655e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x122865a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x122865fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x1228666f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x122866e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x122867530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x122867c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x122867f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x122868700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1228689c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x122868fd0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.088.838 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.088.848 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x127b04bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x127b05040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x127b054b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x127b05920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x127b05d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x127b06200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x127b06670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x127b06ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x127b06f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x127b073c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x127b07830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x127b07f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x127b08a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x127b091f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x127b09a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x127b0a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x127b0a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x127b0af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x127b0b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x127b0bdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x127b0c4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x127b0cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x127b0d310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x127b0da30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x127b0e150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x127b0e410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x127b0e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x127b0eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x127b0efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x127b0f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x127b0f890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x127b0fdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x127b10230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x127b104f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x127b10960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x127b10dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x127b11240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x127b116b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x127b11b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x127b11f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x127b12400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x127b12870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x127b12ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x127b13150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x127b135c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x127b13a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x127b13ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x127b14310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x127b14780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x127b14bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x127b15060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x127b154d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x127b15940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x127b15db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x127b16220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x127b16690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x127b16c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x127b17100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x127b17570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x127b179e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x127b17e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x127b182c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x127b18730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x127b18ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x127b19010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x127b19480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x127b198f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x127b19d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x127b1a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x127b1a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x127b1aab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x127b1af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x127b1b390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x127b1b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x127b1bc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x127b1c0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x127b1c550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x127b1c9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x127b1ce30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x127b1d2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x127b1d710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x127b1db80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x127b1dff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x127b1e460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x127b1e8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x127b1ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x127b1f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x127b1f620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x127b1fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x127b1ff00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x127b20370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x127b207e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x127b20c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x127b210c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x127b21530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x127b219a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x127b21e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x127b22280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x127b226f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x127b22b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x127b22fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x127b23440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x127b238b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x127b23d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x127b24190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x127b24600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x127b24a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x127b24ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x127b25350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x127b257c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x127b25c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x127b260a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x127b26510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x127b26980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x127b26df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x127b27260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x127b276d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x127b27b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x127b27fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x127b28420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x127b28890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x127b28d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x127b29170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x127b295e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x127b29a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x127b29ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x127b2a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x127b2a7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x127b2ac10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x127b2b080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x127b2b4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x127b2b960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x127b2bdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x127b2c240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x127b2c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x127b2cb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x127b2cf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x127b2d400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x127b2d870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x127b2dce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x127b2e150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x127b2e5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x127b2ea30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x127b2eea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x127b2f310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x127b2f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x127b2fbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x127b30060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x127b304d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x127b30940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x127b30db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x127b31220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x127b31690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x127b31b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x127b31f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x127b323e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x127b32850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x127b32cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x127b33130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x127b335a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x127b33a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x127b33e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x127b342f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x127b34760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x127b34bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x127b35040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x127b35c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x127b35f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x127b361f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x127b36660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x127b36ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x127b36f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x127b373b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x127b37820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x127b37c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x127b38100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x127b38570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x127b389e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x127b38e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x127b392c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x127b39730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x127b39ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x127b3a010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x127b3a480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x127b3a8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x127b3ad60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x127b3b1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x127b3b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x127b3bab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x127b3bf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x127b3c390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x127b3c800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x127b3cc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x127b3d0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x127b3d550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x127b3d9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x127b3de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x127b3e2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x127b3e710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x127b3eb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x127b3eff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x127b3f460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x127b3f9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x127b3fed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x127b40340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x127b407b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x127b40c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x127b41090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x127b415b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x127b41ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x127b42630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x127b428f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x127b42eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x127b43470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x127b43a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x127b43ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x127b445b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x127b44b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x127b45130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x127b456f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x127b45cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x127b46270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x127b46830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x127b46df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x127b473b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x127b47970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x127b47f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x127b484f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x127b48ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x127b49070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x127b49630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x127b49bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x127b4a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x127b4a770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x127b4ad30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x127b4b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x127b4b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x127b4be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x127b4c430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x127b4c9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x127b4cfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x127b4d570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x127b4db30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x127b4e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x127b4e6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x127b4ec70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x127b4f230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x127b4f7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x127b4fdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x127b50370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x127b50930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x127b50ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x127b514b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x127b51a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x127b52030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x127b525f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x127b52bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x127b53170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x127b53730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x127b53cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x127b542b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x127b54870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x127b54e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x127b553f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x127b559b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x127b55f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x127b56530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x127b56af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x127b56ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x127b574f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x127b579f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x127b57ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x127b583f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x127b588f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x127b58df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x127b592f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x127b597f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x127b59cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x127b5a1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x127b5a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x127b5abf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x127b5b0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x127b5b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x127b5c000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x127b5c720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x127b5ce40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x127b5d560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x127b5d820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x127b5e010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x127b5e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x127b5e8e0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x121f0c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x121f0c9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x121f0ce60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x121f0d2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x121f0d740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x121f0dbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x121f0e020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x121f0e490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x121f0e900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x121f0ed70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x121f0f1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x121f0f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x121f10420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x121f10bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x121f113e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x121f11b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x121f12220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x121f12940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x121f13060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x121f13790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x121f13eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x121f145d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x121f14cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x121f15410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x121f15b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x121f15df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x121f160b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x121f16520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x121f16990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x121f16e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x121f17300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x121f17810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x121f17c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x121f17f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x121f183b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x121f18820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x121f18d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x121f19280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x121f19780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x121f19c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x121f1a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x121f1a680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x121f1ab80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x121f1b080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x121f1b580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x121f1b9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x121f1be60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x121f1c2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x121f1c740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x121f1cbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x121f1d020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x121f1d490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x121f1d900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x121f1dd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x121f1e1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x121f1e9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x121f1ee50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x121f1f110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x121f1f720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x121f1ff10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x121f203b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x121f20850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x121f20cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x121f21190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x121f21630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x121f21ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x121f21f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x121f22410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x121f228b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x121f22d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x121f231f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x121f23690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x121f23b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x121f24080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x121f245d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x121f24b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x121f25070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x121f255c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x121f25b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x121f26060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x121f265b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x121f26b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x121f27050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x121f275a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x121f27af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x121f28040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x121f28590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x121f28ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x121f29030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x121f29580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x121f29ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x121f2a020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x121f2a570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x121f2aac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x121f2b010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x121f2b560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x121f2bab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x121f2c000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x121f2c550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x121f2caa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x121f2cff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x121f2d540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x121f2da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x121f2dfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x121f2e530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x121f2ea80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x121f2efd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x121f2f520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x121f2fa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x121f2ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x121f30510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x121f30a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x121f30fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x121f31450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x121f318f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x121f31d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x121f32230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x121f326d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x121f32b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x121f33010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x121f334b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x121f33950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x121f33df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x121f34290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x121f34730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x121f34bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x121f35070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x121f35510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x121f359b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x121f35e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x121f362f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x121f36790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x121f36c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x121f370d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x121f37570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x121f37a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x121f37eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x121f38350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x121f387f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x121f38c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x121f39130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x121f395d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x121f39a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x121f39f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x121f3a3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x121f3a850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x121f3acf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x121f3b190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x121f3b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x121f3bad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x121f3bf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x121f3c410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x121f3c8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x121f3cd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x121f3d1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x121f3d690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x121f3db30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x121f3dfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x121f3e470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x121f3e910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x121f3edb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x121f3f250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x121f3f6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x121f3fb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x121f40030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x121f404d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x121f40970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x121f40e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x121f412b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x121f41750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x121f41bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x121f42090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x121f42530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x121f429d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x121f42e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x121f43310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x121f437b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x121f43c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x121f440f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x121f44590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x121f44a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x121f44ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x121f45370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x121f45810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x121f45cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x121f46150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x121f465f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x121f46a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x121f46f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x121f473d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x121f47870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x121f47d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x121f481b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x121f48700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x121f48c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x121f491a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x121f496f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x121f499b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x121f49fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x121f4a5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x121f4abe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x121f4b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x121f4b870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x121f4bb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x121f4c140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x121f4c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x121f4cf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x121f4d3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x121f4d880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x121f4dd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x121f4e4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x121f4ea20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x121f4ef70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x121f4f4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x121f4fa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x121f4ff60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x121f504b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x121f50a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x121f50f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x121f514a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x121f519f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x121f51f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x121f52490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x121f529e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x121f52f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x121f53480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x121f539d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x121f53f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x121f54470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x121f549c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x121f54f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x121f55460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x121f559b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x121f55f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x121f56450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x121f569a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x121f56ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x121f57440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x121f57990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x121f57ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x121f58430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x121f58980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x121f58ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x121f59420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x121f59970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x121f59ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x121f5a410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x121f5a960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x121f5aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x121f5b400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x121f5b950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x121f5bea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x121f5c3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x121f5c940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x121f5ce90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x121f5d3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x121f5d930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x121f5de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x121f5e3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x121f5e920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x121f5ee70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x121f5f3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x121f5f910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x121f5fe60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x121f603b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x121f60900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x121f60e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x121f612f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x121f61790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x121f61c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x121f620d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x121f62570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x121f62a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x121f62eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x121f63350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x121f637f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x121f63c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x121f64130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x121f645d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x121f64a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x121f64f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x121f653b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x121f65900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x121f66020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x121f66740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x121f66e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x121f67580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x121f67840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x121f68030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x121f682f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x121f68900 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.922s
user	0m0.243s
sys	0m0.139s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
