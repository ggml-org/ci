Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:301 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- ARM -mcpu not found, -mcpu=native will be used
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod - Success
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sve
-- Performing Test GGML_MACHINE_SUPPORTS_sve - Failed
-- ARM feature DOTPROD enabled
-- ARM feature MATMUL_INT8 enabled
-- ARM feature FMA enabled
-- ARM feature FP16_VECTOR_ARITHMETIC enabled
-- Adding CPU backend variant ggml-cpu: -mcpu=native+dotprod+i8mm+nosve 
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (2.3s)
-- Generating done (0.2s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m2.534s
user	0m0.865s
sys	0m1.229s
++ nproc
+ make -j10
[  3%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  3%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  3%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  3%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  3%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  3%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  3%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  3%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  4%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  5%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o
[  5%] Built target sha256
[  5%] Built target build_info
[  5%] Built target sha1
[  5%] Built target xxhash
[  6%] Linking CXX shared library libggml-base.dylib
[  6%] Built target ggml-base
[  6%] Generate assembly for embedded Metal library
Embedding Metal library
[  6%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[  7%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[  8%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 11%] Linking CXX shared library libggml-blas.dylib
[ 12%] Linking CXX shared library libggml-cpu.dylib
[ 12%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 13%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 13%] Built target ggml-blas
[ 13%] Built target ggml-cpu
[ 14%] Linking C shared library libggml-metal.dylib
[ 14%] Built target ggml-metal
[ 14%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 15%] Linking CXX shared library libggml.dylib
[ 15%] Built target ggml
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 15%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 15%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o
[ 21%] Linking CXX executable ../../bin/llama-gguf-hash
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o
[ 23%] Linking CXX executable ../../bin/llama-gguf
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 25%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 25%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 26%] Linking CXX shared library libllama.dylib
[ 26%] Built target llama-gguf
[ 26%] Built target llama-gguf-hash
[ 26%] Built target llama
[ 26%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 27%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 30%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 30%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 30%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 30%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 30%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 30%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 31%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 31%] Linking C executable ../bin/test-c
[ 33%] Linking CXX executable ../../bin/llama-simple
[ 33%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 33%] Linking CXX executable ../../bin/llama-quantize-stats
[ 34%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 34%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 35%] Linking CXX executable ../../bin/llama-simple-chat
[ 35%] Built target llava
[ 36%] Linking CXX static library libcommon.a
[ 36%] Linking CXX shared library libllava_shared.dylib
[ 36%] Built target llama-quantize-stats
[ 36%] Built target llama-simple
[ 36%] Built target test-c
[ 36%] Built target llama-simple-chat
[ 37%] Linking CXX static library libllava_static.a
[ 37%] Built target common
[ 37%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 40%] Built target llava_static
[ 40%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 42%] Built target llava_shared
[ 43%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 43%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 44%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 44%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 44%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 45%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 46%] Linking CXX executable ../bin/test-tokenizer-0
[ 46%] Linking CXX executable ../bin/test-sampling
[ 47%] Linking CXX executable ../bin/test-grammar-integration
[ 47%] Linking CXX executable ../bin/test-llama-grammar
[ 48%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 48%] Linking CXX executable ../bin/test-grammar-parser
[ 48%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 49%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 49%] Built target test-tokenizer-1-bpe
[ 50%] Linking CXX executable ../bin/test-log
[ 50%] Built target test-tokenizer-1-spm
[ 50%] Linking CXX executable ../bin/test-arg-parser
[ 50%] Built target test-tokenizer-0
[ 50%] Built target test-sampling
[ 50%] Built target test-llama-grammar
[ 50%] Built target test-grammar-parser
[ 50%] Built target test-grammar-integration
[ 50%] Built target test-json-schema-to-grammar
[ 50%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 54%] Built target test-log
[ 55%] Building CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o
[ 55%] Built target test-arg-parser
[ 56%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 57%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 57%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 57%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 57%] Linking CXX executable ../bin/test-chat-template
[ 57%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 58%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 58%] Linking CXX executable ../bin/test-gguf
[ 58%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 59%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 59%] Linking CXX executable ../bin/test-backend-ops
[ 59%] Linking CXX executable ../bin/test-model-load-cancel
[ 61%] Linking CXX executable ../bin/test-autorelease
[ 61%] Linking CXX executable ../bin/test-quantize-fns
[ 62%] Linking CXX executable ../bin/test-barrier
[ 62%] Linking CXX executable ../bin/test-quantize-perf
[ 62%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 63%] Linking CXX executable ../../bin/llama-batched-bench
[ 63%] Built target test-chat-template
[ 63%] Built target test-gguf
[ 63%] Built target test-backend-ops
[ 63%] Built target test-model-load-cancel
[ 63%] Built target test-autorelease
[ 64%] Linking CXX executable ../bin/test-rope
[ 64%] Built target test-quantize-fns
[ 64%] Built target test-quantize-perf
[ 64%] Built target test-barrier
[ 64%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 64%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 64%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 64%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 65%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 65%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 65%] Built target llama-batched-bench
[ 66%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 67%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 68%] Linking CXX executable ../../bin/llama-batched
[ 68%] Linking CXX executable ../../bin/llama-gguf-split
[ 69%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 70%] Linking CXX executable ../../bin/llama-embedding
[ 70%] Built target test-rope
[ 70%] Linking CXX executable ../../bin/llama-gritlm
[ 70%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 71%] Linking CXX executable ../../bin/llama-eval-callback
[ 71%] Linking CXX executable ../../bin/llama-infill
[ 71%] Linking CXX executable ../../bin/llama-imatrix
[ 72%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 73%] Linking CXX executable ../../bin/llama-bench
[ 73%] Built target llama-gritlm
[ 73%] Built target llama-gguf-split
[ 73%] Built target llama-gbnf-validator
[ 73%] Built target llama-batched
[ 73%] Built target llama-embedding
[ 73%] Built target llama-eval-callback
[ 73%] Linking CXX executable ../../bin/llama-lookahead
[ 73%] Built target llama-infill
[ 73%] Built target llama-imatrix
[ 75%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 75%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 76%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 76%] Built target llama-bench
[ 77%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 78%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 78%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 79%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 80%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 80%] Linking CXX executable ../../bin/llama-lookup-create
[ 80%] Linking CXX executable ../../bin/llama-lookup
[ 81%] Linking CXX executable ../../bin/llama-lookup-merge
[ 81%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 81%] Linking CXX executable ../../bin/llama-lookup-stats
[ 81%] Linking CXX executable ../../bin/llama-parallel
[ 82%] Linking CXX executable ../../bin/llama-cli
[ 82%] Linking CXX executable ../../bin/llama-passkey
[ 82%] Built target llama-lookahead
[ 82%] Linking CXX executable ../../bin/llama-perplexity
[ 82%] Linking CXX executable ../../bin/llama-quantize
[ 83%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 83%] Built target llama-lookup-merge
[ 83%] Built target llama-lookup-create
[ 83%] Built target llama-parallel
[ 83%] Built target llama-lookup
[ 83%] Built target llama-lookup-stats
[ 83%] Built target llama-cli
[ 83%] Built target llama-passkey
[ 83%] Built target llama-perplexity
[ 83%] Linking CXX executable ../../bin/llama-retrieval
[ 84%] Generating loading.html.hpp
[ 84%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 84%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 85%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 86%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 86%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 86%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 86%] Built target llama-quantize
[ 86%] Generating index.html.gz.hpp
[ 86%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 87%] Linking CXX executable ../../bin/llama-speculative-simple
[ 87%] Linking CXX executable ../../bin/llama-save-load-state
[ 88%] Linking CXX executable ../../bin/llama-speculative
[ 89%] Linking CXX executable ../../bin/llama-tokenize
[ 89%] Linking CXX executable ../../bin/llama-run
[ 89%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 90%] Linking CXX executable ../../bin/llama-tts
[ 91%] Linking CXX executable ../../bin/llama-gen-docs
[ 91%] Built target llama-retrieval
[ 92%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 92%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 92%] Built target llama-speculative-simple
[ 92%] Built target llama-tokenize
[ 92%] Built target llama-run
[ 92%] Built target llama-speculative
[ 92%] Built target llama-tts
[ 92%] Built target llama-save-load-state
[ 92%] Built target llama-gen-docs
[ 93%] Linking CXX executable ../../bin/llama-cvector-generator
[ 93%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 95%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 95%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 95%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 96%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 96%] Built target llama-convert-llama2c-to-ggml
[ 97%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 98%] Linking CXX executable ../../bin/llama-vdot
[ 99%] Linking CXX executable ../../bin/llama-export-lora
[ 99%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 99%] Linking CXX executable ../../bin/llama-llava-cli
[ 99%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 99%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Built target llama-cvector-generator
[ 99%] Built target llama-llava-cli
[ 99%] Built target llama-q8dot
[ 99%] Built target llama-qwen2vl-cli
[ 99%] Built target llama-vdot
[ 99%] Built target llama-export-lora
[ 99%] Built target llama-minicpmv-cli
[ 99%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m3.036s
user	0m6.112s
sys	0m9.986s

main: quantize time =  6011.74 ms
main:    total time =  6011.74 ms

main: quantize time =  1967.60 ms
main:    total time =  1967.60 ms

main: quantize time =  1793.54 ms
main:    total time =  1793.54 ms

main: quantize time =  3185.05 ms
main:    total time =  3185.05 ms

main: quantize time =  2696.58 ms
main:    total time =  2696.58 ms

main: quantize time =  5054.62 ms
main:    total time =  5054.62 ms

main: quantize time =  5703.82 ms
main:    total time =  5703.82 ms

main: quantize time =  6810.57 ms
main:    total time =  6810.57 ms

main: quantize time =  5991.79 ms
main:    total time =  5991.79 ms

main: quantize time =  4578.38 ms
main:    total time =  4578.38 ms
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.115 I build: 4454 (8eceb888) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.206 I main: llama backend init
0.00.000.211 I main: load the model and apply lora adapter, if any
0.00.030.143 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.042.485 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.042.497 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.042.500 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.042.501 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.042.501 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.042.502 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.042.503 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.042.504 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.042.504 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.042.505 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.042.506 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.042.506 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.042.507 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.042.508 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.042.510 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.042.511 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.042.511 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.049.492 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.051.730 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.056.417 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.056.421 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.056.422 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.056.426 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.056.426 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.056.427 I llama_model_loader: - type  f32:  194 tensors
0.00.056.427 I llama_model_loader: - type  f16:   98 tensors
0.00.077.576 I llm_load_vocab: special tokens cache size = 25
0.00.083.668 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.083.672 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.083.673 I llm_load_print_meta: arch             = gptneox
0.00.083.673 I llm_load_print_meta: vocab type       = BPE
0.00.083.673 I llm_load_print_meta: n_vocab          = 50304
0.00.083.673 I llm_load_print_meta: n_merges         = 50009
0.00.083.674 I llm_load_print_meta: vocab_only       = 0
0.00.083.674 I llm_load_print_meta: n_ctx_train      = 2048
0.00.083.679 I llm_load_print_meta: n_embd           = 2048
0.00.083.680 I llm_load_print_meta: n_layer          = 24
0.00.083.684 I llm_load_print_meta: n_head           = 16
0.00.083.685 I llm_load_print_meta: n_head_kv        = 16
0.00.083.685 I llm_load_print_meta: n_rot            = 32
0.00.083.685 I llm_load_print_meta: n_swa            = 0
0.00.083.685 I llm_load_print_meta: n_embd_head_k    = 128
0.00.083.685 I llm_load_print_meta: n_embd_head_v    = 128
0.00.083.686 I llm_load_print_meta: n_gqa            = 1
0.00.083.687 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.083.687 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.083.688 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.083.688 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.083.688 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.083.688 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.083.689 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.083.689 I llm_load_print_meta: n_ff             = 8192
0.00.083.689 I llm_load_print_meta: n_expert         = 0
0.00.083.689 I llm_load_print_meta: n_expert_used    = 0
0.00.083.689 I llm_load_print_meta: causal attn      = 1
0.00.083.690 I llm_load_print_meta: pooling type     = 0
0.00.083.691 I llm_load_print_meta: rope type        = 2
0.00.083.691 I llm_load_print_meta: rope scaling     = linear
0.00.083.691 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.083.692 I llm_load_print_meta: freq_scale_train = 1
0.00.083.692 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.083.692 I llm_load_print_meta: rope_finetuned   = unknown
0.00.083.692 I llm_load_print_meta: ssm_d_conv       = 0
0.00.083.694 I llm_load_print_meta: ssm_d_inner      = 0
0.00.083.694 I llm_load_print_meta: ssm_d_state      = 0
0.00.083.694 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.083.694 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.083.694 I llm_load_print_meta: model type       = 1.4B
0.00.083.695 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.083.695 I llm_load_print_meta: model params     = 1.41 B
0.00.083.696 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.083.696 I llm_load_print_meta: general.name     = 1.4B
0.00.083.697 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.083.697 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.083.697 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.083.697 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.083.697 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.083.698 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.083.698 I llm_load_print_meta: max token length = 1024
0.00.085.967 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.085.967 I llm_load_tensors: offloading output layer to GPU
0.00.085.968 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.085.987 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.085.989 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.086.335 I llama_new_context_with_model: n_seq_max     = 1
0.00.086.336 I llama_new_context_with_model: n_ctx         = 2048
0.00.086.336 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.086.337 I llama_new_context_with_model: n_batch       = 2048
0.00.086.337 I llama_new_context_with_model: n_ubatch      = 512
0.00.086.337 I llama_new_context_with_model: flash_attn    = 0
0.00.086.337 I llama_new_context_with_model: freq_base     = 10000.0
0.00.086.338 I llama_new_context_with_model: freq_scale    = 1
0.00.086.338 I ggml_metal_init: allocating
0.00.086.342 I ggml_metal_init: found device: Apple M4
0.00.086.343 I ggml_metal_init: picking default device: Apple M4
0.00.087.011 I ggml_metal_init: using embedded metal library
0.00.128.799 I ggml_metal_init: GPU name:   Apple M4
0.00.128.804 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.128.805 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.128.805 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.128.805 I ggml_metal_init: simdgroup reduction   = true
0.00.128.806 I ggml_metal_init: simdgroup matrix mul. = true
0.00.128.806 I ggml_metal_init: has bfloat            = true
0.00.128.806 I ggml_metal_init: use bfloat            = true
0.00.128.807 I ggml_metal_init: hasUnifiedMemory      = true
0.00.128.808 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.238.265 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.273.458 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.273.464 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.273.487 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.274.502 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.274.503 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.274.504 I llama_new_context_with_model: graph nodes  = 967
0.00.274.504 I llama_new_context_with_model: graph splits = 2
0.00.274.506 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.274.639 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.274.640 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.361.885 I main: llama threadpool init, n_threads = 4
0.00.361.931 I 
0.00.361.963 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.361.965 I 
0.00.362.264 I sampler seed: 1234
0.00.362.272 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.362.308 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.362.310 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.362.310 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.217.334 I llama_perf_sampler_print:    sampling time =       1.33 ms /    71 runs   (    0.02 ms per token, 53423.63 tokens per second)
0.02.217.335 I llama_perf_context_print:        load time =     331.73 ms
0.02.217.336 I llama_perf_context_print: prompt eval time =      44.21 ms /     7 tokens (    6.32 ms per token,   158.35 tokens per second)
0.02.217.336 I llama_perf_context_print:        eval time =    1807.92 ms /    63 runs   (   28.70 ms per token,    34.85 tokens per second)
0.02.217.337 I llama_perf_context_print:       total time =    1855.46 ms /    70 tokens
0.02.217.575 I ggml_metal_free: deallocating

real	0m2.504s
user	0m0.140s
sys	0m0.116s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.047 I build: 4454 (8eceb888) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.080 I main: llama backend init
0.00.000.083 I main: load the model and apply lora adapter, if any
0.00.009.823 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.028.424 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.028.431 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.028.433 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.028.438 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.028.439 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.028.439 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.028.439 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.028.440 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.028.440 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.028.441 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.028.441 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.028.441 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.028.442 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.028.442 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.028.444 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.028.444 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.028.445 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.032.348 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.033.419 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.037.326 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.037.328 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.037.328 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.037.328 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.037.329 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.037.329 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.037.330 I llama_model_loader: - type  f32:  194 tensors
0.00.037.330 I llama_model_loader: - type q8_0:   98 tensors
0.00.060.239 I llm_load_vocab: special tokens cache size = 25
0.00.066.661 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.066.665 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.066.666 I llm_load_print_meta: arch             = gptneox
0.00.066.666 I llm_load_print_meta: vocab type       = BPE
0.00.066.666 I llm_load_print_meta: n_vocab          = 50304
0.00.066.667 I llm_load_print_meta: n_merges         = 50009
0.00.066.667 I llm_load_print_meta: vocab_only       = 0
0.00.066.667 I llm_load_print_meta: n_ctx_train      = 2048
0.00.066.668 I llm_load_print_meta: n_embd           = 2048
0.00.066.668 I llm_load_print_meta: n_layer          = 24
0.00.066.673 I llm_load_print_meta: n_head           = 16
0.00.066.674 I llm_load_print_meta: n_head_kv        = 16
0.00.066.674 I llm_load_print_meta: n_rot            = 32
0.00.066.674 I llm_load_print_meta: n_swa            = 0
0.00.066.674 I llm_load_print_meta: n_embd_head_k    = 128
0.00.066.674 I llm_load_print_meta: n_embd_head_v    = 128
0.00.066.675 I llm_load_print_meta: n_gqa            = 1
0.00.066.676 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.066.676 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.066.677 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.066.677 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.066.678 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.066.678 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.066.678 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.066.679 I llm_load_print_meta: n_ff             = 8192
0.00.066.679 I llm_load_print_meta: n_expert         = 0
0.00.066.679 I llm_load_print_meta: n_expert_used    = 0
0.00.066.679 I llm_load_print_meta: causal attn      = 1
0.00.066.679 I llm_load_print_meta: pooling type     = 0
0.00.066.680 I llm_load_print_meta: rope type        = 2
0.00.066.683 I llm_load_print_meta: rope scaling     = linear
0.00.066.684 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.066.684 I llm_load_print_meta: freq_scale_train = 1
0.00.066.684 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.066.686 I llm_load_print_meta: rope_finetuned   = unknown
0.00.066.686 I llm_load_print_meta: ssm_d_conv       = 0
0.00.066.686 I llm_load_print_meta: ssm_d_inner      = 0
0.00.066.686 I llm_load_print_meta: ssm_d_state      = 0
0.00.066.686 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.066.686 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.066.687 I llm_load_print_meta: model type       = 1.4B
0.00.066.687 I llm_load_print_meta: model ftype      = Q8_0
0.00.066.687 I llm_load_print_meta: model params     = 1.41 B
0.00.066.688 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.066.688 I llm_load_print_meta: general.name     = 1.4B
0.00.066.688 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.066.688 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.066.688 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.066.689 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.066.689 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.066.693 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.066.693 I llm_load_print_meta: max token length = 1024
0.00.069.112 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.069.113 I llm_load_tensors: offloading output layer to GPU
0.00.069.113 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.069.124 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.069.125 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.069.553 I llama_new_context_with_model: n_seq_max     = 1
0.00.069.554 I llama_new_context_with_model: n_ctx         = 2048
0.00.069.554 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.069.554 I llama_new_context_with_model: n_batch       = 2048
0.00.069.554 I llama_new_context_with_model: n_ubatch      = 512
0.00.069.554 I llama_new_context_with_model: flash_attn    = 0
0.00.069.555 I llama_new_context_with_model: freq_base     = 10000.0
0.00.069.555 I llama_new_context_with_model: freq_scale    = 1
0.00.069.555 I ggml_metal_init: allocating
0.00.069.558 I ggml_metal_init: found device: Apple M4
0.00.069.560 I ggml_metal_init: picking default device: Apple M4
0.00.070.288 I ggml_metal_init: using embedded metal library
0.00.072.851 I ggml_metal_init: GPU name:   Apple M4
0.00.072.852 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.072.853 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.072.853 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.072.853 I ggml_metal_init: simdgroup reduction   = true
0.00.072.853 I ggml_metal_init: simdgroup matrix mul. = true
0.00.072.854 I ggml_metal_init: has bfloat            = true
0.00.072.854 I ggml_metal_init: use bfloat            = true
0.00.072.855 I ggml_metal_init: hasUnifiedMemory      = true
0.00.072.855 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.083.217 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.108.887 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.108.894 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.108.918 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.110.204 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.110.207 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.110.207 I llama_new_context_with_model: graph nodes  = 967
0.00.110.207 I llama_new_context_with_model: graph splits = 2
0.00.110.211 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.110.339 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.110.340 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.335.287 I main: llama threadpool init, n_threads = 4
0.01.335.321 I 
0.01.335.344 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.335.344 I 
0.01.335.579 I sampler seed: 1234
0.01.335.583 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.335.624 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.335.626 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.335.626 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.412.828 I llama_perf_sampler_print:    sampling time =       1.22 ms /    71 runs   (    0.02 ms per token, 58340.18 tokens per second)
0.02.412.829 I llama_perf_context_print:        load time =    1325.46 ms
0.02.412.830 I llama_perf_context_print: prompt eval time =      43.12 ms /     7 tokens (    6.16 ms per token,   162.35 tokens per second)
0.02.412.831 I llama_perf_context_print:        eval time =    1031.05 ms /    63 runs   (   16.37 ms per token,    61.10 tokens per second)
0.02.412.831 I llama_perf_context_print:       total time =    1077.54 ms /    70 tokens
0.02.413.061 I ggml_metal_free: deallocating

real	0m2.431s
user	0m0.116s
sys	0m0.216s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.046 I build: 4454 (8eceb888) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.078 I main: llama backend init
0.00.000.080 I main: load the model and apply lora adapter, if any
0.00.010.873 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.018.727 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.018.732 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.734 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.735 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.735 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.735 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.736 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.737 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.737 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.738 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.738 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.738 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.739 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.739 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.741 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.741 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.741 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.597 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.615 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.468 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.470 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.470 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.470 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.471 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.471 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.027.471 I llama_model_loader: - type  f32:  194 tensors
0.00.027.472 I llama_model_loader: - type q4_0:   97 tensors
0.00.027.472 I llama_model_loader: - type q6_K:    1 tensors
0.00.048.544 I llm_load_vocab: special tokens cache size = 25
0.00.054.593 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.054.596 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.054.596 I llm_load_print_meta: arch             = gptneox
0.00.054.596 I llm_load_print_meta: vocab type       = BPE
0.00.054.597 I llm_load_print_meta: n_vocab          = 50304
0.00.054.597 I llm_load_print_meta: n_merges         = 50009
0.00.054.597 I llm_load_print_meta: vocab_only       = 0
0.00.054.597 I llm_load_print_meta: n_ctx_train      = 2048
0.00.054.597 I llm_load_print_meta: n_embd           = 2048
0.00.054.598 I llm_load_print_meta: n_layer          = 24
0.00.054.603 I llm_load_print_meta: n_head           = 16
0.00.054.604 I llm_load_print_meta: n_head_kv        = 16
0.00.054.604 I llm_load_print_meta: n_rot            = 32
0.00.054.605 I llm_load_print_meta: n_swa            = 0
0.00.054.605 I llm_load_print_meta: n_embd_head_k    = 128
0.00.054.605 I llm_load_print_meta: n_embd_head_v    = 128
0.00.054.606 I llm_load_print_meta: n_gqa            = 1
0.00.054.606 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.054.607 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.054.608 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.054.608 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.054.608 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.054.608 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.054.609 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.054.609 I llm_load_print_meta: n_ff             = 8192
0.00.054.610 I llm_load_print_meta: n_expert         = 0
0.00.054.610 I llm_load_print_meta: n_expert_used    = 0
0.00.054.610 I llm_load_print_meta: causal attn      = 1
0.00.054.610 I llm_load_print_meta: pooling type     = 0
0.00.054.610 I llm_load_print_meta: rope type        = 2
0.00.054.610 I llm_load_print_meta: rope scaling     = linear
0.00.054.611 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.054.611 I llm_load_print_meta: freq_scale_train = 1
0.00.054.611 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.054.612 I llm_load_print_meta: rope_finetuned   = unknown
0.00.054.612 I llm_load_print_meta: ssm_d_conv       = 0
0.00.054.612 I llm_load_print_meta: ssm_d_inner      = 0
0.00.054.612 I llm_load_print_meta: ssm_d_state      = 0
0.00.054.612 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.054.612 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.054.613 I llm_load_print_meta: model type       = 1.4B
0.00.054.613 I llm_load_print_meta: model ftype      = Q4_0
0.00.054.613 I llm_load_print_meta: model params     = 1.41 B
0.00.054.614 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.054.614 I llm_load_print_meta: general.name     = 1.4B
0.00.054.614 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.054.615 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.054.615 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.054.616 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.054.617 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.054.617 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.054.617 I llm_load_print_meta: max token length = 1024
0.00.056.866 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.056.866 I llm_load_tensors: offloading output layer to GPU
0.00.056.866 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.056.878 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.056.879 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.057.262 I llama_new_context_with_model: n_seq_max     = 1
0.00.057.262 I llama_new_context_with_model: n_ctx         = 2048
0.00.057.263 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.057.263 I llama_new_context_with_model: n_batch       = 2048
0.00.057.263 I llama_new_context_with_model: n_ubatch      = 512
0.00.057.263 I llama_new_context_with_model: flash_attn    = 0
0.00.057.264 I llama_new_context_with_model: freq_base     = 10000.0
0.00.057.264 I llama_new_context_with_model: freq_scale    = 1
0.00.057.265 I ggml_metal_init: allocating
0.00.057.271 I ggml_metal_init: found device: Apple M4
0.00.057.274 I ggml_metal_init: picking default device: Apple M4
0.00.057.960 I ggml_metal_init: using embedded metal library
0.00.060.510 I ggml_metal_init: GPU name:   Apple M4
0.00.060.511 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.060.512 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.060.512 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.060.512 I ggml_metal_init: simdgroup reduction   = true
0.00.060.512 I ggml_metal_init: simdgroup matrix mul. = true
0.00.060.513 I ggml_metal_init: has bfloat            = true
0.00.060.513 I ggml_metal_init: use bfloat            = true
0.00.060.513 I ggml_metal_init: hasUnifiedMemory      = true
0.00.060.514 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.070.239 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.094.507 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.094.515 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.094.536 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.095.594 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.095.596 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.095.596 I llama_new_context_with_model: graph nodes  = 967
0.00.095.596 I llama_new_context_with_model: graph splits = 2
0.00.095.603 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.095.739 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.095.739 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.651.482 I main: llama threadpool init, n_threads = 4
0.00.651.533 I 
0.00.651.563 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.651.564 I 
0.00.651.792 I sampler seed: 1234
0.00.651.799 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.651.824 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.651.824 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.651.824 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.326.439 I llama_perf_sampler_print:    sampling time =       1.20 ms /    71 runs   (    0.02 ms per token, 59265.44 tokens per second)
0.01.326.439 I llama_perf_context_print:        load time =     640.60 ms
0.01.326.440 I llama_perf_context_print: prompt eval time =      39.76 ms /     7 tokens (    5.68 ms per token,   176.05 tokens per second)
0.01.326.441 I llama_perf_context_print:        eval time =     631.93 ms /    63 runs   (   10.03 ms per token,    99.69 tokens per second)
0.01.326.442 I llama_perf_context_print:       total time =     674.96 ms /    70 tokens
0.01.326.689 I ggml_metal_free: deallocating

real	0m1.346s
user	0m0.112s
sys	0m0.147s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.044 I build: 4454 (8eceb888) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.075 I main: llama backend init
0.00.000.077 I main: load the model and apply lora adapter, if any
0.00.008.906 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.749 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.754 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.755 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.756 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.756 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.756 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.757 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.761 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.761 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.763 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.763 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.763 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.764 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.764 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.768 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.768 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.769 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.583 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.602 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.355 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.356 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.356 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.357 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.357 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.357 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.025.358 I llama_model_loader: - type  f32:  194 tensors
0.00.025.358 I llama_model_loader: - type q4_1:   97 tensors
0.00.025.358 I llama_model_loader: - type q6_K:    1 tensors
0.00.046.397 I llm_load_vocab: special tokens cache size = 25
0.00.052.576 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.579 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.579 I llm_load_print_meta: arch             = gptneox
0.00.052.580 I llm_load_print_meta: vocab type       = BPE
0.00.052.580 I llm_load_print_meta: n_vocab          = 50304
0.00.052.580 I llm_load_print_meta: n_merges         = 50009
0.00.052.580 I llm_load_print_meta: vocab_only       = 0
0.00.052.581 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.581 I llm_load_print_meta: n_embd           = 2048
0.00.052.581 I llm_load_print_meta: n_layer          = 24
0.00.052.584 I llm_load_print_meta: n_head           = 16
0.00.052.585 I llm_load_print_meta: n_head_kv        = 16
0.00.052.586 I llm_load_print_meta: n_rot            = 32
0.00.052.586 I llm_load_print_meta: n_swa            = 0
0.00.052.587 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.588 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.588 I llm_load_print_meta: n_gqa            = 1
0.00.052.589 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.590 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.590 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.591 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.591 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.591 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.591 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.592 I llm_load_print_meta: n_ff             = 8192
0.00.052.592 I llm_load_print_meta: n_expert         = 0
0.00.052.592 I llm_load_print_meta: n_expert_used    = 0
0.00.052.594 I llm_load_print_meta: causal attn      = 1
0.00.052.595 I llm_load_print_meta: pooling type     = 0
0.00.052.596 I llm_load_print_meta: rope type        = 2
0.00.052.596 I llm_load_print_meta: rope scaling     = linear
0.00.052.596 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.596 I llm_load_print_meta: freq_scale_train = 1
0.00.052.597 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.597 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.597 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.597 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.597 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.597 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.598 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.598 I llm_load_print_meta: model type       = 1.4B
0.00.052.602 I llm_load_print_meta: model ftype      = Q4_1
0.00.052.603 I llm_load_print_meta: model params     = 1.41 B
0.00.052.604 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.052.604 I llm_load_print_meta: general.name     = 1.4B
0.00.052.604 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.605 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.605 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.605 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.605 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.052.605 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.606 I llm_load_print_meta: max token length = 1024
0.00.054.644 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.644 I llm_load_tensors: offloading output layer to GPU
0.00.054.644 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.655 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.054.656 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.055.002 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.003 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.003 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.003 I llama_new_context_with_model: n_batch       = 2048
0.00.055.003 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.004 I llama_new_context_with_model: flash_attn    = 0
0.00.055.004 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.004 I llama_new_context_with_model: freq_scale    = 1
0.00.055.005 I ggml_metal_init: allocating
0.00.055.011 I ggml_metal_init: found device: Apple M4
0.00.055.013 I ggml_metal_init: picking default device: Apple M4
0.00.055.618 I ggml_metal_init: using embedded metal library
0.00.057.955 I ggml_metal_init: GPU name:   Apple M4
0.00.057.957 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.957 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.957 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.958 I ggml_metal_init: simdgroup reduction   = true
0.00.057.958 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.958 I ggml_metal_init: has bfloat            = true
0.00.057.960 I ggml_metal_init: use bfloat            = true
0.00.057.960 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.964 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.641 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.087.478 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.087.487 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.087.506 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.088.461 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.088.462 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.088.462 I llama_new_context_with_model: graph nodes  = 967
0.00.088.462 I llama_new_context_with_model: graph splits = 2
0.00.088.465 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.088.594 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.088.594 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.683.720 I main: llama threadpool init, n_threads = 4
0.00.683.762 I 
0.00.683.804 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.683.805 I 
0.00.684.040 I sampler seed: 1234
0.00.684.045 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.684.077 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.684.079 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.684.079 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.405.226 I llama_perf_sampler_print:    sampling time =       1.09 ms /    71 runs   (    0.02 ms per token, 65137.61 tokens per second)
0.01.405.226 I llama_perf_context_print:        load time =     674.81 ms
0.01.405.227 I llama_perf_context_print: prompt eval time =      39.61 ms /     7 tokens (    5.66 ms per token,   176.73 tokens per second)
0.01.405.228 I llama_perf_context_print:        eval time =     678.73 ms /    63 runs   (   10.77 ms per token,    92.82 tokens per second)
0.01.405.228 I llama_perf_context_print:       total time =     721.51 ms /    70 tokens
0.01.405.468 I ggml_metal_free: deallocating

real	0m1.422s
user	0m0.110s
sys	0m0.144s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4454 (8eceb888) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.073 I main: llama backend init
0.00.000.075 I main: load the model and apply lora adapter, if any
0.00.009.642 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.183 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.017.188 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.189 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.190 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.190 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.190 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.191 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.192 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.192 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.192 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.193 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.195 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.195 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.196 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.198 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.198 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.198 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.928 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.060 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.721 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.723 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.723 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.723 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.723 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.724 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.025.724 I llama_model_loader: - type  f32:  194 tensors
0.00.025.725 I llama_model_loader: - type q5_0:   97 tensors
0.00.025.725 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.954 I llm_load_vocab: special tokens cache size = 25
0.00.051.880 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.882 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.883 I llm_load_print_meta: arch             = gptneox
0.00.051.883 I llm_load_print_meta: vocab type       = BPE
0.00.051.883 I llm_load_print_meta: n_vocab          = 50304
0.00.051.884 I llm_load_print_meta: n_merges         = 50009
0.00.051.884 I llm_load_print_meta: vocab_only       = 0
0.00.051.884 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.884 I llm_load_print_meta: n_embd           = 2048
0.00.051.884 I llm_load_print_meta: n_layer          = 24
0.00.051.887 I llm_load_print_meta: n_head           = 16
0.00.051.888 I llm_load_print_meta: n_head_kv        = 16
0.00.051.888 I llm_load_print_meta: n_rot            = 32
0.00.051.888 I llm_load_print_meta: n_swa            = 0
0.00.051.888 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.890 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.891 I llm_load_print_meta: n_gqa            = 1
0.00.051.891 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.892 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.893 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.893 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.893 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.893 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.893 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.894 I llm_load_print_meta: n_ff             = 8192
0.00.051.894 I llm_load_print_meta: n_expert         = 0
0.00.051.894 I llm_load_print_meta: n_expert_used    = 0
0.00.051.894 I llm_load_print_meta: causal attn      = 1
0.00.051.895 I llm_load_print_meta: pooling type     = 0
0.00.051.895 I llm_load_print_meta: rope type        = 2
0.00.051.895 I llm_load_print_meta: rope scaling     = linear
0.00.051.895 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.900 I llm_load_print_meta: freq_scale_train = 1
0.00.051.900 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.902 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.902 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.902 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.902 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.902 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.903 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.903 I llm_load_print_meta: model type       = 1.4B
0.00.051.903 I llm_load_print_meta: model ftype      = Q5_0
0.00.051.905 I llm_load_print_meta: model params     = 1.41 B
0.00.051.906 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.051.906 I llm_load_print_meta: general.name     = 1.4B
0.00.051.906 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.907 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.907 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.907 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.908 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.908 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.908 I llm_load_print_meta: max token length = 1024
0.00.053.686 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.686 I llm_load_tensors: offloading output layer to GPU
0.00.053.687 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.692 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.053.693 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.054.020 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.021 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.021 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.021 I llama_new_context_with_model: n_batch       = 2048
0.00.054.021 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.021 I llama_new_context_with_model: flash_attn    = 0
0.00.054.022 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.022 I llama_new_context_with_model: freq_scale    = 1
0.00.054.022 I ggml_metal_init: allocating
0.00.054.025 I ggml_metal_init: found device: Apple M4
0.00.054.027 I ggml_metal_init: picking default device: Apple M4
0.00.054.615 I ggml_metal_init: using embedded metal library
0.00.056.960 I ggml_metal_init: GPU name:   Apple M4
0.00.056.962 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.962 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.962 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.963 I ggml_metal_init: simdgroup reduction   = true
0.00.056.963 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.963 I ggml_metal_init: has bfloat            = true
0.00.056.963 I ggml_metal_init: use bfloat            = true
0.00.056.964 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.964 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.627 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.085.739 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.744 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.762 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.861 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.863 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.863 I llama_new_context_with_model: graph nodes  = 967
0.00.086.863 I llama_new_context_with_model: graph splits = 2
0.00.086.866 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.087.006 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.006 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.720.464 I main: llama threadpool init, n_threads = 4
0.00.720.507 I 
0.00.720.537 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.720.537 I 
0.00.720.767 I sampler seed: 1234
0.00.720.772 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.720.820 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.720.827 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.720.827 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.512.632 I llama_perf_sampler_print:    sampling time =       1.26 ms /    71 runs   (    0.02 ms per token, 56170.89 tokens per second)
0.01.512.632 I llama_perf_context_print:        load time =     710.81 ms
0.01.512.637 I llama_perf_context_print: prompt eval time =      43.13 ms /     7 tokens (    6.16 ms per token,   162.28 tokens per second)
0.01.512.638 I llama_perf_context_print:        eval time =     745.76 ms /    63 runs   (   11.84 ms per token,    84.48 tokens per second)
0.01.512.638 I llama_perf_context_print:       total time =     792.17 ms /    70 tokens
0.01.512.943 I ggml_metal_free: deallocating

real	0m1.531s
user	0m0.110s
sys	0m0.142s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4454 (8eceb888) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.069 I main: llama backend init
0.00.000.071 I main: load the model and apply lora adapter, if any
0.00.008.956 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.787 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.794 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.796 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.796 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.798 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.799 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.799 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.800 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.800 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.801 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.801 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.801 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.802 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.802 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.806 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.807 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.807 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.501 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.502 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.286 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.289 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.290 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.290 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.291 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.291 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.292 I llama_model_loader: - type  f32:  194 tensors
0.00.025.292 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.292 I llama_model_loader: - type q6_K:    1 tensors
0.00.047.101 I llm_load_vocab: special tokens cache size = 25
0.00.053.247 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.053.255 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.053.256 I llm_load_print_meta: arch             = gptneox
0.00.053.256 I llm_load_print_meta: vocab type       = BPE
0.00.053.257 I llm_load_print_meta: n_vocab          = 50304
0.00.053.257 I llm_load_print_meta: n_merges         = 50009
0.00.053.257 I llm_load_print_meta: vocab_only       = 0
0.00.053.257 I llm_load_print_meta: n_ctx_train      = 2048
0.00.053.257 I llm_load_print_meta: n_embd           = 2048
0.00.053.258 I llm_load_print_meta: n_layer          = 24
0.00.053.263 I llm_load_print_meta: n_head           = 16
0.00.053.263 I llm_load_print_meta: n_head_kv        = 16
0.00.053.264 I llm_load_print_meta: n_rot            = 32
0.00.053.264 I llm_load_print_meta: n_swa            = 0
0.00.053.264 I llm_load_print_meta: n_embd_head_k    = 128
0.00.053.264 I llm_load_print_meta: n_embd_head_v    = 128
0.00.053.265 I llm_load_print_meta: n_gqa            = 1
0.00.053.265 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.053.267 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.053.267 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.053.269 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.053.269 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.053.269 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.053.269 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.053.270 I llm_load_print_meta: n_ff             = 8192
0.00.053.270 I llm_load_print_meta: n_expert         = 0
0.00.053.270 I llm_load_print_meta: n_expert_used    = 0
0.00.053.270 I llm_load_print_meta: causal attn      = 1
0.00.053.271 I llm_load_print_meta: pooling type     = 0
0.00.053.271 I llm_load_print_meta: rope type        = 2
0.00.053.272 I llm_load_print_meta: rope scaling     = linear
0.00.053.273 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.053.273 I llm_load_print_meta: freq_scale_train = 1
0.00.053.273 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.053.273 I llm_load_print_meta: rope_finetuned   = unknown
0.00.053.273 I llm_load_print_meta: ssm_d_conv       = 0
0.00.053.273 I llm_load_print_meta: ssm_d_inner      = 0
0.00.053.273 I llm_load_print_meta: ssm_d_state      = 0
0.00.053.274 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.053.274 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.053.274 I llm_load_print_meta: model type       = 1.4B
0.00.053.274 I llm_load_print_meta: model ftype      = Q5_1
0.00.053.275 I llm_load_print_meta: model params     = 1.41 B
0.00.053.275 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.053.275 I llm_load_print_meta: general.name     = 1.4B
0.00.053.276 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.053.276 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.053.276 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.053.276 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.053.276 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.053.277 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.053.277 I llm_load_print_meta: max token length = 1024
0.00.055.222 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.055.222 I llm_load_tensors: offloading output layer to GPU
0.00.055.222 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.055.233 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.055.235 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.055.620 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.621 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.622 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.622 I llama_new_context_with_model: n_batch       = 2048
0.00.055.622 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.622 I llama_new_context_with_model: flash_attn    = 0
0.00.055.622 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.623 I llama_new_context_with_model: freq_scale    = 1
0.00.055.623 I ggml_metal_init: allocating
0.00.055.627 I ggml_metal_init: found device: Apple M4
0.00.055.629 I ggml_metal_init: picking default device: Apple M4
0.00.056.312 I ggml_metal_init: using embedded metal library
0.00.058.948 I ggml_metal_init: GPU name:   Apple M4
0.00.058.949 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.950 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.950 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.950 I ggml_metal_init: simdgroup reduction   = true
0.00.058.951 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.951 I ggml_metal_init: has bfloat            = true
0.00.058.951 I ggml_metal_init: use bfloat            = true
0.00.058.953 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.954 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.135 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.088.923 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.088.938 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.088.957 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.089.960 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.089.961 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.089.962 I llama_new_context_with_model: graph nodes  = 967
0.00.089.962 I llama_new_context_with_model: graph splits = 2
0.00.089.965 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.090.106 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.090.107 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.779.762 I main: llama threadpool init, n_threads = 4
0.00.779.796 I 
0.00.779.817 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.779.819 I 
0.00.779.931 I sampler seed: 1234
0.00.779.936 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.779.949 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.779.949 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.779.949 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.618.016 I llama_perf_sampler_print:    sampling time =       1.20 ms /    71 runs   (    0.02 ms per token, 59019.12 tokens per second)
0.01.618.017 I llama_perf_context_print:        load time =     770.80 ms
0.01.618.018 I llama_perf_context_print: prompt eval time =      42.13 ms /     7 tokens (    6.02 ms per token,   166.13 tokens per second)
0.01.618.019 I llama_perf_context_print:        eval time =     793.00 ms /    63 runs   (   12.59 ms per token,    79.45 tokens per second)
0.01.618.019 I llama_perf_context_print:       total time =     838.26 ms /    70 tokens
0.01.618.275 I ggml_metal_free: deallocating

real	0m1.636s
user	0m0.111s
sys	0m0.148s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4454 (8eceb888) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.073 I main: llama backend init
0.00.000.075 I main: load the model and apply lora adapter, if any
0.00.011.936 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.018.616 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.018.621 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.623 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.623 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.624 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.624 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.624 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.625 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.625 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.627 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.627 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.627 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.628 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.628 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.630 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.630 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.630 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.332 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.381 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.158 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.159 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.159 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.159 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.160 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.160 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.027.160 I llama_model_loader: - type  f32:  194 tensors
0.00.027.161 I llama_model_loader: - type q2_K:   49 tensors
0.00.027.161 I llama_model_loader: - type q3_K:   48 tensors
0.00.027.161 I llama_model_loader: - type q6_K:    1 tensors
0.00.048.009 I llm_load_vocab: special tokens cache size = 25
0.00.054.197 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.054.200 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.054.201 I llm_load_print_meta: arch             = gptneox
0.00.054.201 I llm_load_print_meta: vocab type       = BPE
0.00.054.201 I llm_load_print_meta: n_vocab          = 50304
0.00.054.202 I llm_load_print_meta: n_merges         = 50009
0.00.054.202 I llm_load_print_meta: vocab_only       = 0
0.00.054.202 I llm_load_print_meta: n_ctx_train      = 2048
0.00.054.202 I llm_load_print_meta: n_embd           = 2048
0.00.054.202 I llm_load_print_meta: n_layer          = 24
0.00.054.205 I llm_load_print_meta: n_head           = 16
0.00.054.205 I llm_load_print_meta: n_head_kv        = 16
0.00.054.206 I llm_load_print_meta: n_rot            = 32
0.00.054.206 I llm_load_print_meta: n_swa            = 0
0.00.054.206 I llm_load_print_meta: n_embd_head_k    = 128
0.00.054.206 I llm_load_print_meta: n_embd_head_v    = 128
0.00.054.207 I llm_load_print_meta: n_gqa            = 1
0.00.054.208 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.054.208 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.054.209 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.054.209 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.054.209 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.054.209 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.054.210 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.054.210 I llm_load_print_meta: n_ff             = 8192
0.00.054.211 I llm_load_print_meta: n_expert         = 0
0.00.054.211 I llm_load_print_meta: n_expert_used    = 0
0.00.054.211 I llm_load_print_meta: causal attn      = 1
0.00.054.211 I llm_load_print_meta: pooling type     = 0
0.00.054.211 I llm_load_print_meta: rope type        = 2
0.00.054.212 I llm_load_print_meta: rope scaling     = linear
0.00.054.212 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.054.212 I llm_load_print_meta: freq_scale_train = 1
0.00.054.215 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.054.215 I llm_load_print_meta: rope_finetuned   = unknown
0.00.054.215 I llm_load_print_meta: ssm_d_conv       = 0
0.00.054.215 I llm_load_print_meta: ssm_d_inner      = 0
0.00.054.215 I llm_load_print_meta: ssm_d_state      = 0
0.00.054.215 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.054.215 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.054.216 I llm_load_print_meta: model type       = 1.4B
0.00.054.216 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.054.217 I llm_load_print_meta: model params     = 1.41 B
0.00.054.217 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.054.217 I llm_load_print_meta: general.name     = 1.4B
0.00.054.217 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.054.218 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.054.218 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.054.218 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.054.218 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.054.219 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.054.221 I llm_load_print_meta: max token length = 1024
0.00.056.100 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.056.101 I llm_load_tensors: offloading output layer to GPU
0.00.056.101 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.056.111 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.056.113 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.056.433 I llama_new_context_with_model: n_seq_max     = 1
0.00.056.434 I llama_new_context_with_model: n_ctx         = 2048
0.00.056.434 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.056.434 I llama_new_context_with_model: n_batch       = 2048
0.00.056.434 I llama_new_context_with_model: n_ubatch      = 512
0.00.056.434 I llama_new_context_with_model: flash_attn    = 0
0.00.056.435 I llama_new_context_with_model: freq_base     = 10000.0
0.00.056.435 I llama_new_context_with_model: freq_scale    = 1
0.00.056.436 I ggml_metal_init: allocating
0.00.056.439 I ggml_metal_init: found device: Apple M4
0.00.056.440 I ggml_metal_init: picking default device: Apple M4
0.00.057.022 I ggml_metal_init: using embedded metal library
0.00.059.353 I ggml_metal_init: GPU name:   Apple M4
0.00.059.354 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.059.355 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.059.355 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.059.355 I ggml_metal_init: simdgroup reduction   = true
0.00.059.356 I ggml_metal_init: simdgroup matrix mul. = true
0.00.059.356 I ggml_metal_init: has bfloat            = true
0.00.059.356 I ggml_metal_init: use bfloat            = true
0.00.059.356 I ggml_metal_init: hasUnifiedMemory      = true
0.00.059.357 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.069.194 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.089.935 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.089.943 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.089.961 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.091.022 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.091.023 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.091.023 I llama_new_context_with_model: graph nodes  = 967
0.00.091.023 I llama_new_context_with_model: graph splits = 2
0.00.091.026 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.091.173 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.091.174 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.466.045 I main: llama threadpool init, n_threads = 4
0.00.466.092 I 
0.00.466.115 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.466.116 I 
0.00.466.359 I sampler seed: 1234
0.00.466.362 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.466.378 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.466.379 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.466.380 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.142.106 I llama_perf_sampler_print:    sampling time =       1.18 ms /    71 runs   (    0.02 ms per token, 60271.65 tokens per second)
0.01.142.107 I llama_perf_context_print:        load time =     454.10 ms
0.01.142.108 I llama_perf_context_print: prompt eval time =      35.90 ms /     7 tokens (    5.13 ms per token,   194.96 tokens per second)
0.01.142.109 I llama_perf_context_print:        eval time =     636.81 ms /    63 runs   (   10.11 ms per token,    98.93 tokens per second)
0.01.142.109 I llama_perf_context_print:       total time =     676.07 ms /    70 tokens
0.01.142.343 I ggml_metal_free: deallocating

real	0m1.163s
user	0m0.110s
sys	0m0.117s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4454 (8eceb888) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.068 I main: llama backend init
0.00.000.071 I main: load the model and apply lora adapter, if any
0.00.009.566 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.029 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.017.034 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.035 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.036 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.036 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.036 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.037 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.041 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.041 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.041 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.042 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.042 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.042 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.043 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.046 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.047 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.047 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.874 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.913 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.688 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.689 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.690 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.690 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.690 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.691 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.691 I llama_model_loader: - type  f32:  194 tensors
0.00.025.692 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.692 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.692 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.692 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.933 I llm_load_vocab: special tokens cache size = 25
0.00.051.887 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.890 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.890 I llm_load_print_meta: arch             = gptneox
0.00.051.891 I llm_load_print_meta: vocab type       = BPE
0.00.051.891 I llm_load_print_meta: n_vocab          = 50304
0.00.051.891 I llm_load_print_meta: n_merges         = 50009
0.00.051.891 I llm_load_print_meta: vocab_only       = 0
0.00.051.891 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.892 I llm_load_print_meta: n_embd           = 2048
0.00.051.892 I llm_load_print_meta: n_layer          = 24
0.00.051.895 I llm_load_print_meta: n_head           = 16
0.00.051.895 I llm_load_print_meta: n_head_kv        = 16
0.00.051.896 I llm_load_print_meta: n_rot            = 32
0.00.051.896 I llm_load_print_meta: n_swa            = 0
0.00.051.896 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.896 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.897 I llm_load_print_meta: n_gqa            = 1
0.00.051.898 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.898 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.899 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.899 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.902 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.902 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.902 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.903 I llm_load_print_meta: n_ff             = 8192
0.00.051.904 I llm_load_print_meta: n_expert         = 0
0.00.051.906 I llm_load_print_meta: n_expert_used    = 0
0.00.051.906 I llm_load_print_meta: causal attn      = 1
0.00.051.906 I llm_load_print_meta: pooling type     = 0
0.00.051.906 I llm_load_print_meta: rope type        = 2
0.00.051.906 I llm_load_print_meta: rope scaling     = linear
0.00.051.907 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.907 I llm_load_print_meta: freq_scale_train = 1
0.00.051.907 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.908 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.908 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.908 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.908 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.908 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.908 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.913 I llm_load_print_meta: model type       = 1.4B
0.00.051.913 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.051.913 I llm_load_print_meta: model params     = 1.41 B
0.00.051.914 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.051.914 I llm_load_print_meta: general.name     = 1.4B
0.00.051.915 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.915 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.915 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.915 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.915 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.916 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.916 I llm_load_print_meta: max token length = 1024
0.00.053.852 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.852 I llm_load_tensors: offloading output layer to GPU
0.00.053.853 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.863 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.053.864 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.054.213 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.214 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.214 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.214 I llama_new_context_with_model: n_batch       = 2048
0.00.054.214 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.214 I llama_new_context_with_model: flash_attn    = 0
0.00.054.215 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.215 I llama_new_context_with_model: freq_scale    = 1
0.00.054.216 I ggml_metal_init: allocating
0.00.054.222 I ggml_metal_init: found device: Apple M4
0.00.054.224 I ggml_metal_init: picking default device: Apple M4
0.00.054.800 I ggml_metal_init: using embedded metal library
0.00.057.111 I ggml_metal_init: GPU name:   Apple M4
0.00.057.114 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.114 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.114 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.115 I ggml_metal_init: simdgroup reduction   = true
0.00.057.115 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.115 I ggml_metal_init: has bfloat            = true
0.00.057.115 I ggml_metal_init: use bfloat            = true
0.00.057.116 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.120 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.715 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.085.592 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.600 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.622 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.590 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.592 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.592 I llama_new_context_with_model: graph nodes  = 967
0.00.086.592 I llama_new_context_with_model: graph splits = 2
0.00.086.595 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.086.729 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.086.730 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.535.648 I main: llama threadpool init, n_threads = 4
0.00.535.689 I 
0.00.535.717 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.535.719 I 
0.00.535.945 I sampler seed: 1234
0.00.535.953 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.536.006 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.536.009 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.536.009 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.278.597 I llama_perf_sampler_print:    sampling time =       1.21 ms /    71 runs   (    0.02 ms per token, 58629.23 tokens per second)
0.01.278.598 I llama_perf_context_print:        load time =     526.08 ms
0.01.278.599 I llama_perf_context_print: prompt eval time =      44.47 ms /     7 tokens (    6.35 ms per token,   157.40 tokens per second)
0.01.278.599 I llama_perf_context_print:        eval time =     695.13 ms /    63 runs   (   11.03 ms per token,    90.63 tokens per second)
0.01.278.600 I llama_perf_context_print:       total time =     742.95 ms /    70 tokens
0.01.278.841 I ggml_metal_free: deallocating

real	0m1.295s
user	0m0.110s
sys	0m0.121s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4454 (8eceb888) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.070 I main: llama backend init
0.00.000.072 I main: load the model and apply lora adapter, if any
0.00.008.766 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.462 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.467 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.470 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.471 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.471 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.471 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.471 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.475 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.476 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.476 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.480 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.480 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.481 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.481 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.485 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.485 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.486 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.313 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.328 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.131 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.132 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.133 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.133 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.133 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.134 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.134 I llama_model_loader: - type  f32:  194 tensors
0.00.025.135 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.135 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.135 I llama_model_loader: - type q6_K:   13 tensors
0.00.046.040 I llm_load_vocab: special tokens cache size = 25
0.00.051.965 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.968 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.968 I llm_load_print_meta: arch             = gptneox
0.00.051.969 I llm_load_print_meta: vocab type       = BPE
0.00.051.969 I llm_load_print_meta: n_vocab          = 50304
0.00.051.969 I llm_load_print_meta: n_merges         = 50009
0.00.051.969 I llm_load_print_meta: vocab_only       = 0
0.00.051.970 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.970 I llm_load_print_meta: n_embd           = 2048
0.00.051.970 I llm_load_print_meta: n_layer          = 24
0.00.051.973 I llm_load_print_meta: n_head           = 16
0.00.051.973 I llm_load_print_meta: n_head_kv        = 16
0.00.051.974 I llm_load_print_meta: n_rot            = 32
0.00.051.974 I llm_load_print_meta: n_swa            = 0
0.00.051.974 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.974 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.977 I llm_load_print_meta: n_gqa            = 1
0.00.051.978 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.979 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.979 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.980 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.980 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.981 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.981 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.981 I llm_load_print_meta: n_ff             = 8192
0.00.051.982 I llm_load_print_meta: n_expert         = 0
0.00.051.983 I llm_load_print_meta: n_expert_used    = 0
0.00.051.985 I llm_load_print_meta: causal attn      = 1
0.00.051.985 I llm_load_print_meta: pooling type     = 0
0.00.051.985 I llm_load_print_meta: rope type        = 2
0.00.051.985 I llm_load_print_meta: rope scaling     = linear
0.00.051.986 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.986 I llm_load_print_meta: freq_scale_train = 1
0.00.051.986 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.987 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.987 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.987 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.987 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.988 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.988 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.992 I llm_load_print_meta: model type       = 1.4B
0.00.051.993 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.051.993 I llm_load_print_meta: model params     = 1.41 B
0.00.051.993 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.051.993 I llm_load_print_meta: general.name     = 1.4B
0.00.051.994 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.995 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.995 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.996 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.996 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.996 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.996 I llm_load_print_meta: max token length = 1024
0.00.053.809 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.809 I llm_load_tensors: offloading output layer to GPU
0.00.053.809 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.815 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.053.816 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.054.147 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.147 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.147 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.148 I llama_new_context_with_model: n_batch       = 2048
0.00.054.148 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.148 I llama_new_context_with_model: flash_attn    = 0
0.00.054.148 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.149 I llama_new_context_with_model: freq_scale    = 1
0.00.054.149 I ggml_metal_init: allocating
0.00.054.152 I ggml_metal_init: found device: Apple M4
0.00.054.154 I ggml_metal_init: picking default device: Apple M4
0.00.054.744 I ggml_metal_init: using embedded metal library
0.00.057.072 I ggml_metal_init: GPU name:   Apple M4
0.00.057.073 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.074 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.074 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.076 I ggml_metal_init: simdgroup reduction   = true
0.00.057.076 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.076 I ggml_metal_init: has bfloat            = true
0.00.057.076 I ggml_metal_init: use bfloat            = true
0.00.057.077 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.077 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.962 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.086.620 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.626 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.643 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.726 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.727 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.728 I llama_new_context_with_model: graph nodes  = 967
0.00.087.728 I llama_new_context_with_model: graph splits = 2
0.00.087.735 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.087.865 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.866 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.611.572 I main: llama threadpool init, n_threads = 4
0.00.611.608 I 
0.00.611.629 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.611.629 I 
0.00.611.874 I sampler seed: 1234
0.00.611.878 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.611.910 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.611.912 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.611.912 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.370.775 I llama_perf_sampler_print:    sampling time =       1.23 ms /    71 runs   (    0.02 ms per token, 57583.13 tokens per second)
0.01.370.775 I llama_perf_context_print:        load time =     602.80 ms
0.01.370.776 I llama_perf_context_print: prompt eval time =      47.08 ms /     7 tokens (    6.73 ms per token,   148.67 tokens per second)
0.01.370.777 I llama_perf_context_print:        eval time =     708.86 ms /    63 runs   (   11.25 ms per token,    88.87 tokens per second)
0.01.370.777 I llama_perf_context_print:       total time =     759.21 ms /    70 tokens
0.01.371.026 I ggml_metal_free: deallocating

real	0m1.387s
user	0m0.111s
sys	0m0.136s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4454 (8eceb888) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.082 I main: llama backend init
0.00.000.085 I main: load the model and apply lora adapter, if any
0.00.009.495 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.880 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.885 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.887 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.887 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.887 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.888 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.888 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.889 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.889 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.890 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.890 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.891 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.891 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.892 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.894 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.894 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.894 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.811 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.814 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.618 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.619 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.619 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.619 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.620 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.620 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.621 I llama_model_loader: - type  f32:  194 tensors
0.00.025.621 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.621 I llama_model_loader: - type q6_K:   37 tensors
0.00.045.924 I llm_load_vocab: special tokens cache size = 25
0.00.051.905 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.907 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.907 I llm_load_print_meta: arch             = gptneox
0.00.051.908 I llm_load_print_meta: vocab type       = BPE
0.00.051.908 I llm_load_print_meta: n_vocab          = 50304
0.00.051.908 I llm_load_print_meta: n_merges         = 50009
0.00.051.908 I llm_load_print_meta: vocab_only       = 0
0.00.051.909 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.909 I llm_load_print_meta: n_embd           = 2048
0.00.051.909 I llm_load_print_meta: n_layer          = 24
0.00.051.912 I llm_load_print_meta: n_head           = 16
0.00.051.914 I llm_load_print_meta: n_head_kv        = 16
0.00.051.914 I llm_load_print_meta: n_rot            = 32
0.00.051.914 I llm_load_print_meta: n_swa            = 0
0.00.051.914 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.915 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.915 I llm_load_print_meta: n_gqa            = 1
0.00.051.916 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.917 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.917 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.918 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.918 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.918 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.918 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.919 I llm_load_print_meta: n_ff             = 8192
0.00.051.919 I llm_load_print_meta: n_expert         = 0
0.00.051.919 I llm_load_print_meta: n_expert_used    = 0
0.00.051.919 I llm_load_print_meta: causal attn      = 1
0.00.051.920 I llm_load_print_meta: pooling type     = 0
0.00.051.920 I llm_load_print_meta: rope type        = 2
0.00.051.920 I llm_load_print_meta: rope scaling     = linear
0.00.051.920 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.921 I llm_load_print_meta: freq_scale_train = 1
0.00.051.921 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.921 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.921 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.921 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.922 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.922 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.922 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.922 I llm_load_print_meta: model type       = 1.4B
0.00.051.924 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.051.925 I llm_load_print_meta: model params     = 1.41 B
0.00.051.925 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.051.925 I llm_load_print_meta: general.name     = 1.4B
0.00.051.926 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.926 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.926 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.926 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.927 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.927 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.927 I llm_load_print_meta: max token length = 1024
0.00.053.910 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.911 I llm_load_tensors: offloading output layer to GPU
0.00.053.911 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.922 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.053.923 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.054.314 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.315 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.315 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.315 I llama_new_context_with_model: n_batch       = 2048
0.00.054.316 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.316 I llama_new_context_with_model: flash_attn    = 0
0.00.054.316 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.316 I llama_new_context_with_model: freq_scale    = 1
0.00.054.317 I ggml_metal_init: allocating
0.00.054.320 I ggml_metal_init: found device: Apple M4
0.00.054.322 I ggml_metal_init: picking default device: Apple M4
0.00.054.897 I ggml_metal_init: using embedded metal library
0.00.057.363 I ggml_metal_init: GPU name:   Apple M4
0.00.057.365 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.366 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.366 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.366 I ggml_metal_init: simdgroup reduction   = true
0.00.057.367 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.367 I ggml_metal_init: has bfloat            = true
0.00.057.367 I ggml_metal_init: use bfloat            = true
0.00.057.367 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.368 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.651 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.087.803 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.087.817 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.087.846 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.088.819 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.088.821 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.088.821 I llama_new_context_with_model: graph nodes  = 967
0.00.088.821 I llama_new_context_with_model: graph splits = 2
0.00.088.824 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.088.966 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.088.966 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.703.532 I main: llama threadpool init, n_threads = 4
0.00.703.570 I 
0.00.703.593 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.703.593 I 
0.00.703.817 I sampler seed: 1234
0.00.703.822 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.703.856 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.703.857 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.703.857 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.557.851 I llama_perf_sampler_print:    sampling time =       1.19 ms /    71 runs   (    0.02 ms per token, 59563.76 tokens per second)
0.01.557.852 I llama_perf_context_print:        load time =     694.03 ms
0.01.557.853 I llama_perf_context_print: prompt eval time =      55.93 ms /     7 tokens (    7.99 ms per token,   125.16 tokens per second)
0.01.557.853 I llama_perf_context_print:        eval time =     795.28 ms /    63 runs   (   12.62 ms per token,    79.22 tokens per second)
0.01.557.854 I llama_perf_context_print:       total time =     854.32 ms /    70 tokens
0.01.558.113 I ggml_metal_free: deallocating

real	0m1.577s
user	0m0.111s
sys	0m0.159s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.046 I build: 4454 (8eceb888) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.076 I main: llama backend init
0.00.000.079 I main: load the model and apply lora adapter, if any
0.00.008.823 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.344 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.349 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.351 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.352 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.352 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.352 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.355 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.356 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.356 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.356 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.357 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.357 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.358 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.358 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.361 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.361 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.362 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.167 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.299 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.320 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.322 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.322 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.323 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.323 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.323 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.324 I llama_model_loader: - type  f32:  194 tensors
0.00.025.325 I llama_model_loader: - type q6_K:   98 tensors
0.00.046.572 I llm_load_vocab: special tokens cache size = 25
0.00.052.840 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.846 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.846 I llm_load_print_meta: arch             = gptneox
0.00.052.847 I llm_load_print_meta: vocab type       = BPE
0.00.052.847 I llm_load_print_meta: n_vocab          = 50304
0.00.052.847 I llm_load_print_meta: n_merges         = 50009
0.00.052.847 I llm_load_print_meta: vocab_only       = 0
0.00.052.848 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.848 I llm_load_print_meta: n_embd           = 2048
0.00.052.848 I llm_load_print_meta: n_layer          = 24
0.00.052.858 I llm_load_print_meta: n_head           = 16
0.00.052.858 I llm_load_print_meta: n_head_kv        = 16
0.00.052.859 I llm_load_print_meta: n_rot            = 32
0.00.052.859 I llm_load_print_meta: n_swa            = 0
0.00.052.859 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.861 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.861 I llm_load_print_meta: n_gqa            = 1
0.00.052.862 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.863 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.863 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.865 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.866 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.867 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.867 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.868 I llm_load_print_meta: n_ff             = 8192
0.00.052.868 I llm_load_print_meta: n_expert         = 0
0.00.052.868 I llm_load_print_meta: n_expert_used    = 0
0.00.052.868 I llm_load_print_meta: causal attn      = 1
0.00.052.868 I llm_load_print_meta: pooling type     = 0
0.00.052.868 I llm_load_print_meta: rope type        = 2
0.00.052.869 I llm_load_print_meta: rope scaling     = linear
0.00.052.869 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.869 I llm_load_print_meta: freq_scale_train = 1
0.00.052.869 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.870 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.870 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.870 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.870 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.870 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.870 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.871 I llm_load_print_meta: model type       = 1.4B
0.00.052.871 I llm_load_print_meta: model ftype      = Q6_K
0.00.052.871 I llm_load_print_meta: model params     = 1.41 B
0.00.052.872 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.052.872 I llm_load_print_meta: general.name     = 1.4B
0.00.052.873 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.873 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.873 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.874 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.875 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.052.875 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.875 I llm_load_print_meta: max token length = 1024
0.00.054.969 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.970 I llm_load_tensors: offloading output layer to GPU
0.00.054.970 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.981 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.054.982 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.055.325 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.326 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.326 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.326 I llama_new_context_with_model: n_batch       = 2048
0.00.055.326 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.327 I llama_new_context_with_model: flash_attn    = 0
0.00.055.327 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.327 I llama_new_context_with_model: freq_scale    = 1
0.00.055.328 I ggml_metal_init: allocating
0.00.055.331 I ggml_metal_init: found device: Apple M4
0.00.055.334 I ggml_metal_init: picking default device: Apple M4
0.00.055.971 I ggml_metal_init: using embedded metal library
0.00.058.328 I ggml_metal_init: GPU name:   Apple M4
0.00.058.329 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.330 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.330 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.330 I ggml_metal_init: simdgroup reduction   = true
0.00.058.330 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.331 I ggml_metal_init: has bfloat            = true
0.00.058.331 I ggml_metal_init: use bfloat            = true
0.00.058.331 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.332 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.460 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.087.116 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.087.124 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.087.145 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.088.070 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.088.071 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.088.071 I llama_new_context_with_model: graph nodes  = 967
0.00.088.072 I llama_new_context_with_model: graph splits = 2
0.00.088.074 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.088.207 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.088.207 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.742.940 I main: llama threadpool init, n_threads = 4
0.00.742.988 I 
0.00.743.012 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.743.014 I 
0.00.743.285 I sampler seed: 1234
0.00.743.290 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.743.333 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.743.335 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.743.335 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.618.691 I llama_perf_sampler_print:    sampling time =       1.35 ms /    71 runs   (    0.02 ms per token, 52709.73 tokens per second)
0.01.618.691 I llama_perf_context_print:        load time =     734.11 ms
0.01.618.692 I llama_perf_context_print: prompt eval time =      54.42 ms /     7 tokens (    7.77 ms per token,   128.64 tokens per second)
0.01.618.693 I llama_perf_context_print:        eval time =     818.09 ms /    63 runs   (   12.99 ms per token,    77.01 tokens per second)
0.01.618.693 I llama_perf_context_print:       total time =     875.75 ms /    70 tokens
0.01.618.935 I ggml_metal_free: deallocating

real	0m1.640s
user	0m0.110s
sys	0m0.132s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.641 I build: 4454 (8eceb888) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.024.783 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.039.455 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.039.461 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.039.463 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.039.464 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.039.465 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.039.466 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.039.466 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.039.469 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.039.470 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.039.470 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.039.471 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.039.472 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.039.472 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.039.473 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.039.474 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.039.475 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.039.475 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.047.273 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.049.247 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.056.110 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.056.112 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.056.113 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.056.113 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.056.114 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.056.114 I llama_model_loader: - type  f32:  194 tensors
0.00.056.115 I llama_model_loader: - type  f16:   98 tensors
0.00.084.419 I llm_load_vocab: special tokens cache size = 25
0.00.091.151 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.091.153 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.091.154 I llm_load_print_meta: arch             = gptneox
0.00.091.154 I llm_load_print_meta: vocab type       = BPE
0.00.091.154 I llm_load_print_meta: n_vocab          = 50304
0.00.091.154 I llm_load_print_meta: n_merges         = 50009
0.00.091.155 I llm_load_print_meta: vocab_only       = 0
0.00.091.155 I llm_load_print_meta: n_ctx_train      = 2048
0.00.091.155 I llm_load_print_meta: n_embd           = 2048
0.00.091.155 I llm_load_print_meta: n_layer          = 24
0.00.091.158 I llm_load_print_meta: n_head           = 16
0.00.091.158 I llm_load_print_meta: n_head_kv        = 16
0.00.091.159 I llm_load_print_meta: n_rot            = 32
0.00.091.159 I llm_load_print_meta: n_swa            = 0
0.00.091.159 I llm_load_print_meta: n_embd_head_k    = 128
0.00.091.159 I llm_load_print_meta: n_embd_head_v    = 128
0.00.091.160 I llm_load_print_meta: n_gqa            = 1
0.00.091.160 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.091.161 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.091.161 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.091.162 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.091.162 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.091.162 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.091.162 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.091.163 I llm_load_print_meta: n_ff             = 8192
0.00.091.163 I llm_load_print_meta: n_expert         = 0
0.00.091.163 I llm_load_print_meta: n_expert_used    = 0
0.00.091.163 I llm_load_print_meta: causal attn      = 1
0.00.091.163 I llm_load_print_meta: pooling type     = 0
0.00.091.164 I llm_load_print_meta: rope type        = 2
0.00.091.164 I llm_load_print_meta: rope scaling     = linear
0.00.091.165 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.091.165 I llm_load_print_meta: freq_scale_train = 1
0.00.091.165 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.091.165 I llm_load_print_meta: rope_finetuned   = unknown
0.00.091.165 I llm_load_print_meta: ssm_d_conv       = 0
0.00.091.166 I llm_load_print_meta: ssm_d_inner      = 0
0.00.091.166 I llm_load_print_meta: ssm_d_state      = 0
0.00.091.166 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.091.167 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.091.167 I llm_load_print_meta: model type       = 1.4B
0.00.091.168 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.091.168 I llm_load_print_meta: model params     = 1.41 B
0.00.091.168 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.091.169 I llm_load_print_meta: general.name     = 1.4B
0.00.091.169 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.091.169 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.091.169 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.091.169 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.091.169 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.091.171 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.091.171 I llm_load_print_meta: max token length = 1024
0.00.093.674 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.093.674 I llm_load_tensors: offloading output layer to GPU
0.00.093.674 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.093.685 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.093.686 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.094.042 I llama_new_context_with_model: n_seq_max     = 1
0.00.094.043 I llama_new_context_with_model: n_ctx         = 128
0.00.094.043 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.094.043 I llama_new_context_with_model: n_batch       = 128
0.00.094.043 I llama_new_context_with_model: n_ubatch      = 128
0.00.094.043 I llama_new_context_with_model: flash_attn    = 0
0.00.094.044 I llama_new_context_with_model: freq_base     = 10000.0
0.00.094.044 I llama_new_context_with_model: freq_scale    = 1
0.00.094.044 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.094.045 I ggml_metal_init: allocating
0.00.094.047 I ggml_metal_init: found device: Apple M4
0.00.094.049 I ggml_metal_init: picking default device: Apple M4
0.00.094.646 I ggml_metal_init: using embedded metal library
0.00.097.214 I ggml_metal_init: GPU name:   Apple M4
0.00.097.216 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.097.216 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.097.217 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.097.217 I ggml_metal_init: simdgroup reduction   = true
0.00.097.217 I ggml_metal_init: simdgroup matrix mul. = true
0.00.097.217 I ggml_metal_init: has bfloat            = true
0.00.097.217 I ggml_metal_init: use bfloat            = true
0.00.097.218 I ggml_metal_init: hasUnifiedMemory      = true
0.00.097.218 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.106.356 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.107.713 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.107.716 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.107.729 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.108.626 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.108.627 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.108.627 I llama_new_context_with_model: graph nodes  = 967
0.00.108.627 I llama_new_context_with_model: graph splits = 2
0.00.108.629 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.108.629 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.406.999 I 
0.01.407.038 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.407.080 I perplexity: tokenizing the input ..
0.01.419.827 I perplexity: tokenization took 12.747 ms
0.01.419.832 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.540.999 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.542.874 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.542.925 I llama_perf_context_print:        load time =    1382.20 ms
0.01.542.927 I llama_perf_context_print: prompt eval time =     120.78 ms /   128 tokens (    0.94 ms per token,  1059.80 tokens per second)
0.01.542.928 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.542.929 I llama_perf_context_print:       total time =     135.93 ms /   129 tokens
0.01.543.543 I ggml_metal_free: deallocating

real	0m1.737s
user	0m0.125s
sys	0m0.234s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.400 I build: 4454 (8eceb888) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.015.940 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.028.766 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.028.776 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.028.779 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.028.780 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.028.780 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.028.781 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.028.781 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.028.783 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.028.783 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.028.784 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.028.788 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.028.789 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.028.789 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.028.790 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.028.792 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.028.793 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.028.793 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.034.716 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.036.325 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.041.848 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.041.851 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.041.851 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.041.852 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.041.852 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.041.852 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.041.853 I llama_model_loader: - type  f32:  194 tensors
0.00.041.853 I llama_model_loader: - type q8_0:   98 tensors
0.00.066.994 I llm_load_vocab: special tokens cache size = 25
0.00.073.196 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.073.199 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.073.199 I llm_load_print_meta: arch             = gptneox
0.00.073.199 I llm_load_print_meta: vocab type       = BPE
0.00.073.200 I llm_load_print_meta: n_vocab          = 50304
0.00.073.200 I llm_load_print_meta: n_merges         = 50009
0.00.073.200 I llm_load_print_meta: vocab_only       = 0
0.00.073.200 I llm_load_print_meta: n_ctx_train      = 2048
0.00.073.200 I llm_load_print_meta: n_embd           = 2048
0.00.073.200 I llm_load_print_meta: n_layer          = 24
0.00.073.203 I llm_load_print_meta: n_head           = 16
0.00.073.204 I llm_load_print_meta: n_head_kv        = 16
0.00.073.204 I llm_load_print_meta: n_rot            = 32
0.00.073.205 I llm_load_print_meta: n_swa            = 0
0.00.073.205 I llm_load_print_meta: n_embd_head_k    = 128
0.00.073.205 I llm_load_print_meta: n_embd_head_v    = 128
0.00.073.206 I llm_load_print_meta: n_gqa            = 1
0.00.073.206 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.073.207 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.073.210 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.073.211 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.073.212 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.073.213 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.073.213 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.073.213 I llm_load_print_meta: n_ff             = 8192
0.00.073.214 I llm_load_print_meta: n_expert         = 0
0.00.073.214 I llm_load_print_meta: n_expert_used    = 0
0.00.073.214 I llm_load_print_meta: causal attn      = 1
0.00.073.214 I llm_load_print_meta: pooling type     = 0
0.00.073.214 I llm_load_print_meta: rope type        = 2
0.00.073.214 I llm_load_print_meta: rope scaling     = linear
0.00.073.215 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.073.220 I llm_load_print_meta: freq_scale_train = 1
0.00.073.220 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.073.220 I llm_load_print_meta: rope_finetuned   = unknown
0.00.073.221 I llm_load_print_meta: ssm_d_conv       = 0
0.00.073.222 I llm_load_print_meta: ssm_d_inner      = 0
0.00.073.223 I llm_load_print_meta: ssm_d_state      = 0
0.00.073.223 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.073.223 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.073.223 I llm_load_print_meta: model type       = 1.4B
0.00.073.224 I llm_load_print_meta: model ftype      = Q8_0
0.00.073.224 I llm_load_print_meta: model params     = 1.41 B
0.00.073.225 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.073.225 I llm_load_print_meta: general.name     = 1.4B
0.00.073.225 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.073.225 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.073.225 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.073.225 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.073.227 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.073.227 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.073.227 I llm_load_print_meta: max token length = 1024
0.00.075.655 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.075.655 I llm_load_tensors: offloading output layer to GPU
0.00.075.655 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.075.667 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.075.668 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.076.026 I llama_new_context_with_model: n_seq_max     = 1
0.00.076.027 I llama_new_context_with_model: n_ctx         = 128
0.00.076.027 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.076.027 I llama_new_context_with_model: n_batch       = 128
0.00.076.027 I llama_new_context_with_model: n_ubatch      = 128
0.00.076.028 I llama_new_context_with_model: flash_attn    = 0
0.00.076.028 I llama_new_context_with_model: freq_base     = 10000.0
0.00.076.028 I llama_new_context_with_model: freq_scale    = 1
0.00.076.029 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.076.030 I ggml_metal_init: allocating
0.00.076.036 I ggml_metal_init: found device: Apple M4
0.00.076.039 I ggml_metal_init: picking default device: Apple M4
0.00.076.701 I ggml_metal_init: using embedded metal library
0.00.079.194 I ggml_metal_init: GPU name:   Apple M4
0.00.079.196 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.079.196 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.079.197 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.079.197 I ggml_metal_init: simdgroup reduction   = true
0.00.079.197 I ggml_metal_init: simdgroup matrix mul. = true
0.00.079.197 I ggml_metal_init: has bfloat            = true
0.00.079.197 I ggml_metal_init: use bfloat            = true
0.00.079.198 I ggml_metal_init: hasUnifiedMemory      = true
0.00.079.199 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.090.422 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.091.777 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.091.781 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.091.803 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.092.710 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.092.711 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.092.711 I llama_new_context_with_model: graph nodes  = 967
0.00.092.711 I llama_new_context_with_model: graph splits = 2
0.00.092.713 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.092.713 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.959.295 I 
0.00.959.319 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.959.329 I perplexity: tokenizing the input ..
0.00.969.031 I perplexity: tokenization took 9.701 ms
0.00.969.034 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.095.774 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.096.942 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.096.971 I llama_perf_context_print:        load time =     943.35 ms
0.01.096.972 I llama_perf_context_print: prompt eval time =     126.52 ms /   128 tokens (    0.99 ms per token,  1011.71 tokens per second)
0.01.096.972 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.096.973 I llama_perf_context_print:       total time =     137.68 ms /   129 tokens
0.01.097.435 I ggml_metal_free: deallocating

real	0m1.122s
user	0m0.102s
sys	0m0.159s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.262 I build: 4454 (8eceb888) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.941 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.768 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.016.773 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.775 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.775 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.776 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.776 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.776 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.777 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.778 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.778 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.778 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.779 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.779 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.780 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.783 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.784 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.784 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.551 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.604 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.407 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.408 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.408 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.408 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.409 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.409 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.025.409 I llama_model_loader: - type  f32:  194 tensors
0.00.025.410 I llama_model_loader: - type q4_0:   97 tensors
0.00.025.410 I llama_model_loader: - type q6_K:    1 tensors
0.00.046.320 I llm_load_vocab: special tokens cache size = 25
0.00.052.336 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.339 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.339 I llm_load_print_meta: arch             = gptneox
0.00.052.339 I llm_load_print_meta: vocab type       = BPE
0.00.052.340 I llm_load_print_meta: n_vocab          = 50304
0.00.052.340 I llm_load_print_meta: n_merges         = 50009
0.00.052.340 I llm_load_print_meta: vocab_only       = 0
0.00.052.340 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.340 I llm_load_print_meta: n_embd           = 2048
0.00.052.341 I llm_load_print_meta: n_layer          = 24
0.00.052.343 I llm_load_print_meta: n_head           = 16
0.00.052.344 I llm_load_print_meta: n_head_kv        = 16
0.00.052.345 I llm_load_print_meta: n_rot            = 32
0.00.052.345 I llm_load_print_meta: n_swa            = 0
0.00.052.345 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.345 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.346 I llm_load_print_meta: n_gqa            = 1
0.00.052.347 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.347 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.348 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.348 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.349 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.349 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.349 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.350 I llm_load_print_meta: n_ff             = 8192
0.00.052.350 I llm_load_print_meta: n_expert         = 0
0.00.052.350 I llm_load_print_meta: n_expert_used    = 0
0.00.052.351 I llm_load_print_meta: causal attn      = 1
0.00.052.351 I llm_load_print_meta: pooling type     = 0
0.00.052.351 I llm_load_print_meta: rope type        = 2
0.00.052.351 I llm_load_print_meta: rope scaling     = linear
0.00.052.351 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.352 I llm_load_print_meta: freq_scale_train = 1
0.00.052.352 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.352 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.354 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.354 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.354 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.354 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.354 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.355 I llm_load_print_meta: model type       = 1.4B
0.00.052.355 I llm_load_print_meta: model ftype      = Q4_0
0.00.052.356 I llm_load_print_meta: model params     = 1.41 B
0.00.052.356 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.052.356 I llm_load_print_meta: general.name     = 1.4B
0.00.052.357 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.358 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.358 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.359 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.359 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.052.359 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.359 I llm_load_print_meta: max token length = 1024
0.00.054.333 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.334 I llm_load_tensors: offloading output layer to GPU
0.00.054.334 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.344 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.054.346 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.054.681 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.681 I llama_new_context_with_model: n_ctx         = 128
0.00.054.681 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.682 I llama_new_context_with_model: n_batch       = 128
0.00.054.682 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.682 I llama_new_context_with_model: flash_attn    = 0
0.00.054.682 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.683 I llama_new_context_with_model: freq_scale    = 1
0.00.054.683 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.683 I ggml_metal_init: allocating
0.00.054.690 I ggml_metal_init: found device: Apple M4
0.00.054.692 I ggml_metal_init: picking default device: Apple M4
0.00.055.238 I ggml_metal_init: using embedded metal library
0.00.057.533 I ggml_metal_init: GPU name:   Apple M4
0.00.057.535 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.535 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.536 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.536 I ggml_metal_init: simdgroup reduction   = true
0.00.057.536 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.536 I ggml_metal_init: has bfloat            = true
0.00.057.536 I ggml_metal_init: use bfloat            = true
0.00.057.537 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.537 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.124 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.068.386 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.388 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.402 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.069.229 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.069.229 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.069.230 I llama_new_context_with_model: graph nodes  = 967
0.00.069.230 I llama_new_context_with_model: graph splits = 2
0.00.069.231 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.069.232 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.589.150 I 
0.00.589.232 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.589.257 I perplexity: tokenizing the input ..
0.00.597.555 I perplexity: tokenization took 8.297 ms
0.00.597.559 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.720.677 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.721.889 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.721.913 I llama_perf_context_print:        load time =     579.20 ms
0.00.721.914 I llama_perf_context_print: prompt eval time =     122.89 ms /   128 tokens (    0.96 ms per token,  1041.56 tokens per second)
0.00.721.915 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.721.915 I llama_perf_context_print:       total time =     132.77 ms /   129 tokens
0.00.722.398 I ggml_metal_free: deallocating

real	0m0.738s
user	0m0.079s
sys	0m0.088s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.098 I build: 4454 (8eceb888) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.902 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.862 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.015.867 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.869 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.869 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.869 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.870 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.870 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.871 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.871 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.872 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.873 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.875 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.875 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.876 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.879 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.880 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.880 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.654 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.698 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.438 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.439 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.440 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.440 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.440 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.440 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.024.441 I llama_model_loader: - type  f32:  194 tensors
0.00.024.441 I llama_model_loader: - type q4_1:   97 tensors
0.00.024.442 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.325 I llm_load_vocab: special tokens cache size = 25
0.00.051.294 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.297 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.297 I llm_load_print_meta: arch             = gptneox
0.00.051.297 I llm_load_print_meta: vocab type       = BPE
0.00.051.298 I llm_load_print_meta: n_vocab          = 50304
0.00.051.298 I llm_load_print_meta: n_merges         = 50009
0.00.051.298 I llm_load_print_meta: vocab_only       = 0
0.00.051.298 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.298 I llm_load_print_meta: n_embd           = 2048
0.00.051.298 I llm_load_print_meta: n_layer          = 24
0.00.051.301 I llm_load_print_meta: n_head           = 16
0.00.051.301 I llm_load_print_meta: n_head_kv        = 16
0.00.051.303 I llm_load_print_meta: n_rot            = 32
0.00.051.304 I llm_load_print_meta: n_swa            = 0
0.00.051.304 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.304 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.305 I llm_load_print_meta: n_gqa            = 1
0.00.051.306 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.306 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.307 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.307 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.307 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.307 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.308 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.308 I llm_load_print_meta: n_ff             = 8192
0.00.051.309 I llm_load_print_meta: n_expert         = 0
0.00.051.309 I llm_load_print_meta: n_expert_used    = 0
0.00.051.309 I llm_load_print_meta: causal attn      = 1
0.00.051.309 I llm_load_print_meta: pooling type     = 0
0.00.051.310 I llm_load_print_meta: rope type        = 2
0.00.051.314 I llm_load_print_meta: rope scaling     = linear
0.00.051.316 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.318 I llm_load_print_meta: freq_scale_train = 1
0.00.051.318 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.318 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.318 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.318 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.318 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.319 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.319 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.319 I llm_load_print_meta: model type       = 1.4B
0.00.051.319 I llm_load_print_meta: model ftype      = Q4_1
0.00.051.320 I llm_load_print_meta: model params     = 1.41 B
0.00.051.321 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.051.321 I llm_load_print_meta: general.name     = 1.4B
0.00.051.321 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.322 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.322 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.322 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.322 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.322 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.322 I llm_load_print_meta: max token length = 1024
0.00.053.324 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.324 I llm_load_tensors: offloading output layer to GPU
0.00.053.324 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.335 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.053.336 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.053.696 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.696 I llama_new_context_with_model: n_ctx         = 128
0.00.053.696 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.697 I llama_new_context_with_model: n_batch       = 128
0.00.053.697 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.697 I llama_new_context_with_model: flash_attn    = 0
0.00.053.697 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.698 I llama_new_context_with_model: freq_scale    = 1
0.00.053.698 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.699 I ggml_metal_init: allocating
0.00.053.705 I ggml_metal_init: found device: Apple M4
0.00.053.707 I ggml_metal_init: picking default device: Apple M4
0.00.054.309 I ggml_metal_init: using embedded metal library
0.00.056.630 I ggml_metal_init: GPU name:   Apple M4
0.00.056.632 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.632 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.633 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.633 I ggml_metal_init: simdgroup reduction   = true
0.00.056.633 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.633 I ggml_metal_init: has bfloat            = true
0.00.056.633 I ggml_metal_init: use bfloat            = true
0.00.056.634 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.634 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.041 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.067.382 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.385 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.398 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.229 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.230 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.231 I llama_new_context_with_model: graph nodes  = 967
0.00.068.231 I llama_new_context_with_model: graph splits = 2
0.00.068.232 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.232 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.674.858 I 
0.00.674.885 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.674.896 I perplexity: tokenizing the input ..
0.00.682.911 I perplexity: tokenization took 8.015 ms
0.00.682.915 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.805.810 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.806.972 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.807.000 I llama_perf_context_print:        load time =     665.95 ms
0.00.807.001 I llama_perf_context_print: prompt eval time =     122.67 ms /   128 tokens (    0.96 ms per token,  1043.47 tokens per second)
0.00.807.002 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.807.002 I llama_perf_context_print:       total time =     132.14 ms /   129 tokens
0.00.807.508 I ggml_metal_free: deallocating

real	0m0.822s
user	0m0.079s
sys	0m0.107s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.085 I build: 4454 (8eceb888) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.369 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.377 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.016.382 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.384 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.385 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.385 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.385 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.386 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.387 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.387 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.390 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.390 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.390 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.391 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.391 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.395 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.396 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.396 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.112 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.107 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.831 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.833 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.833 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.833 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.834 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.834 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.024.835 I llama_model_loader: - type  f32:  194 tensors
0.00.024.835 I llama_model_loader: - type q5_0:   97 tensors
0.00.024.835 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.913 I llm_load_vocab: special tokens cache size = 25
0.00.051.038 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.041 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.041 I llm_load_print_meta: arch             = gptneox
0.00.051.042 I llm_load_print_meta: vocab type       = BPE
0.00.051.042 I llm_load_print_meta: n_vocab          = 50304
0.00.051.042 I llm_load_print_meta: n_merges         = 50009
0.00.051.042 I llm_load_print_meta: vocab_only       = 0
0.00.051.042 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.043 I llm_load_print_meta: n_embd           = 2048
0.00.051.043 I llm_load_print_meta: n_layer          = 24
0.00.051.045 I llm_load_print_meta: n_head           = 16
0.00.051.048 I llm_load_print_meta: n_head_kv        = 16
0.00.051.048 I llm_load_print_meta: n_rot            = 32
0.00.051.048 I llm_load_print_meta: n_swa            = 0
0.00.051.048 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.048 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.049 I llm_load_print_meta: n_gqa            = 1
0.00.051.050 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.055 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.056 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.057 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.058 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.058 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.058 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.059 I llm_load_print_meta: n_ff             = 8192
0.00.051.059 I llm_load_print_meta: n_expert         = 0
0.00.051.059 I llm_load_print_meta: n_expert_used    = 0
0.00.051.059 I llm_load_print_meta: causal attn      = 1
0.00.051.059 I llm_load_print_meta: pooling type     = 0
0.00.051.059 I llm_load_print_meta: rope type        = 2
0.00.051.060 I llm_load_print_meta: rope scaling     = linear
0.00.051.060 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.063 I llm_load_print_meta: freq_scale_train = 1
0.00.051.064 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.065 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.065 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.065 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.065 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.065 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.065 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.065 I llm_load_print_meta: model type       = 1.4B
0.00.051.066 I llm_load_print_meta: model ftype      = Q5_0
0.00.051.066 I llm_load_print_meta: model params     = 1.41 B
0.00.051.067 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.051.067 I llm_load_print_meta: general.name     = 1.4B
0.00.051.067 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.068 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.070 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.070 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.071 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.071 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.071 I llm_load_print_meta: max token length = 1024
0.00.053.025 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.026 I llm_load_tensors: offloading output layer to GPU
0.00.053.026 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.036 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.053.038 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.053.401 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.401 I llama_new_context_with_model: n_ctx         = 128
0.00.053.402 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.402 I llama_new_context_with_model: n_batch       = 128
0.00.053.402 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.402 I llama_new_context_with_model: flash_attn    = 0
0.00.053.403 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.403 I llama_new_context_with_model: freq_scale    = 1
0.00.053.403 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.404 I ggml_metal_init: allocating
0.00.053.409 I ggml_metal_init: found device: Apple M4
0.00.053.412 I ggml_metal_init: picking default device: Apple M4
0.00.053.987 I ggml_metal_init: using embedded metal library
0.00.056.303 I ggml_metal_init: GPU name:   Apple M4
0.00.056.304 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.305 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.305 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.305 I ggml_metal_init: simdgroup reduction   = true
0.00.056.305 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.305 I ggml_metal_init: has bfloat            = true
0.00.056.306 I ggml_metal_init: use bfloat            = true
0.00.056.306 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.307 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.835 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.067.063 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.066 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.081 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.943 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.944 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.944 I llama_new_context_with_model: graph nodes  = 967
0.00.067.945 I llama_new_context_with_model: graph splits = 2
0.00.067.946 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.946 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.686.957 I 
0.00.687.000 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.687.016 I perplexity: tokenizing the input ..
0.00.695.194 I perplexity: tokenization took 8.177 ms
0.00.695.197 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.830.065 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.831.236 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.831.254 I llama_perf_context_print:        load time =     677.58 ms
0.00.831.259 I llama_perf_context_print: prompt eval time =     134.64 ms /   128 tokens (    1.05 ms per token,   950.70 tokens per second)
0.00.831.260 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.831.261 I llama_perf_context_print:       total time =     144.30 ms /   129 tokens
0.00.831.595 I ggml_metal_free: deallocating

real	0m0.846s
user	0m0.078s
sys	0m0.113s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.086 I build: 4454 (8eceb888) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.829 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.016 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.021 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.022 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.023 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.023 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.023 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.024 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.025 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.025 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.025 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.026 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.026 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.026 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.027 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.028 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.029 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.029 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.792 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.784 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.539 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.540 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.541 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.541 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.541 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.542 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.542 I llama_model_loader: - type  f32:  194 tensors
0.00.024.543 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.543 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.503 I llm_load_vocab: special tokens cache size = 25
0.00.050.409 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.412 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.412 I llm_load_print_meta: arch             = gptneox
0.00.050.413 I llm_load_print_meta: vocab type       = BPE
0.00.050.413 I llm_load_print_meta: n_vocab          = 50304
0.00.050.413 I llm_load_print_meta: n_merges         = 50009
0.00.050.413 I llm_load_print_meta: vocab_only       = 0
0.00.050.414 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.414 I llm_load_print_meta: n_embd           = 2048
0.00.050.414 I llm_load_print_meta: n_layer          = 24
0.00.050.417 I llm_load_print_meta: n_head           = 16
0.00.050.418 I llm_load_print_meta: n_head_kv        = 16
0.00.050.418 I llm_load_print_meta: n_rot            = 32
0.00.050.418 I llm_load_print_meta: n_swa            = 0
0.00.050.418 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.418 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.419 I llm_load_print_meta: n_gqa            = 1
0.00.050.420 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.420 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.421 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.421 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.421 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.422 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.422 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.422 I llm_load_print_meta: n_ff             = 8192
0.00.050.423 I llm_load_print_meta: n_expert         = 0
0.00.050.423 I llm_load_print_meta: n_expert_used    = 0
0.00.050.424 I llm_load_print_meta: causal attn      = 1
0.00.050.425 I llm_load_print_meta: pooling type     = 0
0.00.050.425 I llm_load_print_meta: rope type        = 2
0.00.050.425 I llm_load_print_meta: rope scaling     = linear
0.00.050.425 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.426 I llm_load_print_meta: freq_scale_train = 1
0.00.050.426 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.426 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.426 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.426 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.427 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.427 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.427 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.429 I llm_load_print_meta: model type       = 1.4B
0.00.050.429 I llm_load_print_meta: model ftype      = Q5_1
0.00.050.430 I llm_load_print_meta: model params     = 1.41 B
0.00.050.430 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.050.430 I llm_load_print_meta: general.name     = 1.4B
0.00.050.431 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.431 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.431 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.431 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.432 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.433 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.433 I llm_load_print_meta: max token length = 1024
0.00.052.409 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.409 I llm_load_tensors: offloading output layer to GPU
0.00.052.409 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.420 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.052.421 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.052.750 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.751 I llama_new_context_with_model: n_ctx         = 128
0.00.052.751 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.751 I llama_new_context_with_model: n_batch       = 128
0.00.052.751 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.751 I llama_new_context_with_model: flash_attn    = 0
0.00.052.751 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.752 I llama_new_context_with_model: freq_scale    = 1
0.00.052.752 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.752 I ggml_metal_init: allocating
0.00.052.755 I ggml_metal_init: found device: Apple M4
0.00.052.757 I ggml_metal_init: picking default device: Apple M4
0.00.053.301 I ggml_metal_init: using embedded metal library
0.00.055.596 I ggml_metal_init: GPU name:   Apple M4
0.00.055.597 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.598 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.598 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.598 I ggml_metal_init: simdgroup reduction   = true
0.00.055.598 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.598 I ggml_metal_init: has bfloat            = true
0.00.055.599 I ggml_metal_init: use bfloat            = true
0.00.055.599 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.600 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.132 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.393 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.395 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.410 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.336 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.337 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.337 I llama_new_context_with_model: graph nodes  = 967
0.00.067.338 I llama_new_context_with_model: graph splits = 2
0.00.067.339 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.339 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.724.402 I 
0.00.724.452 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.724.472 I perplexity: tokenizing the input ..
0.00.732.153 I perplexity: tokenization took 7.679 ms
0.00.732.164 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.867.178 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.868.338 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.868.363 I llama_perf_context_print:        load time =     715.57 ms
0.00.868.364 I llama_perf_context_print: prompt eval time =     134.79 ms /   128 tokens (    1.05 ms per token,   949.65 tokens per second)
0.00.868.365 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.868.365 I llama_perf_context_print:       total time =     143.96 ms /   129 tokens
0.00.868.836 I ggml_metal_free: deallocating

real	0m0.883s
user	0m0.078s
sys	0m0.109s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.086 I build: 4454 (8eceb888) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.391 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.140 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.146 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.147 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.148 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.148 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.148 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.149 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.150 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.150 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.151 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.153 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.154 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.154 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.154 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.156 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.156 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.156 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.866 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.908 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.630 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.631 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.631 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.631 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.632 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.632 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.632 I llama_model_loader: - type  f32:  194 tensors
0.00.024.633 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.633 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.633 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.289 I llm_load_vocab: special tokens cache size = 25
0.00.051.289 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.292 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.292 I llm_load_print_meta: arch             = gptneox
0.00.051.293 I llm_load_print_meta: vocab type       = BPE
0.00.051.293 I llm_load_print_meta: n_vocab          = 50304
0.00.051.293 I llm_load_print_meta: n_merges         = 50009
0.00.051.293 I llm_load_print_meta: vocab_only       = 0
0.00.051.293 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.294 I llm_load_print_meta: n_embd           = 2048
0.00.051.294 I llm_load_print_meta: n_layer          = 24
0.00.051.297 I llm_load_print_meta: n_head           = 16
0.00.051.297 I llm_load_print_meta: n_head_kv        = 16
0.00.051.298 I llm_load_print_meta: n_rot            = 32
0.00.051.298 I llm_load_print_meta: n_swa            = 0
0.00.051.298 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.298 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.299 I llm_load_print_meta: n_gqa            = 1
0.00.051.300 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.301 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.301 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.302 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.302 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.302 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.302 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.303 I llm_load_print_meta: n_ff             = 8192
0.00.051.303 I llm_load_print_meta: n_expert         = 0
0.00.051.303 I llm_load_print_meta: n_expert_used    = 0
0.00.051.305 I llm_load_print_meta: causal attn      = 1
0.00.051.305 I llm_load_print_meta: pooling type     = 0
0.00.051.305 I llm_load_print_meta: rope type        = 2
0.00.051.306 I llm_load_print_meta: rope scaling     = linear
0.00.051.306 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.306 I llm_load_print_meta: freq_scale_train = 1
0.00.051.307 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.307 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.307 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.307 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.307 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.307 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.308 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.308 I llm_load_print_meta: model type       = 1.4B
0.00.051.308 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.051.309 I llm_load_print_meta: model params     = 1.41 B
0.00.051.309 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.051.309 I llm_load_print_meta: general.name     = 1.4B
0.00.051.310 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.310 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.310 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.310 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.311 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.311 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.311 I llm_load_print_meta: max token length = 1024
0.00.052.879 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.879 I llm_load_tensors: offloading output layer to GPU
0.00.052.879 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.889 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.052.891 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.053.219 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.219 I llama_new_context_with_model: n_ctx         = 128
0.00.053.220 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.220 I llama_new_context_with_model: n_batch       = 128
0.00.053.220 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.220 I llama_new_context_with_model: flash_attn    = 0
0.00.053.220 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.221 I llama_new_context_with_model: freq_scale    = 1
0.00.053.221 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.222 I ggml_metal_init: allocating
0.00.053.227 I ggml_metal_init: found device: Apple M4
0.00.053.230 I ggml_metal_init: picking default device: Apple M4
0.00.053.773 I ggml_metal_init: using embedded metal library
0.00.056.112 I ggml_metal_init: GPU name:   Apple M4
0.00.056.113 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.114 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.114 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.114 I ggml_metal_init: simdgroup reduction   = true
0.00.056.114 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.115 I ggml_metal_init: has bfloat            = true
0.00.056.115 I ggml_metal_init: use bfloat            = true
0.00.056.115 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.116 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.580 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.860 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.869 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.887 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.714 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.715 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.715 I llama_new_context_with_model: graph nodes  = 967
0.00.067.715 I llama_new_context_with_model: graph splits = 2
0.00.067.716 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.717 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.406.311 I 
0.00.406.337 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.406.347 I perplexity: tokenizing the input ..
0.00.414.476 I perplexity: tokenization took 8.128 ms
0.00.414.484 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.546.968 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.548.164 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.548.193 I llama_perf_context_print:        load time =     396.92 ms
0.00.548.193 I llama_perf_context_print: prompt eval time =     132.25 ms /   128 tokens (    1.03 ms per token,   967.87 tokens per second)
0.00.548.194 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.548.194 I llama_perf_context_print:       total time =     141.88 ms /   129 tokens
0.00.548.727 I ggml_metal_free: deallocating

real	0m0.564s
user	0m0.079s
sys	0m0.075s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.090 I build: 4454 (8eceb888) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.874 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.061 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.066 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.068 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.069 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.069 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.069 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.071 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.072 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.073 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.073 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.074 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.074 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.074 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.075 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.076 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.076 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.077 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.780 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.775 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.511 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.513 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.513 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.513 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.514 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.514 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.515 I llama_model_loader: - type  f32:  194 tensors
0.00.024.515 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.515 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.515 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.516 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.428 I llm_load_vocab: special tokens cache size = 25
0.00.051.533 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.536 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.536 I llm_load_print_meta: arch             = gptneox
0.00.051.536 I llm_load_print_meta: vocab type       = BPE
0.00.051.537 I llm_load_print_meta: n_vocab          = 50304
0.00.051.537 I llm_load_print_meta: n_merges         = 50009
0.00.051.537 I llm_load_print_meta: vocab_only       = 0
0.00.051.537 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.537 I llm_load_print_meta: n_embd           = 2048
0.00.051.537 I llm_load_print_meta: n_layer          = 24
0.00.051.540 I llm_load_print_meta: n_head           = 16
0.00.051.541 I llm_load_print_meta: n_head_kv        = 16
0.00.051.541 I llm_load_print_meta: n_rot            = 32
0.00.051.542 I llm_load_print_meta: n_swa            = 0
0.00.051.542 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.542 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.543 I llm_load_print_meta: n_gqa            = 1
0.00.051.543 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.544 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.544 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.545 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.545 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.545 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.545 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.546 I llm_load_print_meta: n_ff             = 8192
0.00.051.546 I llm_load_print_meta: n_expert         = 0
0.00.051.547 I llm_load_print_meta: n_expert_used    = 0
0.00.051.547 I llm_load_print_meta: causal attn      = 1
0.00.051.547 I llm_load_print_meta: pooling type     = 0
0.00.051.547 I llm_load_print_meta: rope type        = 2
0.00.051.547 I llm_load_print_meta: rope scaling     = linear
0.00.051.548 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.548 I llm_load_print_meta: freq_scale_train = 1
0.00.051.548 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.549 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.549 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.549 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.549 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.549 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.549 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.549 I llm_load_print_meta: model type       = 1.4B
0.00.051.550 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.051.550 I llm_load_print_meta: model params     = 1.41 B
0.00.051.551 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.051.551 I llm_load_print_meta: general.name     = 1.4B
0.00.051.551 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.551 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.552 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.552 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.552 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.552 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.553 I llm_load_print_meta: max token length = 1024
0.00.053.528 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.528 I llm_load_tensors: offloading output layer to GPU
0.00.053.529 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.539 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.053.540 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.053.881 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.882 I llama_new_context_with_model: n_ctx         = 128
0.00.053.882 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.882 I llama_new_context_with_model: n_batch       = 128
0.00.053.882 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.882 I llama_new_context_with_model: flash_attn    = 0
0.00.053.883 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.883 I llama_new_context_with_model: freq_scale    = 1
0.00.053.883 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.884 I ggml_metal_init: allocating
0.00.053.890 I ggml_metal_init: found device: Apple M4
0.00.053.892 I ggml_metal_init: picking default device: Apple M4
0.00.054.451 I ggml_metal_init: using embedded metal library
0.00.056.771 I ggml_metal_init: GPU name:   Apple M4
0.00.056.772 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.773 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.773 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.773 I ggml_metal_init: simdgroup reduction   = true
0.00.056.774 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.774 I ggml_metal_init: has bfloat            = true
0.00.056.774 I ggml_metal_init: use bfloat            = true
0.00.056.774 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.775 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.091 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.067.404 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.409 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.425 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.260 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.261 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.262 I llama_new_context_with_model: graph nodes  = 967
0.00.068.262 I llama_new_context_with_model: graph splits = 2
0.00.068.263 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.263 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.474.874 I 
0.00.474.906 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.474.919 I perplexity: tokenizing the input ..
0.00.482.794 I perplexity: tokenization took 7.874 ms
0.00.482.798 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.615.218 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.616.389 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.616.411 I llama_perf_context_print:        load time =     466.00 ms
0.00.616.412 I llama_perf_context_print: prompt eval time =     132.19 ms /   128 tokens (    1.03 ms per token,   968.28 tokens per second)
0.00.616.412 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.616.413 I llama_perf_context_print:       total time =     141.54 ms /   129 tokens
0.00.616.840 I ggml_metal_free: deallocating

real	0m0.630s
user	0m0.079s
sys	0m0.083s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.087 I build: 4454 (8eceb888) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.692 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.743 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.015.748 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.750 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.750 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.750 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.751 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.751 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.752 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.753 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.753 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.753 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.754 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.754 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.755 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.756 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.756 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.757 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.471 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.473 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.210 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.211 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.211 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.211 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.212 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.212 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.212 I llama_model_loader: - type  f32:  194 tensors
0.00.024.213 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.213 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.213 I llama_model_loader: - type q6_K:   13 tensors
0.00.044.889 I llm_load_vocab: special tokens cache size = 25
0.00.050.869 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.872 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.873 I llm_load_print_meta: arch             = gptneox
0.00.050.873 I llm_load_print_meta: vocab type       = BPE
0.00.050.873 I llm_load_print_meta: n_vocab          = 50304
0.00.050.873 I llm_load_print_meta: n_merges         = 50009
0.00.050.874 I llm_load_print_meta: vocab_only       = 0
0.00.050.874 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.874 I llm_load_print_meta: n_embd           = 2048
0.00.050.874 I llm_load_print_meta: n_layer          = 24
0.00.050.876 I llm_load_print_meta: n_head           = 16
0.00.050.877 I llm_load_print_meta: n_head_kv        = 16
0.00.050.877 I llm_load_print_meta: n_rot            = 32
0.00.050.877 I llm_load_print_meta: n_swa            = 0
0.00.050.878 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.878 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.879 I llm_load_print_meta: n_gqa            = 1
0.00.050.879 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.880 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.881 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.881 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.883 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.884 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.884 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.884 I llm_load_print_meta: n_ff             = 8192
0.00.050.884 I llm_load_print_meta: n_expert         = 0
0.00.050.885 I llm_load_print_meta: n_expert_used    = 0
0.00.050.885 I llm_load_print_meta: causal attn      = 1
0.00.050.885 I llm_load_print_meta: pooling type     = 0
0.00.050.886 I llm_load_print_meta: rope type        = 2
0.00.050.893 I llm_load_print_meta: rope scaling     = linear
0.00.050.895 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.895 I llm_load_print_meta: freq_scale_train = 1
0.00.050.895 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.897 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.897 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.897 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.897 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.897 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.897 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.897 I llm_load_print_meta: model type       = 1.4B
0.00.050.898 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.050.898 I llm_load_print_meta: model params     = 1.41 B
0.00.050.899 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.050.899 I llm_load_print_meta: general.name     = 1.4B
0.00.050.899 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.899 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.900 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.900 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.900 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.900 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.900 I llm_load_print_meta: max token length = 1024
0.00.052.434 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.435 I llm_load_tensors: offloading output layer to GPU
0.00.052.435 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.445 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.052.446 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.052.815 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.816 I llama_new_context_with_model: n_ctx         = 128
0.00.052.816 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.816 I llama_new_context_with_model: n_batch       = 128
0.00.052.816 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.817 I llama_new_context_with_model: flash_attn    = 0
0.00.052.817 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.817 I llama_new_context_with_model: freq_scale    = 1
0.00.052.818 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.818 I ggml_metal_init: allocating
0.00.052.825 I ggml_metal_init: found device: Apple M4
0.00.052.827 I ggml_metal_init: picking default device: Apple M4
0.00.053.383 I ggml_metal_init: using embedded metal library
0.00.055.694 I ggml_metal_init: GPU name:   Apple M4
0.00.055.696 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.696 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.697 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.697 I ggml_metal_init: simdgroup reduction   = true
0.00.055.697 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.697 I ggml_metal_init: has bfloat            = true
0.00.055.697 I ggml_metal_init: use bfloat            = true
0.00.055.698 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.698 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.008 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.246 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.253 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.273 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.109 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.110 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.110 I llama_new_context_with_model: graph nodes  = 967
0.00.067.111 I llama_new_context_with_model: graph splits = 2
0.00.067.112 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.112 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.559.434 I 
0.00.559.465 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.559.478 I perplexity: tokenizing the input ..
0.00.568.646 I perplexity: tokenization took 9.166 ms
0.00.568.649 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.702.153 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.703.566 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.703.590 I llama_perf_context_print:        load time =     550.73 ms
0.00.703.591 I llama_perf_context_print: prompt eval time =     133.27 ms /   128 tokens (    1.04 ms per token,   960.47 tokens per second)
0.00.703.591 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.703.591 I llama_perf_context_print:       total time =     144.16 ms /   129 tokens
0.00.703.932 I ggml_metal_free: deallocating

real	0m0.717s
user	0m0.080s
sys	0m0.099s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.101 I build: 4454 (8eceb888) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.412 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.836 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.843 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.845 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.845 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.845 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.846 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.846 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.847 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.847 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.851 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.851 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.851 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.851 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.852 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.856 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.857 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.857 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.763 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.867 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.803 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.805 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.805 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.806 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.806 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.806 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.807 I llama_model_loader: - type  f32:  194 tensors
0.00.025.807 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.808 I llama_model_loader: - type q6_K:   37 tensors
0.00.047.524 I llm_load_vocab: special tokens cache size = 25
0.00.053.677 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.053.682 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.053.682 I llm_load_print_meta: arch             = gptneox
0.00.053.683 I llm_load_print_meta: vocab type       = BPE
0.00.053.683 I llm_load_print_meta: n_vocab          = 50304
0.00.053.683 I llm_load_print_meta: n_merges         = 50009
0.00.053.683 I llm_load_print_meta: vocab_only       = 0
0.00.053.683 I llm_load_print_meta: n_ctx_train      = 2048
0.00.053.685 I llm_load_print_meta: n_embd           = 2048
0.00.053.685 I llm_load_print_meta: n_layer          = 24
0.00.053.689 I llm_load_print_meta: n_head           = 16
0.00.053.689 I llm_load_print_meta: n_head_kv        = 16
0.00.053.689 I llm_load_print_meta: n_rot            = 32
0.00.053.690 I llm_load_print_meta: n_swa            = 0
0.00.053.692 I llm_load_print_meta: n_embd_head_k    = 128
0.00.053.692 I llm_load_print_meta: n_embd_head_v    = 128
0.00.053.693 I llm_load_print_meta: n_gqa            = 1
0.00.053.693 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.053.694 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.053.698 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.053.699 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.053.699 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.053.699 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.053.699 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.053.700 I llm_load_print_meta: n_ff             = 8192
0.00.053.700 I llm_load_print_meta: n_expert         = 0
0.00.053.700 I llm_load_print_meta: n_expert_used    = 0
0.00.053.702 I llm_load_print_meta: causal attn      = 1
0.00.053.702 I llm_load_print_meta: pooling type     = 0
0.00.053.702 I llm_load_print_meta: rope type        = 2
0.00.053.702 I llm_load_print_meta: rope scaling     = linear
0.00.053.704 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.053.704 I llm_load_print_meta: freq_scale_train = 1
0.00.053.705 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.053.705 I llm_load_print_meta: rope_finetuned   = unknown
0.00.053.705 I llm_load_print_meta: ssm_d_conv       = 0
0.00.053.705 I llm_load_print_meta: ssm_d_inner      = 0
0.00.053.705 I llm_load_print_meta: ssm_d_state      = 0
0.00.053.705 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.053.705 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.053.705 I llm_load_print_meta: model type       = 1.4B
0.00.053.706 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.053.706 I llm_load_print_meta: model params     = 1.41 B
0.00.053.707 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.053.707 I llm_load_print_meta: general.name     = 1.4B
0.00.053.707 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.053.707 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.053.708 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.053.708 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.053.708 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.053.708 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.053.708 I llm_load_print_meta: max token length = 1024
0.00.055.728 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.055.729 I llm_load_tensors: offloading output layer to GPU
0.00.055.729 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.055.740 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.055.741 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.056.123 I llama_new_context_with_model: n_seq_max     = 1
0.00.056.124 I llama_new_context_with_model: n_ctx         = 128
0.00.056.124 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.056.124 I llama_new_context_with_model: n_batch       = 128
0.00.056.124 I llama_new_context_with_model: n_ubatch      = 128
0.00.056.124 I llama_new_context_with_model: flash_attn    = 0
0.00.056.125 I llama_new_context_with_model: freq_base     = 10000.0
0.00.056.125 I llama_new_context_with_model: freq_scale    = 1
0.00.056.125 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.056.126 I ggml_metal_init: allocating
0.00.056.129 I ggml_metal_init: found device: Apple M4
0.00.056.131 I ggml_metal_init: picking default device: Apple M4
0.00.056.740 I ggml_metal_init: using embedded metal library
0.00.059.185 I ggml_metal_init: GPU name:   Apple M4
0.00.059.187 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.059.187 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.059.188 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.059.188 I ggml_metal_init: simdgroup reduction   = true
0.00.059.190 I ggml_metal_init: simdgroup matrix mul. = true
0.00.059.190 I ggml_metal_init: has bfloat            = true
0.00.059.190 I ggml_metal_init: use bfloat            = true
0.00.059.191 I ggml_metal_init: hasUnifiedMemory      = true
0.00.059.196 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.445 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.069.771 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.069.773 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.069.789 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.070.622 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.070.622 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.070.623 I llama_new_context_with_model: graph nodes  = 967
0.00.070.623 I llama_new_context_with_model: graph splits = 2
0.00.070.624 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.070.625 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.635.329 I 
0.00.635.364 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.635.377 I perplexity: tokenizing the input ..
0.00.643.053 I perplexity: tokenization took 7.675 ms
0.00.643.058 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.783.941 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.785.193 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.785.220 I llama_perf_context_print:        load time =     625.91 ms
0.00.785.221 I llama_perf_context_print: prompt eval time =     140.65 ms /   128 tokens (    1.10 ms per token,   910.08 tokens per second)
0.00.785.222 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.785.222 I llama_perf_context_print:       total time =     149.89 ms /   129 tokens
0.00.785.727 I ggml_metal_free: deallocating

real	0m0.801s
user	0m0.080s
sys	0m0.116s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.087 I build: 4454 (8eceb888) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.995 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.815 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.820 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.826 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.826 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.827 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.827 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.827 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.828 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.829 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.829 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.829 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.830 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.830 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.831 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.834 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.835 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.835 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.578 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.575 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.315 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.316 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.317 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.317 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.317 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.318 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.318 I llama_model_loader: - type  f32:  194 tensors
0.00.024.318 I llama_model_loader: - type q6_K:   98 tensors
0.00.045.297 I llm_load_vocab: special tokens cache size = 25
0.00.051.362 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.365 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.365 I llm_load_print_meta: arch             = gptneox
0.00.051.365 I llm_load_print_meta: vocab type       = BPE
0.00.051.366 I llm_load_print_meta: n_vocab          = 50304
0.00.051.366 I llm_load_print_meta: n_merges         = 50009
0.00.051.366 I llm_load_print_meta: vocab_only       = 0
0.00.051.366 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.366 I llm_load_print_meta: n_embd           = 2048
0.00.051.367 I llm_load_print_meta: n_layer          = 24
0.00.051.370 I llm_load_print_meta: n_head           = 16
0.00.051.371 I llm_load_print_meta: n_head_kv        = 16
0.00.051.372 I llm_load_print_meta: n_rot            = 32
0.00.051.373 I llm_load_print_meta: n_swa            = 0
0.00.051.373 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.373 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.374 I llm_load_print_meta: n_gqa            = 1
0.00.051.374 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.375 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.376 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.376 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.376 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.376 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.377 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.377 I llm_load_print_meta: n_ff             = 8192
0.00.051.377 I llm_load_print_meta: n_expert         = 0
0.00.051.378 I llm_load_print_meta: n_expert_used    = 0
0.00.051.378 I llm_load_print_meta: causal attn      = 1
0.00.051.383 I llm_load_print_meta: pooling type     = 0
0.00.051.383 I llm_load_print_meta: rope type        = 2
0.00.051.383 I llm_load_print_meta: rope scaling     = linear
0.00.051.384 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.386 I llm_load_print_meta: freq_scale_train = 1
0.00.051.388 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.388 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.388 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.388 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.388 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.388 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.388 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.389 I llm_load_print_meta: model type       = 1.4B
0.00.051.389 I llm_load_print_meta: model ftype      = Q6_K
0.00.051.390 I llm_load_print_meta: model params     = 1.41 B
0.00.051.390 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.051.390 I llm_load_print_meta: general.name     = 1.4B
0.00.051.390 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.391 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.391 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.391 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.391 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.391 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.392 I llm_load_print_meta: max token length = 1024
0.00.053.522 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.522 I llm_load_tensors: offloading output layer to GPU
0.00.053.523 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.533 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.053.535 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.053.955 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.956 I llama_new_context_with_model: n_ctx         = 128
0.00.053.956 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.956 I llama_new_context_with_model: n_batch       = 128
0.00.053.956 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.956 I llama_new_context_with_model: flash_attn    = 0
0.00.053.957 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.957 I llama_new_context_with_model: freq_scale    = 1
0.00.053.957 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.958 I ggml_metal_init: allocating
0.00.053.961 I ggml_metal_init: found device: Apple M4
0.00.053.962 I ggml_metal_init: picking default device: Apple M4
0.00.054.540 I ggml_metal_init: using embedded metal library
0.00.056.970 I ggml_metal_init: GPU name:   Apple M4
0.00.056.971 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.972 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.972 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.972 I ggml_metal_init: simdgroup reduction   = true
0.00.056.972 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.973 I ggml_metal_init: has bfloat            = true
0.00.056.973 I ggml_metal_init: use bfloat            = true
0.00.056.975 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.975 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.772 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.068.057 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.061 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.075 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.069.059 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.069.060 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.069.060 I llama_new_context_with_model: graph nodes  = 967
0.00.069.061 I llama_new_context_with_model: graph splits = 2
0.00.069.062 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.069.062 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.644.566 I 
0.00.644.589 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.644.600 I perplexity: tokenizing the input ..
0.00.652.943 I perplexity: tokenization took 8.343 ms
0.00.652.951 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.792.902 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.794.100 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.794.130 I llama_perf_context_print:        load time =     635.57 ms
0.00.794.132 I llama_perf_context_print: prompt eval time =     139.72 ms /   128 tokens (    1.09 ms per token,   916.09 tokens per second)
0.00.794.133 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.794.133 I llama_perf_context_print:       total time =     149.57 ms /   129 tokens
0.00.794.471 I ggml_metal_free: deallocating

real	0m0.808s
user	0m0.080s
sys	0m0.114s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.254 I build: 4454 (8eceb888) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.023.070 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.037.728 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.037.733 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.037.735 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.037.736 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.037.736 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.037.736 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.037.737 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.037.741 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.037.741 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.037.741 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.037.743 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.037.744 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.037.744 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.037.750 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.037.753 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.037.754 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.037.754 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.045.028 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.047.009 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.053.258 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.053.260 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.053.261 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.053.261 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.053.262 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.053.262 I llama_model_loader: - type  f32:  194 tensors
0.00.053.263 I llama_model_loader: - type  f16:   98 tensors
0.00.080.665 I llm_load_vocab: special tokens cache size = 25
0.00.087.205 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.087.208 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.087.208 I llm_load_print_meta: arch             = gptneox
0.00.087.209 I llm_load_print_meta: vocab type       = BPE
0.00.087.209 I llm_load_print_meta: n_vocab          = 50304
0.00.087.209 I llm_load_print_meta: n_merges         = 50009
0.00.087.209 I llm_load_print_meta: vocab_only       = 0
0.00.087.209 I llm_load_print_meta: n_ctx_train      = 2048
0.00.087.209 I llm_load_print_meta: n_embd           = 2048
0.00.087.209 I llm_load_print_meta: n_layer          = 24
0.00.087.212 I llm_load_print_meta: n_head           = 16
0.00.087.215 I llm_load_print_meta: n_head_kv        = 16
0.00.087.215 I llm_load_print_meta: n_rot            = 32
0.00.087.215 I llm_load_print_meta: n_swa            = 0
0.00.087.215 I llm_load_print_meta: n_embd_head_k    = 128
0.00.087.215 I llm_load_print_meta: n_embd_head_v    = 128
0.00.087.216 I llm_load_print_meta: n_gqa            = 1
0.00.087.216 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.087.217 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.087.217 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.087.218 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.087.218 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.087.218 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.087.218 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.087.219 I llm_load_print_meta: n_ff             = 8192
0.00.087.219 I llm_load_print_meta: n_expert         = 0
0.00.087.219 I llm_load_print_meta: n_expert_used    = 0
0.00.087.220 I llm_load_print_meta: causal attn      = 1
0.00.087.220 I llm_load_print_meta: pooling type     = 0
0.00.087.220 I llm_load_print_meta: rope type        = 2
0.00.087.220 I llm_load_print_meta: rope scaling     = linear
0.00.087.220 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.087.221 I llm_load_print_meta: freq_scale_train = 1
0.00.087.221 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.087.221 I llm_load_print_meta: rope_finetuned   = unknown
0.00.087.221 I llm_load_print_meta: ssm_d_conv       = 0
0.00.087.221 I llm_load_print_meta: ssm_d_inner      = 0
0.00.087.223 I llm_load_print_meta: ssm_d_state      = 0
0.00.087.223 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.087.223 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.087.223 I llm_load_print_meta: model type       = 1.4B
0.00.087.224 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.087.227 I llm_load_print_meta: model params     = 1.41 B
0.00.087.228 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.087.228 I llm_load_print_meta: general.name     = 1.4B
0.00.087.228 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.087.228 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.087.228 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.087.229 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.087.229 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.087.229 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.087.229 I llm_load_print_meta: max token length = 1024
0.00.089.634 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.089.635 I llm_load_tensors: offloading output layer to GPU
0.00.089.635 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.089.645 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.089.647 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.089.983 I llama_new_context_with_model: n_seq_max     = 1
0.00.089.984 I llama_new_context_with_model: n_ctx         = 128
0.00.089.984 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.089.984 I llama_new_context_with_model: n_batch       = 128
0.00.089.984 I llama_new_context_with_model: n_ubatch      = 128
0.00.089.985 I llama_new_context_with_model: flash_attn    = 0
0.00.089.985 I llama_new_context_with_model: freq_base     = 10000.0
0.00.089.985 I llama_new_context_with_model: freq_scale    = 1
0.00.089.986 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.089.986 I ggml_metal_init: allocating
0.00.089.989 I ggml_metal_init: found device: Apple M4
0.00.089.991 I ggml_metal_init: picking default device: Apple M4
0.00.090.586 I ggml_metal_init: using embedded metal library
0.00.093.126 I ggml_metal_init: GPU name:   Apple M4
0.00.093.128 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.093.128 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.093.128 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.093.129 I ggml_metal_init: simdgroup reduction   = true
0.00.093.129 I ggml_metal_init: simdgroup matrix mul. = true
0.00.093.129 I ggml_metal_init: has bfloat            = true
0.00.093.129 I ggml_metal_init: use bfloat            = true
0.00.093.130 I ggml_metal_init: hasUnifiedMemory      = true
0.00.093.130 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.102.196 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.103.426 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.103.430 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.103.445 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.104.351 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.104.352 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.104.352 I llama_new_context_with_model: graph nodes  = 967
0.00.104.352 I llama_new_context_with_model: graph splits = 2
0.00.104.353 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.104.354 I 
0.00.104.379 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.104.381 I compute_imatrix: tokenizing the input ..
0.00.111.035 I compute_imatrix: tokenization took 6.654 ms
0.00.111.036 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.570.201 I compute_imatrix: 1.46 seconds per pass - ETA 0.02 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.573.343 I llama_perf_context_print:        load time =    1547.13 ms
0.01.573.344 I llama_perf_context_print: prompt eval time =    1458.54 ms /   128 tokens (   11.39 ms per token,    87.76 tokens per second)
0.01.573.345 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.573.345 I llama_perf_context_print:       total time =    1550.26 ms /   129 tokens
0.01.574.365 I ggml_metal_free: deallocating

real	0m1.761s
user	0m0.169s
sys	0m0.246s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4454 (8eceb888)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 'Ä'
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x128e07590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x128e07ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x128e08250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x128e08800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x128e08db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x128e09360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x128e09910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x128e09ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x128e0a470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x128e0a970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x128e0ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x128e0b370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x128e0be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x128e0c640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x128e0ce50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x128e0d570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x128e0dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x128e0e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x128e0ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x128e0f2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x128e0f9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x128e100e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x128e10800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x128e110a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x128e117c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x128e11a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x128e12090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x128e12d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x128e13240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x128e13500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x128e139a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x128e13c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x128e144f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x128e14a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x128e14cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x128e15190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x128e15630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x128e15ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x128e15f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x128e16410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x128e168b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x128e16d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x128e171f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x128e17690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x128e17950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x128e17f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x128e18570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x128e18e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x128e194a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x128e19ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x128e1a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x128e1a6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x128e1ace0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x128e1b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x128e1bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x128e1bf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x128e1c420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x128e1c6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x128e1ccf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x128e1d4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x128e1d7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x128e1dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x128e1e0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x128e1e580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x128e1ea20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x128e1eec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x128e1f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x128e1f800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x128e1fca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x128e20140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x128e205e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x128e20a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x128e20f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x128e21470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x128e219c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x128e21f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x128e22460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x128e229b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x128e22f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x128e23450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x128e239a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x128e23ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x128e24440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x128e24990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x128e24ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x128e25430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x128e25980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x128e25ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x128e26420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x128e26970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x128e26ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x128e27410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x128e27960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x128e27eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x128e28400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x128e28950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x128e28ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x128e18b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x128e29310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x128e29ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x128e2a010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x128e2a560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x128e2aab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x128e2b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x128e2b550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x128e2baa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x128e2bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x128e2c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x128e2ca90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x128e2cfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x128e2d530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x128e2da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x128e2dfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x128e2e470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x128e2e910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x128e2edb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x128e2f250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x128e2f6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x128e2fb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x128e30030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x128e304d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x128e30970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x128e30e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x128e312b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x128e31750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x128e31bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x128e32090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x128e32530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x128e329d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x128e32e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x128e33310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x128e337b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x128e33c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x128e340f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x128e34590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x128e34a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x128e34ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x128e35370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x128e35810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x128e35cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x128e36150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x128e365f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x128e36a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x128e36f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x128e373d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x128e37870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x128e37d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x128e381b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x128e38650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x128e38af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x128e38f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x128e39430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x128e398d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x128e39d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x128e3a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x128e3a6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x128e3ab50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x128e3aff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x128e3b490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x128e3b930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x128e3bdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x128e3c270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x128e3c710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x128e3cbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x128e3d050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x128e3d4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x128e3d990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x128e3de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x128e3e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x128e3e770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x128e3ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x128e3f0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x128e3f550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x128e3f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x128e3fe90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x128e40330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x128e407d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x128e40c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x128e41110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x128e415b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x128e41a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x128e41ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x128e42390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x128e42830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x128e42cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x128e43170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x128e43610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x128e43ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x128e43f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x128e443f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x128e44890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x128e44d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x128e451d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x128e45720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x128e45c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x128e461c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x128e46710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x128e469d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x128e46fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x128e475f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x128e47c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x128e483f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x128e48890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x128e48b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x128e49160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x128e49770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x128e49f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x128e4a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x128e4a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x128e4ad40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x128e4b4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x128e4ba40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x128e4bf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x128e4c4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x128e4ca30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x128e4cf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x128e4d4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x128e4da20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x128e4df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x128e4e4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x128e4ea10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x128e4ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x128e4f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x128e4fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x128e4ff50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x128e504a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x128e509f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x128e50f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x128e51490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x128e519e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x128e51f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x128e52480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x128e529d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x128e52f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x128e53470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x128e539c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x128e53f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x128e54460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x128e549b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x128e54f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x128e55450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x128e559a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x128e55ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x128e56440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x128e56990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x128e56ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x128e57430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x128e57980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x128e57ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x128e58420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x128e58970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x128e58ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x128e59410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x128e59960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x128e59eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x128e5a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x128e5a950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x128e5aea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x128e5b3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x128e5b940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x128e5be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x128e5c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x128e5c930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x128e5ce80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x128e5d3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x128e5d920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x128e5de70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x128e5e310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x128e5e7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x128e5ec50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x128e5f0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x128e5f590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x128e5fa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x128e5fed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x128e60370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x128e60810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x128e60cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x128e61150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x128e615f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x128e61a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x128e61f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x128e623d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x128e62920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x128e63040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x128e63760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x128e63e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x128e645a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x128e64860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x128e65050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x128e65310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x128e65920 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
0.00.141.596 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.141.600 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x128e655d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x128e48e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x128e46c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x128e478b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x128e1a990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x128e1a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x128e1c9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x128e49420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x128e11d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x128e18830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x128e19150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x128e19760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x128e17c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x128e19d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x128e10d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x128e1cfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x128e295d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x128e64b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x128e13f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x128e141e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x128e49a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x128e47ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x128e12350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x128e12610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x128e128d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x128e65d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x128e66040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x128e66300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x128e665c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x128e66880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x128e66b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x128e66e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x128e670c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x128e67380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x128e67640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x128e67900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x128e67bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x128e67e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x128e68140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x128e68400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x128e686c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x128e68980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x128e68c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x128e68f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x128e691c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x128e69480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x128e69740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x128e69a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x128e69cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x128e69f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x128e6a240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x128e6a500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x128e6a7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x128e6aa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x128e6ad40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x128e6b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x128e6b2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x128e6b580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x128e6b840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x128e6bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x128e6bdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x128e6c080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x128e6c340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x128e6c600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x128e6c8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x128e6cb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x128e6ce40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x128e6d100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x128e6d3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x128e6d680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x128e6d940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x128e6dc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x128e6dec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x128e6e180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x128e6e440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x128e6e700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x128e6e9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x128e6ec80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x128e6ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x128e6f200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x128e6f4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x128e6f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x128e6fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x128e6fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x128e6ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x128e70280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x128e70540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x128e70800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x128e70ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x128e70d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x128e71040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x128e71300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x128e715c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x128e71880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x128e71b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x128e71e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x128e720c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x128e72380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x128e72640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x128e72900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x128e72bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x128e72e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x128e73140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x128e73400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x128e736c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x128e73980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x128e73c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x128e73f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x128e741c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x128e74480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x128e74740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x128e74a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x128e74cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x128e74f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x128e75240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x128e75500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x128e757c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x128e75a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x128e75d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x128e76000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x128e762c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x128e76580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x128e76840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x128e76b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x128e76dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x128e77080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x128e77340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x128e77600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x128e778c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x128e77b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x128e77e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x128e78100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x128e783c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x128e78680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x128e78940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x128e78c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x128e78ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x128e79180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x128e79440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x128e79700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x128e799c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x128e79c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x128e79f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x128e7a200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x128e7a4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x128e7a780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x128e7aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x128e7ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x128e7afc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x128e7b280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x128e7b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x128e7b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x128e7bac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x128e7bd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x128e7c040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x128e7c300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x128e7c5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x128e7c880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x128e7cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x128e7ce00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x128e7d0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x128e7d380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x128e7d640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x128e7d900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x128e7dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x128e7de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x128e7e140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x128e7e400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x128e7e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x128e7e980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x128e7ec40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x128e7ef00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x128e7f1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x128e7f480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x128e7f740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x128e7fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x128e7fcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x128e7ff80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x128e80240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x128e80500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x128e807c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x128e80a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x128e80d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x128e81000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x128e812c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x128e81580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x128e81840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x128e81b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x128e81dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x128e82080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x128e82340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x128e82600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x128e828c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x128e82b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x128e82e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x128e83100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x128e833c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x128e83680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x128e83940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x128e83c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x128e83ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x128e84180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x128e84440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x128e84700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x128e849c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x128e84c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x128e84f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x128e85200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x128e854c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x128e85960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x128e86110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x128e863d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x128e86690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x128e86b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x128e86f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x128e873e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x128e87850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x128e87cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x128e88130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x128e885a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x128e88a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x128e88e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x128e892f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x128e89760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x128e89bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x128e8a040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x128e8a4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x128e8a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x128e8ad90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x128e8b200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x128e8b670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x128e8bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x128e8bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x128e8c3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x128e8c830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x128e8cca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x128e8d110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x128e8d580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x128e8d9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x128e8de60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x128e8e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x128e8e740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x128e8ebb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x128e8f020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x128e8f490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x128e8f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x128e8fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x128e901e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x128e90650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x128e90ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x128e90f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x128e913a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x128e91810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x128e91c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x128e920f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x128e92560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x128e929d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x128e92e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x128e932b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x128e93720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x128e93b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x128e94000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x128e94470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x128e948e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x128e94d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x128e951c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x128e95630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x128e95aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x128e95f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x128e96380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x128e967f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x128e96c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x128e970d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x128e97540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x128e979b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x128e97e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x128e98290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x128e98700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x128e98b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x128e98fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x128e99450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x128e998c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x128e99d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x128e9a7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x128e9aec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x128e9b5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x128e9bd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x128e9bfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x128e9c7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x128e9ca70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x128e9d080 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x128e99ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x128e9cd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x128e9c280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x128e9d4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x128e9d7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x128e9da60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x128e9dd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x128e9dfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x128e9e2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x128e9e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x128e9e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x128e9eae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x128e9f0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x128e9f680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x128e9fcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x128e9ff70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x128ea0230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x128ea04f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x128ea07b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x128ea0a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x128ea0d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x128ea0ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x128ea12b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x128ea1570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x128ea1830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x128ea1af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x128ea1db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x128ea2070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x128ea2330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x128ea25f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x128ea28b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x128ea2b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x128ea2e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x128ea30f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x128ea33b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x128ea3670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x128ea3930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x128ea3bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x128ea3eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x128ea4170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x128ea4430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x128ea46f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x128ea49b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x128ea4c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x128ea4f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x128ea51f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x128ea54b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x128ea5770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x128ea5a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x128ea5cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x128ea5fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x128ea6270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x128ea6530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x128ea67f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x128ea6ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x128ea6d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x128ea7030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x128ea72f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x128ea75b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x128ea7870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x128ea7b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x128ea7df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x128ea80b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x128ea8370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x128ea8630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x128ea88f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x128ea8bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x128ea8e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x128ea9130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x128ea93f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x128ea96b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x128ea9970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x128ea9c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x128ea9ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x128eaa1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x128eaa470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x128eaa730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x128eaa9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x128eaacb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x128eaaf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x128eab230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x128eab4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x128eab7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x128eaba70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x128eabd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x128eabff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x128eac2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x128eac570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x128eac830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x128eacaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x128eacdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x128ead070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x128ead330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x128ead5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x128ead8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x128eadb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x128eade30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x128eae0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x128eae3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x128eae670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x128eae930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x128eaebf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x128eaeeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x128eaf170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x128eaf430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x128eaf6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x128eaf9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x128eafc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x128eaff30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x128eb01f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x128eb04b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x128eb0770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x128eb0a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x128eb0cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x128eb0fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x128eb1270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x128eb1530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x128eb17f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x128eb1ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x128eb1d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x128eb2030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x128eb22f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x128eb25b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x128eb2870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x128eb2b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x128eb2df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x128eb30b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x128eb3370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x128eb3630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x128eb38f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x128eb3bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x128eb3e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x128eb4130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x128eb43f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x128eb46b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x128eb4970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x128eb4c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x128eb4ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x128eb51b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x128eb5470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x128eb5730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x128eb59f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x128eb5cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x128eb5f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x128eb6230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x128eb64f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x128eb67b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x128eb6a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x128eb6d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x128eb6ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x128eb72b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x128eb7570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x128eb7830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x128eb7af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x128eb7db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x128eb8070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x128eb8330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x128eb85f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x128eb88b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x128eb8b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x128eb8e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x128eb90f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x128eb93b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x128eb9670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x128eb9930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x128eb9bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x128eb9eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x128eba170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x128eba430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x128eba6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x128eba9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x128ebac70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x128ebaf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x128ebb1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x128ebb4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x128ebb770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x128ebba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x128ebbcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x128ebbfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x128ebc270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x128ebc530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x128ebc7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x128ebcab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x128ebcd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x128ebd030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x128ebd2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x128ebd5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x128ebd870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x128ebdb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x128ebddf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x128ebe0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x128ebe370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x128ebe630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x128ebe8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x128ebebb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x128ebee70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x128ebf130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x128ebf3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x128ebf6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x128ebf970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x128ebfc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x128ebfef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x128ec01b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x128ec0470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x128ec0730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x128ec09f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x128ec0cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x128ec0f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x128ec1230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x128ec14f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x128ec1ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x128ec1d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x128ec2040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x128ec2300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x128ec25c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x128ec2880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x128ec2b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x128ec2e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x128ec30c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x128ec3380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x128ec3640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x128ec3900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x128ec3bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x128ec3e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x128ec4140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x128ec4400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x128ec46c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x128ec4980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x128ec4c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x128ec4f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x128ec51c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x128ec5480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x128ec5740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x128ec5a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x128ec5cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x128ec5f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x128ec6240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x128ec6500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x128ec67c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x128ec6a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x128ec6d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x128ec7000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x128ec72c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x128ec7580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x128ec7840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x128ec7b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x128ec7dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x128ec8080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x128ec8340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x128ec8600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x128ec88c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x128ec8b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x128ec8e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x128ec9100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x128ec93c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x128ec9680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x128ec9940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x128ec9c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x128ec9ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x128eca180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x128eca440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x128eca700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x128eca9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x128ecac80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x128ecaf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x128ecb200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x128ecb4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x128ecb780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x128ecba40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x128ecbd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x128ecbfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x128ecc280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x128ecc680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x128ecc940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x128eccc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x128ecd070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x128ecd4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x128ecd950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x128ecddc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x128ece230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x128ece6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x128eceb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x128ecef80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x128ecfaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x128ed0210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x128ed0930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x128ed1050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x128ed1310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x128ed15d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x128ed1b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x128ed1f70 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.770s
user	0m0.294s
sys	0m0.318s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4454 (8eceb888)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 'Ä'
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13d0098f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13d00a000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13d00a5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13d00ab60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13d00b110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13d00b6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13d00bc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13d00c220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13d00c7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13d00ccd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13d00d1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13d00d6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13d00e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13d00e9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13d00f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13d00f8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13d00fff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13d010710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13d010e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13d011600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13d011d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13d012440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13d012b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13d013400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13d013b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13d013de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13d0143f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13d015060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13d0155a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13d015860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13d015d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13d015fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13d016850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13d016d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13d017050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13d0174f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13d017990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13d017e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13d0182d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13d018770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13d018c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13d0190b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13d019550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13d0199f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13d019cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13d01a2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13d01a8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13d01b1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13d01b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13d01be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13d01c420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13d01ca30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13d01d040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13d01d650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13d01de40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13d01e2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13d01e780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13d01ea40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13d01f050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13d01f840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13d01fb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13d01ffa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13d020440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13d0208e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13d020d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13d021220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13d0216c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13d021b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13d022000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13d0224a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13d022940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13d022de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13d023280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13d0237d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13d023d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13d024270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13d0247c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13d024d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13d025260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13d0257b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13d025d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13d026250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13d0267a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13d026cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13d027240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13d027790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13d027ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13d028230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13d028780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13d028cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13d029220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13d029770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13d029cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13d02a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13d02a760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13d02acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13d02b200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13d01aee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13d02b670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13d02be20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13d02c370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13d02c8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13d02ce10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13d02d360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13d02d8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13d02de00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13d02e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13d02e8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13d02edf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13d02f340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13d02f890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13d02fde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13d030330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13d0307d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13d030c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13d031110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13d0315b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13d031a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13d031ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13d032390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13d032830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13d032cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13d033170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13d033610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13d033ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13d033f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13d0343f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13d034890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13d034d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13d0351d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13d035670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13d035b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13d035fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13d036450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13d0368f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13d036d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13d037230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13d0376d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13d037b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13d038010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13d0384b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13d038950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13d038df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13d039290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13d039730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13d039bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13d03a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13d03a510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13d03a9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13d03ae50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13d03b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13d03b790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13d03bc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13d03c0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13d03c570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13d03ca10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13d03ceb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13d03d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13d03d7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13d03dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13d03e130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13d03e5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13d03ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13d03ef10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13d03f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13d03f850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13d03fcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13d040190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13d040630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13d040ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13d040f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13d041410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13d0418b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13d041d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13d0421f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13d042690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13d042b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13d042fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13d043470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13d043910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13d043db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13d044250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13d0446f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13d044b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13d045030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13d0454d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13d045970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13d045e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13d0462b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13d046750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13d046bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13d047090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13d047530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13d047a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13d047fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13d048520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13d048a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13d048d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13d049340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13d049950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13d049f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13d04a750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13d04abf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13d04aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13d04b4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13d04bad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13d04c2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13d04c760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13d04cc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13d04d0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13d04d850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13d04dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13d04e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13d04e840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13d04ed90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13d04f2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13d04f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13d04fd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13d0502d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13d050820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13d050d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13d0512c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13d051810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13d051d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13d0522b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13d052800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13d052d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13d0532a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13d0537f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13d053d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13d054290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13d0547e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13d054d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13d055280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13d0557d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13d055d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13d056270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13d0567c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13d056d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13d057260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13d0577b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13d057d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13d058250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13d0587a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13d058cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13d059240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13d059790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13d059ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13d05a230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13d05a780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13d05acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13d05b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13d05b770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13d05bcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13d05c210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13d05c760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13d05ccb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13d05d200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13d05d750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13d05dca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13d05e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13d05e740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13d05ec90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13d05f1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13d05f730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13d05fc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13d0601d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13be06be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13be06ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13be07310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13be07780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13be07bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13be08060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13be084d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13be08940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13be08db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13be09220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13be09690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13be09b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13be09f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13be0a3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13be0a850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13be0acc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13be0b130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13be0be20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13be0c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13be0cc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13be0cf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13be0d710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13be0d9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13be0dfe0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
0.00.108.695 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.108.698 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13be10780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13be10bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13be111b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13be11760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13be11d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13be122c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13be12870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13be12e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13be133d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13be138d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13be13dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13be142d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13be14df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13be155a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13be15db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13be164d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13be16bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13be17310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13be17a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13be18200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13be18920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13be19040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13be19760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13be19e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13be1a5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13be1a860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13be1ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13be1b480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13be1ba90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13be1c280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13be1c720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13be1c9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13be1d270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13be1d7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13be1da70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13be1df10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13be1e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13be1e850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13be1ecf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13be1f190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13be1f630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13be1fad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13be1ff70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13be20410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13be206d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13be20ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13be212f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13be21900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13be21f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13be22520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13be22b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13be23140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13be23750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13be23d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13be24550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13be249f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13be24e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13be25150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13be25760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13be25f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13be263f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13be26890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13be26d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13be271d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13be27670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13be27b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13be27fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13be28450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13be288f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13be28d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13be29230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13be296d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13be29b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13be2a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13be2a610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13be2ab60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13be2b0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13be2b600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13be2bb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13be2c0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13be2c5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13be2cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13be2d090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13be2d5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13be2db30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13be2e080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13be2e5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13be2eb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13be2f070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13be2f5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13be2fb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13be30060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13be305b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13be30b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13be31050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13be315a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13be31af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13be32040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13be32590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13be32ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13be33030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13be33580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13be33ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13be34020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13be34570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13be34ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13be35010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13be35560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13be35ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13be36000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13be36550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13be36aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13be36ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13be37490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13be37930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13be37dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13be38270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13be38710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13be38bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13be39050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13be394f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13be39990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13be39e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13be3a2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13be3a770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13be3ac10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13be3b0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13be3b550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13be3b9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13be3be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13be3c330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13be3c7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13be3cc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13be3d110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13be3d5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13be3da50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13be3def0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13be3e390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13be3e830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13be3ecd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13be3f170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13be3f610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13be3fab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13be3ff50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13be403f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13be40890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13be40d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13be411d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13be41670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13be41b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13be41fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13be42450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13be428f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13be42d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13be43230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13be436d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13be43b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13be44010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13be444b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13be44950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13be44df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13be45290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13be45730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13be45bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13be46070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13be46510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13be469b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13be46e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13be472f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13be47790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13be47c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13be480d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13be48570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13be48a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13be48eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13be49350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13be497f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13be49c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13be4a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13be4a5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13be4aa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13be4af10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13be4b3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13be4b850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13be4bcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13be4c190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13be4c630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13be4cad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13be4cf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13be4d410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13be4d8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13be4dd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13be4e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13be4e740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13be4ec90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13be4f1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13be4f730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13be4f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13be50000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13be50610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13be50c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13be51410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13be518b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13be51b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13be52180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13be52790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13be52f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13be53420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13be538c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13be53d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13be54510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13be54a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13be54fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13be55500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13be55a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13be55fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13be564f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13be56a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13be56f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13be574e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13be57a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13be57f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13be584d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13be58a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13be58f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13be594c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13be59a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13be59f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13be5a4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13be5aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13be5af50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13be5b4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13be5b9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13be5bf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13be5c490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13be5c9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13be5cf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13be5d480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13be5d9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13be5df20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13be5e470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13be5e9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13be5ef10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13be5f460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13be5f9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13be5ff00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13be60450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13be609a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13be60ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13be61440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13be61990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13be61ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13be62430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13be62980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13be62ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13be63420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13be63970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13be63ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13be64410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13be64960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13be64eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13be65400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13be65950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13be65ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13be663f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13be66940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13be66e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13be67330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13be677d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13be67c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13be68110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13be685b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13be68a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13be68ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13be69390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13be69830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13be69cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13be6a170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13be6a610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13be6aab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13be6af50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13be6b3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13be6b940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13be6c060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13be6c780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13be6cea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13be6d5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13be6d880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13be6e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13be6e330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13be6e940 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13d104fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13d105420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13d105890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13d105d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13d106170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13d1065e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13d106a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13d106ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13d107330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13d1077a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13d107c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13d108270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13d108d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13d109540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13d109d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13d10a470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13d10ab90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13d10b2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13d10b9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13d10c1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13d10c8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13d10cfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13d10d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13d10de20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13d10e540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13d10e800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13d10eac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13d10ef30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13d10f3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13d10f810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13d10fc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13d1101b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13d110620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13d1108e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13d110d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13d1111c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13d111630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13d111aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13d111f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13d112380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13d1127f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13d112c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13d1130d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13d113540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13d1139b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13d113e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13d114290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13d114700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13d114b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13d114fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13d115450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13d1158c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13d115d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13d1161a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13d116610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13d116a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13d116ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13d1174f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13d117960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13d117dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13d118240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13d1186b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13d118b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13d118f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13d119400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13d119870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13d119ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13d11a150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13d11a5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13d11aa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13d11aea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13d11b310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13d11b780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13d11bbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13d11c060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13d11c4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13d11c940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13d11cdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13d11d220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13d11d690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13d11db00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13d11df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13d11e3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13d11e850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13d11ecc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13d11f130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13d11f5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13d11fa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13d11fe80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13d1202f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13d120760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13d120bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13d121040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13d1214b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13d121920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13d121d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13d122200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13d122670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13d122ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13d122f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13d1233c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13d123830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13d123ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13d124530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13d1247f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13d124c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13d1250d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13d125540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13d1259b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13d125e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13d126290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13d126700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13d126b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13d126fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13d127450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13d1278c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13d127d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13d1281a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13d128610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13d128a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13d128ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13d129360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13d1297d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13d129c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13d12a0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13d12a520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13d12a990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13d12ae00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13d12b270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13d12b6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13d12bb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13d12bfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13d12c430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13d12c8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13d12cd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13d12d180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13d12d5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13d12da60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13d12ded0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13d12e340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13d12e7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13d12ec20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13d12f090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13d12f500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13d12f970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13d12fde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13d130250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13d1306c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13d130b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13d130fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13d131410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13d131880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13d131cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13d132160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13d1325d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13d132a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13d132eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13d133320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13d133790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13d133c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13d134070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13d1344e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13d134950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13d134dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13d135230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13d1356a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13d135b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13d135f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13d1363f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13d136860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13d136cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13d137140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13d1375b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13d137a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13d137e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13d138300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13d138770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13d138be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13d139050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13d1394c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13d139930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13d139da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13d13a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13d13a680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13d13aaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13d13af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13d13b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13d13b840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13d13bcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13d13c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13d13c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13d13ca00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13d13ce70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13d13d2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13d13d750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13d13dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13d13e030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13d13e4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13d13e910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13d13ed80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13d13f1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13d13f660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13d13fad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13d13ff40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13d1403b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13d140820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13d140c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13d141100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13d141570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13d1419e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13d142560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13d142820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13d142ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13d142f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13d1433c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13d143830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13d143ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13d144110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13d144580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13d1449f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13d144e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13d1452d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13d145740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13d145bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13d146020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13d146490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13d146900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13d146d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13d1471e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13d147650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13d147ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13d147f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13d1483a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13d148810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13d148c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13d1490f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13d149560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13d1499d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13d149e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13d14a2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13d14a720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13d14ab90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13d14b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13d14b470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13d14b8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13d14bd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13d14c1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13d14c630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13d14caa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13d14cf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13d14d380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13d14d7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13d14dc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13d14e0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13d14e540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13d14e9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13d14ee20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13d14f290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13d14f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13d14fb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13d14ffe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13d150450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13d1508c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13d150d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13d1511a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13d151610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13d151a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13d151ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13d152360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13d1527d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13d152c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13d1530b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13d153520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13d153990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13d153e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13d154270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13d1546e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13d154b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13d154fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13d155430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13d1558a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13d155d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13d156180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13d156bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13d157310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13d157a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13d158150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13d158410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13d158880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13d158e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13d159490 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.936s
user	0m0.257s
sys	0m0.136s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
