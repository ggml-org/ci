+ cmake -DCMAKE_BUILD_TYPE=Debug -DLLAMA_FATAL_WARNINGS=ON ..
-- The C compiler identification is GNU 11.4.0
-- The CXX compiler identification is GNU 11.4.0
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.34.1") 
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE  
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: x86_64
-- Including CPU backend
-- Found OpenMP_C: -fopenmp (found version "4.5") 
-- Found OpenMP_CXX: -fopenmp (found version "4.5") 
-- Found OpenMP: TRUE (found version "4.5")  
-- x86 detected
-- Adding CPU backend variant ggml-cpu: -march=native 
-- Configuring done (0.6s)
-- Generating done (0.1s)
-- Build files have been written to: /home/ggml/work/llama.cpp/build-ci-debug

real	0m0.789s
user	0m0.576s
sys	0m0.215s
++ nproc
+ make -j4
[  0%] Generating build details from Git
[  1%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  2%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  2%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
-- Found Git: /usr/bin/git (found version "2.34.1") 
[  2%] Built target sha256
[  2%] Built target xxhash
[  3%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  3%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  3%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  4%] Built target sha1
[  5%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o
[  5%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  6%] Linking CXX shared library ../../bin/libggml-base.so
[  6%] Built target build_info
[  6%] Built target ggml-base
[  6%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[  6%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[  8%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 10%] Linking CXX shared library ../../bin/libggml-cpu.so
[ 10%] Built target ggml-cpu
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 11%] Linking CXX shared library ../../bin/libggml.so
[ 11%] Built target ggml
[ 12%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o
[ 12%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 12%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 12%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 13%] Linking CXX executable ../../bin/llama-gguf
[ 14%] Linking CXX executable ../../bin/llama-gguf-hash
[ 14%] Built target llama-gguf-hash
[ 14%] Built target llama-gguf
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-graph.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-io.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o
/home/ggml/work/llama.cpp/src/llama-graph.cpp: In function ‘int32_t llama_relative_position_bucket(llama_pos, llama_pos, uint64_t, bool)’:
/home/ggml/work/llama.cpp/src/llama-graph.cpp:31:61: error: ‘logf’ was not declared in this scope
   31 |     int32_t relative_position_if_large = floorf(max_exact + logf(1.0 * relative_position / max_exact) * (n_buckets - max_exact) / log(1.0 * max_distance / max_exact));
      |                                                             ^~~~
/home/ggml/work/llama.cpp/src/llama-graph.cpp:31:131: error: ‘log’ was not declared in this scope; did you mean ‘long’?
   31 |     int32_t relative_position_if_large = floorf(max_exact + logf(1.0 * relative_position / max_exact) * (n_buckets - max_exact) / log(1.0 * max_distance / max_exact));
      |                                                                                                                                   ^~~
      |                                                                                                                                   long
/home/ggml/work/llama.cpp/src/llama-graph.cpp:31:42: error: ‘floorf’ was not declared in this scope
   31 |     int32_t relative_position_if_large = floorf(max_exact + logf(1.0 * relative_position / max_exact) * (n_buckets - max_exact) / log(1.0 * max_distance / max_exact));
      |                                          ^~~~~~
/home/ggml/work/llama.cpp/src/llama-graph.cpp: In member function ‘virtual void llm_graph_input_mean::set_input(const llama_ubatch*)’:
/home/ggml/work/llama.cpp/src/llama-graph.cpp:161:9: error: ‘memset’ was not declared in this scope
  161 |         memset(mean->data, 0, n_tokens * n_tokens * ggml_element_size(mean));
      |         ^~~~~~
/home/ggml/work/llama.cpp/src/llama-graph.cpp:10:1: note: ‘memset’ is defined in header ‘<cstring>’; did you forget to ‘#include <cstring>’?
    9 | #include <cassert>
  +++ |+#include <cstring>
   10 | 
/home/ggml/work/llama.cpp/src/llama-graph.cpp: In member function ‘virtual void llm_graph_input_cls::set_input(const llama_ubatch*)’:
/home/ggml/work/llama.cpp/src/llama-graph.cpp:204:9: error: ‘memset’ was not declared in this scope
  204 |         memset(cls->data, 0, n_tokens * ggml_element_size(cls));
      |         ^~~~~~
/home/ggml/work/llama.cpp/src/llama-graph.cpp:204:9: note: ‘memset’ is defined in header ‘<cstring>’; did you forget to ‘#include <cstring>’?
/home/ggml/work/llama.cpp/src/llama-graph.cpp:231:9: error: ‘memset’ was not declared in this scope
  231 |         memset(cls->data, 0, n_tokens * ggml_element_size(cls));
      |         ^~~~~~
/home/ggml/work/llama.cpp/src/llama-graph.cpp:231:9: note: ‘memset’ is defined in header ‘<cstring>’; did you forget to ‘#include <cstring>’?
/home/ggml/work/llama.cpp/src/llama-graph.cpp: In member function ‘virtual void llm_graph_input_attn_base::set_input(const llama_ubatch*)’:
/home/ggml/work/llama.cpp/src/llama-graph.cpp:351:44: error: ‘INFINITY’ was not declared in this scope
  351 |                                 float f = -INFINITY;
      |                                            ^~~~~~~~
/home/ggml/work/llama.cpp/src/llama-graph.cpp:390:44: error: ‘INFINITY’ was not declared in this scope
  390 |                                 float f = -INFINITY;
      |                                            ^~~~~~~~
/home/ggml/work/llama.cpp/src/llama-graph.cpp:408:78: error: ‘INFINITY’ was not declared in this scope
  408 |                             data[h*(n_tokens*n_tokens) + tj*n_stride + i] = -INFINITY;
      |                                                                              ^~~~~~~~
/home/ggml/work/llama.cpp/src/llama-graph.cpp: In member function ‘virtual void llm_graph_input_attn_kv_self::set_input(const llama_ubatch*)’:
/home/ggml/work/llama.cpp/src/llama-graph.cpp:452:38: error: ‘INFINITY’ was not declared in this scope
  452 |                                 f = -INFINITY;
      |                                      ^~~~~~~~
/home/ggml/work/llama.cpp/src/llama-graph.cpp:468:42: error: ‘INFINITY’ was not declared in this scope
  468 |                                     f = -INFINITY;
      |                                          ^~~~~~~~
/home/ggml/work/llama.cpp/src/llama-graph.cpp:479:69: error: ‘INFINITY’ was not declared in this scope
  479 |                             data[h*(n_kv*n_tokens) + i*n_kv + j] = -INFINITY;
      |                                                                     ^~~~~~~~
/home/ggml/work/llama.cpp/src/llama-graph.cpp:487:73: error: ‘INFINITY’ was not declared in this scope
  487 |                             data_swa[h*(n_kv*n_tokens) + i*n_kv + j] = -INFINITY;
      |                                                                         ^~~~~~~~
/home/ggml/work/llama.cpp/src/llama-graph.cpp:513:44: error: ‘INFINITY’ was not declared in this scope
  513 |                                 float f = -INFINITY;
      |                                            ^~~~~~~~
/home/ggml/work/llama.cpp/src/llama-graph.cpp:531:78: error: ‘INFINITY’ was not declared in this scope
  531 |                             data[h*(n_tokens*n_tokens) + tj*n_stride + i] = -INFINITY;
      |                                                                              ^~~~~~~~
/home/ggml/work/llama.cpp/src/llama-graph.cpp: In member function ‘virtual void llm_graph_input_attn_dec::set_input(const llama_ubatch*)’:
/home/ggml/work/llama.cpp/src/llama-graph.cpp:555:32: error: ‘INFINITY’ was not declared in this scope
  555 |                     float f = -INFINITY;
      |                                ^~~~~~~~
/home/ggml/work/llama.cpp/src/llama-graph.cpp:568:63: error: ‘INFINITY’ was not declared in this scope
  568 |                     data[h*(n_enc*n_tokens) + i*n_enc + j] = -INFINITY;
      |                                                               ^~~~~~~~
/home/ggml/work/llama.cpp/src/llama-graph.cpp: In member function ‘void llm_graph_context::cb(ggml_tensor*, const char*, int) const’:
/home/ggml/work/llama.cpp/src/llama-graph.cpp:652:13: error: ‘strcmp’ was not declared in this scope
  652 |         if (strcmp(name, "kqv_merged_cont") == 0) {
      |             ^~~~~~
/home/ggml/work/llama.cpp/src/llama-graph.cpp:652:13: note: ‘strcmp’ is defined in header ‘<cstring>’; did you forget to ‘#include <cstring>’?
/home/ggml/work/llama.cpp/src/llama-graph.cpp:662:25: error: ‘strcmp’ was not declared in this scope
  662 |         if (il != -1 && strcmp(name, "norm") == 0) {
      |                         ^~~~~~
/home/ggml/work/llama.cpp/src/llama-graph.cpp:662:25: note: ‘strcmp’ is defined in header ‘<cstring>’; did you forget to ‘#include <cstring>’?
/home/ggml/work/llama.cpp/src/llama-graph.cpp: In member function ‘ggml_tensor* llm_graph_context::build_rwkv6_time_mix(ggml_cgraph*, ggml_tensor*, ggml_tensor*, ggml_tensor*, ggml_tensor*, const llama_ubatch&, int) const’:
/home/ggml/work/llama.cpp/src/llama-graph.cpp:1959:74: error: ‘pow’ was not declared in this scope
 1959 |         wkv_output = ggml_gated_linear_attn(ctx0, k, v, r, w, wkv_state, pow(head_size, -0.5f));
      |                                                                          ^~~
make[2]: *** [src/CMakeFiles/llama.dir/build.make:174: src/CMakeFiles/llama.dir/llama-graph.cpp.o] Error 1
make[2]: *** Waiting for unfinished jobs....
make[1]: *** [CMakeFiles/Makefile2:1771: src/CMakeFiles/llama.dir/all] Error 2
make: *** [Makefile:146: all] Error 2

real	0m4.008s
user	0m11.748s
sys	0m1.326s
