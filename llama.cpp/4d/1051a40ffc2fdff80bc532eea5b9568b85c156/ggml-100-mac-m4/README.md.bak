### ctest_debug

Runs ctest in debug mode
- status: 0
```
+ ctest --output-on-failure -L main -E test-opt
Test project /Users/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-tokenizer-0-bert-bge
 1/29 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.32 sec
      Start  2: test-tokenizer-0-command-r
 2/29 Test  #2: test-tokenizer-0-command-r ........   Passed    1.17 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/29 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.17 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/29 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.47 sec
      Start  5: test-tokenizer-0-falcon
 5/29 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.30 sec
      Start  6: test-tokenizer-0-gpt-2
 6/29 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.27 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/29 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.72 sec
      Start  8: test-tokenizer-0-llama-spm
 8/29 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.10 sec
      Start  9: test-tokenizer-0-mpt
 9/29 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.24 sec
      Start 10: test-tokenizer-0-phi-3
10/29 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.09 sec
      Start 11: test-tokenizer-0-qwen2
11/29 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.64 sec
      Start 12: test-tokenizer-0-refact
12/29 Test #12: test-tokenizer-0-refact ...........   Passed    0.23 sec
      Start 13: test-tokenizer-0-starcoder
13/29 Test #13: test-tokenizer-0-starcoder ........   Passed    0.23 sec
      Start 14: test-sampling
14/29 Test #14: test-sampling .....................   Passed    2.24 sec
      Start 15: test-grammar-parser
15/29 Test #15: test-grammar-parser ...............   Passed    0.18 sec
      Start 16: test-grammar-integration
16/29 Test #16: test-grammar-integration ..........   Passed    0.23 sec
      Start 17: test-llama-grammar
17/29 Test #17: test-llama-grammar ................   Passed    0.19 sec
      Start 18: test-chat
18/29 Test #18: test-chat .........................   Passed   18.70 sec
      Start 19: test-json-schema-to-grammar
19/29 Test #19: test-json-schema-to-grammar .......   Passed    2.30 sec
      Start 20: test-tokenizer-1-llama-spm
20/29 Test #20: test-tokenizer-1-llama-spm ........   Passed    1.09 sec
      Start 21: test-log
21/29 Test #21: test-log ..........................   Passed    0.23 sec
      Start 22: test-arg-parser
22/29 Test #22: test-arg-parser ...................   Passed    0.28 sec
      Start 23: test-chat-template
23/29 Test #23: test-chat-template ................   Passed    3.05 sec
      Start 24: test-gguf
24/29 Test #24: test-gguf .........................   Passed    0.99 sec
      Start 25: test-backend-ops
25/29 Test #25: test-backend-ops ..................   Passed  105.89 sec
      Start 28: test-barrier
26/29 Test #28: test-barrier ......................   Passed    0.85 sec
      Start 29: test-quantize-fns
27/29 Test #29: test-quantize-fns .................   Passed   26.35 sec
      Start 30: test-quantize-perf
28/29 Test #30: test-quantize-perf ................   Passed    0.33 sec
      Start 31: test-rope
29/29 Test #31: test-rope .........................   Passed    0.22 sec

100% tests passed, 0 tests failed out of 29

Label Time Summary:
main    = 169.07 sec*proc (29 tests)

Total Test time (real) = 169.08 sec

real	2m49.143s
user	4m41.255s
sys	0m5.659s
```

### ctest_release

Runs ctest in release mode
- status: 0
```
+ ctest --output-on-failure -L main
Test project /Users/ggml/work/llama.cpp/build-ci-release
      Start  1: test-tokenizer-0-bert-bge
 1/29 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.17 sec
      Start  2: test-tokenizer-0-command-r
 2/29 Test  #2: test-tokenizer-0-command-r ........   Passed    0.23 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/29 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.04 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/29 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.08 sec
      Start  5: test-tokenizer-0-falcon
 5/29 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.06 sec
      Start  6: test-tokenizer-0-gpt-2
 6/29 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.05 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/29 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.14 sec
      Start  8: test-tokenizer-0-llama-spm
 8/29 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.03 sec
      Start  9: test-tokenizer-0-mpt
 9/29 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.05 sec
      Start 10: test-tokenizer-0-phi-3
10/29 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.03 sec
      Start 11: test-tokenizer-0-qwen2
11/29 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.11 sec
      Start 12: test-tokenizer-0-refact
12/29 Test #12: test-tokenizer-0-refact ...........   Passed    0.05 sec
      Start 13: test-tokenizer-0-starcoder
13/29 Test #13: test-tokenizer-0-starcoder ........   Passed    0.05 sec
      Start 14: test-sampling
14/29 Test #14: test-sampling .....................   Passed    0.93 sec
      Start 15: test-grammar-parser
15/29 Test #15: test-grammar-parser ...............   Passed    0.18 sec
      Start 16: test-grammar-integration
16/29 Test #16: test-grammar-integration ..........   Passed    0.20 sec
      Start 17: test-llama-grammar
17/29 Test #17: test-llama-grammar ................   Passed    0.21 sec
      Start 18: test-chat
18/29 Test #18: test-chat .........................   Passed    1.89 sec
      Start 19: test-json-schema-to-grammar
19/29 Test #19: test-json-schema-to-grammar .......   Passed    2.25 sec
      Start 20: test-tokenizer-1-llama-spm
20/29 Test #20: test-tokenizer-1-llama-spm ........   Passed    0.32 sec
      Start 21: test-log
21/29 Test #21: test-log ..........................   Passed    0.18 sec
      Start 22: test-arg-parser
22/29 Test #22: test-arg-parser ...................   Passed    0.22 sec
      Start 23: test-chat-template
23/29 Test #23: test-chat-template ................   Passed    0.52 sec
      Start 24: test-gguf
24/29 Test #24: test-gguf .........................   Passed    0.52 sec
      Start 25: test-backend-ops
25/29 Test #25: test-backend-ops ..................   Passed   24.58 sec
      Start 28: test-barrier
26/29 Test #28: test-barrier ......................   Passed    0.93 sec
      Start 29: test-quantize-fns
27/29 Test #29: test-quantize-fns .................   Passed   14.13 sec
      Start 30: test-quantize-perf
28/29 Test #30: test-quantize-perf ................   Passed    0.22 sec
      Start 31: test-rope
29/29 Test #31: test-rope .........................   Passed    0.21 sec

100% tests passed, 0 tests failed out of 29

Label Time Summary:
main    =  49.57 sec*proc (29 tests)

Total Test time (real) =  49.58 sec

real	0m49.593s
user	0m54.936s
sys	0m5.210s
```
### embd_bge_small

BGE Small (BERT):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.203 I build: 4772 (4d1051a4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.029.667 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.036.355 I llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.036.364 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.036.367 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.036.368 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.036.369 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.036.370 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.036.371 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.036.374 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.036.375 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.036.376 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.036.376 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.036.377 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.036.381 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.036.382 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.036.383 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.036.384 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.036.384 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.036.385 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.036.386 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.041.452 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.042.904 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.042.906 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.042.907 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.042.907 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.042.908 I llama_model_loader: - kv  22:               tokenizer.ggml.mask_token_id u32              = 103
0.00.042.908 I llama_model_loader: - kv  23:               general.quantization_version u32              = 2
0.00.042.909 I llama_model_loader: - type  f32:  124 tensors
0.00.042.909 I llama_model_loader: - type  f16:   73 tensors
0.00.042.910 I print_info: file format = GGUF V3 (latest)
0.00.042.911 I print_info: file type   = F16
0.00.042.912 I print_info: file size   = 63.84 MiB (16.12 BPW) 
0.00.047.796 I load: special tokens cache size = 5
0.00.050.064 I load: token to piece cache size = 0.2032 MB
0.00.050.068 I print_info: arch             = bert
0.00.050.069 I print_info: vocab_only       = 0
0.00.050.069 I print_info: n_ctx_train      = 512
0.00.050.069 I print_info: n_embd           = 384
0.00.050.070 I print_info: n_layer          = 12
0.00.050.073 I print_info: n_head           = 12
0.00.050.074 I print_info: n_head_kv        = 12
0.00.050.075 I print_info: n_rot            = 32
0.00.050.075 I print_info: n_swa            = 0
0.00.050.075 I print_info: n_embd_head_k    = 32
0.00.050.075 I print_info: n_embd_head_v    = 32
0.00.050.076 I print_info: n_gqa            = 1
0.00.050.077 I print_info: n_embd_k_gqa     = 384
0.00.050.078 I print_info: n_embd_v_gqa     = 384
0.00.050.079 I print_info: f_norm_eps       = 1.0e-12
0.00.050.079 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.080 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.080 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.080 I print_info: f_logit_scale    = 0.0e+00
0.00.050.081 I print_info: n_ff             = 1536
0.00.050.081 I print_info: n_expert         = 0
0.00.050.082 I print_info: n_expert_used    = 0
0.00.050.082 I print_info: causal attn      = 0
0.00.050.082 I print_info: pooling type     = 2
0.00.050.085 I print_info: rope type        = 2
0.00.050.085 I print_info: rope scaling     = linear
0.00.050.086 I print_info: freq_base_train  = 10000.0
0.00.050.087 I print_info: freq_scale_train = 1
0.00.050.087 I print_info: n_ctx_orig_yarn  = 512
0.00.050.087 I print_info: rope_finetuned   = unknown
0.00.050.087 I print_info: ssm_d_conv       = 0
0.00.050.088 I print_info: ssm_d_inner      = 0
0.00.050.088 I print_info: ssm_d_state      = 0
0.00.050.088 I print_info: ssm_dt_rank      = 0
0.00.050.088 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.089 I print_info: model type       = 33M
0.00.050.089 I print_info: model params     = 33.21 M
0.00.050.090 I print_info: general.name     = Bge Small
0.00.050.090 I print_info: vocab type       = WPM
0.00.050.090 I print_info: n_vocab          = 30522
0.00.050.091 I print_info: n_merges         = 0
0.00.050.091 I print_info: BOS token        = 101 '[CLS]'
0.00.050.091 I print_info: UNK token        = 100 '[UNK]'
0.00.050.092 I print_info: SEP token        = 102 '[SEP]'
0.00.050.092 I print_info: PAD token        = 0 '[PAD]'
0.00.050.092 I print_info: MASK token       = 103 '[MASK]'
0.00.050.093 I print_info: LF token         = 0 '[PAD]'
0.00.050.093 I print_info: max token length = 21
0.00.050.094 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.053.686 I load_tensors: offloading 12 repeating layers to GPU
0.00.053.688 I load_tensors: offloading output layer to GPU
0.00.053.689 I load_tensors: offloaded 13/13 layers to GPU
0.00.053.716 I load_tensors: Metal_Mapped model buffer size =    40.73 MiB
0.00.053.718 I load_tensors:   CPU_Mapped model buffer size =    23.11 MiB
................................................
0.00.054.062 I llama_init_from_model: n_seq_max     = 1
0.00.054.063 I llama_init_from_model: n_ctx         = 512
0.00.054.064 I llama_init_from_model: n_ctx_per_seq = 512
0.00.054.064 I llama_init_from_model: n_batch       = 2048
0.00.054.064 I llama_init_from_model: n_ubatch      = 2048
0.00.054.065 I llama_init_from_model: flash_attn    = 0
0.00.054.065 I llama_init_from_model: freq_base     = 10000.0
0.00.054.066 I llama_init_from_model: freq_scale    = 1
0.00.054.067 I ggml_metal_init: allocating
0.00.054.073 I ggml_metal_init: found device: Apple M4
0.00.054.078 I ggml_metal_init: picking default device: Apple M4
0.00.054.953 I ggml_metal_init: using embedded metal library
0.00.059.513 I ggml_metal_init: GPU name:   Apple M4
0.00.059.516 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.059.517 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.059.517 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.059.518 I ggml_metal_init: simdgroup reduction   = true
0.00.059.518 I ggml_metal_init: simdgroup matrix mul. = true
0.00.059.518 I ggml_metal_init: has residency sets    = true
0.00.059.518 I ggml_metal_init: has bfloat            = true
0.00.059.519 I ggml_metal_init: use bfloat            = true
0.00.059.519 I ggml_metal_init: hasUnifiedMemory      = true
0.00.059.520 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.072.683 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.073.393 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.073.396 I llama_init_from_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.073.418 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.074.600 I llama_init_from_model:      Metal compute buffer size =    16.00 MiB
0.00.074.602 I llama_init_from_model:        CPU compute buffer size =     2.51 MiB
0.00.074.602 I llama_init_from_model: graph nodes  = 429
0.00.074.602 I llama_init_from_model: graph splits = 2
0.00.074.603 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.074.604 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.080.045 I 
0.00.080.073 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.080.758 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.043984 -0.019895  0.007670 -0.000838  0.001372 -0.037036  0.109436  0.042589  0.092046 -0.015918  0.006773 -0.035692 -0.017889  0.015040  0.018126  0.015860 -0.011308  0.010431 -0.085223 -0.008454  0.091368 -0.017067 -0.060328 -0.024484  0.027524  0.076059  0.027984 -0.014564  0.017645 -0.033268 -0.037874 -0.019012  0.068670 -0.009845 -0.025030  0.072346 -0.046570  0.011026 -0.050252  0.047693  0.032387 -0.011761  0.022055  0.049645  0.010465  0.005795 -0.028860  0.008933 -0.018507 -0.051466 -0.046036  0.030497 -0.035418  0.054204 -0.069650  0.044238  0.029786  0.046305  0.073410 -0.042580  0.076109  0.038851 -0.181182  0.082503  0.042250 -0.064549 -0.060097 -0.017853  0.006472  0.005873  0.017196 -0.026615  0.064578  0.112612  0.035159 -0.067416  0.027073 -0.067295 -0.033493 -0.033219  0.033252  0.013523 -0.003346 -0.037473 -0.052055  0.055155 -0.001982 -0.038276  0.064447  0.028824 -0.043334 -0.029236 -0.039462  0.036315  0.008379 -0.015481 -0.036573  0.018130  0.028600  0.342831 -0.044484  0.056114  0.017624 -0.020867 -0.066808  0.000146 -0.037885 -0.030063 -0.008541 -0.021595  0.000548 -0.003219  0.004020  0.018908 -0.008548  0.025835  0.049433  0.000091  0.050920 -0.042479 -0.031884  0.023606  0.030691 -0.023148 -0.046271 -0.079278  0.115198  0.046757  0.027829 -0.040710  0.067777 -0.022955  0.010332 -0.032937 -0.018305  0.043844  0.024268  0.052413  0.007469  0.008905  0.011244 -0.074655 -0.065561 -0.026750 -0.041192 -0.023884  0.026710  0.006912  0.027741  0.052873 -0.036673  0.057707 -0.000170  0.031764 -0.019760 -0.022083  0.041053 -0.058903  0.019619  0.043144  0.043582  0.041593 -0.022527  0.027037 -0.021833  0.005430 -0.041317 -0.001239  0.024452  0.002088  0.044333 -0.022745  0.043662  0.064763  0.055430  0.037060 -0.000919  0.046111  0.045790 -0.008487  0.063057 -0.073235 -0.011935  0.032107  0.023957  0.014725 -0.033688  0.001083 -0.015827 -0.019002  0.047872  0.110842  0.028428  0.031366 -0.013288 -0.057499  0.006644  0.005140 -0.012262 -0.051443 -0.000965 -0.017642 -0.019440 -0.040937  0.009209 -0.057957  0.050961  0.052340 -0.009617 -0.040259 -0.014075 -0.024877 -0.017251  0.006302  0.006588 -0.026919  0.015600  0.030760  0.002578  0.023206 -0.022199 -0.098557 -0.051105 -0.278031 -0.014994 -0.061553 -0.027218  0.017666 -0.010939 -0.017077  0.035055  0.046994 -0.015432  0.015232 -0.025477  0.047844 -0.005949 -0.000749 -0.061008 -0.068946 -0.060377 -0.035942  0.043319 -0.055047  0.015075  0.000537 -0.058175 -0.010439  0.012643  0.151495  0.127111 -0.013622  0.041993 -0.025672  0.014026 -0.001037 -0.150469  0.044857  0.005320 -0.036274 -0.029801 -0.020192 -0.034891  0.010235  0.033548 -0.048178 -0.051795 -0.017463 -0.023492  0.047349  0.052076 -0.016783 -0.055452  0.025836 -0.005707  0.010710  0.038705  0.008191 -0.009764 -0.105779 -0.027430 -0.096101  0.025068 -0.011260  0.092355  0.056101  0.003758  0.027791  0.002092 -0.051085 -0.039890 -0.013542 -0.044956 -0.015335  0.002910 -0.043498 -0.077951  0.065217 -0.006840 -0.001630 -0.014642  0.071560  0.023714 -0.037180  0.009175  0.001562 -0.032268  0.015468  0.037880  0.000337 -0.053207  0.021321 -0.039829  0.000031  0.013400  0.019798 -0.057879  0.006473 -0.049531 -0.267858  0.039156 -0.067972  0.038248 -0.012333  0.041485 -0.016117  0.052383 -0.071363  0.011369  0.024719 -0.007232  0.082087  0.028535 -0.021504  0.040494 -0.004570 -0.074594 -0.014749  0.020037  0.002304  0.023148  0.197212 -0.043222 -0.025983 -0.004959 -0.019278  0.074258  0.001716 -0.031979 -0.036599 -0.045082  0.000547 -0.011560  0.018126 -0.029461 -0.008463  0.006419  0.050799 -0.014959  0.006176  0.026096 -0.030809  0.048055  0.114093 -0.040826 -0.011457  0.005419 -0.003599  0.025154 -0.059142  0.013753 -0.010399  0.038709  0.051449  0.035413  0.035031 -0.017027  0.026360 -0.014490 -0.050032  0.003219  0.054121  0.039723 -0.039127 

0.00.084.631 I llama_perf_context_print:        load time =      50.37 ms
0.00.084.632 I llama_perf_context_print: prompt eval time =       3.74 ms /     9 tokens (    0.42 ms per token,  2406.42 tokens per second)
0.00.084.633 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.084.633 I llama_perf_context_print:       total time =       4.59 ms /    10 tokens
0.00.084.781 I ggml_metal_free: deallocating

real	0m0.293s
user	0m0.055s
sys	0m0.039s
```
- q8_0:
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.044 I build: 4772 (4d1051a4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.507 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.012.342 I llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.012.346 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.012.348 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.012.349 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.012.350 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.012.350 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.012.350 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.012.351 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.012.352 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.012.352 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.012.352 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.012.353 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.012.355 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.012.355 I llama_model_loader: - kv  11:                      bert.attention.causal bool             = false
0.00.012.356 I llama_model_loader: - kv  12:                          bert.pooling_type u32              = 2
0.00.012.356 I llama_model_loader: - kv  13:            tokenizer.ggml.token_type_count u32              = 2
0.00.012.356 I llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = bert
0.00.012.356 I llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.014.881 I llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.015.609 I llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.015.611 I llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.015.611 I llama_model_loader: - kv  19:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.015.611 I llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 0
0.00.015.612 I llama_model_loader: - kv  21:               tokenizer.ggml.mask_token_id u32              = 103
0.00.015.612 I llama_model_loader: - kv  22:               general.quantization_version u32              = 2
0.00.015.612 I llama_model_loader: - kv  23:                          general.file_type u32              = 7
0.00.015.613 I llama_model_loader: - type  f32:  124 tensors
0.00.015.613 I llama_model_loader: - type q8_0:   73 tensors
0.00.015.614 I print_info: file format = GGUF V3 (latest)
0.00.015.614 I print_info: file type   = Q8_0
0.00.015.615 I print_info: file size   = 34.38 MiB (8.68 BPW) 
0.00.018.315 I load: special tokens cache size = 5
0.00.019.692 I load: token to piece cache size = 0.2032 MB
0.00.019.695 I print_info: arch             = bert
0.00.019.695 I print_info: vocab_only       = 0
0.00.019.695 I print_info: n_ctx_train      = 512
0.00.019.696 I print_info: n_embd           = 384
0.00.019.696 I print_info: n_layer          = 12
0.00.019.699 I print_info: n_head           = 12
0.00.019.699 I print_info: n_head_kv        = 12
0.00.019.699 I print_info: n_rot            = 32
0.00.019.699 I print_info: n_swa            = 0
0.00.019.700 I print_info: n_embd_head_k    = 32
0.00.019.700 I print_info: n_embd_head_v    = 32
0.00.019.700 I print_info: n_gqa            = 1
0.00.019.702 I print_info: n_embd_k_gqa     = 384
0.00.019.702 I print_info: n_embd_v_gqa     = 384
0.00.019.703 I print_info: f_norm_eps       = 1.0e-12
0.00.019.703 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.019.703 I print_info: f_clamp_kqv      = 0.0e+00
0.00.019.704 I print_info: f_max_alibi_bias = 0.0e+00
0.00.019.704 I print_info: f_logit_scale    = 0.0e+00
0.00.019.705 I print_info: n_ff             = 1536
0.00.019.705 I print_info: n_expert         = 0
0.00.019.705 I print_info: n_expert_used    = 0
0.00.019.705 I print_info: causal attn      = 0
0.00.019.705 I print_info: pooling type     = 2
0.00.019.705 I print_info: rope type        = 2
0.00.019.706 I print_info: rope scaling     = linear
0.00.019.706 I print_info: freq_base_train  = 10000.0
0.00.019.706 I print_info: freq_scale_train = 1
0.00.019.706 I print_info: n_ctx_orig_yarn  = 512
0.00.019.707 I print_info: rope_finetuned   = unknown
0.00.019.709 I print_info: ssm_d_conv       = 0
0.00.019.709 I print_info: ssm_d_inner      = 0
0.00.019.709 I print_info: ssm_d_state      = 0
0.00.019.709 I print_info: ssm_dt_rank      = 0
0.00.019.709 I print_info: ssm_dt_b_c_rms   = 0
0.00.019.709 I print_info: model type       = 33M
0.00.019.710 I print_info: model params     = 33.21 M
0.00.019.710 I print_info: general.name     = Bge Small
0.00.019.710 I print_info: vocab type       = WPM
0.00.019.711 I print_info: n_vocab          = 30522
0.00.019.711 I print_info: n_merges         = 0
0.00.019.711 I print_info: BOS token        = 101 '[CLS]'
0.00.019.711 I print_info: UNK token        = 100 '[UNK]'
0.00.019.711 I print_info: SEP token        = 102 '[SEP]'
0.00.019.713 I print_info: PAD token        = 0 '[PAD]'
0.00.019.713 I print_info: MASK token       = 103 '[MASK]'
0.00.019.713 I print_info: LF token         = 0 '[PAD]'
0.00.019.714 I print_info: max token length = 21
0.00.019.714 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.021.323 I load_tensors: offloading 12 repeating layers to GPU
0.00.021.324 I load_tensors: offloading output layer to GPU
0.00.021.324 I load_tensors: offloaded 13/13 layers to GPU
0.00.021.330 I load_tensors: Metal_Mapped model buffer size =    21.75 MiB
0.00.021.330 I load_tensors:   CPU_Mapped model buffer size =    12.63 MiB
................................................
0.00.021.497 I llama_init_from_model: n_seq_max     = 1
0.00.021.498 I llama_init_from_model: n_ctx         = 512
0.00.021.498 I llama_init_from_model: n_ctx_per_seq = 512
0.00.021.498 I llama_init_from_model: n_batch       = 2048
0.00.021.498 I llama_init_from_model: n_ubatch      = 2048
0.00.021.499 I llama_init_from_model: flash_attn    = 0
0.00.021.499 I llama_init_from_model: freq_base     = 10000.0
0.00.021.499 I llama_init_from_model: freq_scale    = 1
0.00.021.500 I ggml_metal_init: allocating
0.00.021.504 I ggml_metal_init: found device: Apple M4
0.00.021.507 I ggml_metal_init: picking default device: Apple M4
0.00.022.040 I ggml_metal_init: using embedded metal library
0.00.024.665 I ggml_metal_init: GPU name:   Apple M4
0.00.024.667 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.024.667 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.024.667 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.024.668 I ggml_metal_init: simdgroup reduction   = true
0.00.024.668 I ggml_metal_init: simdgroup matrix mul. = true
0.00.024.668 I ggml_metal_init: has residency sets    = true
0.00.024.668 I ggml_metal_init: has bfloat            = true
0.00.024.668 I ggml_metal_init: use bfloat            = true
0.00.024.669 I ggml_metal_init: hasUnifiedMemory      = true
0.00.024.669 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.034.898 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.035.501 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.035.503 I llama_init_from_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.035.517 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.036.455 I llama_init_from_model:      Metal compute buffer size =    16.00 MiB
0.00.036.456 I llama_init_from_model:        CPU compute buffer size =     2.51 MiB
0.00.036.456 I llama_init_from_model: graph nodes  = 429
0.00.036.456 I llama_init_from_model: graph splits = 2
0.00.036.458 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.036.458 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.039.816 I 
0.00.039.836 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.040.348 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.044994 -0.019858  0.007986 -0.001410  0.001531 -0.036276  0.109404  0.043235  0.092836 -0.015201  0.005971 -0.036330 -0.018159  0.015015  0.017667  0.014510 -0.011792  0.011812 -0.084764 -0.008081  0.091650 -0.017638 -0.061084 -0.025066  0.026879  0.076180  0.028256 -0.014273  0.017420 -0.033553 -0.037520 -0.018280  0.068843 -0.010129 -0.024896  0.071968 -0.046406  0.011471 -0.050333  0.048931  0.032317 -0.012107  0.021812  0.049796  0.010366  0.005264 -0.028665  0.008581 -0.018508 -0.052358 -0.046410  0.029426 -0.034989  0.053289 -0.069849  0.043767  0.029291  0.046759  0.073664 -0.043090  0.075213  0.038641 -0.180496  0.081648  0.043389 -0.065615 -0.059878 -0.017387  0.006913  0.005155  0.017165 -0.026679  0.064347  0.112564  0.034871 -0.067783  0.027124 -0.066989 -0.034296 -0.033344  0.033189  0.014325 -0.004253 -0.036945 -0.052008  0.054151 -0.001715 -0.038112  0.063016  0.028668 -0.041665 -0.028879 -0.039263  0.036879  0.007718 -0.015657 -0.036270  0.018699  0.029792  0.344948 -0.043880  0.056487  0.018022 -0.021448 -0.065281  0.000253 -0.037910 -0.029663 -0.008828 -0.020643  0.001162 -0.003510  0.003091  0.018576 -0.009637  0.025316  0.049298 -0.000937  0.051568 -0.042363 -0.031550  0.023315  0.030044 -0.023267 -0.045483 -0.079644  0.114489  0.047503  0.027208 -0.041477  0.067717 -0.022845  0.010232 -0.033486 -0.017199  0.044319  0.022956  0.052194  0.008122  0.008112  0.009951 -0.074643 -0.065035 -0.025938 -0.040806 -0.024533  0.027409  0.006078  0.027693  0.052243 -0.036587  0.058682  0.001467  0.032191 -0.019964 -0.021840  0.041805 -0.058801  0.019348  0.042626  0.043999  0.040911 -0.021909  0.028025 -0.022801  0.005520 -0.041266 -0.000085  0.024325  0.001532  0.044802 -0.022888  0.043056  0.065030  0.056292  0.037666 -0.001004  0.047521  0.045624 -0.008614  0.062270 -0.073313 -0.011718  0.032708  0.023348  0.014650 -0.033924  0.001158 -0.015872 -0.018974  0.047982  0.110740  0.029177  0.030881 -0.012325 -0.057836  0.006939  0.004378 -0.011864 -0.051754 -0.003018 -0.017766 -0.020051 -0.041018  0.009118 -0.058568  0.050632  0.051795 -0.008968 -0.040668 -0.014765 -0.024320 -0.015980  0.005823  0.007147 -0.027335  0.016275  0.030734  0.002342  0.023167 -0.022169 -0.098059 -0.050901 -0.277240 -0.015100 -0.061566 -0.026219  0.017141 -0.010285 -0.017522  0.034725  0.047897 -0.016594  0.015489 -0.024132  0.048613 -0.004993 -0.000001 -0.060860 -0.068346 -0.059622 -0.036157  0.043605 -0.056353  0.014713  0.000292 -0.058627 -0.010995  0.012274  0.151597  0.127025 -0.012582  0.042765 -0.025227  0.013629 -0.000381 -0.150686  0.043974  0.004714 -0.036955 -0.029586 -0.019853 -0.034063  0.009522  0.034004 -0.048909 -0.051765 -0.017243 -0.025080  0.047588  0.050981 -0.017553 -0.056746  0.024054 -0.006285  0.011003  0.038719  0.008083 -0.008595 -0.106361 -0.027365 -0.096966  0.025294 -0.011096  0.091579  0.056180  0.004494  0.028066  0.001727 -0.051388 -0.039497 -0.013817 -0.045907 -0.015238  0.002920 -0.043938 -0.077716  0.066102 -0.006195 -0.000885 -0.013768  0.071562  0.023826 -0.036363  0.007848  0.001487 -0.032916  0.016336  0.037378  0.000625 -0.052389  0.021549 -0.039695 -0.000026  0.013496  0.020436 -0.057513  0.005900 -0.049906 -0.267546  0.038686 -0.067509  0.037450 -0.011471  0.041547 -0.015904  0.051347 -0.072313  0.012571  0.024808 -0.007420  0.082069  0.028175 -0.021956  0.040736 -0.003945 -0.074192 -0.014923  0.020417  0.002644  0.023207  0.197144 -0.043068 -0.025876 -0.004886 -0.018582  0.073936  0.001341 -0.031663 -0.037140 -0.044262  0.000049 -0.010741  0.018225 -0.028045 -0.008828  0.006047  0.050183 -0.015017  0.007037  0.025894 -0.030628  0.048075  0.112795 -0.039982 -0.011695  0.005039 -0.002800  0.025072 -0.059720  0.014182 -0.010019  0.038100  0.051088  0.034726  0.035947 -0.017035  0.027243 -0.015391 -0.051195  0.003744  0.053928  0.040014 -0.039140 

0.00.043.358 I llama_perf_context_print:        load time =      30.30 ms
0.00.043.360 I llama_perf_context_print: prompt eval time =       2.89 ms /     9 tokens (    0.32 ms per token,  3119.58 tokens per second)
0.00.043.361 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.043.361 I llama_perf_context_print:       total time =       3.54 ms /    10 tokens
0.00.043.548 I ggml_metal_free: deallocating

real	0m0.055s
user	0m0.032s
sys	0m0.015s
```
### rerank_tiny

Rerank Tiny (Jina):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/rerank-tiny/ggml-model-f16.gguf -p 'what is panda?</s></s>hi\nwhat is panda?</s></s>it'\''s a bear\nwhat is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.' -ngl 99 -c 0 --pooling rank --embd-normalize -1 --verbose-prompt
0.00.000.276 I build: 4772 (4d1051a4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.022.093 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.034.933 I llama_model_loader: loaded meta data with 28 key-value pairs and 70 tensors from ../models-mnt/rerank-tiny/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.034.938 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.034.940 I llama_model_loader: - kv   0:                       general.architecture str              = jina-bert-v2
0.00.034.941 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.034.945 I llama_model_loader: - kv   2:                               general.name str              = Jina Bert Implementation
0.00.034.946 I llama_model_loader: - kv   3:                       general.organization str              = Jinaai
0.00.034.947 I llama_model_loader: - kv   4:                         general.size_label str              = 33M
0.00.034.950 I llama_model_loader: - kv   5:                   jina-bert-v2.block_count u32              = 4
0.00.034.951 I llama_model_loader: - kv   6:                jina-bert-v2.context_length u32              = 8192
0.00.034.952 I llama_model_loader: - kv   7:              jina-bert-v2.embedding_length u32              = 384
0.00.034.952 I llama_model_loader: - kv   8:           jina-bert-v2.feed_forward_length u32              = 1536
0.00.034.953 I llama_model_loader: - kv   9:          jina-bert-v2.attention.head_count u32              = 12
0.00.034.957 I llama_model_loader: - kv  10:  jina-bert-v2.attention.layer_norm_epsilon f32              = 0.000000
0.00.034.957 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.034.958 I llama_model_loader: - kv  12:              jina-bert-v2.attention.causal bool             = false
0.00.034.960 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.034.961 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = jina-v1-en
0.00.041.909 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,61056]   = ["<s>", "<pad>", "</s>", "<unk>", "<m...
0.00.044.118 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,61056]   = [3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, ...
0.00.048.594 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,39382]   = ["t h", "i n", "a n", "e r", "th e", ...
0.00.048.595 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.048.596 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
0.00.048.596 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 3
0.00.048.597 I llama_model_loader: - kv  21:          tokenizer.ggml.seperator_token_id u32              = 2
0.00.048.597 I llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 1
0.00.048.597 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 4
0.00.048.598 I llama_model_loader: - kv  24:            tokenizer.ggml.token_type_count u32              = 2
0.00.048.598 I llama_model_loader: - kv  25:               tokenizer.ggml.add_bos_token bool             = true
0.00.048.599 I llama_model_loader: - kv  26:               tokenizer.ggml.add_eos_token bool             = true
0.00.048.599 I llama_model_loader: - kv  27:               general.quantization_version u32              = 2
0.00.048.599 I llama_model_loader: - type  f32:   40 tensors
0.00.048.600 I llama_model_loader: - type  f16:   30 tensors
0.00.048.600 I print_info: file format = GGUF V3 (latest)
0.00.048.601 I print_info: file type   = F16
0.00.048.602 I print_info: file size   = 62.78 MiB (16.01 BPW) 
0.00.052.872 W load: empty token at index 5
0.00.057.903 W load: model vocab missing newline token, using special_pad_id instead
0.00.059.393 W load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
0.00.059.426 I load: special tokens cache size = 5
0.00.324.818 I load: token to piece cache size = 1.5060 MB
0.00.324.823 I print_info: arch             = jina-bert-v2
0.00.324.824 I print_info: vocab_only       = 0
0.00.324.824 I print_info: n_ctx_train      = 8192
0.00.324.824 I print_info: n_embd           = 384
0.00.324.824 I print_info: n_layer          = 4
0.00.324.828 I print_info: n_head           = 12
0.00.324.829 I print_info: n_head_kv        = 12
0.00.324.829 I print_info: n_rot            = 32
0.00.324.829 I print_info: n_swa            = 0
0.00.324.832 I print_info: n_embd_head_k    = 32
0.00.324.832 I print_info: n_embd_head_v    = 32
0.00.324.833 I print_info: n_gqa            = 1
0.00.324.833 I print_info: n_embd_k_gqa     = 384
0.00.324.834 I print_info: n_embd_v_gqa     = 384
0.00.324.835 I print_info: f_norm_eps       = 1.0e-12
0.00.324.835 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.324.835 I print_info: f_clamp_kqv      = 0.0e+00
0.00.324.836 I print_info: f_max_alibi_bias = 8.0e+00
0.00.324.836 I print_info: f_logit_scale    = 0.0e+00
0.00.324.837 I print_info: n_ff             = 1536
0.00.324.837 I print_info: n_expert         = 0
0.00.324.837 I print_info: n_expert_used    = 0
0.00.324.837 I print_info: causal attn      = 0
0.00.324.837 I print_info: pooling type     = -1
0.00.324.837 I print_info: rope type        = -1
0.00.324.840 I print_info: rope scaling     = linear
0.00.324.840 I print_info: freq_base_train  = 10000.0
0.00.324.840 I print_info: freq_scale_train = 1
0.00.324.841 I print_info: n_ctx_orig_yarn  = 8192
0.00.324.841 I print_info: rope_finetuned   = unknown
0.00.324.841 I print_info: ssm_d_conv       = 0
0.00.324.841 I print_info: ssm_d_inner      = 0
0.00.324.841 I print_info: ssm_d_state      = 0
0.00.324.841 I print_info: ssm_dt_rank      = 0
0.00.324.841 I print_info: ssm_dt_b_c_rms   = 0
0.00.324.842 I print_info: model type       = 33M
0.00.324.842 I print_info: model params     = 32.90 M
0.00.324.843 I print_info: general.name     = Jina Bert Implementation
0.00.324.843 I print_info: vocab type       = BPE
0.00.324.844 I print_info: n_vocab          = 61056
0.00.324.844 I print_info: n_merges         = 39382
0.00.324.844 I print_info: BOS token        = 0 '<s>'
0.00.324.844 I print_info: EOS token        = 2 '</s>'
0.00.324.844 I print_info: UNK token        = 3 '<unk>'
0.00.324.845 I print_info: SEP token        = 2 '</s>'
0.00.324.845 I print_info: PAD token        = 1 '<pad>'
0.00.324.845 I print_info: MASK token       = 4 '<mask>'
0.00.324.846 I print_info: EOG token        = 2 '</s>'
0.00.324.846 I print_info: max token length = 45
0.00.324.846 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.326.442 I load_tensors: offloading 4 repeating layers to GPU
0.00.326.443 I load_tensors: offloading output layer to GPU
0.00.326.443 I load_tensors: offloaded 5/5 layers to GPU
0.00.326.466 I load_tensors: Metal_Mapped model buffer size =    18.06 MiB
0.00.326.469 I load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
....................
0.00.326.695 I llama_init_from_model: n_seq_max     = 1
0.00.326.696 I llama_init_from_model: n_ctx         = 8192
0.00.326.696 I llama_init_from_model: n_ctx_per_seq = 8192
0.00.326.696 I llama_init_from_model: n_batch       = 2048
0.00.326.696 I llama_init_from_model: n_ubatch      = 2048
0.00.326.697 I llama_init_from_model: flash_attn    = 0
0.00.326.697 I llama_init_from_model: freq_base     = 10000.0
0.00.326.697 I llama_init_from_model: freq_scale    = 1
0.00.326.698 I ggml_metal_init: allocating
0.00.326.701 I ggml_metal_init: found device: Apple M4
0.00.326.705 I ggml_metal_init: picking default device: Apple M4
0.00.327.401 I ggml_metal_init: using embedded metal library
0.00.330.265 I ggml_metal_init: GPU name:   Apple M4
0.00.330.267 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.330.267 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.330.268 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.330.268 I ggml_metal_init: simdgroup reduction   = true
0.00.330.268 I ggml_metal_init: simdgroup matrix mul. = true
0.00.330.269 I ggml_metal_init: has residency sets    = true
0.00.330.269 I ggml_metal_init: has bfloat            = true
0.00.330.269 I ggml_metal_init: use bfloat            = true
0.00.330.269 I ggml_metal_init: hasUnifiedMemory      = true
0.00.330.270 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.340.226 I llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 4, can_shift = 1
0.00.343.254 I llama_kv_cache_init:      Metal KV buffer size =    48.00 MiB
0.00.343.256 I llama_init_from_model: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB
0.00.343.275 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.349.634 I llama_init_from_model:      Metal compute buffer size =   220.01 MiB
0.00.349.635 I llama_init_from_model:        CPU compute buffer size =    22.02 MiB
0.00.349.635 I llama_init_from_model: graph nodes  = 154
0.00.349.636 I llama_init_from_model: graph splits = 2
0.00.349.637 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 8192
0.00.349.638 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.356.748 I 
0.00.356.789 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.356.884 I main: prompt 0: 'what is panda?</s></s>hi'
0.00.356.885 I main: number of tokens in prompt = 9
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 23233 -> 'hi'
     2 -> '</s>'


0.00.356.888 I main: prompt 1: 'what is panda?</s></s>it's a bear'
0.00.356.888 I main: number of tokens in prompt = 13
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21153 -> 'it'
    37 -> '''
    87 -> 's'
    69 -> 'a'
 25706 -> 'bear'
     2 -> '</s>'


0.00.356.892 I main: prompt 2: 'what is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'
0.00.356.892 I main: number of tokens in prompt = 40
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21215 -> 'he'
 28390 -> 'giant'
 49990 -> 'panda'
    38 -> '('
 21163 -> 'il'
 26237 -> 'uro'
 21223 -> 'po'
 23179 -> 'da'
 36906 -> 'melan'
 26791 -> 'ole'
    89 -> 'u'
 21402 -> 'ca'
 21686 -> '),'
 23314 -> 'sometimes'
 22517 -> 'called'
    69 -> 'a'
 49990 -> 'panda'
 25706 -> 'bear'
 21142 -> 'or'
 22810 -> 'simply'
 49990 -> 'panda'
    42 -> ','
 21152 -> 'is'
    69 -> 'a'
 25706 -> 'bear'
 25677 -> 'species'
 28930 -> 'ende'
 22024 -> 'mic'
 21148 -> 'to'
    76 -> 'h'
 22344 -> 'ina'
    44 -> '.'
     2 -> '</s>'


0.00.357.372 I batch_decode: n_tokens = 62, n_seq = 3

rerank score 0:    0.023
rerank score 1:    0.024
rerank score 2:    0.199

0.00.360.860 I llama_perf_context_print:        load time =     334.65 ms
0.00.360.864 I llama_perf_context_print: prompt eval time =       3.48 ms /    62 tokens (    0.06 ms per token, 17826.34 tokens per second)
0.00.360.865 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.360.867 I llama_perf_context_print:       total time =       4.11 ms /    63 tokens
0.00.361.127 I ggml_metal_free: deallocating

real	0m1.083s
user	0m0.332s
sys	0m0.048s
  - rerank score 0 @ 0.023 OK
  - rerank score 1 @ 0.024 OK
  - rerank score 2 @ 0.199 OK
```
### pythia_1_4b

Pythia 1.4B:
- status: 0
- perplexity:
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
- imatrix:
```
Final estimate: PPL = 10.1498 +/- 3.22650
```
- f16: 
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.208 I build: 4772 (4d1051a4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.401 I main: llama backend init
0.00.000.415 I main: load the model and apply lora adapter, if any
0.00.030.439 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.044.294 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.044.307 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.044.322 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.044.323 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.044.324 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.044.325 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.044.326 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.044.330 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.044.334 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.044.335 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.044.335 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.044.336 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.044.337 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.044.338 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.044.341 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.044.342 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.044.342 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.053.070 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.055.766 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.063.141 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.063.144 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.063.145 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.063.145 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.063.146 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.063.146 I llama_model_loader: - type  f32:  194 tensors
0.00.063.147 I llama_model_loader: - type  f16:   98 tensors
0.00.063.148 I print_info: file format = GGUF V3 (latest)
0.00.063.149 I print_info: file type   = all F32 (guessed)
0.00.063.150 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.076.877 I load: special tokens cache size = 25
0.00.085.359 I load: token to piece cache size = 0.2984 MB
0.00.085.362 I print_info: arch             = gptneox
0.00.085.363 I print_info: vocab_only       = 0
0.00.085.363 I print_info: n_ctx_train      = 2048
0.00.085.363 I print_info: n_embd           = 2048
0.00.085.363 I print_info: n_layer          = 24
0.00.085.366 I print_info: n_head           = 16
0.00.085.367 I print_info: n_head_kv        = 16
0.00.085.367 I print_info: n_rot            = 32
0.00.085.368 I print_info: n_swa            = 0
0.00.085.369 I print_info: n_embd_head_k    = 128
0.00.085.369 I print_info: n_embd_head_v    = 128
0.00.085.370 I print_info: n_gqa            = 1
0.00.085.371 I print_info: n_embd_k_gqa     = 2048
0.00.085.372 I print_info: n_embd_v_gqa     = 2048
0.00.085.372 I print_info: f_norm_eps       = 1.0e-05
0.00.085.373 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.085.373 I print_info: f_clamp_kqv      = 0.0e+00
0.00.085.373 I print_info: f_max_alibi_bias = 0.0e+00
0.00.085.373 I print_info: f_logit_scale    = 0.0e+00
0.00.085.374 I print_info: n_ff             = 8192
0.00.085.374 I print_info: n_expert         = 0
0.00.085.374 I print_info: n_expert_used    = 0
0.00.085.374 I print_info: causal attn      = 1
0.00.085.375 I print_info: pooling type     = 0
0.00.085.375 I print_info: rope type        = 2
0.00.085.375 I print_info: rope scaling     = linear
0.00.085.376 I print_info: freq_base_train  = 10000.0
0.00.085.376 I print_info: freq_scale_train = 1
0.00.085.376 I print_info: n_ctx_orig_yarn  = 2048
0.00.085.376 I print_info: rope_finetuned   = unknown
0.00.085.377 I print_info: ssm_d_conv       = 0
0.00.085.377 I print_info: ssm_d_inner      = 0
0.00.085.377 I print_info: ssm_d_state      = 0
0.00.085.377 I print_info: ssm_dt_rank      = 0
0.00.085.377 I print_info: ssm_dt_b_c_rms   = 0
0.00.085.378 I print_info: model type       = 1.4B
0.00.085.378 I print_info: model params     = 1.41 B
0.00.085.378 I print_info: general.name     = 1.4B
0.00.085.379 I print_info: vocab type       = BPE
0.00.085.379 I print_info: n_vocab          = 50304
0.00.085.379 I print_info: n_merges         = 50009
0.00.085.379 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.085.380 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.085.380 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.085.381 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.085.382 I print_info: LF token         = 187 ''
0.00.085.382 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.085.382 I print_info: max token length = 1024
0.00.085.383 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.129.245 I load_tensors: offloading 24 repeating layers to GPU
0.00.129.248 I load_tensors: offloading output layer to GPU
0.00.129.249 I load_tensors: offloaded 25/25 layers to GPU
0.00.129.271 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.129.273 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.129.911 I llama_init_from_model: n_seq_max     = 1
0.00.129.912 I llama_init_from_model: n_ctx         = 2048
0.00.129.912 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.129.912 I llama_init_from_model: n_batch       = 2048
0.00.129.912 I llama_init_from_model: n_ubatch      = 512
0.00.129.913 I llama_init_from_model: flash_attn    = 0
0.00.129.913 I llama_init_from_model: freq_base     = 10000.0
0.00.129.913 I llama_init_from_model: freq_scale    = 1
0.00.129.915 I ggml_metal_init: allocating
0.00.129.969 I ggml_metal_init: found device: Apple M4
0.00.129.976 I ggml_metal_init: picking default device: Apple M4
0.00.130.632 I ggml_metal_init: using embedded metal library
0.00.139.893 I ggml_metal_init: GPU name:   Apple M4
0.00.139.894 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.139.895 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.139.895 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.139.895 I ggml_metal_init: simdgroup reduction   = true
0.00.139.895 I ggml_metal_init: simdgroup matrix mul. = true
0.00.139.896 I ggml_metal_init: has residency sets    = true
0.00.139.896 I ggml_metal_init: has bfloat            = true
0.00.139.896 I ggml_metal_init: use bfloat            = true
0.00.139.896 I ggml_metal_init: hasUnifiedMemory      = true
0.00.139.897 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.168.744 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.197.763 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.197.768 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.197.811 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.201.991 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.201.993 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.201.993 I llama_init_from_model: graph nodes  = 967
0.00.201.994 I llama_init_from_model: graph splits = 2
0.00.202.001 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.202.129 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.202.129 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.268.002 I main: llama threadpool init, n_threads = 4
0.00.268.049 I 
0.00.268.080 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.268.081 I 
0.00.268.260 I sampler seed: 1234
0.00.268.265 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.268.291 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.268.292 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.268.292 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.103.587 I llama_perf_sampler_print:    sampling time =       1.17 ms /    71 runs   (    0.02 ms per token, 60683.76 tokens per second)
0.02.103.588 I llama_perf_context_print:        load time =     236.65 ms
0.02.103.588 I llama_perf_context_print: prompt eval time =      43.77 ms /     7 tokens (    6.25 ms per token,   159.92 tokens per second)
0.02.103.589 I llama_perf_context_print:        eval time =    1788.69 ms /    63 runs   (   28.39 ms per token,    35.22 tokens per second)
0.02.103.589 I llama_perf_context_print:       total time =    1836.49 ms /    70 tokens
0.02.103.829 I ggml_metal_free: deallocating

real	0m2.428s
user	0m0.133s
sys	0m0.135s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.637 I build: 4772 (4d1051a4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.023.681 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.036.700 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.036.714 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.036.718 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.036.719 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.036.719 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.036.720 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.036.720 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.036.723 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.036.724 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.036.724 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.036.725 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.036.726 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.036.726 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.036.727 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.036.733 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.036.733 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.036.744 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.044.894 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.047.495 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.055.049 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.055.054 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.055.055 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.055.055 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.055.056 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.055.057 I llama_model_loader: - type  f32:  194 tensors
0.00.055.057 I llama_model_loader: - type  f16:   98 tensors
0.00.055.059 I print_info: file format = GGUF V3 (latest)
0.00.055.060 I print_info: file type   = all F32 (guessed)
0.00.055.062 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.069.127 I load: special tokens cache size = 25
0.00.077.584 I load: token to piece cache size = 0.2984 MB
0.00.077.589 I print_info: arch             = gptneox
0.00.077.590 I print_info: vocab_only       = 0
0.00.077.590 I print_info: n_ctx_train      = 2048
0.00.077.590 I print_info: n_embd           = 2048
0.00.077.591 I print_info: n_layer          = 24
0.00.077.594 I print_info: n_head           = 16
0.00.077.595 I print_info: n_head_kv        = 16
0.00.077.595 I print_info: n_rot            = 32
0.00.077.595 I print_info: n_swa            = 0
0.00.077.596 I print_info: n_embd_head_k    = 128
0.00.077.596 I print_info: n_embd_head_v    = 128
0.00.077.597 I print_info: n_gqa            = 1
0.00.077.598 I print_info: n_embd_k_gqa     = 2048
0.00.077.598 I print_info: n_embd_v_gqa     = 2048
0.00.077.599 I print_info: f_norm_eps       = 1.0e-05
0.00.077.599 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.077.599 I print_info: f_clamp_kqv      = 0.0e+00
0.00.077.600 I print_info: f_max_alibi_bias = 0.0e+00
0.00.077.600 I print_info: f_logit_scale    = 0.0e+00
0.00.077.601 I print_info: n_ff             = 8192
0.00.077.601 I print_info: n_expert         = 0
0.00.077.603 I print_info: n_expert_used    = 0
0.00.077.605 I print_info: causal attn      = 1
0.00.077.605 I print_info: pooling type     = 0
0.00.077.605 I print_info: rope type        = 2
0.00.077.606 I print_info: rope scaling     = linear
0.00.077.606 I print_info: freq_base_train  = 10000.0
0.00.077.606 I print_info: freq_scale_train = 1
0.00.077.607 I print_info: n_ctx_orig_yarn  = 2048
0.00.077.607 I print_info: rope_finetuned   = unknown
0.00.077.607 I print_info: ssm_d_conv       = 0
0.00.077.607 I print_info: ssm_d_inner      = 0
0.00.077.607 I print_info: ssm_d_state      = 0
0.00.077.607 I print_info: ssm_dt_rank      = 0
0.00.077.607 I print_info: ssm_dt_b_c_rms   = 0
0.00.077.612 I print_info: model type       = 1.4B
0.00.077.613 I print_info: model params     = 1.41 B
0.00.077.613 I print_info: general.name     = 1.4B
0.00.077.614 I print_info: vocab type       = BPE
0.00.077.614 I print_info: n_vocab          = 50304
0.00.077.614 I print_info: n_merges         = 50009
0.00.077.614 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.077.615 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.077.615 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.077.615 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.077.617 I print_info: LF token         = 187 ''
0.00.077.617 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.077.617 I print_info: max token length = 1024
0.00.077.618 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.423.936 I load_tensors: offloading 24 repeating layers to GPU
0.01.423.942 I load_tensors: offloading output layer to GPU
0.01.423.942 I load_tensors: offloaded 25/25 layers to GPU
0.01.423.963 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.423.966 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.01.424.939 I llama_init_from_model: n_seq_max     = 1
0.01.424.941 I llama_init_from_model: n_ctx         = 128
0.01.424.941 I llama_init_from_model: n_ctx_per_seq = 128
0.01.424.941 I llama_init_from_model: n_batch       = 128
0.01.424.941 I llama_init_from_model: n_ubatch      = 128
0.01.424.942 I llama_init_from_model: flash_attn    = 0
0.01.424.943 I llama_init_from_model: freq_base     = 10000.0
0.01.424.943 I llama_init_from_model: freq_scale    = 1
0.01.424.944 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.424.945 I ggml_metal_init: allocating
0.01.424.982 I ggml_metal_init: found device: Apple M4
0.01.424.993 I ggml_metal_init: picking default device: Apple M4
0.01.426.002 I ggml_metal_init: using embedded metal library
0.01.430.069 I ggml_metal_init: GPU name:   Apple M4
0.01.430.071 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.430.072 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.430.072 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.430.073 I ggml_metal_init: simdgroup reduction   = true
0.01.430.073 I ggml_metal_init: simdgroup matrix mul. = true
0.01.430.073 I ggml_metal_init: has residency sets    = true
0.01.430.073 I ggml_metal_init: has bfloat            = true
0.01.430.073 I ggml_metal_init: use bfloat            = true
0.01.430.074 I ggml_metal_init: hasUnifiedMemory      = true
0.01.430.075 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.442.708 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.444.624 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.444.630 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.444.660 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.446.439 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.446.441 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.446.441 I llama_init_from_model: graph nodes  = 967
0.01.446.441 I llama_init_from_model: graph splits = 2
0.01.446.443 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.446.443 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.481.131 I 
0.01.481.170 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.481.175 I perplexity: tokenizing the input ..
0.01.485.728 I perplexity: tokenization took 4.551 ms
0.01.485.733 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.604.073 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.605.402 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.605.434 I llama_perf_context_print:        load time =    1457.44 ms
0.01.605.436 I llama_perf_context_print: prompt eval time =     118.03 ms /   128 tokens (    0.92 ms per token,  1084.46 tokens per second)
0.01.605.436 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.605.437 I llama_perf_context_print:       total time =     124.30 ms /   129 tokens
0.01.605.784 I ggml_metal_free: deallocating

real	0m1.795s
user	0m0.101s
sys	0m0.249s
```
- q8_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.054 I build: 4772 (4d1051a4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.092 I main: llama backend init
0.00.000.094 I main: load the model and apply lora adapter, if any
0.00.009.814 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.026.220 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.026.226 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.026.230 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.026.231 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.026.231 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.026.231 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.026.232 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.026.233 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.026.234 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.026.234 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.026.234 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.026.235 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.026.235 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.026.236 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.026.239 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.026.239 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.026.240 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.030.073 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.031.178 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.034.966 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.034.967 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.034.967 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.034.968 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.034.968 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.034.969 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.034.969 I llama_model_loader: - type  f32:  194 tensors
0.00.034.970 I llama_model_loader: - type q8_0:   98 tensors
0.00.034.971 I print_info: file format = GGUF V3 (latest)
0.00.034.971 I print_info: file type   = Q8_0
0.00.034.973 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.044.295 I load: special tokens cache size = 25
0.00.051.304 I load: token to piece cache size = 0.2984 MB
0.00.051.308 I print_info: arch             = gptneox
0.00.051.309 I print_info: vocab_only       = 0
0.00.051.309 I print_info: n_ctx_train      = 2048
0.00.051.309 I print_info: n_embd           = 2048
0.00.051.309 I print_info: n_layer          = 24
0.00.051.315 I print_info: n_head           = 16
0.00.051.316 I print_info: n_head_kv        = 16
0.00.051.316 I print_info: n_rot            = 32
0.00.051.316 I print_info: n_swa            = 0
0.00.051.316 I print_info: n_embd_head_k    = 128
0.00.051.321 I print_info: n_embd_head_v    = 128
0.00.051.322 I print_info: n_gqa            = 1
0.00.051.324 I print_info: n_embd_k_gqa     = 2048
0.00.051.324 I print_info: n_embd_v_gqa     = 2048
0.00.051.325 I print_info: f_norm_eps       = 1.0e-05
0.00.051.325 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.326 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.327 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.327 I print_info: f_logit_scale    = 0.0e+00
0.00.051.328 I print_info: n_ff             = 8192
0.00.051.328 I print_info: n_expert         = 0
0.00.051.328 I print_info: n_expert_used    = 0
0.00.051.328 I print_info: causal attn      = 1
0.00.051.328 I print_info: pooling type     = 0
0.00.051.328 I print_info: rope type        = 2
0.00.051.329 I print_info: rope scaling     = linear
0.00.051.329 I print_info: freq_base_train  = 10000.0
0.00.051.329 I print_info: freq_scale_train = 1
0.00.051.330 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.334 I print_info: rope_finetuned   = unknown
0.00.051.334 I print_info: ssm_d_conv       = 0
0.00.051.334 I print_info: ssm_d_inner      = 0
0.00.051.334 I print_info: ssm_d_state      = 0
0.00.051.334 I print_info: ssm_dt_rank      = 0
0.00.051.334 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.335 I print_info: model type       = 1.4B
0.00.051.335 I print_info: model params     = 1.41 B
0.00.051.335 I print_info: general.name     = 1.4B
0.00.051.336 I print_info: vocab type       = BPE
0.00.051.336 I print_info: n_vocab          = 50304
0.00.051.336 I print_info: n_merges         = 50009
0.00.051.337 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.337 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.337 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.337 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.337 I print_info: LF token         = 187 ''
0.00.051.338 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.338 I print_info: max token length = 1024
0.00.051.338 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.139.171 I load_tensors: offloading 24 repeating layers to GPU
0.01.139.176 I load_tensors: offloading output layer to GPU
0.01.139.178 I load_tensors: offloaded 25/25 layers to GPU
0.01.139.203 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.01.139.208 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.01.140.695 I llama_init_from_model: n_seq_max     = 1
0.01.140.697 I llama_init_from_model: n_ctx         = 2048
0.01.140.697 I llama_init_from_model: n_ctx_per_seq = 2048
0.01.140.697 I llama_init_from_model: n_batch       = 2048
0.01.140.698 I llama_init_from_model: n_ubatch      = 512
0.01.140.698 I llama_init_from_model: flash_attn    = 0
0.01.140.699 I llama_init_from_model: freq_base     = 10000.0
0.01.140.700 I llama_init_from_model: freq_scale    = 1
0.01.140.701 I ggml_metal_init: allocating
0.01.140.720 I ggml_metal_init: found device: Apple M4
0.01.140.728 I ggml_metal_init: picking default device: Apple M4
0.01.142.093 I ggml_metal_init: using embedded metal library
0.01.147.508 I ggml_metal_init: GPU name:   Apple M4
0.01.147.511 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.147.512 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.147.512 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.147.513 I ggml_metal_init: simdgroup reduction   = true
0.01.147.513 I ggml_metal_init: simdgroup matrix mul. = true
0.01.147.513 I ggml_metal_init: has residency sets    = true
0.01.147.513 I ggml_metal_init: has bfloat            = true
0.01.147.513 I ggml_metal_init: use bfloat            = true
0.01.147.514 I ggml_metal_init: hasUnifiedMemory      = true
0.01.147.518 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.163.912 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.217.167 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.01.217.175 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.01.217.211 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.221.491 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.01.221.493 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.01.221.493 I llama_init_from_model: graph nodes  = 967
0.01.221.493 I llama_init_from_model: graph splits = 2
0.01.221.498 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.01.221.634 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.221.635 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.279.208 I main: llama threadpool init, n_threads = 4
0.01.279.252 I 
0.01.279.273 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.279.275 I 
0.01.279.436 I sampler seed: 1234
0.01.279.441 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.279.452 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.279.452 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.279.452 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.373.185 I llama_perf_sampler_print:    sampling time =       1.27 ms /    71 runs   (    0.02 ms per token, 55773.76 tokens per second)
0.02.373.186 I llama_perf_context_print:        load time =    1268.65 ms
0.02.373.187 I llama_perf_context_print: prompt eval time =      49.23 ms /     7 tokens (    7.03 ms per token,   142.19 tokens per second)
0.02.373.187 I llama_perf_context_print:        eval time =    1041.69 ms /    63 runs   (   16.53 ms per token,    60.48 tokens per second)
0.02.373.188 I llama_perf_context_print:       total time =    1094.72 ms /    70 tokens
0.02.373.464 I ggml_metal_free: deallocating

real	0m2.392s
user	0m0.110s
sys	0m0.260s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.290 I build: 4772 (4d1051a4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.997 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.210 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.017.216 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.218 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.223 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.224 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.224 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.224 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.225 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.225 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.226 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.226 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.226 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.227 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.227 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.229 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.229 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.230 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.052 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.201 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.022 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.024 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.024 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.024 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.025 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.025 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.026.026 I llama_model_loader: - type  f32:  194 tensors
0.00.026.026 I llama_model_loader: - type q8_0:   98 tensors
0.00.026.027 I print_info: file format = GGUF V3 (latest)
0.00.026.027 I print_info: file type   = Q8_0
0.00.026.028 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.034.480 I load: special tokens cache size = 25
0.00.040.299 I load: token to piece cache size = 0.2984 MB
0.00.040.303 I print_info: arch             = gptneox
0.00.040.303 I print_info: vocab_only       = 0
0.00.040.304 I print_info: n_ctx_train      = 2048
0.00.040.304 I print_info: n_embd           = 2048
0.00.040.304 I print_info: n_layer          = 24
0.00.040.308 I print_info: n_head           = 16
0.00.040.309 I print_info: n_head_kv        = 16
0.00.040.309 I print_info: n_rot            = 32
0.00.040.309 I print_info: n_swa            = 0
0.00.040.309 I print_info: n_embd_head_k    = 128
0.00.040.313 I print_info: n_embd_head_v    = 128
0.00.040.313 I print_info: n_gqa            = 1
0.00.040.314 I print_info: n_embd_k_gqa     = 2048
0.00.040.314 I print_info: n_embd_v_gqa     = 2048
0.00.040.315 I print_info: f_norm_eps       = 1.0e-05
0.00.040.315 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.315 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.315 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.316 I print_info: f_logit_scale    = 0.0e+00
0.00.040.316 I print_info: n_ff             = 8192
0.00.040.316 I print_info: n_expert         = 0
0.00.040.317 I print_info: n_expert_used    = 0
0.00.040.317 I print_info: causal attn      = 1
0.00.040.317 I print_info: pooling type     = 0
0.00.040.317 I print_info: rope type        = 2
0.00.040.317 I print_info: rope scaling     = linear
0.00.040.319 I print_info: freq_base_train  = 10000.0
0.00.040.319 I print_info: freq_scale_train = 1
0.00.040.319 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.320 I print_info: rope_finetuned   = unknown
0.00.040.320 I print_info: ssm_d_conv       = 0
0.00.040.320 I print_info: ssm_d_inner      = 0
0.00.040.320 I print_info: ssm_d_state      = 0
0.00.040.320 I print_info: ssm_dt_rank      = 0
0.00.040.320 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.320 I print_info: model type       = 1.4B
0.00.040.322 I print_info: model params     = 1.41 B
0.00.040.322 I print_info: general.name     = 1.4B
0.00.040.322 I print_info: vocab type       = BPE
0.00.040.323 I print_info: n_vocab          = 50304
0.00.040.323 I print_info: n_merges         = 50009
0.00.040.323 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.323 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.323 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.323 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.324 I print_info: LF token         = 187 ''
0.00.040.324 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.324 I print_info: max token length = 1024
0.00.040.325 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.827.822 I load_tensors: offloading 24 repeating layers to GPU
0.00.827.828 I load_tensors: offloading output layer to GPU
0.00.827.829 I load_tensors: offloaded 25/25 layers to GPU
0.00.827.857 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.827.860 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.829.457 I llama_init_from_model: n_seq_max     = 1
0.00.829.459 I llama_init_from_model: n_ctx         = 128
0.00.829.459 I llama_init_from_model: n_ctx_per_seq = 128
0.00.829.460 I llama_init_from_model: n_batch       = 128
0.00.829.460 I llama_init_from_model: n_ubatch      = 128
0.00.829.460 I llama_init_from_model: flash_attn    = 0
0.00.829.461 I llama_init_from_model: freq_base     = 10000.0
0.00.829.462 I llama_init_from_model: freq_scale    = 1
0.00.829.462 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.829.463 I ggml_metal_init: allocating
0.00.829.555 I ggml_metal_init: found device: Apple M4
0.00.829.565 I ggml_metal_init: picking default device: Apple M4
0.00.830.919 I ggml_metal_init: using embedded metal library
0.00.836.073 I ggml_metal_init: GPU name:   Apple M4
0.00.836.076 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.836.076 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.836.077 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.836.077 I ggml_metal_init: simdgroup reduction   = true
0.00.836.078 I ggml_metal_init: simdgroup matrix mul. = true
0.00.836.078 I ggml_metal_init: has residency sets    = true
0.00.836.078 I ggml_metal_init: has bfloat            = true
0.00.836.079 I ggml_metal_init: use bfloat            = true
0.00.836.079 I ggml_metal_init: hasUnifiedMemory      = true
0.00.836.081 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.851.063 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.854.457 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.854.461 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.854.502 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.857.361 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.857.363 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.857.363 I llama_init_from_model: graph nodes  = 967
0.00.857.364 I llama_init_from_model: graph splits = 2
0.00.857.367 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.857.367 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.884.290 I 
0.00.884.378 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.884.389 I perplexity: tokenizing the input ..
0.00.891.783 I perplexity: tokenization took 7.391 ms
0.00.891.792 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.030.632 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.031.963 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.031.986 I llama_perf_context_print:        load time =     874.28 ms
0.01.031.987 I llama_perf_context_print: prompt eval time =     137.87 ms /   128 tokens (    1.08 ms per token,   928.39 tokens per second)
0.01.031.988 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.031.989 I llama_perf_context_print:       total time =     147.70 ms /   129 tokens
0.01.032.363 I ggml_metal_free: deallocating

real	0m1.048s
user	0m0.077s
sys	0m0.158s
```
- q4_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.056 I build: 4772 (4d1051a4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.097 I main: llama backend init
0.00.000.099 I main: load the model and apply lora adapter, if any
0.00.010.679 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.421 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.018.426 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.428 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.429 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.429 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.429 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.430 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.431 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.431 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.431 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.432 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.432 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.432 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.433 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.435 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.435 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.435 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.274 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.367 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.174 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.027.176 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.176 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.177 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.177 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.177 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.027.178 I llama_model_loader: - type  f32:  194 tensors
0.00.027.178 I llama_model_loader: - type q4_0:   97 tensors
0.00.027.178 I llama_model_loader: - type q6_K:    1 tensors
0.00.027.179 I print_info: file format = GGUF V3 (latest)
0.00.027.179 I print_info: file type   = Q4_0
0.00.027.180 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.035.470 I load: special tokens cache size = 25
0.00.041.609 I load: token to piece cache size = 0.2984 MB
0.00.041.612 I print_info: arch             = gptneox
0.00.041.612 I print_info: vocab_only       = 0
0.00.041.613 I print_info: n_ctx_train      = 2048
0.00.041.613 I print_info: n_embd           = 2048
0.00.041.613 I print_info: n_layer          = 24
0.00.041.618 I print_info: n_head           = 16
0.00.041.619 I print_info: n_head_kv        = 16
0.00.041.622 I print_info: n_rot            = 32
0.00.041.622 I print_info: n_swa            = 0
0.00.041.622 I print_info: n_embd_head_k    = 128
0.00.041.622 I print_info: n_embd_head_v    = 128
0.00.041.623 I print_info: n_gqa            = 1
0.00.041.624 I print_info: n_embd_k_gqa     = 2048
0.00.041.624 I print_info: n_embd_v_gqa     = 2048
0.00.041.625 I print_info: f_norm_eps       = 1.0e-05
0.00.041.626 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.626 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.626 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.626 I print_info: f_logit_scale    = 0.0e+00
0.00.041.627 I print_info: n_ff             = 8192
0.00.041.627 I print_info: n_expert         = 0
0.00.041.627 I print_info: n_expert_used    = 0
0.00.041.627 I print_info: causal attn      = 1
0.00.041.629 I print_info: pooling type     = 0
0.00.041.629 I print_info: rope type        = 2
0.00.041.629 I print_info: rope scaling     = linear
0.00.041.630 I print_info: freq_base_train  = 10000.0
0.00.041.630 I print_info: freq_scale_train = 1
0.00.041.630 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.630 I print_info: rope_finetuned   = unknown
0.00.041.631 I print_info: ssm_d_conv       = 0
0.00.041.631 I print_info: ssm_d_inner      = 0
0.00.041.631 I print_info: ssm_d_state      = 0
0.00.041.631 I print_info: ssm_dt_rank      = 0
0.00.041.631 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.631 I print_info: model type       = 1.4B
0.00.041.632 I print_info: model params     = 1.41 B
0.00.041.633 I print_info: general.name     = 1.4B
0.00.041.638 I print_info: vocab type       = BPE
0.00.041.640 I print_info: n_vocab          = 50304
0.00.041.640 I print_info: n_merges         = 50009
0.00.041.640 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.641 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.641 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.641 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.641 I print_info: LF token         = 187 ''
0.00.041.641 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.641 I print_info: max token length = 1024
0.00.041.642 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.583.412 I load_tensors: offloading 24 repeating layers to GPU
0.00.583.428 I load_tensors: offloading output layer to GPU
0.00.583.429 I load_tensors: offloaded 25/25 layers to GPU
0.00.583.461 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.583.462 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.585.006 I llama_init_from_model: n_seq_max     = 1
0.00.585.010 I llama_init_from_model: n_ctx         = 2048
0.00.585.011 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.585.011 I llama_init_from_model: n_batch       = 2048
0.00.585.012 I llama_init_from_model: n_ubatch      = 512
0.00.585.012 I llama_init_from_model: flash_attn    = 0
0.00.585.015 I llama_init_from_model: freq_base     = 10000.0
0.00.585.015 I llama_init_from_model: freq_scale    = 1
0.00.585.018 I ggml_metal_init: allocating
0.00.585.095 I ggml_metal_init: found device: Apple M4
0.00.585.109 I ggml_metal_init: picking default device: Apple M4
0.00.586.875 I ggml_metal_init: using embedded metal library
0.00.593.051 I ggml_metal_init: GPU name:   Apple M4
0.00.593.057 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.593.057 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.593.058 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.593.059 I ggml_metal_init: simdgroup reduction   = true
0.00.593.059 I ggml_metal_init: simdgroup matrix mul. = true
0.00.593.060 I ggml_metal_init: has residency sets    = true
0.00.593.060 I ggml_metal_init: has bfloat            = true
0.00.593.060 I ggml_metal_init: use bfloat            = true
0.00.593.061 I ggml_metal_init: hasUnifiedMemory      = true
0.00.593.063 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.612.591 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.667.281 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.667.290 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.667.331 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.671.898 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.671.900 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.671.901 I llama_init_from_model: graph nodes  = 967
0.00.671.901 I llama_init_from_model: graph splits = 2
0.00.671.906 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.672.021 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.672.021 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.726.237 I main: llama threadpool init, n_threads = 4
0.00.726.285 I 
0.00.726.311 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.726.312 I 
0.00.726.456 I sampler seed: 1234
0.00.726.460 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.726.472 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.726.472 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.726.472 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.419.086 I llama_perf_sampler_print:    sampling time =       1.43 ms /    71 runs   (    0.02 ms per token, 49754.73 tokens per second)
0.01.419.087 I llama_perf_context_print:        load time =     714.80 ms
0.01.419.087 I llama_perf_context_print: prompt eval time =      49.03 ms /     7 tokens (    7.00 ms per token,   142.78 tokens per second)
0.01.419.089 I llama_perf_context_print:        eval time =     640.68 ms /    63 runs   (   10.17 ms per token,    98.33 tokens per second)
0.01.419.089 I llama_perf_context_print:       total time =     693.60 ms /    70 tokens
0.01.419.362 I ggml_metal_free: deallocating

real	0m1.438s
user	0m0.111s
sys	0m0.202s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.251 I build: 4772 (4d1051a4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.244 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.526 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.018.531 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.533 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.537 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.537 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.538 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.538 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.539 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.539 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.540 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.540 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.540 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.541 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.541 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.543 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.543 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.543 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.337 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.527 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.265 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.027.266 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.267 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.267 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.268 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.268 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.027.269 I llama_model_loader: - type  f32:  194 tensors
0.00.027.269 I llama_model_loader: - type q4_0:   97 tensors
0.00.027.269 I llama_model_loader: - type q6_K:    1 tensors
0.00.027.270 I print_info: file format = GGUF V3 (latest)
0.00.027.270 I print_info: file type   = Q4_0
0.00.027.276 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.035.743 I load: special tokens cache size = 25
0.00.041.901 I load: token to piece cache size = 0.2984 MB
0.00.041.907 I print_info: arch             = gptneox
0.00.041.907 I print_info: vocab_only       = 0
0.00.041.909 I print_info: n_ctx_train      = 2048
0.00.041.909 I print_info: n_embd           = 2048
0.00.041.909 I print_info: n_layer          = 24
0.00.041.913 I print_info: n_head           = 16
0.00.041.916 I print_info: n_head_kv        = 16
0.00.041.916 I print_info: n_rot            = 32
0.00.041.916 I print_info: n_swa            = 0
0.00.041.916 I print_info: n_embd_head_k    = 128
0.00.041.916 I print_info: n_embd_head_v    = 128
0.00.041.917 I print_info: n_gqa            = 1
0.00.041.917 I print_info: n_embd_k_gqa     = 2048
0.00.041.918 I print_info: n_embd_v_gqa     = 2048
0.00.041.918 I print_info: f_norm_eps       = 1.0e-05
0.00.041.919 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.919 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.920 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.920 I print_info: f_logit_scale    = 0.0e+00
0.00.041.922 I print_info: n_ff             = 8192
0.00.041.923 I print_info: n_expert         = 0
0.00.041.923 I print_info: n_expert_used    = 0
0.00.041.923 I print_info: causal attn      = 1
0.00.041.924 I print_info: pooling type     = 0
0.00.041.924 I print_info: rope type        = 2
0.00.041.924 I print_info: rope scaling     = linear
0.00.041.924 I print_info: freq_base_train  = 10000.0
0.00.041.924 I print_info: freq_scale_train = 1
0.00.041.925 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.925 I print_info: rope_finetuned   = unknown
0.00.041.925 I print_info: ssm_d_conv       = 0
0.00.041.925 I print_info: ssm_d_inner      = 0
0.00.041.925 I print_info: ssm_d_state      = 0
0.00.041.925 I print_info: ssm_dt_rank      = 0
0.00.041.925 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.926 I print_info: model type       = 1.4B
0.00.041.926 I print_info: model params     = 1.41 B
0.00.041.926 I print_info: general.name     = 1.4B
0.00.041.927 I print_info: vocab type       = BPE
0.00.041.927 I print_info: n_vocab          = 50304
0.00.041.927 I print_info: n_merges         = 50009
0.00.041.928 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.928 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.928 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.930 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.930 I print_info: LF token         = 187 ''
0.00.041.930 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.930 I print_info: max token length = 1024
0.00.041.931 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.598.974 I load_tensors: offloading 24 repeating layers to GPU
0.00.598.988 I load_tensors: offloading output layer to GPU
0.00.598.989 I load_tensors: offloaded 25/25 layers to GPU
0.00.599.024 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.599.025 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.600.523 I llama_init_from_model: n_seq_max     = 1
0.00.600.526 I llama_init_from_model: n_ctx         = 128
0.00.600.526 I llama_init_from_model: n_ctx_per_seq = 128
0.00.600.527 I llama_init_from_model: n_batch       = 128
0.00.600.527 I llama_init_from_model: n_ubatch      = 128
0.00.600.527 I llama_init_from_model: flash_attn    = 0
0.00.600.530 I llama_init_from_model: freq_base     = 10000.0
0.00.600.530 I llama_init_from_model: freq_scale    = 1
0.00.600.531 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.600.533 I ggml_metal_init: allocating
0.00.600.621 I ggml_metal_init: found device: Apple M4
0.00.600.635 I ggml_metal_init: picking default device: Apple M4
0.00.602.488 I ggml_metal_init: using embedded metal library
0.00.607.969 I ggml_metal_init: GPU name:   Apple M4
0.00.607.977 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.607.978 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.607.978 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.607.979 I ggml_metal_init: simdgroup reduction   = true
0.00.607.980 I ggml_metal_init: simdgroup matrix mul. = true
0.00.607.980 I ggml_metal_init: has residency sets    = true
0.00.607.980 I ggml_metal_init: has bfloat            = true
0.00.607.980 I ggml_metal_init: use bfloat            = true
0.00.607.982 I ggml_metal_init: hasUnifiedMemory      = true
0.00.607.984 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.627.484 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.631.130 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.631.137 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.631.188 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.634.532 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.634.534 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.634.535 I llama_init_from_model: graph nodes  = 967
0.00.634.535 I llama_init_from_model: graph splits = 2
0.00.634.538 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.634.538 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.664.050 I 
0.00.664.100 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.664.105 I perplexity: tokenizing the input ..
0.00.668.797 I perplexity: tokenization took 4.69 ms
0.00.668.801 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.792.453 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.793.804 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.793.821 I llama_perf_context_print:        load time =     652.80 ms
0.00.793.822 I llama_perf_context_print: prompt eval time =     123.43 ms /   128 tokens (    0.96 ms per token,  1037.06 tokens per second)
0.00.793.823 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.793.823 I llama_perf_context_print:       total time =     129.77 ms /   129 tokens
0.00.794.197 I ggml_metal_free: deallocating

real	0m0.811s
user	0m0.077s
sys	0m0.142s
```
- q4_1:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.052 I build: 4772 (4d1051a4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.089 I main: llama backend init
0.00.000.092 I main: load the model and apply lora adapter, if any
0.00.008.749 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.131 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.017.142 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.144 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.144 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.145 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.146 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.146 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.147 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.147 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.148 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.148 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.148 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.149 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.149 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.151 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.151 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.152 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.928 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.993 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.685 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.686 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.686 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.687 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.687 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.687 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.025.688 I llama_model_loader: - type  f32:  194 tensors
0.00.025.688 I llama_model_loader: - type q4_1:   97 tensors
0.00.025.688 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.689 I print_info: file format = GGUF V3 (latest)
0.00.025.689 I print_info: file type   = Q4_1
0.00.025.690 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.033.624 I load: special tokens cache size = 25
0.00.039.624 I load: token to piece cache size = 0.2984 MB
0.00.039.627 I print_info: arch             = gptneox
0.00.039.627 I print_info: vocab_only       = 0
0.00.039.627 I print_info: n_ctx_train      = 2048
0.00.039.628 I print_info: n_embd           = 2048
0.00.039.628 I print_info: n_layer          = 24
0.00.039.631 I print_info: n_head           = 16
0.00.039.631 I print_info: n_head_kv        = 16
0.00.039.632 I print_info: n_rot            = 32
0.00.039.633 I print_info: n_swa            = 0
0.00.039.634 I print_info: n_embd_head_k    = 128
0.00.039.634 I print_info: n_embd_head_v    = 128
0.00.039.635 I print_info: n_gqa            = 1
0.00.039.635 I print_info: n_embd_k_gqa     = 2048
0.00.039.640 I print_info: n_embd_v_gqa     = 2048
0.00.039.641 I print_info: f_norm_eps       = 1.0e-05
0.00.039.641 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.642 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.642 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.642 I print_info: f_logit_scale    = 0.0e+00
0.00.039.643 I print_info: n_ff             = 8192
0.00.039.647 I print_info: n_expert         = 0
0.00.039.648 I print_info: n_expert_used    = 0
0.00.039.648 I print_info: causal attn      = 1
0.00.039.649 I print_info: pooling type     = 0
0.00.039.649 I print_info: rope type        = 2
0.00.039.649 I print_info: rope scaling     = linear
0.00.039.650 I print_info: freq_base_train  = 10000.0
0.00.039.650 I print_info: freq_scale_train = 1
0.00.039.650 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.650 I print_info: rope_finetuned   = unknown
0.00.039.650 I print_info: ssm_d_conv       = 0
0.00.039.651 I print_info: ssm_d_inner      = 0
0.00.039.651 I print_info: ssm_d_state      = 0
0.00.039.651 I print_info: ssm_dt_rank      = 0
0.00.039.651 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.651 I print_info: model type       = 1.4B
0.00.039.652 I print_info: model params     = 1.41 B
0.00.039.652 I print_info: general.name     = 1.4B
0.00.039.652 I print_info: vocab type       = BPE
0.00.039.652 I print_info: n_vocab          = 50304
0.00.039.652 I print_info: n_merges         = 50009
0.00.039.653 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.653 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.653 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.653 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.653 I print_info: LF token         = 187 ''
0.00.039.655 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.656 I print_info: max token length = 1024
0.00.039.656 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.659.167 I load_tensors: offloading 24 repeating layers to GPU
0.00.659.180 I load_tensors: offloading output layer to GPU
0.00.659.181 I load_tensors: offloaded 25/25 layers to GPU
0.00.659.213 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.659.221 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.660.826 I llama_init_from_model: n_seq_max     = 1
0.00.660.828 I llama_init_from_model: n_ctx         = 2048
0.00.660.829 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.660.830 I llama_init_from_model: n_batch       = 2048
0.00.660.830 I llama_init_from_model: n_ubatch      = 512
0.00.660.830 I llama_init_from_model: flash_attn    = 0
0.00.660.833 I llama_init_from_model: freq_base     = 10000.0
0.00.660.834 I llama_init_from_model: freq_scale    = 1
0.00.660.836 I ggml_metal_init: allocating
0.00.660.912 I ggml_metal_init: found device: Apple M4
0.00.660.925 I ggml_metal_init: picking default device: Apple M4
0.00.662.781 I ggml_metal_init: using embedded metal library
0.00.669.182 I ggml_metal_init: GPU name:   Apple M4
0.00.669.187 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.669.188 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.669.189 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.669.189 I ggml_metal_init: simdgroup reduction   = true
0.00.669.190 I ggml_metal_init: simdgroup matrix mul. = true
0.00.669.190 I ggml_metal_init: has residency sets    = true
0.00.669.190 I ggml_metal_init: has bfloat            = true
0.00.669.190 I ggml_metal_init: use bfloat            = true
0.00.669.191 I ggml_metal_init: hasUnifiedMemory      = true
0.00.669.193 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.688.458 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.741.324 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.741.333 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.741.373 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.745.738 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.745.740 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.745.741 I llama_init_from_model: graph nodes  = 967
0.00.745.741 I llama_init_from_model: graph splits = 2
0.00.745.746 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.745.869 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.745.870 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.802.147 I main: llama threadpool init, n_threads = 4
0.00.802.206 I 
0.00.802.231 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.802.233 I 
0.00.802.387 I sampler seed: 1234
0.00.802.392 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.802.413 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.802.413 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.802.413 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.533.565 I llama_perf_sampler_print:    sampling time =       1.22 ms /    71 runs   (    0.02 ms per token, 58244.46 tokens per second)
0.01.533.565 I llama_perf_context_print:        load time =     792.66 ms
0.01.533.568 I llama_perf_context_print: prompt eval time =      48.94 ms /     7 tokens (    6.99 ms per token,   143.04 tokens per second)
0.01.533.570 I llama_perf_context_print:        eval time =     679.57 ms /    63 runs   (   10.79 ms per token,    92.71 tokens per second)
0.01.533.573 I llama_perf_context_print:       total time =     732.15 ms /    70 tokens
0.01.533.870 I ggml_metal_free: deallocating

real	0m1.550s
user	0m0.110s
sys	0m0.204s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.108 I build: 4772 (4d1051a4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.965 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.079 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.084 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.086 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.087 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.087 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.088 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.088 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.089 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.089 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.089 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.090 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.090 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.090 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.091 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.093 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.093 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.094 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.876 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.094 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.923 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.924 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.924 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.925 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.925 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.926 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.024.926 I llama_model_loader: - type  f32:  194 tensors
0.00.024.926 I llama_model_loader: - type q4_1:   97 tensors
0.00.024.927 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.927 I print_info: file format = GGUF V3 (latest)
0.00.024.928 I print_info: file type   = Q4_1
0.00.024.931 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.033.500 I load: special tokens cache size = 25
0.00.039.611 I load: token to piece cache size = 0.2984 MB
0.00.039.616 I print_info: arch             = gptneox
0.00.039.616 I print_info: vocab_only       = 0
0.00.039.616 I print_info: n_ctx_train      = 2048
0.00.039.616 I print_info: n_embd           = 2048
0.00.039.617 I print_info: n_layer          = 24
0.00.039.621 I print_info: n_head           = 16
0.00.039.622 I print_info: n_head_kv        = 16
0.00.039.622 I print_info: n_rot            = 32
0.00.039.622 I print_info: n_swa            = 0
0.00.039.622 I print_info: n_embd_head_k    = 128
0.00.039.622 I print_info: n_embd_head_v    = 128
0.00.039.623 I print_info: n_gqa            = 1
0.00.039.624 I print_info: n_embd_k_gqa     = 2048
0.00.039.624 I print_info: n_embd_v_gqa     = 2048
0.00.039.625 I print_info: f_norm_eps       = 1.0e-05
0.00.039.625 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.625 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.626 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.626 I print_info: f_logit_scale    = 0.0e+00
0.00.039.626 I print_info: n_ff             = 8192
0.00.039.627 I print_info: n_expert         = 0
0.00.039.627 I print_info: n_expert_used    = 0
0.00.039.627 I print_info: causal attn      = 1
0.00.039.627 I print_info: pooling type     = 0
0.00.039.627 I print_info: rope type        = 2
0.00.039.627 I print_info: rope scaling     = linear
0.00.039.628 I print_info: freq_base_train  = 10000.0
0.00.039.628 I print_info: freq_scale_train = 1
0.00.039.628 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.628 I print_info: rope_finetuned   = unknown
0.00.039.628 I print_info: ssm_d_conv       = 0
0.00.039.628 I print_info: ssm_d_inner      = 0
0.00.039.629 I print_info: ssm_d_state      = 0
0.00.039.629 I print_info: ssm_dt_rank      = 0
0.00.039.629 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.629 I print_info: model type       = 1.4B
0.00.039.629 I print_info: model params     = 1.41 B
0.00.039.630 I print_info: general.name     = 1.4B
0.00.039.630 I print_info: vocab type       = BPE
0.00.039.630 I print_info: n_vocab          = 50304
0.00.039.630 I print_info: n_merges         = 50009
0.00.039.631 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.631 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.631 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.634 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.634 I print_info: LF token         = 187 ''
0.00.039.635 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.635 I print_info: max token length = 1024
0.00.039.635 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.655.068 I load_tensors: offloading 24 repeating layers to GPU
0.00.655.088 I load_tensors: offloading output layer to GPU
0.00.655.088 I load_tensors: offloaded 25/25 layers to GPU
0.00.655.122 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.655.124 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.656.788 I llama_init_from_model: n_seq_max     = 1
0.00.656.791 I llama_init_from_model: n_ctx         = 128
0.00.656.792 I llama_init_from_model: n_ctx_per_seq = 128
0.00.656.792 I llama_init_from_model: n_batch       = 128
0.00.656.793 I llama_init_from_model: n_ubatch      = 128
0.00.656.793 I llama_init_from_model: flash_attn    = 0
0.00.656.795 I llama_init_from_model: freq_base     = 10000.0
0.00.656.796 I llama_init_from_model: freq_scale    = 1
0.00.656.796 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.656.798 I ggml_metal_init: allocating
0.00.656.880 I ggml_metal_init: found device: Apple M4
0.00.656.894 I ggml_metal_init: picking default device: Apple M4
0.00.658.690 I ggml_metal_init: using embedded metal library
0.00.665.486 I ggml_metal_init: GPU name:   Apple M4
0.00.665.496 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.665.496 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.665.497 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.665.498 I ggml_metal_init: simdgroup reduction   = true
0.00.665.498 I ggml_metal_init: simdgroup matrix mul. = true
0.00.665.498 I ggml_metal_init: has residency sets    = true
0.00.665.498 I ggml_metal_init: has bfloat            = true
0.00.665.499 I ggml_metal_init: use bfloat            = true
0.00.665.500 I ggml_metal_init: hasUnifiedMemory      = true
0.00.665.504 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.684.179 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.687.695 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.687.710 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.687.753 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.691.009 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.691.010 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.691.011 I llama_init_from_model: graph nodes  = 967
0.00.691.011 I llama_init_from_model: graph splits = 2
0.00.691.015 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.691.015 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.719.907 I 
0.00.719.993 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.720.001 I perplexity: tokenizing the input ..
0.00.727.290 I perplexity: tokenization took 7.284 ms
0.00.727.298 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.863.552 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.864.925 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.864.948 I llama_perf_context_print:        load time =     710.93 ms
0.00.864.949 I llama_perf_context_print: prompt eval time =     135.35 ms /   128 tokens (    1.06 ms per token,   945.72 tokens per second)
0.00.864.950 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.864.950 I llama_perf_context_print:       total time =     145.05 ms /   129 tokens
0.00.865.329 I ggml_metal_free: deallocating

real	0m0.880s
user	0m0.081s
sys	0m0.131s
```
- q5_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.050 I build: 4772 (4d1051a4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.086 I main: llama backend init
0.00.000.088 I main: load the model and apply lora adapter, if any
0.00.010.071 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.029 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.018.034 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.035 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.036 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.036 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.037 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.038 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.039 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.039 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.040 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.040 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.042 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.042 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.043 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.044 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.048 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.048 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.788 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.920 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.650 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.652 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.652 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.652 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.653 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.653 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.026.654 I llama_model_loader: - type  f32:  194 tensors
0.00.026.654 I llama_model_loader: - type q5_0:   97 tensors
0.00.026.654 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.655 I print_info: file format = GGUF V3 (latest)
0.00.026.655 I print_info: file type   = Q5_0
0.00.026.656 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.034.622 I load: special tokens cache size = 25
0.00.040.350 I load: token to piece cache size = 0.2984 MB
0.00.040.353 I print_info: arch             = gptneox
0.00.040.353 I print_info: vocab_only       = 0
0.00.040.354 I print_info: n_ctx_train      = 2048
0.00.040.354 I print_info: n_embd           = 2048
0.00.040.354 I print_info: n_layer          = 24
0.00.040.357 I print_info: n_head           = 16
0.00.040.358 I print_info: n_head_kv        = 16
0.00.040.358 I print_info: n_rot            = 32
0.00.040.358 I print_info: n_swa            = 0
0.00.040.358 I print_info: n_embd_head_k    = 128
0.00.040.358 I print_info: n_embd_head_v    = 128
0.00.040.359 I print_info: n_gqa            = 1
0.00.040.360 I print_info: n_embd_k_gqa     = 2048
0.00.040.361 I print_info: n_embd_v_gqa     = 2048
0.00.040.361 I print_info: f_norm_eps       = 1.0e-05
0.00.040.361 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.362 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.363 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.365 I print_info: f_logit_scale    = 0.0e+00
0.00.040.365 I print_info: n_ff             = 8192
0.00.040.365 I print_info: n_expert         = 0
0.00.040.366 I print_info: n_expert_used    = 0
0.00.040.367 I print_info: causal attn      = 1
0.00.040.367 I print_info: pooling type     = 0
0.00.040.367 I print_info: rope type        = 2
0.00.040.368 I print_info: rope scaling     = linear
0.00.040.368 I print_info: freq_base_train  = 10000.0
0.00.040.368 I print_info: freq_scale_train = 1
0.00.040.368 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.369 I print_info: rope_finetuned   = unknown
0.00.040.369 I print_info: ssm_d_conv       = 0
0.00.040.369 I print_info: ssm_d_inner      = 0
0.00.040.369 I print_info: ssm_d_state      = 0
0.00.040.369 I print_info: ssm_dt_rank      = 0
0.00.040.369 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.369 I print_info: model type       = 1.4B
0.00.040.370 I print_info: model params     = 1.41 B
0.00.040.370 I print_info: general.name     = 1.4B
0.00.040.370 I print_info: vocab type       = BPE
0.00.040.371 I print_info: n_vocab          = 50304
0.00.040.371 I print_info: n_merges         = 50009
0.00.040.371 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.371 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.371 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.372 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.372 I print_info: LF token         = 187 ''
0.00.040.372 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.372 I print_info: max token length = 1024
0.00.040.373 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.693.645 I load_tensors: offloading 24 repeating layers to GPU
0.00.693.655 I load_tensors: offloading output layer to GPU
0.00.693.656 I load_tensors: offloaded 25/25 layers to GPU
0.00.693.693 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.693.695 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.695.238 I llama_init_from_model: n_seq_max     = 1
0.00.695.240 I llama_init_from_model: n_ctx         = 2048
0.00.695.240 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.695.241 I llama_init_from_model: n_batch       = 2048
0.00.695.242 I llama_init_from_model: n_ubatch      = 512
0.00.695.242 I llama_init_from_model: flash_attn    = 0
0.00.695.243 I llama_init_from_model: freq_base     = 10000.0
0.00.695.244 I llama_init_from_model: freq_scale    = 1
0.00.695.245 I ggml_metal_init: allocating
0.00.695.266 I ggml_metal_init: found device: Apple M4
0.00.695.276 I ggml_metal_init: picking default device: Apple M4
0.00.696.846 I ggml_metal_init: using embedded metal library
0.00.703.032 I ggml_metal_init: GPU name:   Apple M4
0.00.703.036 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.703.036 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.703.037 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.703.038 I ggml_metal_init: simdgroup reduction   = true
0.00.703.038 I ggml_metal_init: simdgroup matrix mul. = true
0.00.703.038 I ggml_metal_init: has residency sets    = true
0.00.703.038 I ggml_metal_init: has bfloat            = true
0.00.703.039 I ggml_metal_init: use bfloat            = true
0.00.703.039 I ggml_metal_init: hasUnifiedMemory      = true
0.00.703.041 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.721.306 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.773.003 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.773.015 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.773.050 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.777.181 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.777.183 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.777.184 I llama_init_from_model: graph nodes  = 967
0.00.777.184 I llama_init_from_model: graph splits = 2
0.00.777.188 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.777.315 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.777.316 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.836.361 I main: llama threadpool init, n_threads = 4
0.00.836.406 I 
0.00.836.430 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.836.432 I 
0.00.836.576 I sampler seed: 1234
0.00.836.580 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.836.604 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.836.605 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.836.605 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.626.189 I llama_perf_sampler_print:    sampling time =       1.38 ms /    71 runs   (    0.02 ms per token, 51300.58 tokens per second)
0.01.626.190 I llama_perf_context_print:        load time =     825.57 ms
0.01.626.191 I llama_perf_context_print: prompt eval time =      53.16 ms /     7 tokens (    7.59 ms per token,   131.69 tokens per second)
0.01.626.192 I llama_perf_context_print:        eval time =     733.56 ms /    63 runs   (   11.64 ms per token,    85.88 tokens per second)
0.01.626.192 I llama_perf_context_print:       total time =     790.55 ms /    70 tokens
0.01.626.450 I ggml_metal_free: deallocating

real	0m1.645s
user	0m0.109s
sys	0m0.204s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.105 I build: 4772 (4d1051a4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.969 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.415 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.017.420 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.424 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.425 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.425 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.425 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.426 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.427 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.427 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.428 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.428 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.428 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.429 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.429 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.431 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.431 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.432 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.129 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.183 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.930 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.932 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.932 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.932 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.933 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.933 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.025.934 I llama_model_loader: - type  f32:  194 tensors
0.00.025.934 I llama_model_loader: - type q5_0:   97 tensors
0.00.025.934 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.935 I print_info: file format = GGUF V3 (latest)
0.00.025.935 I print_info: file type   = Q5_0
0.00.025.937 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.034.445 I load: special tokens cache size = 25
0.00.040.223 I load: token to piece cache size = 0.2984 MB
0.00.040.230 I print_info: arch             = gptneox
0.00.040.230 I print_info: vocab_only       = 0
0.00.040.230 I print_info: n_ctx_train      = 2048
0.00.040.234 I print_info: n_embd           = 2048
0.00.040.234 I print_info: n_layer          = 24
0.00.040.238 I print_info: n_head           = 16
0.00.040.244 I print_info: n_head_kv        = 16
0.00.040.245 I print_info: n_rot            = 32
0.00.040.245 I print_info: n_swa            = 0
0.00.040.246 I print_info: n_embd_head_k    = 128
0.00.040.246 I print_info: n_embd_head_v    = 128
0.00.040.247 I print_info: n_gqa            = 1
0.00.040.248 I print_info: n_embd_k_gqa     = 2048
0.00.040.250 I print_info: n_embd_v_gqa     = 2048
0.00.040.250 I print_info: f_norm_eps       = 1.0e-05
0.00.040.251 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.251 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.251 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.251 I print_info: f_logit_scale    = 0.0e+00
0.00.040.252 I print_info: n_ff             = 8192
0.00.040.252 I print_info: n_expert         = 0
0.00.040.252 I print_info: n_expert_used    = 0
0.00.040.252 I print_info: causal attn      = 1
0.00.040.252 I print_info: pooling type     = 0
0.00.040.252 I print_info: rope type        = 2
0.00.040.253 I print_info: rope scaling     = linear
0.00.040.253 I print_info: freq_base_train  = 10000.0
0.00.040.253 I print_info: freq_scale_train = 1
0.00.040.254 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.255 I print_info: rope_finetuned   = unknown
0.00.040.255 I print_info: ssm_d_conv       = 0
0.00.040.255 I print_info: ssm_d_inner      = 0
0.00.040.255 I print_info: ssm_d_state      = 0
0.00.040.255 I print_info: ssm_dt_rank      = 0
0.00.040.255 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.256 I print_info: model type       = 1.4B
0.00.040.256 I print_info: model params     = 1.41 B
0.00.040.256 I print_info: general.name     = 1.4B
0.00.040.257 I print_info: vocab type       = BPE
0.00.040.257 I print_info: n_vocab          = 50304
0.00.040.257 I print_info: n_merges         = 50009
0.00.040.257 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.257 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.259 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.259 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.259 I print_info: LF token         = 187 ''
0.00.040.259 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.260 I print_info: max token length = 1024
0.00.040.260 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.685.815 I load_tensors: offloading 24 repeating layers to GPU
0.00.685.829 I load_tensors: offloading output layer to GPU
0.00.685.830 I load_tensors: offloaded 25/25 layers to GPU
0.00.685.864 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.685.871 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.687.427 I llama_init_from_model: n_seq_max     = 1
0.00.687.430 I llama_init_from_model: n_ctx         = 128
0.00.687.430 I llama_init_from_model: n_ctx_per_seq = 128
0.00.687.431 I llama_init_from_model: n_batch       = 128
0.00.687.431 I llama_init_from_model: n_ubatch      = 128
0.00.687.432 I llama_init_from_model: flash_attn    = 0
0.00.687.434 I llama_init_from_model: freq_base     = 10000.0
0.00.687.435 I llama_init_from_model: freq_scale    = 1
0.00.687.435 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.687.437 I ggml_metal_init: allocating
0.00.687.516 I ggml_metal_init: found device: Apple M4
0.00.687.530 I ggml_metal_init: picking default device: Apple M4
0.00.689.294 I ggml_metal_init: using embedded metal library
0.00.696.126 I ggml_metal_init: GPU name:   Apple M4
0.00.696.134 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.696.135 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.696.135 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.696.136 I ggml_metal_init: simdgroup reduction   = true
0.00.696.136 I ggml_metal_init: simdgroup matrix mul. = true
0.00.696.136 I ggml_metal_init: has residency sets    = true
0.00.696.137 I ggml_metal_init: has bfloat            = true
0.00.696.137 I ggml_metal_init: use bfloat            = true
0.00.696.138 I ggml_metal_init: hasUnifiedMemory      = true
0.00.696.141 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.713.767 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.717.164 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.717.167 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.717.215 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.720.516 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.720.518 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.720.519 I llama_init_from_model: graph nodes  = 967
0.00.720.519 I llama_init_from_model: graph splits = 2
0.00.720.523 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.720.523 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.746.985 I 
0.00.747.056 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.747.063 I perplexity: tokenizing the input ..
0.00.753.869 I perplexity: tokenization took 6.804 ms
0.00.753.882 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.889.809 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.891.358 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.891.381 I llama_perf_context_print:        load time =     737.00 ms
0.00.891.382 I llama_perf_context_print: prompt eval time =     134.96 ms /   128 tokens (    1.05 ms per token,   948.42 tokens per second)
0.00.891.382 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.891.383 I llama_perf_context_print:       total time =     144.40 ms /   129 tokens
0.00.891.744 I ggml_metal_free: deallocating

real	0m0.908s
user	0m0.079s
sys	0m0.129s
```
- q5_1:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.044 I build: 4772 (4d1051a4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.078 I main: llama backend init
0.00.000.080 I main: load the model and apply lora adapter, if any
0.00.008.708 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.163 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.167 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.172 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.173 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.173 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.174 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.174 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.176 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.177 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.177 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.177 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.178 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.178 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.179 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.181 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.181 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.181 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.914 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.968 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.665 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.666 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.667 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.667 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.667 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.668 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.668 I llama_model_loader: - type  f32:  194 tensors
0.00.024.668 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.669 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.669 I print_info: file format = GGUF V3 (latest)
0.00.024.670 I print_info: file type   = Q5_1
0.00.024.670 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.032.532 I load: special tokens cache size = 25
0.00.038.324 I load: token to piece cache size = 0.2984 MB
0.00.038.327 I print_info: arch             = gptneox
0.00.038.327 I print_info: vocab_only       = 0
0.00.038.327 I print_info: n_ctx_train      = 2048
0.00.038.328 I print_info: n_embd           = 2048
0.00.038.328 I print_info: n_layer          = 24
0.00.038.330 I print_info: n_head           = 16
0.00.038.331 I print_info: n_head_kv        = 16
0.00.038.331 I print_info: n_rot            = 32
0.00.038.331 I print_info: n_swa            = 0
0.00.038.331 I print_info: n_embd_head_k    = 128
0.00.038.333 I print_info: n_embd_head_v    = 128
0.00.038.334 I print_info: n_gqa            = 1
0.00.038.335 I print_info: n_embd_k_gqa     = 2048
0.00.038.335 I print_info: n_embd_v_gqa     = 2048
0.00.038.336 I print_info: f_norm_eps       = 1.0e-05
0.00.038.336 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.336 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.336 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.337 I print_info: f_logit_scale    = 0.0e+00
0.00.038.337 I print_info: n_ff             = 8192
0.00.038.337 I print_info: n_expert         = 0
0.00.038.337 I print_info: n_expert_used    = 0
0.00.038.338 I print_info: causal attn      = 1
0.00.038.338 I print_info: pooling type     = 0
0.00.038.338 I print_info: rope type        = 2
0.00.038.338 I print_info: rope scaling     = linear
0.00.038.339 I print_info: freq_base_train  = 10000.0
0.00.038.339 I print_info: freq_scale_train = 1
0.00.038.339 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.340 I print_info: rope_finetuned   = unknown
0.00.038.344 I print_info: ssm_d_conv       = 0
0.00.038.344 I print_info: ssm_d_inner      = 0
0.00.038.344 I print_info: ssm_d_state      = 0
0.00.038.344 I print_info: ssm_dt_rank      = 0
0.00.038.344 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.346 I print_info: model type       = 1.4B
0.00.038.346 I print_info: model params     = 1.41 B
0.00.038.346 I print_info: general.name     = 1.4B
0.00.038.347 I print_info: vocab type       = BPE
0.00.038.347 I print_info: n_vocab          = 50304
0.00.038.347 I print_info: n_merges         = 50009
0.00.038.347 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.348 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.348 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.348 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.348 I print_info: LF token         = 187 ''
0.00.038.349 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.349 I print_info: max token length = 1024
0.00.038.349 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.636.287 I load_tensors: offloading 24 repeating layers to GPU
0.00.636.299 I load_tensors: offloading output layer to GPU
0.00.636.300 I load_tensors: offloaded 25/25 layers to GPU
0.00.636.340 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.636.341 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.638.165 I llama_init_from_model: n_seq_max     = 1
0.00.638.168 I llama_init_from_model: n_ctx         = 2048
0.00.638.168 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.638.169 I llama_init_from_model: n_batch       = 2048
0.00.638.169 I llama_init_from_model: n_ubatch      = 512
0.00.638.170 I llama_init_from_model: flash_attn    = 0
0.00.638.171 I llama_init_from_model: freq_base     = 10000.0
0.00.638.172 I llama_init_from_model: freq_scale    = 1
0.00.638.174 I ggml_metal_init: allocating
0.00.638.283 I ggml_metal_init: found device: Apple M4
0.00.638.296 I ggml_metal_init: picking default device: Apple M4
0.00.640.019 I ggml_metal_init: using embedded metal library
0.00.646.543 I ggml_metal_init: GPU name:   Apple M4
0.00.646.547 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.646.548 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.646.548 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.646.549 I ggml_metal_init: simdgroup reduction   = true
0.00.646.549 I ggml_metal_init: simdgroup matrix mul. = true
0.00.646.549 I ggml_metal_init: has residency sets    = true
0.00.646.549 I ggml_metal_init: has bfloat            = true
0.00.646.550 I ggml_metal_init: use bfloat            = true
0.00.646.551 I ggml_metal_init: hasUnifiedMemory      = true
0.00.646.552 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.664.408 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.717.197 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.717.203 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.717.238 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.722.231 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.722.233 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.722.233 I llama_init_from_model: graph nodes  = 967
0.00.722.233 I llama_init_from_model: graph splits = 2
0.00.722.238 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.722.367 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.722.368 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.779.519 I main: llama threadpool init, n_threads = 4
0.00.779.569 I 
0.00.779.591 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.779.591 I 
0.00.779.736 I sampler seed: 1234
0.00.779.741 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.779.787 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.779.791 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.779.791 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.609.576 I llama_perf_sampler_print:    sampling time =       1.39 ms /    71 runs   (    0.02 ms per token, 51115.91 tokens per second)
0.01.609.577 I llama_perf_context_print:        load time =     770.07 ms
0.01.609.580 I llama_perf_context_print: prompt eval time =      42.23 ms /     7 tokens (    6.03 ms per token,   165.78 tokens per second)
0.01.609.580 I llama_perf_context_print:        eval time =     784.58 ms /    63 runs   (   12.45 ms per token,    80.30 tokens per second)
0.01.609.581 I llama_perf_context_print:       total time =     830.79 ms /    70 tokens
0.01.609.828 I ggml_metal_free: deallocating

real	0m1.626s
user	0m0.110s
sys	0m0.230s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.111 I build: 4772 (4d1051a4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.022 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.775 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.015.781 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.783 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.783 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.783 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.784 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.784 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.785 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.785 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.786 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.786 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.786 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.786 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.789 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.792 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.793 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.793 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.580 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.696 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.537 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.539 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.539 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.540 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.540 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.540 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.541 I llama_model_loader: - type  f32:  194 tensors
0.00.024.541 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.542 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.542 I print_info: file format = GGUF V3 (latest)
0.00.024.548 I print_info: file type   = Q5_1
0.00.024.548 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.032.683 I load: special tokens cache size = 25
0.00.038.507 I load: token to piece cache size = 0.2984 MB
0.00.038.512 I print_info: arch             = gptneox
0.00.038.512 I print_info: vocab_only       = 0
0.00.038.512 I print_info: n_ctx_train      = 2048
0.00.038.512 I print_info: n_embd           = 2048
0.00.038.512 I print_info: n_layer          = 24
0.00.038.517 I print_info: n_head           = 16
0.00.038.517 I print_info: n_head_kv        = 16
0.00.038.518 I print_info: n_rot            = 32
0.00.038.518 I print_info: n_swa            = 0
0.00.038.518 I print_info: n_embd_head_k    = 128
0.00.038.521 I print_info: n_embd_head_v    = 128
0.00.038.522 I print_info: n_gqa            = 1
0.00.038.522 I print_info: n_embd_k_gqa     = 2048
0.00.038.523 I print_info: n_embd_v_gqa     = 2048
0.00.038.523 I print_info: f_norm_eps       = 1.0e-05
0.00.038.524 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.524 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.524 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.524 I print_info: f_logit_scale    = 0.0e+00
0.00.038.525 I print_info: n_ff             = 8192
0.00.038.525 I print_info: n_expert         = 0
0.00.038.525 I print_info: n_expert_used    = 0
0.00.038.525 I print_info: causal attn      = 1
0.00.038.525 I print_info: pooling type     = 0
0.00.038.526 I print_info: rope type        = 2
0.00.038.527 I print_info: rope scaling     = linear
0.00.038.528 I print_info: freq_base_train  = 10000.0
0.00.038.530 I print_info: freq_scale_train = 1
0.00.038.530 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.530 I print_info: rope_finetuned   = unknown
0.00.038.531 I print_info: ssm_d_conv       = 0
0.00.038.531 I print_info: ssm_d_inner      = 0
0.00.038.531 I print_info: ssm_d_state      = 0
0.00.038.531 I print_info: ssm_dt_rank      = 0
0.00.038.531 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.531 I print_info: model type       = 1.4B
0.00.038.531 I print_info: model params     = 1.41 B
0.00.038.531 I print_info: general.name     = 1.4B
0.00.038.532 I print_info: vocab type       = BPE
0.00.038.532 I print_info: n_vocab          = 50304
0.00.038.533 I print_info: n_merges         = 50009
0.00.038.533 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.535 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.535 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.535 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.535 I print_info: LF token         = 187 ''
0.00.038.536 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.536 I print_info: max token length = 1024
0.00.038.536 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.602.964 I load_tensors: offloading 24 repeating layers to GPU
0.00.602.981 I load_tensors: offloading output layer to GPU
0.00.602.981 I load_tensors: offloaded 25/25 layers to GPU
0.00.603.013 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.603.015 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.604.507 I llama_init_from_model: n_seq_max     = 1
0.00.604.510 I llama_init_from_model: n_ctx         = 128
0.00.604.510 I llama_init_from_model: n_ctx_per_seq = 128
0.00.604.511 I llama_init_from_model: n_batch       = 128
0.00.604.511 I llama_init_from_model: n_ubatch      = 128
0.00.604.512 I llama_init_from_model: flash_attn    = 0
0.00.604.514 I llama_init_from_model: freq_base     = 10000.0
0.00.604.515 I llama_init_from_model: freq_scale    = 1
0.00.604.515 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.604.518 I ggml_metal_init: allocating
0.00.604.612 I ggml_metal_init: found device: Apple M4
0.00.604.626 I ggml_metal_init: picking default device: Apple M4
0.00.606.233 I ggml_metal_init: using embedded metal library
0.00.612.741 I ggml_metal_init: GPU name:   Apple M4
0.00.612.746 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.612.747 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.612.747 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.612.748 I ggml_metal_init: simdgroup reduction   = true
0.00.612.748 I ggml_metal_init: simdgroup matrix mul. = true
0.00.612.749 I ggml_metal_init: has residency sets    = true
0.00.612.749 I ggml_metal_init: has bfloat            = true
0.00.612.749 I ggml_metal_init: use bfloat            = true
0.00.612.750 I ggml_metal_init: hasUnifiedMemory      = true
0.00.612.754 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.630.159 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.633.708 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.633.714 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.633.786 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.637.010 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.637.012 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.637.012 I llama_init_from_model: graph nodes  = 967
0.00.637.012 I llama_init_from_model: graph splits = 2
0.00.637.015 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.637.015 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.668.774 I 
0.00.668.859 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.668.868 I perplexity: tokenizing the input ..
0.00.675.841 I perplexity: tokenization took 6.968 ms
0.00.675.849 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.824.773 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.826.116 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.826.137 I llama_perf_context_print:        load time =     659.74 ms
0.00.826.138 I llama_perf_context_print: prompt eval time =     147.98 ms /   128 tokens (    1.16 ms per token,   864.98 tokens per second)
0.00.826.138 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.826.139 I llama_perf_context_print:       total time =     157.37 ms /   129 tokens
0.00.826.519 I ggml_metal_free: deallocating

real	0m0.841s
user	0m0.079s
sys	0m0.140s
```
- q2_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.052 I build: 4772 (4d1051a4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.089 I main: llama backend init
0.00.000.091 I main: load the model and apply lora adapter, if any
0.00.009.786 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.548 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.553 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.555 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.555 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.556 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.556 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.556 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.558 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.559 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.559 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.559 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.560 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.560 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.561 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.562 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.562 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.562 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.332 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.401 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.179 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.180 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.181 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.181 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.181 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.182 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.182 I llama_model_loader: - type  f32:  194 tensors
0.00.025.182 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.183 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.183 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.184 I print_info: file format = GGUF V3 (latest)
0.00.025.184 I print_info: file type   = Q2_K - Medium
0.00.025.185 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.033.352 I load: special tokens cache size = 25
0.00.039.256 I load: token to piece cache size = 0.2984 MB
0.00.039.259 I print_info: arch             = gptneox
0.00.039.259 I print_info: vocab_only       = 0
0.00.039.259 I print_info: n_ctx_train      = 2048
0.00.039.259 I print_info: n_embd           = 2048
0.00.039.260 I print_info: n_layer          = 24
0.00.039.263 I print_info: n_head           = 16
0.00.039.263 I print_info: n_head_kv        = 16
0.00.039.263 I print_info: n_rot            = 32
0.00.039.265 I print_info: n_swa            = 0
0.00.039.265 I print_info: n_embd_head_k    = 128
0.00.039.265 I print_info: n_embd_head_v    = 128
0.00.039.266 I print_info: n_gqa            = 1
0.00.039.267 I print_info: n_embd_k_gqa     = 2048
0.00.039.267 I print_info: n_embd_v_gqa     = 2048
0.00.039.268 I print_info: f_norm_eps       = 1.0e-05
0.00.039.268 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.268 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.268 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.268 I print_info: f_logit_scale    = 0.0e+00
0.00.039.269 I print_info: n_ff             = 8192
0.00.039.269 I print_info: n_expert         = 0
0.00.039.270 I print_info: n_expert_used    = 0
0.00.039.270 I print_info: causal attn      = 1
0.00.039.270 I print_info: pooling type     = 0
0.00.039.270 I print_info: rope type        = 2
0.00.039.270 I print_info: rope scaling     = linear
0.00.039.271 I print_info: freq_base_train  = 10000.0
0.00.039.271 I print_info: freq_scale_train = 1
0.00.039.271 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.272 I print_info: rope_finetuned   = unknown
0.00.039.272 I print_info: ssm_d_conv       = 0
0.00.039.272 I print_info: ssm_d_inner      = 0
0.00.039.272 I print_info: ssm_d_state      = 0
0.00.039.272 I print_info: ssm_dt_rank      = 0
0.00.039.273 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.273 I print_info: model type       = 1.4B
0.00.039.273 I print_info: model params     = 1.41 B
0.00.039.273 I print_info: general.name     = 1.4B
0.00.039.274 I print_info: vocab type       = BPE
0.00.039.276 I print_info: n_vocab          = 50304
0.00.039.276 I print_info: n_merges         = 50009
0.00.039.276 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.276 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.277 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.277 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.277 I print_info: LF token         = 187 ''
0.00.039.277 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.277 I print_info: max token length = 1024
0.00.039.278 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.348.663 I load_tensors: offloading 24 repeating layers to GPU
0.00.348.677 I load_tensors: offloading output layer to GPU
0.00.348.677 I load_tensors: offloaded 25/25 layers to GPU
0.00.348.704 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.348.706 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.349.927 I llama_init_from_model: n_seq_max     = 1
0.00.349.940 I llama_init_from_model: n_ctx         = 2048
0.00.349.940 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.349.941 I llama_init_from_model: n_batch       = 2048
0.00.349.941 I llama_init_from_model: n_ubatch      = 512
0.00.349.941 I llama_init_from_model: flash_attn    = 0
0.00.349.943 I llama_init_from_model: freq_base     = 10000.0
0.00.349.944 I llama_init_from_model: freq_scale    = 1
0.00.349.950 I ggml_metal_init: allocating
0.00.350.026 I ggml_metal_init: found device: Apple M4
0.00.350.041 I ggml_metal_init: picking default device: Apple M4
0.00.351.896 I ggml_metal_init: using embedded metal library
0.00.358.336 I ggml_metal_init: GPU name:   Apple M4
0.00.358.351 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.358.352 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.358.353 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.358.353 I ggml_metal_init: simdgroup reduction   = true
0.00.358.353 I ggml_metal_init: simdgroup matrix mul. = true
0.00.358.354 I ggml_metal_init: has residency sets    = true
0.00.358.354 I ggml_metal_init: has bfloat            = true
0.00.358.355 I ggml_metal_init: use bfloat            = true
0.00.358.359 I ggml_metal_init: hasUnifiedMemory      = true
0.00.358.380 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.379.614 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.428.395 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.428.404 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.428.447 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.432.507 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.432.509 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.432.510 I llama_init_from_model: graph nodes  = 967
0.00.432.510 I llama_init_from_model: graph splits = 2
0.00.432.515 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.432.645 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.432.645 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.496.026 I main: llama threadpool init, n_threads = 4
0.00.496.066 I 
0.00.496.089 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.496.089 I 
0.00.496.261 I sampler seed: 1234
0.00.496.266 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.496.307 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.496.309 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.496.309 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.174.070 I llama_perf_sampler_print:    sampling time =       1.34 ms /    71 runs   (    0.02 ms per token, 52985.07 tokens per second)
0.01.174.071 I llama_perf_context_print:        load time =     485.50 ms
0.01.174.072 I llama_perf_context_print: prompt eval time =      40.47 ms /     7 tokens (    5.78 ms per token,   172.98 tokens per second)
0.01.174.073 I llama_perf_context_print:        eval time =     634.45 ms /    63 runs   (   10.07 ms per token,    99.30 tokens per second)
0.01.174.073 I llama_perf_context_print:       total time =     678.78 ms /    70 tokens
0.01.174.294 I ggml_metal_free: deallocating

real	0m1.193s
user	0m0.113s
sys	0m0.162s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.117 I build: 4772 (4d1051a4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.005 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.044 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.017.050 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.052 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.053 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.053 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.053 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.054 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.055 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.055 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.055 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.056 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.056 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.057 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.057 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.060 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.060 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.060 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.809 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.896 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.640 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.641 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.642 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.642 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.642 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.643 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.643 I llama_model_loader: - type  f32:  194 tensors
0.00.025.644 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.644 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.644 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.645 I print_info: file format = GGUF V3 (latest)
0.00.025.645 I print_info: file type   = Q2_K - Medium
0.00.025.647 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.033.931 I load: special tokens cache size = 25
0.00.039.965 I load: token to piece cache size = 0.2984 MB
0.00.039.969 I print_info: arch             = gptneox
0.00.039.969 I print_info: vocab_only       = 0
0.00.039.970 I print_info: n_ctx_train      = 2048
0.00.039.970 I print_info: n_embd           = 2048
0.00.039.970 I print_info: n_layer          = 24
0.00.039.974 I print_info: n_head           = 16
0.00.039.975 I print_info: n_head_kv        = 16
0.00.039.975 I print_info: n_rot            = 32
0.00.039.975 I print_info: n_swa            = 0
0.00.039.976 I print_info: n_embd_head_k    = 128
0.00.039.976 I print_info: n_embd_head_v    = 128
0.00.039.977 I print_info: n_gqa            = 1
0.00.039.977 I print_info: n_embd_k_gqa     = 2048
0.00.039.978 I print_info: n_embd_v_gqa     = 2048
0.00.039.979 I print_info: f_norm_eps       = 1.0e-05
0.00.039.979 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.979 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.979 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.979 I print_info: f_logit_scale    = 0.0e+00
0.00.039.980 I print_info: n_ff             = 8192
0.00.039.980 I print_info: n_expert         = 0
0.00.039.980 I print_info: n_expert_used    = 0
0.00.039.981 I print_info: causal attn      = 1
0.00.039.981 I print_info: pooling type     = 0
0.00.039.981 I print_info: rope type        = 2
0.00.039.981 I print_info: rope scaling     = linear
0.00.039.981 I print_info: freq_base_train  = 10000.0
0.00.039.982 I print_info: freq_scale_train = 1
0.00.039.982 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.982 I print_info: rope_finetuned   = unknown
0.00.039.982 I print_info: ssm_d_conv       = 0
0.00.039.982 I print_info: ssm_d_inner      = 0
0.00.039.982 I print_info: ssm_d_state      = 0
0.00.039.982 I print_info: ssm_dt_rank      = 0
0.00.039.983 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.983 I print_info: model type       = 1.4B
0.00.039.983 I print_info: model params     = 1.41 B
0.00.039.983 I print_info: general.name     = 1.4B
0.00.039.984 I print_info: vocab type       = BPE
0.00.039.984 I print_info: n_vocab          = 50304
0.00.039.987 I print_info: n_merges         = 50009
0.00.039.987 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.987 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.987 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.987 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.988 I print_info: LF token         = 187 ''
0.00.039.988 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.988 I print_info: max token length = 1024
0.00.039.988 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.348.431 I load_tensors: offloading 24 repeating layers to GPU
0.00.348.446 I load_tensors: offloading output layer to GPU
0.00.348.447 I load_tensors: offloaded 25/25 layers to GPU
0.00.348.478 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.348.480 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.350.224 I llama_init_from_model: n_seq_max     = 1
0.00.350.235 I llama_init_from_model: n_ctx         = 128
0.00.350.235 I llama_init_from_model: n_ctx_per_seq = 128
0.00.350.236 I llama_init_from_model: n_batch       = 128
0.00.350.236 I llama_init_from_model: n_ubatch      = 128
0.00.350.237 I llama_init_from_model: flash_attn    = 0
0.00.350.238 I llama_init_from_model: freq_base     = 10000.0
0.00.350.239 I llama_init_from_model: freq_scale    = 1
0.00.350.239 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.350.241 I ggml_metal_init: allocating
0.00.350.318 I ggml_metal_init: found device: Apple M4
0.00.350.332 I ggml_metal_init: picking default device: Apple M4
0.00.352.186 I ggml_metal_init: using embedded metal library
0.00.357.664 I ggml_metal_init: GPU name:   Apple M4
0.00.357.681 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.357.682 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.357.682 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.357.683 I ggml_metal_init: simdgroup reduction   = true
0.00.357.683 I ggml_metal_init: simdgroup matrix mul. = true
0.00.357.683 I ggml_metal_init: has residency sets    = true
0.00.357.684 I ggml_metal_init: has bfloat            = true
0.00.357.684 I ggml_metal_init: use bfloat            = true
0.00.357.686 I ggml_metal_init: hasUnifiedMemory      = true
0.00.357.691 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.378.643 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.382.216 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.382.234 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.382.303 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.385.713 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.385.715 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.385.716 I llama_init_from_model: graph nodes  = 967
0.00.385.716 I llama_init_from_model: graph splits = 2
0.00.385.720 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.385.720 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.414.031 I 
0.00.414.119 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.414.125 I perplexity: tokenizing the input ..
0.00.421.463 I perplexity: tokenization took 7.335 ms
0.00.421.469 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.554.738 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.556.081 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.556.103 I llama_perf_context_print:        load time =     404.02 ms
0.00.556.104 I llama_perf_context_print: prompt eval time =     132.34 ms /   128 tokens (    1.03 ms per token,   967.18 tokens per second)
0.00.556.105 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.556.105 I llama_perf_context_print:       total time =     142.08 ms /   129 tokens
0.00.556.475 I ggml_metal_free: deallocating

real	0m0.573s
user	0m0.082s
sys	0m0.096s
```
- q3_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4772 (4d1051a4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.084 I main: llama backend init
0.00.000.086 I main: load the model and apply lora adapter, if any
0.00.008.756 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.421 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.426 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.432 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.433 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.433 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.434 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.434 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.435 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.435 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.436 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.436 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.436 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.437 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.437 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.438 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.439 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.439 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.236 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.367 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.133 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.134 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.135 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.135 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.135 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.135 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.136 I llama_model_loader: - type  f32:  194 tensors
0.00.025.136 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.136 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.137 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.137 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.138 I print_info: file format = GGUF V3 (latest)
0.00.025.138 I print_info: file type   = Q3_K - Medium
0.00.025.139 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.033.118 I load: special tokens cache size = 25
0.00.039.035 I load: token to piece cache size = 0.2984 MB
0.00.039.038 I print_info: arch             = gptneox
0.00.039.038 I print_info: vocab_only       = 0
0.00.039.038 I print_info: n_ctx_train      = 2048
0.00.039.038 I print_info: n_embd           = 2048
0.00.039.038 I print_info: n_layer          = 24
0.00.039.041 I print_info: n_head           = 16
0.00.039.042 I print_info: n_head_kv        = 16
0.00.039.042 I print_info: n_rot            = 32
0.00.039.043 I print_info: n_swa            = 0
0.00.039.043 I print_info: n_embd_head_k    = 128
0.00.039.045 I print_info: n_embd_head_v    = 128
0.00.039.046 I print_info: n_gqa            = 1
0.00.039.047 I print_info: n_embd_k_gqa     = 2048
0.00.039.051 I print_info: n_embd_v_gqa     = 2048
0.00.039.052 I print_info: f_norm_eps       = 1.0e-05
0.00.039.053 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.053 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.053 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.053 I print_info: f_logit_scale    = 0.0e+00
0.00.039.054 I print_info: n_ff             = 8192
0.00.039.054 I print_info: n_expert         = 0
0.00.039.054 I print_info: n_expert_used    = 0
0.00.039.056 I print_info: causal attn      = 1
0.00.039.057 I print_info: pooling type     = 0
0.00.039.057 I print_info: rope type        = 2
0.00.039.058 I print_info: rope scaling     = linear
0.00.039.058 I print_info: freq_base_train  = 10000.0
0.00.039.058 I print_info: freq_scale_train = 1
0.00.039.058 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.059 I print_info: rope_finetuned   = unknown
0.00.039.059 I print_info: ssm_d_conv       = 0
0.00.039.059 I print_info: ssm_d_inner      = 0
0.00.039.059 I print_info: ssm_d_state      = 0
0.00.039.059 I print_info: ssm_dt_rank      = 0
0.00.039.059 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.060 I print_info: model type       = 1.4B
0.00.039.060 I print_info: model params     = 1.41 B
0.00.039.060 I print_info: general.name     = 1.4B
0.00.039.060 I print_info: vocab type       = BPE
0.00.039.061 I print_info: n_vocab          = 50304
0.00.039.061 I print_info: n_merges         = 50009
0.00.039.061 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.061 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.061 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.061 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.062 I print_info: LF token         = 187 ''
0.00.039.062 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.062 I print_info: max token length = 1024
0.00.039.063 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.445.116 I load_tensors: offloading 24 repeating layers to GPU
0.00.445.129 I load_tensors: offloading output layer to GPU
0.00.445.130 I load_tensors: offloaded 25/25 layers to GPU
0.00.445.163 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.445.164 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.446.778 I llama_init_from_model: n_seq_max     = 1
0.00.446.785 I llama_init_from_model: n_ctx         = 2048
0.00.446.785 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.446.786 I llama_init_from_model: n_batch       = 2048
0.00.446.786 I llama_init_from_model: n_ubatch      = 512
0.00.446.787 I llama_init_from_model: flash_attn    = 0
0.00.446.789 I llama_init_from_model: freq_base     = 10000.0
0.00.446.789 I llama_init_from_model: freq_scale    = 1
0.00.446.791 I ggml_metal_init: allocating
0.00.446.861 I ggml_metal_init: found device: Apple M4
0.00.446.875 I ggml_metal_init: picking default device: Apple M4
0.00.448.687 I ggml_metal_init: using embedded metal library
0.00.454.471 I ggml_metal_init: GPU name:   Apple M4
0.00.454.485 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.454.486 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.454.486 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.454.487 I ggml_metal_init: simdgroup reduction   = true
0.00.454.487 I ggml_metal_init: simdgroup matrix mul. = true
0.00.454.487 I ggml_metal_init: has residency sets    = true
0.00.454.488 I ggml_metal_init: has bfloat            = true
0.00.454.488 I ggml_metal_init: use bfloat            = true
0.00.454.491 I ggml_metal_init: hasUnifiedMemory      = true
0.00.454.496 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.475.257 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.531.320 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.531.327 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.531.364 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.536.178 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.536.180 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.536.180 I llama_init_from_model: graph nodes  = 967
0.00.536.181 I llama_init_from_model: graph splits = 2
0.00.536.187 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.536.324 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.536.325 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.591.405 I main: llama threadpool init, n_threads = 4
0.00.591.449 I 
0.00.591.473 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.591.473 I 
0.00.591.619 I sampler seed: 1234
0.00.591.624 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.591.649 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.591.650 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.591.651 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.344.408 I llama_perf_sampler_print:    sampling time =       1.39 ms /    71 runs   (    0.02 ms per token, 50932.57 tokens per second)
0.01.344.409 I llama_perf_context_print:        load time =     581.90 ms
0.01.344.409 I llama_perf_context_print: prompt eval time =      49.84 ms /     7 tokens (    7.12 ms per token,   140.44 tokens per second)
0.01.344.410 I llama_perf_context_print:        eval time =     699.93 ms /    63 runs   (   11.11 ms per token,    90.01 tokens per second)
0.01.344.410 I llama_perf_context_print:       total time =     753.75 ms /    70 tokens
0.01.344.610 I ggml_metal_free: deallocating

real	0m1.360s
user	0m0.110s
sys	0m0.186s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.120 I build: 4772 (4d1051a4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.845 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.069 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.017.075 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.081 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.081 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.082 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.082 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.082 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.084 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.084 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.084 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.085 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.085 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.085 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.085 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.087 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.087 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.088 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.944 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.054 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.949 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.950 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.950 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.951 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.951 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.952 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.952 I llama_model_loader: - type  f32:  194 tensors
0.00.025.953 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.953 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.953 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.953 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.954 I print_info: file format = GGUF V3 (latest)
0.00.025.954 I print_info: file type   = Q3_K - Medium
0.00.025.955 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.034.333 I load: special tokens cache size = 25
0.00.040.475 I load: token to piece cache size = 0.2984 MB
0.00.040.479 I print_info: arch             = gptneox
0.00.040.480 I print_info: vocab_only       = 0
0.00.040.480 I print_info: n_ctx_train      = 2048
0.00.040.480 I print_info: n_embd           = 2048
0.00.040.480 I print_info: n_layer          = 24
0.00.040.484 I print_info: n_head           = 16
0.00.040.485 I print_info: n_head_kv        = 16
0.00.040.485 I print_info: n_rot            = 32
0.00.040.485 I print_info: n_swa            = 0
0.00.040.486 I print_info: n_embd_head_k    = 128
0.00.040.486 I print_info: n_embd_head_v    = 128
0.00.040.486 I print_info: n_gqa            = 1
0.00.040.487 I print_info: n_embd_k_gqa     = 2048
0.00.040.488 I print_info: n_embd_v_gqa     = 2048
0.00.040.489 I print_info: f_norm_eps       = 1.0e-05
0.00.040.489 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.489 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.489 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.489 I print_info: f_logit_scale    = 0.0e+00
0.00.040.490 I print_info: n_ff             = 8192
0.00.040.490 I print_info: n_expert         = 0
0.00.040.490 I print_info: n_expert_used    = 0
0.00.040.490 I print_info: causal attn      = 1
0.00.040.490 I print_info: pooling type     = 0
0.00.040.491 I print_info: rope type        = 2
0.00.040.491 I print_info: rope scaling     = linear
0.00.040.493 I print_info: freq_base_train  = 10000.0
0.00.040.493 I print_info: freq_scale_train = 1
0.00.040.493 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.493 I print_info: rope_finetuned   = unknown
0.00.040.493 I print_info: ssm_d_conv       = 0
0.00.040.494 I print_info: ssm_d_inner      = 0
0.00.040.494 I print_info: ssm_d_state      = 0
0.00.040.494 I print_info: ssm_dt_rank      = 0
0.00.040.496 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.496 I print_info: model type       = 1.4B
0.00.040.496 I print_info: model params     = 1.41 B
0.00.040.496 I print_info: general.name     = 1.4B
0.00.040.497 I print_info: vocab type       = BPE
0.00.040.497 I print_info: n_vocab          = 50304
0.00.040.497 I print_info: n_merges         = 50009
0.00.040.498 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.498 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.498 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.498 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.498 I print_info: LF token         = 187 ''
0.00.040.500 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.500 I print_info: max token length = 1024
0.00.040.500 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.445.644 I load_tensors: offloading 24 repeating layers to GPU
0.00.445.658 I load_tensors: offloading output layer to GPU
0.00.445.659 I load_tensors: offloaded 25/25 layers to GPU
0.00.445.694 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.445.702 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.447.388 I llama_init_from_model: n_seq_max     = 1
0.00.447.395 I llama_init_from_model: n_ctx         = 128
0.00.447.395 I llama_init_from_model: n_ctx_per_seq = 128
0.00.447.396 I llama_init_from_model: n_batch       = 128
0.00.447.396 I llama_init_from_model: n_ubatch      = 128
0.00.447.396 I llama_init_from_model: flash_attn    = 0
0.00.447.398 I llama_init_from_model: freq_base     = 10000.0
0.00.447.398 I llama_init_from_model: freq_scale    = 1
0.00.447.399 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.447.402 I ggml_metal_init: allocating
0.00.447.502 I ggml_metal_init: found device: Apple M4
0.00.447.516 I ggml_metal_init: picking default device: Apple M4
0.00.449.394 I ggml_metal_init: using embedded metal library
0.00.454.872 I ggml_metal_init: GPU name:   Apple M4
0.00.454.887 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.454.888 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.454.888 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.454.889 I ggml_metal_init: simdgroup reduction   = true
0.00.454.889 I ggml_metal_init: simdgroup matrix mul. = true
0.00.454.890 I ggml_metal_init: has residency sets    = true
0.00.454.890 I ggml_metal_init: has bfloat            = true
0.00.454.890 I ggml_metal_init: use bfloat            = true
0.00.454.893 I ggml_metal_init: hasUnifiedMemory      = true
0.00.454.897 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.475.387 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.478.988 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.478.992 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.479.038 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.482.293 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.482.295 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.482.295 I llama_init_from_model: graph nodes  = 967
0.00.482.296 I llama_init_from_model: graph splits = 2
0.00.482.299 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.482.300 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.507.236 I 
0.00.507.314 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.507.323 I perplexity: tokenizing the input ..
0.00.514.069 I perplexity: tokenization took 6.744 ms
0.00.514.078 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.646.111 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.647.491 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.647.513 I llama_perf_context_print:        load time =     497.38 ms
0.00.647.515 I llama_perf_context_print: prompt eval time =     131.63 ms /   128 tokens (    1.03 ms per token,   972.42 tokens per second)
0.00.647.515 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.647.516 I llama_perf_context_print:       total time =     140.28 ms /   129 tokens
0.00.647.912 I ggml_metal_free: deallocating

real	0m0.664s
user	0m0.080s
sys	0m0.111s
```
- q4_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.050 I build: 4772 (4d1051a4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.086 I main: llama backend init
0.00.000.088 I main: load the model and apply lora adapter, if any
0.00.010.000 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.692 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.017.697 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.698 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.699 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.703 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.703 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.704 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.704 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.705 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.705 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.706 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.706 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.706 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.707 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.708 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.708 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.709 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.467 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.532 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.210 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.211 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.211 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.212 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.212 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.212 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.026.213 I llama_model_loader: - type  f32:  194 tensors
0.00.026.213 I llama_model_loader: - type q4_K:   61 tensors
0.00.026.213 I llama_model_loader: - type q5_K:   24 tensors
0.00.026.213 I llama_model_loader: - type q6_K:   13 tensors
0.00.026.214 I print_info: file format = GGUF V3 (latest)
0.00.026.214 I print_info: file type   = Q4_K - Medium
0.00.026.215 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.034.203 I load: special tokens cache size = 25
0.00.040.146 I load: token to piece cache size = 0.2984 MB
0.00.040.149 I print_info: arch             = gptneox
0.00.040.149 I print_info: vocab_only       = 0
0.00.040.149 I print_info: n_ctx_train      = 2048
0.00.040.150 I print_info: n_embd           = 2048
0.00.040.150 I print_info: n_layer          = 24
0.00.040.153 I print_info: n_head           = 16
0.00.040.154 I print_info: n_head_kv        = 16
0.00.040.154 I print_info: n_rot            = 32
0.00.040.155 I print_info: n_swa            = 0
0.00.040.155 I print_info: n_embd_head_k    = 128
0.00.040.155 I print_info: n_embd_head_v    = 128
0.00.040.156 I print_info: n_gqa            = 1
0.00.040.157 I print_info: n_embd_k_gqa     = 2048
0.00.040.158 I print_info: n_embd_v_gqa     = 2048
0.00.040.158 I print_info: f_norm_eps       = 1.0e-05
0.00.040.159 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.159 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.159 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.159 I print_info: f_logit_scale    = 0.0e+00
0.00.040.160 I print_info: n_ff             = 8192
0.00.040.160 I print_info: n_expert         = 0
0.00.040.160 I print_info: n_expert_used    = 0
0.00.040.160 I print_info: causal attn      = 1
0.00.040.162 I print_info: pooling type     = 0
0.00.040.163 I print_info: rope type        = 2
0.00.040.164 I print_info: rope scaling     = linear
0.00.040.164 I print_info: freq_base_train  = 10000.0
0.00.040.164 I print_info: freq_scale_train = 1
0.00.040.164 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.165 I print_info: rope_finetuned   = unknown
0.00.040.165 I print_info: ssm_d_conv       = 0
0.00.040.165 I print_info: ssm_d_inner      = 0
0.00.040.165 I print_info: ssm_d_state      = 0
0.00.040.165 I print_info: ssm_dt_rank      = 0
0.00.040.165 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.165 I print_info: model type       = 1.4B
0.00.040.166 I print_info: model params     = 1.41 B
0.00.040.166 I print_info: general.name     = 1.4B
0.00.040.166 I print_info: vocab type       = BPE
0.00.040.167 I print_info: n_vocab          = 50304
0.00.040.167 I print_info: n_merges         = 50009
0.00.040.167 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.167 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.167 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.168 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.168 I print_info: LF token         = 187 ''
0.00.040.172 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.172 I print_info: max token length = 1024
0.00.040.174 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.520.785 I load_tensors: offloading 24 repeating layers to GPU
0.00.520.799 I load_tensors: offloading output layer to GPU
0.00.520.800 I load_tensors: offloaded 25/25 layers to GPU
0.00.520.832 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.520.834 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.522.463 I llama_init_from_model: n_seq_max     = 1
0.00.522.471 I llama_init_from_model: n_ctx         = 2048
0.00.522.471 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.522.472 I llama_init_from_model: n_batch       = 2048
0.00.522.472 I llama_init_from_model: n_ubatch      = 512
0.00.522.473 I llama_init_from_model: flash_attn    = 0
0.00.522.474 I llama_init_from_model: freq_base     = 10000.0
0.00.522.475 I llama_init_from_model: freq_scale    = 1
0.00.522.479 I ggml_metal_init: allocating
0.00.522.552 I ggml_metal_init: found device: Apple M4
0.00.522.565 I ggml_metal_init: picking default device: Apple M4
0.00.524.436 I ggml_metal_init: using embedded metal library
0.00.531.141 I ggml_metal_init: GPU name:   Apple M4
0.00.531.145 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.531.146 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.531.146 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.531.147 I ggml_metal_init: simdgroup reduction   = true
0.00.531.147 I ggml_metal_init: simdgroup matrix mul. = true
0.00.531.147 I ggml_metal_init: has residency sets    = true
0.00.531.147 I ggml_metal_init: has bfloat            = true
0.00.531.148 I ggml_metal_init: use bfloat            = true
0.00.531.148 I ggml_metal_init: hasUnifiedMemory      = true
0.00.531.150 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.548.565 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.601.705 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.601.712 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.601.747 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.606.107 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.606.110 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.606.110 I llama_init_from_model: graph nodes  = 967
0.00.606.111 I llama_init_from_model: graph splits = 2
0.00.606.122 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.606.249 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.606.250 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.662.848 I main: llama threadpool init, n_threads = 4
0.00.662.895 I 
0.00.662.920 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.662.922 I 
0.00.663.064 I sampler seed: 1234
0.00.663.069 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.663.081 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.663.081 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.663.081 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.421.737 I llama_perf_sampler_print:    sampling time =       1.42 ms /    71 runs   (    0.02 ms per token, 50176.68 tokens per second)
0.01.421.738 I llama_perf_context_print:        load time =     652.10 ms
0.01.421.739 I llama_perf_context_print: prompt eval time =      48.90 ms /     7 tokens (    6.99 ms per token,   143.16 tokens per second)
0.01.421.739 I llama_perf_context_print:        eval time =     706.78 ms /    63 runs   (   11.22 ms per token,    89.14 tokens per second)
0.01.421.740 I llama_perf_context_print:       total time =     759.64 ms /    70 tokens
0.01.422.021 I ggml_metal_free: deallocating

real	0m1.439s
user	0m0.109s
sys	0m0.189s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.120 I build: 4772 (4d1051a4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.898 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.992 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.015.999 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.000 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.001 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.001 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.002 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.002 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.003 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.003 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.006 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.007 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.007 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.007 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.008 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.009 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.010 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.010 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.806 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.883 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.681 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.683 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.683 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.683 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.684 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.684 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.685 I llama_model_loader: - type  f32:  194 tensors
0.00.024.685 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.685 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.686 I llama_model_loader: - type q6_K:   13 tensors
0.00.024.686 I print_info: file format = GGUF V3 (latest)
0.00.024.687 I print_info: file type   = Q4_K - Medium
0.00.024.688 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.032.862 I load: special tokens cache size = 25
0.00.038.925 I load: token to piece cache size = 0.2984 MB
0.00.038.929 I print_info: arch             = gptneox
0.00.038.930 I print_info: vocab_only       = 0
0.00.038.930 I print_info: n_ctx_train      = 2048
0.00.038.930 I print_info: n_embd           = 2048
0.00.038.930 I print_info: n_layer          = 24
0.00.038.934 I print_info: n_head           = 16
0.00.038.935 I print_info: n_head_kv        = 16
0.00.038.935 I print_info: n_rot            = 32
0.00.038.936 I print_info: n_swa            = 0
0.00.038.936 I print_info: n_embd_head_k    = 128
0.00.038.936 I print_info: n_embd_head_v    = 128
0.00.038.936 I print_info: n_gqa            = 1
0.00.038.937 I print_info: n_embd_k_gqa     = 2048
0.00.038.939 I print_info: n_embd_v_gqa     = 2048
0.00.038.940 I print_info: f_norm_eps       = 1.0e-05
0.00.038.940 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.940 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.940 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.940 I print_info: f_logit_scale    = 0.0e+00
0.00.038.941 I print_info: n_ff             = 8192
0.00.038.941 I print_info: n_expert         = 0
0.00.038.941 I print_info: n_expert_used    = 0
0.00.038.941 I print_info: causal attn      = 1
0.00.038.941 I print_info: pooling type     = 0
0.00.038.943 I print_info: rope type        = 2
0.00.038.944 I print_info: rope scaling     = linear
0.00.038.944 I print_info: freq_base_train  = 10000.0
0.00.038.944 I print_info: freq_scale_train = 1
0.00.038.944 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.944 I print_info: rope_finetuned   = unknown
0.00.038.944 I print_info: ssm_d_conv       = 0
0.00.038.945 I print_info: ssm_d_inner      = 0
0.00.038.945 I print_info: ssm_d_state      = 0
0.00.038.945 I print_info: ssm_dt_rank      = 0
0.00.038.945 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.945 I print_info: model type       = 1.4B
0.00.038.946 I print_info: model params     = 1.41 B
0.00.038.946 I print_info: general.name     = 1.4B
0.00.038.947 I print_info: vocab type       = BPE
0.00.038.947 I print_info: n_vocab          = 50304
0.00.038.948 I print_info: n_merges         = 50009
0.00.038.948 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.948 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.948 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.948 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.949 I print_info: LF token         = 187 ''
0.00.038.949 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.949 I print_info: max token length = 1024
0.00.038.949 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.528.214 I load_tensors: offloading 24 repeating layers to GPU
0.00.528.231 I load_tensors: offloading output layer to GPU
0.00.528.232 I load_tensors: offloaded 25/25 layers to GPU
0.00.528.268 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.528.269 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.529.851 I llama_init_from_model: n_seq_max     = 1
0.00.529.854 I llama_init_from_model: n_ctx         = 128
0.00.529.855 I llama_init_from_model: n_ctx_per_seq = 128
0.00.529.855 I llama_init_from_model: n_batch       = 128
0.00.529.855 I llama_init_from_model: n_ubatch      = 128
0.00.529.856 I llama_init_from_model: flash_attn    = 0
0.00.529.858 I llama_init_from_model: freq_base     = 10000.0
0.00.529.859 I llama_init_from_model: freq_scale    = 1
0.00.529.859 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.529.862 I ggml_metal_init: allocating
0.00.529.951 I ggml_metal_init: found device: Apple M4
0.00.529.966 I ggml_metal_init: picking default device: Apple M4
0.00.531.808 I ggml_metal_init: using embedded metal library
0.00.538.623 I ggml_metal_init: GPU name:   Apple M4
0.00.538.631 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.538.631 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.538.632 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.538.632 I ggml_metal_init: simdgroup reduction   = true
0.00.538.633 I ggml_metal_init: simdgroup matrix mul. = true
0.00.538.633 I ggml_metal_init: has residency sets    = true
0.00.538.633 I ggml_metal_init: has bfloat            = true
0.00.538.634 I ggml_metal_init: use bfloat            = true
0.00.538.635 I ggml_metal_init: hasUnifiedMemory      = true
0.00.538.646 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.556.567 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.560.013 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.560.025 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.560.071 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.563.305 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.563.307 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.563.307 I llama_init_from_model: graph nodes  = 967
0.00.563.307 I llama_init_from_model: graph splits = 2
0.00.563.310 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.563.310 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.592.009 I 
0.00.592.063 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.592.068 I perplexity: tokenizing the input ..
0.00.597.357 I perplexity: tokenization took 5.287 ms
0.00.597.361 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.731.411 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.732.761 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.732.787 I llama_perf_context_print:        load time =     583.11 ms
0.00.732.788 I llama_perf_context_print: prompt eval time =     133.50 ms /   128 tokens (    1.04 ms per token,   958.78 tokens per second)
0.00.732.789 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.732.790 I llama_perf_context_print:       total time =     140.78 ms /   129 tokens
0.00.733.228 I ggml_metal_free: deallocating

real	0m0.748s
user	0m0.078s
sys	0m0.127s
```
- q5_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.049 I build: 4772 (4d1051a4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.085 I main: llama backend init
0.00.000.087 I main: load the model and apply lora adapter, if any
0.00.013.584 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.021.491 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.021.496 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.021.498 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.021.498 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.021.499 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.021.499 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.021.499 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.021.500 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.021.501 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.021.501 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.021.503 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.021.504 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.021.504 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.021.505 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.021.507 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.021.508 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.021.508 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.025.272 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.026.371 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.030.118 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.030.119 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.030.119 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.030.120 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.030.120 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.030.120 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.030.121 I llama_model_loader: - type  f32:  194 tensors
0.00.030.121 I llama_model_loader: - type q5_K:   61 tensors
0.00.030.121 I llama_model_loader: - type q6_K:   37 tensors
0.00.030.122 I print_info: file format = GGUF V3 (latest)
0.00.030.122 I print_info: file type   = Q5_K - Medium
0.00.030.123 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.038.092 I load: special tokens cache size = 25
0.00.044.061 I load: token to piece cache size = 0.2984 MB
0.00.044.064 I print_info: arch             = gptneox
0.00.044.064 I print_info: vocab_only       = 0
0.00.044.064 I print_info: n_ctx_train      = 2048
0.00.044.064 I print_info: n_embd           = 2048
0.00.044.064 I print_info: n_layer          = 24
0.00.044.067 I print_info: n_head           = 16
0.00.044.068 I print_info: n_head_kv        = 16
0.00.044.068 I print_info: n_rot            = 32
0.00.044.068 I print_info: n_swa            = 0
0.00.044.070 I print_info: n_embd_head_k    = 128
0.00.044.071 I print_info: n_embd_head_v    = 128
0.00.044.071 I print_info: n_gqa            = 1
0.00.044.072 I print_info: n_embd_k_gqa     = 2048
0.00.044.073 I print_info: n_embd_v_gqa     = 2048
0.00.044.073 I print_info: f_norm_eps       = 1.0e-05
0.00.044.074 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.044.074 I print_info: f_clamp_kqv      = 0.0e+00
0.00.044.074 I print_info: f_max_alibi_bias = 0.0e+00
0.00.044.074 I print_info: f_logit_scale    = 0.0e+00
0.00.044.075 I print_info: n_ff             = 8192
0.00.044.077 I print_info: n_expert         = 0
0.00.044.077 I print_info: n_expert_used    = 0
0.00.044.077 I print_info: causal attn      = 1
0.00.044.077 I print_info: pooling type     = 0
0.00.044.079 I print_info: rope type        = 2
0.00.044.079 I print_info: rope scaling     = linear
0.00.044.080 I print_info: freq_base_train  = 10000.0
0.00.044.080 I print_info: freq_scale_train = 1
0.00.044.080 I print_info: n_ctx_orig_yarn  = 2048
0.00.044.080 I print_info: rope_finetuned   = unknown
0.00.044.081 I print_info: ssm_d_conv       = 0
0.00.044.081 I print_info: ssm_d_inner      = 0
0.00.044.081 I print_info: ssm_d_state      = 0
0.00.044.081 I print_info: ssm_dt_rank      = 0
0.00.044.081 I print_info: ssm_dt_b_c_rms   = 0
0.00.044.082 I print_info: model type       = 1.4B
0.00.044.086 I print_info: model params     = 1.41 B
0.00.044.086 I print_info: general.name     = 1.4B
0.00.044.087 I print_info: vocab type       = BPE
0.00.044.087 I print_info: n_vocab          = 50304
0.00.044.087 I print_info: n_merges         = 50009
0.00.044.087 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.044.087 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.044.087 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.044.087 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.044.089 I print_info: LF token         = 187 ''
0.00.044.090 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.044.090 I print_info: max token length = 1024
0.00.044.090 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.614.121 I load_tensors: offloading 24 repeating layers to GPU
0.00.614.135 I load_tensors: offloading output layer to GPU
0.00.614.136 I load_tensors: offloaded 25/25 layers to GPU
0.00.614.173 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.614.174 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.615.762 I llama_init_from_model: n_seq_max     = 1
0.00.615.765 I llama_init_from_model: n_ctx         = 2048
0.00.615.765 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.615.766 I llama_init_from_model: n_batch       = 2048
0.00.615.766 I llama_init_from_model: n_ubatch      = 512
0.00.615.766 I llama_init_from_model: flash_attn    = 0
0.00.615.770 I llama_init_from_model: freq_base     = 10000.0
0.00.615.770 I llama_init_from_model: freq_scale    = 1
0.00.615.772 I ggml_metal_init: allocating
0.00.615.844 I ggml_metal_init: found device: Apple M4
0.00.615.857 I ggml_metal_init: picking default device: Apple M4
0.00.617.759 I ggml_metal_init: using embedded metal library
0.00.624.240 I ggml_metal_init: GPU name:   Apple M4
0.00.624.243 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.624.244 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.624.245 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.624.245 I ggml_metal_init: simdgroup reduction   = true
0.00.624.246 I ggml_metal_init: simdgroup matrix mul. = true
0.00.624.246 I ggml_metal_init: has residency sets    = true
0.00.624.246 I ggml_metal_init: has bfloat            = true
0.00.624.246 I ggml_metal_init: use bfloat            = true
0.00.624.247 I ggml_metal_init: hasUnifiedMemory      = true
0.00.624.249 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.641.856 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.694.919 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.694.925 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.694.959 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.699.256 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.699.258 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.699.258 I llama_init_from_model: graph nodes  = 967
0.00.699.258 I llama_init_from_model: graph splits = 2
0.00.699.263 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.699.392 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.699.392 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.760.775 I main: llama threadpool init, n_threads = 4
0.00.760.818 I 
0.00.760.843 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.760.843 I 
0.00.760.988 I sampler seed: 1234
0.00.760.993 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.761.012 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.761.012 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.761.012 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.601.965 I llama_perf_sampler_print:    sampling time =       1.31 ms /    71 runs   (    0.02 ms per token, 54281.35 tokens per second)
0.01.601.966 I llama_perf_context_print:        load time =     746.43 ms
0.01.601.966 I llama_perf_context_print: prompt eval time =      52.61 ms /     7 tokens (    7.52 ms per token,   133.06 tokens per second)
0.01.601.967 I llama_perf_context_print:        eval time =     785.47 ms /    63 runs   (   12.47 ms per token,    80.21 tokens per second)
0.01.601.967 I llama_perf_context_print:       total time =     841.94 ms /    70 tokens
0.01.602.229 I ggml_metal_free: deallocating

real	0m1.620s
user	0m0.108s
sys	0m0.222s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.112 I build: 4772 (4d1051a4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.056 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.052 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.017.058 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.060 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.061 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.061 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.061 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.062 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.063 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.066 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.066 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.066 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.067 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.067 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.067 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.069 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.069 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.070 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.833 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.930 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.721 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.723 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.724 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.724 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.724 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.725 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.725 I llama_model_loader: - type  f32:  194 tensors
0.00.025.726 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.726 I llama_model_loader: - type q6_K:   37 tensors
0.00.025.727 I print_info: file format = GGUF V3 (latest)
0.00.025.727 I print_info: file type   = Q5_K - Medium
0.00.025.729 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.034.236 I load: special tokens cache size = 25
0.00.040.188 I load: token to piece cache size = 0.2984 MB
0.00.040.193 I print_info: arch             = gptneox
0.00.040.193 I print_info: vocab_only       = 0
0.00.040.194 I print_info: n_ctx_train      = 2048
0.00.040.194 I print_info: n_embd           = 2048
0.00.040.194 I print_info: n_layer          = 24
0.00.040.198 I print_info: n_head           = 16
0.00.040.199 I print_info: n_head_kv        = 16
0.00.040.199 I print_info: n_rot            = 32
0.00.040.199 I print_info: n_swa            = 0
0.00.040.199 I print_info: n_embd_head_k    = 128
0.00.040.200 I print_info: n_embd_head_v    = 128
0.00.040.200 I print_info: n_gqa            = 1
0.00.040.201 I print_info: n_embd_k_gqa     = 2048
0.00.040.203 I print_info: n_embd_v_gqa     = 2048
0.00.040.204 I print_info: f_norm_eps       = 1.0e-05
0.00.040.204 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.205 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.205 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.205 I print_info: f_logit_scale    = 0.0e+00
0.00.040.205 I print_info: n_ff             = 8192
0.00.040.206 I print_info: n_expert         = 0
0.00.040.206 I print_info: n_expert_used    = 0
0.00.040.206 I print_info: causal attn      = 1
0.00.040.206 I print_info: pooling type     = 0
0.00.040.206 I print_info: rope type        = 2
0.00.040.206 I print_info: rope scaling     = linear
0.00.040.207 I print_info: freq_base_train  = 10000.0
0.00.040.208 I print_info: freq_scale_train = 1
0.00.040.208 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.208 I print_info: rope_finetuned   = unknown
0.00.040.208 I print_info: ssm_d_conv       = 0
0.00.040.208 I print_info: ssm_d_inner      = 0
0.00.040.210 I print_info: ssm_d_state      = 0
0.00.040.210 I print_info: ssm_dt_rank      = 0
0.00.040.210 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.210 I print_info: model type       = 1.4B
0.00.040.211 I print_info: model params     = 1.41 B
0.00.040.211 I print_info: general.name     = 1.4B
0.00.040.212 I print_info: vocab type       = BPE
0.00.040.212 I print_info: n_vocab          = 50304
0.00.040.212 I print_info: n_merges         = 50009
0.00.040.214 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.214 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.214 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.214 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.214 I print_info: LF token         = 187 ''
0.00.040.215 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.215 I print_info: max token length = 1024
0.00.040.215 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.594.845 I load_tensors: offloading 24 repeating layers to GPU
0.00.594.859 I load_tensors: offloading output layer to GPU
0.00.594.860 I load_tensors: offloaded 25/25 layers to GPU
0.00.594.896 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.594.898 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.596.480 I llama_init_from_model: n_seq_max     = 1
0.00.596.482 I llama_init_from_model: n_ctx         = 128
0.00.596.482 I llama_init_from_model: n_ctx_per_seq = 128
0.00.596.483 I llama_init_from_model: n_batch       = 128
0.00.596.483 I llama_init_from_model: n_ubatch      = 128
0.00.596.483 I llama_init_from_model: flash_attn    = 0
0.00.596.485 I llama_init_from_model: freq_base     = 10000.0
0.00.596.486 I llama_init_from_model: freq_scale    = 1
0.00.596.487 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.596.488 I ggml_metal_init: allocating
0.00.596.539 I ggml_metal_init: found device: Apple M4
0.00.596.552 I ggml_metal_init: picking default device: Apple M4
0.00.598.057 I ggml_metal_init: using embedded metal library
0.00.604.433 I ggml_metal_init: GPU name:   Apple M4
0.00.604.438 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.604.438 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.604.439 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.604.440 I ggml_metal_init: simdgroup reduction   = true
0.00.604.440 I ggml_metal_init: simdgroup matrix mul. = true
0.00.604.440 I ggml_metal_init: has residency sets    = true
0.00.604.440 I ggml_metal_init: has bfloat            = true
0.00.604.441 I ggml_metal_init: use bfloat            = true
0.00.604.442 I ggml_metal_init: hasUnifiedMemory      = true
0.00.604.444 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.621.318 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.624.814 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.624.818 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.624.859 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.628.126 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.628.128 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.628.128 I llama_init_from_model: graph nodes  = 967
0.00.628.129 I llama_init_from_model: graph splits = 2
0.00.628.131 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.628.131 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.660.735 I 
0.00.660.792 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.660.797 I perplexity: tokenizing the input ..
0.00.665.654 I perplexity: tokenization took 4.856 ms
0.00.665.658 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.801.539 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.802.894 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.802.919 I llama_perf_context_print:        load time =     650.67 ms
0.00.802.920 I llama_perf_context_print: prompt eval time =     135.65 ms /   128 tokens (    1.06 ms per token,   943.63 tokens per second)
0.00.802.921 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.802.921 I llama_perf_context_print:       total time =     142.19 ms /   129 tokens
0.00.803.294 I ggml_metal_free: deallocating

real	0m0.819s
user	0m0.076s
sys	0m0.137s
```
- q6_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.050 I build: 4772 (4d1051a4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.086 I main: llama backend init
0.00.000.087 I main: load the model and apply lora adapter, if any
0.00.008.664 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.301 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.017.306 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.308 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.308 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.309 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.309 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.309 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.310 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.310 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.311 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.311 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.311 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.312 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.312 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.314 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.314 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.314 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.085 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.206 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.913 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.914 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.915 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.915 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.915 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.916 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.916 I llama_model_loader: - type  f32:  194 tensors
0.00.025.916 I llama_model_loader: - type q6_K:   98 tensors
0.00.025.917 I print_info: file format = GGUF V3 (latest)
0.00.025.917 I print_info: file type   = Q6_K
0.00.025.918 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.033.782 I load: special tokens cache size = 25
0.00.039.823 I load: token to piece cache size = 0.2984 MB
0.00.039.826 I print_info: arch             = gptneox
0.00.039.826 I print_info: vocab_only       = 0
0.00.039.827 I print_info: n_ctx_train      = 2048
0.00.039.827 I print_info: n_embd           = 2048
0.00.039.827 I print_info: n_layer          = 24
0.00.039.829 I print_info: n_head           = 16
0.00.039.832 I print_info: n_head_kv        = 16
0.00.039.832 I print_info: n_rot            = 32
0.00.039.833 I print_info: n_swa            = 0
0.00.039.833 I print_info: n_embd_head_k    = 128
0.00.039.833 I print_info: n_embd_head_v    = 128
0.00.039.834 I print_info: n_gqa            = 1
0.00.039.834 I print_info: n_embd_k_gqa     = 2048
0.00.039.835 I print_info: n_embd_v_gqa     = 2048
0.00.039.836 I print_info: f_norm_eps       = 1.0e-05
0.00.039.836 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.836 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.836 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.837 I print_info: f_logit_scale    = 0.0e+00
0.00.039.837 I print_info: n_ff             = 8192
0.00.039.837 I print_info: n_expert         = 0
0.00.039.838 I print_info: n_expert_used    = 0
0.00.039.838 I print_info: causal attn      = 1
0.00.039.838 I print_info: pooling type     = 0
0.00.039.838 I print_info: rope type        = 2
0.00.039.838 I print_info: rope scaling     = linear
0.00.039.839 I print_info: freq_base_train  = 10000.0
0.00.039.839 I print_info: freq_scale_train = 1
0.00.039.839 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.839 I print_info: rope_finetuned   = unknown
0.00.039.840 I print_info: ssm_d_conv       = 0
0.00.039.840 I print_info: ssm_d_inner      = 0
0.00.039.841 I print_info: ssm_d_state      = 0
0.00.039.842 I print_info: ssm_dt_rank      = 0
0.00.039.842 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.842 I print_info: model type       = 1.4B
0.00.039.842 I print_info: model params     = 1.41 B
0.00.039.842 I print_info: general.name     = 1.4B
0.00.039.843 I print_info: vocab type       = BPE
0.00.039.843 I print_info: n_vocab          = 50304
0.00.039.843 I print_info: n_merges         = 50009
0.00.039.844 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.844 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.844 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.844 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.844 I print_info: LF token         = 187 ''
0.00.039.845 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.845 I print_info: max token length = 1024
0.00.039.845 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.662.231 I load_tensors: offloading 24 repeating layers to GPU
0.00.662.234 I load_tensors: offloading output layer to GPU
0.00.662.234 I load_tensors: offloaded 25/25 layers to GPU
0.00.662.259 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.662.262 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.663.923 I llama_init_from_model: n_seq_max     = 1
0.00.663.925 I llama_init_from_model: n_ctx         = 2048
0.00.663.926 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.663.926 I llama_init_from_model: n_batch       = 2048
0.00.663.927 I llama_init_from_model: n_ubatch      = 512
0.00.663.927 I llama_init_from_model: flash_attn    = 0
0.00.663.928 I llama_init_from_model: freq_base     = 10000.0
0.00.663.929 I llama_init_from_model: freq_scale    = 1
0.00.663.930 I ggml_metal_init: allocating
0.00.663.972 I ggml_metal_init: found device: Apple M4
0.00.663.987 I ggml_metal_init: picking default device: Apple M4
0.00.665.585 I ggml_metal_init: using embedded metal library
0.00.671.690 I ggml_metal_init: GPU name:   Apple M4
0.00.671.694 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.671.695 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.671.695 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.671.696 I ggml_metal_init: simdgroup reduction   = true
0.00.671.696 I ggml_metal_init: simdgroup matrix mul. = true
0.00.671.696 I ggml_metal_init: has residency sets    = true
0.00.671.697 I ggml_metal_init: has bfloat            = true
0.00.671.697 I ggml_metal_init: use bfloat            = true
0.00.671.698 I ggml_metal_init: hasUnifiedMemory      = true
0.00.671.707 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.689.195 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.739.754 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.739.760 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.739.793 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.743.812 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.743.814 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.743.814 I llama_init_from_model: graph nodes  = 967
0.00.743.815 I llama_init_from_model: graph splits = 2
0.00.743.820 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.743.949 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.743.950 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.809.486 I main: llama threadpool init, n_threads = 4
0.00.809.530 I 
0.00.809.552 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.809.554 I 
0.00.809.715 I sampler seed: 1234
0.00.809.720 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.809.762 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.809.765 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.809.766 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.

He added: Ive

0.01.684.021 I llama_perf_sampler_print:    sampling time =       1.37 ms /    71 runs   (    0.02 ms per token, 51976.57 tokens per second)
0.01.684.022 I llama_perf_context_print:        load time =     800.09 ms
0.01.684.023 I llama_perf_context_print: prompt eval time =      57.83 ms /     7 tokens (    8.26 ms per token,   121.05 tokens per second)
0.01.684.023 I llama_perf_context_print:        eval time =     813.47 ms /    63 runs   (   12.91 ms per token,    77.45 tokens per second)
0.01.684.024 I llama_perf_context_print:       total time =     875.27 ms /    70 tokens
0.01.684.278 I ggml_metal_free: deallocating

real	0m1.701s
user	0m0.108s
sys	0m0.224s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.116 I build: 4772 (4d1051a4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.013 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.141 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.147 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.152 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.153 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.153 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.154 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.154 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.155 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.155 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.156 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.156 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.156 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.157 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.157 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.159 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.159 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.159 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.902 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.975 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.748 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.750 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.750 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.751 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.751 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.752 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.752 I llama_model_loader: - type  f32:  194 tensors
0.00.024.752 I llama_model_loader: - type q6_K:   98 tensors
0.00.024.753 I print_info: file format = GGUF V3 (latest)
0.00.024.754 I print_info: file type   = Q6_K
0.00.024.761 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.032.783 I load: special tokens cache size = 25
0.00.038.774 I load: token to piece cache size = 0.2984 MB
0.00.038.778 I print_info: arch             = gptneox
0.00.038.778 I print_info: vocab_only       = 0
0.00.038.779 I print_info: n_ctx_train      = 2048
0.00.038.779 I print_info: n_embd           = 2048
0.00.038.779 I print_info: n_layer          = 24
0.00.038.783 I print_info: n_head           = 16
0.00.038.784 I print_info: n_head_kv        = 16
0.00.038.784 I print_info: n_rot            = 32
0.00.038.784 I print_info: n_swa            = 0
0.00.038.785 I print_info: n_embd_head_k    = 128
0.00.038.785 I print_info: n_embd_head_v    = 128
0.00.038.786 I print_info: n_gqa            = 1
0.00.038.786 I print_info: n_embd_k_gqa     = 2048
0.00.038.787 I print_info: n_embd_v_gqa     = 2048
0.00.038.788 I print_info: f_norm_eps       = 1.0e-05
0.00.038.788 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.788 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.788 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.788 I print_info: f_logit_scale    = 0.0e+00
0.00.038.789 I print_info: n_ff             = 8192
0.00.038.789 I print_info: n_expert         = 0
0.00.038.789 I print_info: n_expert_used    = 0
0.00.038.789 I print_info: causal attn      = 1
0.00.038.790 I print_info: pooling type     = 0
0.00.038.790 I print_info: rope type        = 2
0.00.038.790 I print_info: rope scaling     = linear
0.00.038.790 I print_info: freq_base_train  = 10000.0
0.00.038.791 I print_info: freq_scale_train = 1
0.00.038.791 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.791 I print_info: rope_finetuned   = unknown
0.00.038.791 I print_info: ssm_d_conv       = 0
0.00.038.791 I print_info: ssm_d_inner      = 0
0.00.038.791 I print_info: ssm_d_state      = 0
0.00.038.791 I print_info: ssm_dt_rank      = 0
0.00.038.791 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.792 I print_info: model type       = 1.4B
0.00.038.792 I print_info: model params     = 1.41 B
0.00.038.792 I print_info: general.name     = 1.4B
0.00.038.793 I print_info: vocab type       = BPE
0.00.038.793 I print_info: n_vocab          = 50304
0.00.038.793 I print_info: n_merges         = 50009
0.00.038.793 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.794 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.794 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.794 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.794 I print_info: LF token         = 187 ''
0.00.038.794 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.794 I print_info: max token length = 1024
0.00.038.795 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.579.053 I load_tensors: offloading 24 repeating layers to GPU
0.00.579.060 I load_tensors: offloading output layer to GPU
0.00.579.061 I load_tensors: offloaded 25/25 layers to GPU
0.00.579.092 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.579.096 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.580.392 I llama_init_from_model: n_seq_max     = 1
0.00.580.394 I llama_init_from_model: n_ctx         = 128
0.00.580.395 I llama_init_from_model: n_ctx_per_seq = 128
0.00.580.395 I llama_init_from_model: n_batch       = 128
0.00.580.396 I llama_init_from_model: n_ubatch      = 128
0.00.580.396 I llama_init_from_model: flash_attn    = 0
0.00.580.397 I llama_init_from_model: freq_base     = 10000.0
0.00.580.398 I llama_init_from_model: freq_scale    = 1
0.00.580.399 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.580.400 I ggml_metal_init: allocating
0.00.580.490 I ggml_metal_init: found device: Apple M4
0.00.580.501 I ggml_metal_init: picking default device: Apple M4
0.00.582.108 I ggml_metal_init: using embedded metal library
0.00.588.210 I ggml_metal_init: GPU name:   Apple M4
0.00.588.214 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.588.215 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.588.215 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.588.216 I ggml_metal_init: simdgroup reduction   = true
0.00.588.216 I ggml_metal_init: simdgroup matrix mul. = true
0.00.588.216 I ggml_metal_init: has residency sets    = true
0.00.588.216 I ggml_metal_init: has bfloat            = true
0.00.588.217 I ggml_metal_init: use bfloat            = true
0.00.588.218 I ggml_metal_init: hasUnifiedMemory      = true
0.00.588.228 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.604.888 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.608.315 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.608.319 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.608.363 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.611.608 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.611.610 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.611.610 I llama_init_from_model: graph nodes  = 967
0.00.611.610 I llama_init_from_model: graph splits = 2
0.00.611.613 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.611.613 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.643.892 I 
0.00.643.938 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.643.943 I perplexity: tokenizing the input ..
0.00.650.531 I perplexity: tokenization took 6.586 ms
0.00.650.537 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.783.330 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.784.842 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.784.866 I llama_perf_context_print:        load time =     634.87 ms
0.00.784.866 I llama_perf_context_print: prompt eval time =     131.86 ms /   128 tokens (    1.03 ms per token,   970.72 tokens per second)
0.00.784.867 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.784.867 I llama_perf_context_print:       total time =     140.98 ms /   129 tokens
0.00.785.256 I ggml_metal_free: deallocating

real	0m0.800s
user	0m0.078s
sys	0m0.128s
```
- save-load-state: 
```
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4772 (4d1051a4)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 ''
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x141e04920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x141e080e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x141e08690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x141e08c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x141e091f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x141e097a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x141e09d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x141e0a300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x141e0a8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x141e0adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x141e0b2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x141e0b7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x141e0c2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x141e0ca80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x141e0d290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x141e0d9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x141e0e0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x141e0e7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x141e0ef10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x141e0f6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x141e0fe00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x141e10520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x141e10c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x141e114e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x141e11c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x141e11ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x141e124d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x141e13140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x141e13680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x141e13940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x141e13de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x141e140a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x141e14930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x141e14e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x141e15130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x141e155d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x141e15a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x141e15f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x141e163b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x141e16850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x141e16cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x141e17190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x141e17630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x141e17ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x141e17d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x141e183a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x141e189b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x141e192d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x141e198e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x141e19ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x141e1a500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x141e1ab10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x141e1b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x141e1b730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x141e1bf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x141e1c3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x141e1c860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x141e1cb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x141e1d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x141e1d920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x141e1dbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x141e1e080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x141e1e520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x141e1e9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x141e1ee60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x141e1f300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x141e1f7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x141e1fc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x141e200e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x141e20580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x141e20a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x141e20ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x141e21360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x141e218b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x141e21e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x141e22350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x141e228a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x141e22df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x141e23340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x141e23890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x141e23de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x141e24330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x141e24880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x141e24dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x141e25320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x141e25870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x141e25dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x141e26310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x141e26860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x141e26db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x141e27300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x141e27850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x141e27da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x141e282f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x141e28840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x141e28d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x141e292e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x141e18fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x141e29750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x141e29f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x141e2a450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x141e2a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x141e2aef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x141e2b440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x141e2b990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x141e2bee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x141e2c430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x141e2c980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x141e2ced0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x141e2d420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x141e2d970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x141e2dec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x141e2e410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x141e2e8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x141e2ed50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x141e2f1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x141e2f690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x141e2fb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x141e2ffd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x141e30470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x141e30910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x141e30db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x141e31250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x141e316f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x141e31b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x141e32030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x141e324d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x141e32970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x141e32e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x141e332b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x141e33750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x141e33bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x141e34090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x141e34530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x141e349d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x141e34e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x141e35310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x141e357b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x141e35c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x141e360f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x141e36590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x141e36a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x141e36ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x141e37370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x141e37810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x141e37cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x141e38150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x141e385f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x141e38a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x141e38f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x141e393d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x141e39870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x141e39d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x141e3a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x141e3a650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x141e3aaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x141e3af90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x141e3b430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x141e3b8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x141e3bd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x141e3c210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x141e3c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x141e3cb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x141e3cff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x141e3d490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x141e3d930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x141e3ddd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x141e3e270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x141e3e710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x141e3ebb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x141e3f050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x141e3f4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x141e3f990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x141e3fe30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x141e402d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x141e40770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x141e40c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x141e410b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x141e41550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x141e419f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x141e41e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x141e42330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x141e427d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x141e42c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x141e43110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x141e435b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x141e43a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x141e43ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x141e44390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x141e44830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x141e44cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x141e45170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x141e45610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x141e45b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x141e460b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x141e46600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x141e46b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x141e46e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x141e47420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x141e47a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x141e48040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x141e48830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x141e48cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x141e48f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x141e495a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x141e49bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x141e4a3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x141e4a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x141e4ace0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x141e4b180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x141e4b930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x141e4be80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x141e4c3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x141e4c920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x141e4ce70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x141e4d3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x141e4d910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x141e4de60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x141e4e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x141e4e900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x141e4ee50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x141e4f3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x141e4f8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x141e4fe40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x141e50390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x141e508e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x141e50e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x141e51380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x141e518d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x141e51e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x141e52370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x141e528c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x141e52e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x141e53360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x141e538b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x141e53e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x141e54350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x141e548a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x141e54df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x141e55340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x141e55890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x141e55de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x141e56330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x141e56880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x141e56dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x141e57320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x141e57870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x141e57dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x141e58310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x141e58860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x141e58db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x141e59300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x141e59850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x141e59da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x141e5a2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x141e5a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x141e5ad90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x141e5b2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x141e5b830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x141e5bd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x141e5c2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x141e5c820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x141e5cd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x141e5d2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x141e5d810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x141e5dd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x141e5e2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x141e5e750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x141e5ebf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x141e5f090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x141e5f530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x141e5f9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x141e5fe70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x141e60310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x141e607b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x141e60c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x141e610f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x141e61590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x141e61a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x141e61ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x141e62370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x141e62810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x141e62cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x141e63150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x141e635f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x141e63a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x141e63f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x141e643d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x141e64870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x141e64d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x141e651b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x141e65650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x141e65ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x141e662c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x141e669e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x141e67100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x141e67820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x141e67ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x141e682d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x141e68590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x141e68ba0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.727.900 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.727.904 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x141e68850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x141e476e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x141e470d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x141e47cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x141e1add0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x141e1a7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x141e1cde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x141e49860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x141e12180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x141e18c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x141e19590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x141e19ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x141e18050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x141e1a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x141e11180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x141e29a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x141e67da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x141e14360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x141e14620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x141e49e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x141e48300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x141e12790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x141e12a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x141e12d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x141e69000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x141e692c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x141e69580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x141e69840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x141e69b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x141e69dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x141e6a080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x141e6a340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x141e6a600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x141e6a8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x141e6ab80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x141e6ae40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x141e6b100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x141e6b3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x141e6b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x141e6b940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x141e6bc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x141e6bec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x141e6c180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x141e6c440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x141e6c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x141e6c9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x141e6cc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x141e6cf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x141e6d200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x141e6d4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x141e6d780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x141e6da40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x141e6dd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x141e6dfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x141e6e280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x141e6e540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x141e6e800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x141e6eac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x141e6ed80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x141e6f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x141e6f300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x124b04230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x124b046a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x124b04be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x124b05050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x124b054c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x124b05930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x124b05da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x124b06210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x124b06680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x124b06af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x124b06f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x124b073d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x124b07840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x124b07cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x124b08120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x124b08590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x124b08a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x124b08e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x124b092e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x124b09750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x124b09bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x124b0a030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x124b0a4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x124b0a910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x124b0ad80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x124b0b1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x124b0b660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x124b0bad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x124b0bf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x124b0c3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x124b0c820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x124b0cc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x124b0d100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x124b0d570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x124b0d9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x124b0de50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x124b0e2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x124b0e730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x124b0eba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x124b0f010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x124b0f480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x124b0f8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x124b0fd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x124b101d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x124b10640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x124b10ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x124b10f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x124b11390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x124b11800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x124b11c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x124b120e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x124b12550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x124b129c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x124b12e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x124b132a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x124b13710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x124b13b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x124b13ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x124b14460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x124b148d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x124b14d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x124b151b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x124b15620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x124b15a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x124b15f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x124b16370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x124b167e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x124b16c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x124b170c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x124b17530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x124b179a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x124b17e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x124b18280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x124b186f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x124b18b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x124b18fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x124b19440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x124b198b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x124b19d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x124b1a190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x124b1a600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x124b1aa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x124b1aee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x124b1b350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x124b1b7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x124b1bc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x124b1c0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x124b1c510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x124b1c980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x124b1cdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x124b1d260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x124b1d6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x124b1db40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x124b1dfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x124b1e420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x124b1e890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x124b1ed00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x124b1f170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x124b1f5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x124b1fa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x124b1fec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x124b20330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x124b207a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x124b20c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x124b21080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x124b21c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x124b21f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x124b22210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x124b22680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x124b22af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x124b22f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x124b233d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x124b23840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x124b23cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x124b24120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x124b24590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x124b24a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x124b24e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x124b252e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x124b25750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x124b25bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x124b26030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x124b264a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x124b26910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x124b26d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x124b271f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x124b27660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x124b27ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x124b27f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x124b283b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x124b28820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x124b28c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x124b29100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x124b29570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x124b299e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x124b29e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x124b2a2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x124b2a730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x124b2aba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x124b2b010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x124b2b480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x124b2b9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x124b2bef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x124b2c360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x124b2c7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x124b2cc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x124b2d0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x124b2d5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x124b2dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x124b2e650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x124b2e910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x124b2eed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x124b2f490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x124b2fa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x124b30010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x124b305d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x124b30b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x124b31150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x124b31710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x124b31cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x124b32290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x124b32850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x124b32e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x124b333d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x124b33990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x124b33f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x124b34510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x124b34ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x124b35090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x124b35650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x124b35c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x124b361d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x124b36790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x124b36d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x124b37310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x124b378d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x124b37e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x124b38450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x124b38a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x124b38fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x124b39590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x124b39b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x124b3a110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x124b3a6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x124b3ac90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x124b3b250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x124b3b810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x124b3bdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x124b3c390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x124b3c950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x124b3cf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x124b3d4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x124b3da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x124b3e050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x124b3e610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x124b3ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x124b3f190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x124b3f750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x124b3fd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x124b402d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x124b40890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x124b40e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x124b41410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x124b419d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x124b41f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x124b42550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x124b42b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x124b43010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x124b43510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x124b43a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x124b43f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x124b44410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x124b44910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x124b44e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x124b45310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x124b45810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x124b45d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x124b46210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x124b46710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x124b46c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x124b47110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x124b47610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x124b47b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x124b48010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x124b48510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x124b48a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x124b48f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x124b49410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x124b49910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x124b49e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x124b4a310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x124b4a810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x124b4b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x124b4b940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x124b4c060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x124b4c780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x124b4ca40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x124b4d230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x124b4d4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x124b4db00 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x115c046e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x115c04b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x115c04fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x115c05430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x115c058a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x115c05d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x115c06180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x115c065f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x115c06a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x115c06ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x115c07340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x115c07a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x115c08580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x115c08d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x115c09540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x115c09c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x115c0a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x115c0aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x115c0b1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x115c0b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x115c0c010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x115c0c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x115c0ce50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x115c0d570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x115c0dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x115c0df50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x115c0e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x115c0e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x115c0eaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x115c0ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x115c0f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x115c0f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x115c0fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x115c10030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x115c104a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x115c10910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x115c10d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x115c111f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x115c11660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x115c11ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x115c11f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x115c123b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x115c12820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x115c12c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x115c13100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x115c13570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x115c139e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x115c13e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x115c142c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x115c14730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x115c14ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x115c15010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x115c15480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x115c158f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x115c15d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x115c161d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x115c16740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x115c16c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x115c170b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x115c17520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x115c17990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x115c17e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x115c18270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x115c186e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x115c18b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x115c18fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x115c19430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x115c198a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x115c19d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x115c1a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x115c1a5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x115c1aa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x115c1aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x115c1b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x115c1b7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x115c1bc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x115c1c090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x115c1c500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x115c1c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x115c1cde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x115c1d250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x115c1d6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x115c1db30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x115c1dfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x115c1e410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x115c1e880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x115c1ecf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x115c1f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x115c1f5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x115c1fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x115c1feb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x115c20320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x115c20790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x115c20c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x115c21070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x115c214e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x115c21950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x115c21dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x115c22620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x115c22b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x115c230f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x115c236a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x115c23c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x115c24200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x115c247b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x115c24d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x115c25310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x115c258c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x115c25e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x115c26420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x115c269d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x115c26f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x115c27530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x115c27ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x115c27fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x115c284e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x115c289e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x115c28ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x115c293e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x115c298e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x115c29de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x115c2a2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x115c2a7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x115c2ace0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x115c2b1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x115c2b6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x115c2bbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x115c2c0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x115c2c5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x115c2cae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x115c2cfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x115c2d4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x115c2d9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x115c2dee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x115c2e3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x115c2e8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x115c2ede0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x115c2f2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x115c2f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x115c2fce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x115c301e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x115c306e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x115c30be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x115c310e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x115c315e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x115c31ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x115c31fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x115c324e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x115c329e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x115c32ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x115c333e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x115c338e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x115c33de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x115c342e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x115c347e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x115c34ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x115c351e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x115c356e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x115c35be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x115c360e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x115c365e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x115c36ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x115c36fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x115c374e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x115c379e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x115c37ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x115c383e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x115c388e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x115c38de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x115c392e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x115c397e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x115c39ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x115c3a1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x115c3a6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x115c3abe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x115c3b0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x115c3b5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x115c3bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x115c3bfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x115c3c4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x115c3c9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x115c3cee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x115c3d3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x115c3d8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x115c3dde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x115c3e2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x115c3e7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x115c3ece0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x115c3f1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x115c3f6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x115c3fbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x115c400e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x115c405e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x115c40ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x115c41090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x115c41640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x115c41bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x115c421a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x115c427b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x115c42dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x115c433d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x115c43bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x115c44060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x115c44320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x115c44930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x115c44f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x115c45730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x115c45bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x115c46070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x115c46510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x115c46cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x115c47210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x115c47760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x115c47cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x115c48200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x115c48750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x115c48ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x115c491f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x115c49740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x115c49c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x115c4a1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x115c4a730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x115c4ac80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x115c4b1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x115c4b720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x115c4bc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x115c4c1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x115c4c710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x115c4cc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x115c4d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x115c4d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x115c4dc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x115c4e1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x115c4e6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x115c4ec40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x115c4f190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x115c4f6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x115c4fc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x115c50180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x115c506d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x115c50c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x115c51170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x115c516c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x115c51c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x115c52160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x115c526b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x115c52c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x115c53150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x115c536a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x115c53bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x115c54140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x115c54690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x115c54be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x115c55130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x115c55680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x115c55bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x115c56120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x115c56670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x115c56bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x115c57110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x115c57660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x115c57bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x115c58100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x115c58650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x115c58ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x115c590f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x115c59640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x115c59ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x115c59f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x115c5a420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x115c5a8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x115c5ad60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x115c5b200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x115c5b6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x115c5bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x115c5bfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x115c5c480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x115c5c920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x115c5cdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x115c5d260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x115c5d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x115c5dba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x115c5e040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x115c5e4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x115c5e980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x115c5ee20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x115c5f2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x115c5f760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x115c5fc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x115c600a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x115c60540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x115c609e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x115c60f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x115c61650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x115c61d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x115c62490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x115c62bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x115c62e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x115c63660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x115c63920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x115c63f30 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.772s
user	0m0.280s
sys	0m0.325s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4772 (4d1051a4)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 ''
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14e60b600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14e60bd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14e60c2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14e60c860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14e60ce10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14e60d3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14e60d970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14e60df20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14e60e4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14e60e9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14e60eed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14e60f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14e60fef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14e6106a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14e610eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14e6115d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14e611cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14e612410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14e612b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14e613300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14e613a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14e614140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14e614860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14e615100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14e615820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14e615ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14e6160f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14e616d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14e6172a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14e617560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14e617a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14e617cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14e618550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14e618a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14e618d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14e6191f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14e619690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14e619b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14e619fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14e61a470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14e61a910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14e61adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14e61b250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14e61b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14e61b9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14e61bfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14e61c5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14e61cef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14e61d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14e61db10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14e61e120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14e61e730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14e61ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14e61f350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14e61fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14e61ffe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14e620480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14e620740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14e620d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14e621540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14e621800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14e621ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14e622140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14e6225e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14e622a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14e622f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14e6233c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14e623860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14e623d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14e6241a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14e624640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14e624ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14e624f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14e6254d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14e625a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14e625f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14e6264c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14e626a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14e626f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14e6274b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14e627a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x14e627f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x14e6284a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x14e6289f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x14e628f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x14e629490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x14e6299e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x14e629f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x14e62a480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x14e62a9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x14e62af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x14e62b470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x14e62b9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x14e62bf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x14e62c460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x14e62c9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x14e62cf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x14e61cbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x14e62d370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x14e62db20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x14e62e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x14e62e5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x14e62eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x14e62f060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x14e62f5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x14e62fb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x14e630050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x14e6305a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x14e630af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x14e631040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x14e631590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x14e631ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x14e632030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14e6324d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14e632970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14e632e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14e6332b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14e633750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14e633bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14e634090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14e634530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14e6349d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14e634e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14e635310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14e6357b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14e635c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14e6360f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14e636590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14e636a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14e636ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14e637370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14e637810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14e637cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14e638150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14e6385f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14e638a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14e638f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14e6393d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14e639870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14e639d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14e63a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14e63a650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14e63aaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14e63af90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14e63b430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14e63b8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14e63bd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14e63c210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14e63c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14e63cb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14e63cff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14e63d490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14e63d930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14e63ddd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14e63e270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14e63e710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14e63ebb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14e63f050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14e63f4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14e63f990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14e63fe30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14e6402d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14e640770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14e640c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14e6410b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14e641550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14e6419f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14e641e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14e642330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14e6427d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14e642c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14e643110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14e6435b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14e643a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14e643ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14e644390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14e644830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14e644cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14e645170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14e645610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14e645ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14e645f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14e6463f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14e646890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14e646d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14e6471d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14e647670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14e647b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14e647fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14e648450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14e6488f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14e648d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14e649230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14e649780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14e649cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14e64a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14e64a770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14e64aa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14e64b040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14e64b650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14e64bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14e64c450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14e64c8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14e64cbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14e64d1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x14e64d7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14e64dfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14e64e460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14e64e900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14e64eda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14e64f550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14e64faa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14e64fff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14e650540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14e650a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14e650fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14e651530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14e651a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14e651fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14e652520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14e652a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14e652fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14e653510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14e653a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14e653fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14e654500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14e654a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14e654fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14e6554f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14e655a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14e655f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14e6564e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14e656a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14e656f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14e6574d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14e657a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14e657f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14e6584c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14e658a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14e658f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14e6594b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14e659a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14e659f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14e65a4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14e65a9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14e65af40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14e65b490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14e65b9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14e65bf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14e65c480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14e65c9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14e65cf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14e65d470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14e65d9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14e65df10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14e65e460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14e65e9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14e65ef00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14e65f450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14e65f9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14e65fef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14e660440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14e660990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14e660ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14e661430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14e661980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14e661ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x14e662370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x14e662810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14e662cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14e663150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14e6635f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14e663a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14e663f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14e6643d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14e664870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14e664d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14e6651b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14e665650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14e665af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14e665f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14e666430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x14e6668d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x14e666d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x14e667210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x14e6676b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x14e667b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x14e667ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x14e668490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x14e668930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x14e668dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x14e669270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14e6697c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14e669ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14e66a600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14e66ad20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14e66b440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14e66b700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x14e66bef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14e66c1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14e66c7c0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.099.439 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.099.443 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14f804ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14f804f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14f8053c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14f805830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14f805ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14f806110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14f806580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14f8069f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14f806e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14f8072d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14f807740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14f807e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14f808940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14f8090f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14f809900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14f80a020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14f80a740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14f80ae60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14f80b580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14f80bd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14f80c470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14f80cb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14f80d2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14f80d9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14f80e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14f80e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14f80e670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14f80eae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14f80ef50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14f80f3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14f80f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14f80fd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14f8101d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14f810490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14f810900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14f810d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14f8111e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14f811650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14f811ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14f811f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14f8123a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14f812810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14f812c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14f8130f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14f813560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14f8139d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14f813e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14f8142b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14f814720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14f814b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14f815000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14f815470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14f8158e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14f815d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14f8161c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14f816630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14f816ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14f8170a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14f817510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14f817980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14f817df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14f818260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14f8186d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14f818b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14f818fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14f819420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14f819890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14f819d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14f81a170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14f81a5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14f81aa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14f81aec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14f81b330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14f81b7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14f81bc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14f81c080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14f81c4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14f81c960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14f81cdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14f81d240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14f81d6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x14f81db20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x14f81df90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x14f81e400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x14f81e870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x14f81ece0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x14f81f150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x14f81f5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x14f81fa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x14f81fea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x14f820310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x14f820780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x14f820bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x14f821060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x14f8214d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x14f821940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x14f821db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x14f822220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x14f822690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x14f822b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x14f822f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x14f8233e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x14f823850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x14f823cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x14f824130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x14f8245a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x14f824a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x14f824e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x14f8252f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x14f825760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x14f825bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x14f826040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x14f8264b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14f826920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14f826d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14f827200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14f827670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14f827ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14f827f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14f8283c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14f828830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14f828ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14f829110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14f829580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14f8299f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14f829e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14f82a2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14f82a740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14f82abb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14f82b020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14f82b490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14f82b900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14f82bd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14f82c1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14f82c650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14f82cac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14f82cf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14f82d3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14f82d810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14f82dc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14f82e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14f82e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14f82e9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14f82ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14f82f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14f82f720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14f82fb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14f830000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14f830470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14f8308e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14f830d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14f8311c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14f831630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14f831aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14f831f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14f832380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14f8327f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14f832c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14f8330d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14f833540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14f8339b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14f833e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14f834290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14f834700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14f834b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14f834fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14f835c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14f835ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14f836190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14f836600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14f836a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14f836ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14f837350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14f8377c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14f837c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14f8380a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14f838510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14f838980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14f838df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14f839260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14f8396d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14f839b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14f839fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14f83a420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14f83a890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14f83ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14f83b170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14f83b5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14f83ba50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14f83bec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14f83c330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14f83c7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14f83cc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14f83d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14f83d4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14f83d960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14f83ddd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14f83e240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14f83e6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14f83eb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14f83ef90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14f83f400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14f83f960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14f83fe70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14f8402e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x14f840750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14f840bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14f841030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14f841550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14f841a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14f8425d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14f842890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14f842e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14f843410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14f8439d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14f843f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14f844550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14f844b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14f8450d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14f845690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14f845c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14f846210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14f8467d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14f846d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14f847350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14f847910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14f847ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14f848490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14f848a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14f849010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14f8495d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14f849b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14f84a150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14f84a710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14f84acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14f84b290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14f84b850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14f84be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14f84c3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14f84c990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14f84cf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14f84d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14f84dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14f84e090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14f84e650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14f84ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14f84f1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14f84f790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14f84fd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14f850310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14f8508d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14f850e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14f851450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14f851a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14f851fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14f852590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14f852b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14f853110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14f8536d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14f853c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14f854250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14f854810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14f854dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14f855390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14f855950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14f855f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14f8564d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x14f856a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x14f856f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14f857490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14f857990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14f857e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14f858390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14f858890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14f858d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14f859290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14f859790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14f859c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14f85a190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14f85a690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14f85ab90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14f85b090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x14f85b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x14f85ba90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x14f85bf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x14f85c490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x14f85c990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x14f85ce90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x14f85d390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x14f85d890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x14f85dd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x14f85e290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14f85e790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14f85f1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14f85f8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14f85ffe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14f860700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14f8609c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x14f8611b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14f861470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14f861a80 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14e66c470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14e64b300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14e64acf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14e64b910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14e61e9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14e61e3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14e620a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14e64d480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14e615da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14e61c890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14e61d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14e61d7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14e61bc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14e61ddd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14e614da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14e621010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14e62d630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14e66b9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14e617f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14e618240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14e64da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14e64bf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14e6163b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14e616670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14e616930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14e66cc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14e66cee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14e66d1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14e66d460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14e66d720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14e66d9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14e66dca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14e66df60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14e66e220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14e66e4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14e66e7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14e66ea60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14e66ed20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14e66efe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14e66f2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14e66f560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14e66f820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14e66fae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14e66fda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14e670060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14e670320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14e6705e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14e6708a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14e670b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14e670e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14e6710e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14e6713a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14e671660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14e671920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14e671be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14e671ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14e672160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14e672420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14e6726e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14e6729a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14e672c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14e672f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14e6731e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14e6734a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14e673760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14e673a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14e673ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14e673fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14e674260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14e674520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14e6747e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14e674aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14e674d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14e675020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14e6752e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14e6755a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14e675860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14e675b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14e675de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14e6760a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14e676360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x14e676620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x14e6768e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x14e676ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x14e676e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x14e677120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x14e6773e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x14e6776a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x14e677960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x14e677c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x14e677ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x14e6781a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x14e678460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x14e678720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x14e6789e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x14e678ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x14e678f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x14e679220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x14e6794e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x14e6797a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x14e679a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x14e679d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x14e679fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x14e67a2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x14e67a560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x14e67a820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x14e67aae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x14e67ada0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x14e67b060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x14e67b320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x14e67b5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x14e67b8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x14e67bb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14e67be20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14e67c0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14e67c3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14e67c660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14e67c920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14e67cbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14e67cea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14e67d160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14e67d420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14e67d6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14e67d9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14e67dc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14e67df20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14e67e1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14e67e4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14e67e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14e67ea20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14e67ece0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14e67efa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14e67f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14e67f520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14e67f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14e67faa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14e67fd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14e680020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14e6802e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14e6805a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14e680860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14e680b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14e680de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14e6810a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14e681360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14e681620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14e6818e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14e681ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14e681e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14e682120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14e6823e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14e6826a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14e682960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14e682c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14e682ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14e6831a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14e683460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14e683720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14e6839e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14e683ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14e683f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14e684220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14e6844e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14e6847a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14e684a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14e684d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14e684fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14e6852a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14e685560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14e685820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14e685ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14e685da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14e686060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14e686320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14e6865e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14e6868a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14e686b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14e686e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14e6870e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14e6873a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14e687660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14e687920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14e687be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14e687ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14e688160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14e688420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14e6886e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14e6889a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14e688c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14e688f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14e6891e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14e6894a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14e689760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14e689a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14e689ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14e689fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14e68a260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14e68a520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14e68a7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14e68aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14e68ad60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14e68b020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14e68b2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14e68b5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14e68b860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x14e68bb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14e68bde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14e68c0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14e68c360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14e68c620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14e68cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14e68ceb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14e68d170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14e68d430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14e68d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14e68d9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14e68dc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14e68df30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14e68e480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14e68e9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14e68ef20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14e68f470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14e68f9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14e68ff10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14e690460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14e6909b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14e690f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14e691450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14e6919a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14e691ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14e692440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14e692990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14e692ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14e693430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14e693980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14e693ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14e694420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14e694970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14e694ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14e695410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14e695960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14e695eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14e696400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14e696950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14e696ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14e6973f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14e697940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14e697e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14e6983e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14e698930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14e698e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14e6993d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14e699920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14e699e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14e69a3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14e69a910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14e69ae60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14e69b3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14e69b900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14e69be50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14e69c3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14e69c8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14e69ce40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14e69d390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14e69d8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14e69de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14e69e380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x14e69e640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x14e69e900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14e69ebc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14e69f030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14e69f4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14e69f910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14e69fd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14e6a01f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14e6a0660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14e6a0ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14e6a0f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14e6a13b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14e6a1820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14e6a1c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14e6a2100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x14e6a2570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x14e6a29e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x14e6a2e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x14e6a32c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x14e6a3730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x14e6a3ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x14e6a4010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x14e6a4480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x14e6a48f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x14e6a4d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14e6a51d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14e6a5c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14e6a6350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14e6a6a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14e6a7190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14e6a7450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x14e6a78c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14e6a7ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14e6a84d0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.956s
user	0m0.231s
sys	0m0.188s
```
### ctest_with_model_debug

Runs ctest with model files in debug mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-debug
    Start 26: test-model-load-cancel
1/2 Test #26: test-model-load-cancel ...........   Passed    0.44 sec
    Start 27: test-autorelease
2/2 Test #27: test-autorelease .................   Passed    1.52 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   1.96 sec*proc (2 tests)

Total Test time (real) =   1.97 sec
        1.99 real         0.52 user         0.24 sys
```
### ctest_with_model_release

Runs ctest with model files in release mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-release
    Start 26: test-model-load-cancel
1/2 Test #26: test-model-load-cancel ...........   Passed    0.24 sec
    Start 27: test-autorelease
2/2 Test #27: test-autorelease .................   Passed    0.30 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   0.55 sec*proc (2 tests)

Total Test time (real) =   0.56 sec
        0.56 real         0.13 user         0.08 sys
```
