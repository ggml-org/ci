### ctest_debug

Runs ctest in debug mode
- status: 0
```
+ ctest --output-on-failure -L main -E test-opt
Test project /Users/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-tokenizer-0-bert-bge
 1/29 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.32 sec
      Start  2: test-tokenizer-0-command-r
 2/29 Test  #2: test-tokenizer-0-command-r ........   Passed    1.15 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/29 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.17 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/29 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.46 sec
      Start  5: test-tokenizer-0-falcon
 5/29 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.29 sec
      Start  6: test-tokenizer-0-gpt-2
 6/29 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.23 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/29 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.69 sec
      Start  8: test-tokenizer-0-llama-spm
 8/29 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.09 sec
      Start  9: test-tokenizer-0-mpt
 9/29 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.24 sec
      Start 10: test-tokenizer-0-phi-3
10/29 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.08 sec
      Start 11: test-tokenizer-0-qwen2
11/29 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.63 sec
      Start 12: test-tokenizer-0-refact
12/29 Test #12: test-tokenizer-0-refact ...........   Passed    0.23 sec
      Start 13: test-tokenizer-0-starcoder
13/29 Test #13: test-tokenizer-0-starcoder ........   Passed    0.23 sec
      Start 14: test-sampling
14/29 Test #14: test-sampling .....................   Passed    2.24 sec
      Start 15: test-grammar-parser
15/29 Test #15: test-grammar-parser ...............   Passed    0.18 sec
      Start 16: test-grammar-integration
16/29 Test #16: test-grammar-integration ..........   Passed    0.24 sec
      Start 17: test-llama-grammar
17/29 Test #17: test-llama-grammar ................   Passed    0.19 sec
      Start 18: test-chat
18/29 Test #18: test-chat .........................   Passed   17.97 sec
      Start 19: test-json-schema-to-grammar
19/29 Test #19: test-json-schema-to-grammar .......   Passed    2.30 sec
      Start 20: test-tokenizer-1-llama-spm
20/29 Test #20: test-tokenizer-1-llama-spm ........   Passed    1.13 sec
      Start 21: test-log
21/29 Test #21: test-log ..........................   Passed    0.21 sec
      Start 22: test-arg-parser
22/29 Test #22: test-arg-parser ...................   Passed    0.28 sec
      Start 23: test-chat-template
23/29 Test #23: test-chat-template ................   Passed    3.00 sec
      Start 24: test-gguf
24/29 Test #24: test-gguf .........................   Passed    0.79 sec
      Start 25: test-backend-ops
25/29 Test #25: test-backend-ops ..................   Passed  193.12 sec
      Start 28: test-barrier
26/29 Test #28: test-barrier ......................   Passed    0.95 sec
      Start 29: test-quantize-fns
27/29 Test #29: test-quantize-fns .................   Passed   26.23 sec
      Start 30: test-quantize-perf
28/29 Test #30: test-quantize-perf ................   Passed    0.34 sec
      Start 31: test-rope
29/29 Test #31: test-rope .........................   Passed    0.21 sec

100% tests passed, 0 tests failed out of 29

Label Time Summary:
main    = 255.16 sec*proc (29 tests)

Total Test time (real) = 255.17 sec

real	4m15.235s
user	8m32.024s
sys	0m7.779s
```

### ctest_release

Runs ctest in release mode
- status: 0
```
+ ctest --output-on-failure -L main
Test project /Users/ggml/work/llama.cpp/build-ci-release
      Start  1: test-tokenizer-0-bert-bge
 1/29 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.14 sec
      Start  2: test-tokenizer-0-command-r
 2/29 Test  #2: test-tokenizer-0-command-r ........   Passed    0.22 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/29 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.04 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/29 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.08 sec
      Start  5: test-tokenizer-0-falcon
 5/29 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.06 sec
      Start  6: test-tokenizer-0-gpt-2
 6/29 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.05 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/29 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.14 sec
      Start  8: test-tokenizer-0-llama-spm
 8/29 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.03 sec
      Start  9: test-tokenizer-0-mpt
 9/29 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.05 sec
      Start 10: test-tokenizer-0-phi-3
10/29 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.03 sec
      Start 11: test-tokenizer-0-qwen2
11/29 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.11 sec
      Start 12: test-tokenizer-0-refact
12/29 Test #12: test-tokenizer-0-refact ...........   Passed    0.05 sec
      Start 13: test-tokenizer-0-starcoder
13/29 Test #13: test-tokenizer-0-starcoder ........   Passed    0.05 sec
      Start 14: test-sampling
14/29 Test #14: test-sampling .....................   Passed    0.93 sec
      Start 15: test-grammar-parser
15/29 Test #15: test-grammar-parser ...............   Passed    0.16 sec
      Start 16: test-grammar-integration
16/29 Test #16: test-grammar-integration ..........   Passed    0.18 sec
      Start 17: test-llama-grammar
17/29 Test #17: test-llama-grammar ................   Passed    0.17 sec
      Start 18: test-chat
18/29 Test #18: test-chat .........................   Passed    1.71 sec
      Start 19: test-json-schema-to-grammar
19/29 Test #19: test-json-schema-to-grammar .......   Passed    2.19 sec
      Start 20: test-tokenizer-1-llama-spm
20/29 Test #20: test-tokenizer-1-llama-spm ........   Passed    0.30 sec
      Start 21: test-log
21/29 Test #21: test-log ..........................   Passed    0.18 sec
      Start 22: test-arg-parser
22/29 Test #22: test-arg-parser ...................   Passed    0.21 sec
      Start 23: test-chat-template
23/29 Test #23: test-chat-template ................   Passed    0.45 sec
      Start 24: test-gguf
24/29 Test #24: test-gguf .........................   Passed    0.42 sec
      Start 25: test-backend-ops
25/29 Test #25: test-backend-ops ..................   Passed   30.84 sec
      Start 28: test-barrier
26/29 Test #28: test-barrier ......................   Passed    0.36 sec
      Start 29: test-quantize-fns
27/29 Test #29: test-quantize-fns .................   Passed   14.17 sec
      Start 30: test-quantize-perf
28/29 Test #30: test-quantize-perf ................   Passed    0.21 sec
      Start 31: test-rope
29/29 Test #31: test-rope .........................   Passed    0.22 sec

100% tests passed, 0 tests failed out of 29

Label Time Summary:
main    =  54.74 sec*proc (29 tests)

Total Test time (real) =  54.76 sec

real	0m54.766s
user	1m17.383s
sys	0m6.347s
```
### embd_bge_small

BGE Small (BERT):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.187 I build: 4671 (4d3465c5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.021.989 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.027.615 I llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.027.623 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.027.626 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.027.627 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.027.627 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.027.628 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.027.629 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.027.630 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.027.631 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.027.632 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.027.633 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.027.633 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.027.637 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.027.637 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.027.642 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.027.643 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.027.643 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.027.644 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.027.644 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.032.908 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.034.113 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.034.115 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.034.115 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.034.116 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.034.116 I llama_model_loader: - kv  22:               tokenizer.ggml.mask_token_id u32              = 103
0.00.034.117 I llama_model_loader: - kv  23:               general.quantization_version u32              = 2
0.00.034.118 I llama_model_loader: - type  f32:  124 tensors
0.00.034.118 I llama_model_loader: - type  f16:   73 tensors
0.00.034.119 I print_info: file format = GGUF V3 (latest)
0.00.034.120 I print_info: file type   = F16
0.00.034.121 I print_info: file size   = 63.84 MiB (16.12 BPW) 
0.00.038.657 I load: special tokens cache size = 5
0.00.040.685 I load: token to piece cache size = 0.2032 MB
0.00.040.689 I print_info: arch             = bert
0.00.040.689 I print_info: vocab_only       = 0
0.00.040.690 I print_info: n_ctx_train      = 512
0.00.040.690 I print_info: n_embd           = 384
0.00.040.690 I print_info: n_layer          = 12
0.00.040.694 I print_info: n_head           = 12
0.00.040.695 I print_info: n_head_kv        = 12
0.00.040.695 I print_info: n_rot            = 32
0.00.040.697 I print_info: n_swa            = 0
0.00.040.698 I print_info: n_embd_head_k    = 32
0.00.040.698 I print_info: n_embd_head_v    = 32
0.00.040.698 I print_info: n_gqa            = 1
0.00.040.699 I print_info: n_embd_k_gqa     = 384
0.00.040.700 I print_info: n_embd_v_gqa     = 384
0.00.040.701 I print_info: f_norm_eps       = 1.0e-12
0.00.040.702 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.702 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.702 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.702 I print_info: f_logit_scale    = 0.0e+00
0.00.040.703 I print_info: n_ff             = 1536
0.00.040.704 I print_info: n_expert         = 0
0.00.040.704 I print_info: n_expert_used    = 0
0.00.040.704 I print_info: causal attn      = 0
0.00.040.704 I print_info: pooling type     = 2
0.00.040.704 I print_info: rope type        = 2
0.00.040.705 I print_info: rope scaling     = linear
0.00.040.706 I print_info: freq_base_train  = 10000.0
0.00.040.706 I print_info: freq_scale_train = 1
0.00.040.706 I print_info: n_ctx_orig_yarn  = 512
0.00.040.706 I print_info: rope_finetuned   = unknown
0.00.040.707 I print_info: ssm_d_conv       = 0
0.00.040.707 I print_info: ssm_d_inner      = 0
0.00.040.707 I print_info: ssm_d_state      = 0
0.00.040.707 I print_info: ssm_dt_rank      = 0
0.00.040.708 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.708 I print_info: model type       = 33M
0.00.040.708 I print_info: model params     = 33.21 M
0.00.040.709 I print_info: general.name     = Bge Small
0.00.040.710 I print_info: vocab type       = WPM
0.00.040.710 I print_info: n_vocab          = 30522
0.00.040.710 I print_info: n_merges         = 0
0.00.040.710 I print_info: BOS token        = 101 '[CLS]'
0.00.040.711 I print_info: UNK token        = 100 '[UNK]'
0.00.040.711 I print_info: SEP token        = 102 '[SEP]'
0.00.040.711 I print_info: PAD token        = 0 '[PAD]'
0.00.040.711 I print_info: MASK token       = 103 '[MASK]'
0.00.040.712 I print_info: LF token         = 0 '[PAD]'
0.00.040.712 I print_info: max token length = 21
0.00.040.713 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.044.024 I load_tensors: offloading 12 repeating layers to GPU
0.00.044.026 I load_tensors: offloading output layer to GPU
0.00.044.026 I load_tensors: offloaded 13/13 layers to GPU
0.00.044.052 I load_tensors: Metal_Mapped model buffer size =    40.73 MiB
0.00.044.054 I load_tensors:   CPU_Mapped model buffer size =    23.11 MiB
................................................
0.00.044.355 I llama_init_from_model: n_seq_max     = 1
0.00.044.356 I llama_init_from_model: n_ctx         = 512
0.00.044.357 I llama_init_from_model: n_ctx_per_seq = 512
0.00.044.357 I llama_init_from_model: n_batch       = 2048
0.00.044.357 I llama_init_from_model: n_ubatch      = 2048
0.00.044.358 I llama_init_from_model: flash_attn    = 0
0.00.044.358 I llama_init_from_model: freq_base     = 10000.0
0.00.044.358 I llama_init_from_model: freq_scale    = 1
0.00.044.359 I ggml_metal_init: allocating
0.00.044.364 I ggml_metal_init: found device: Apple M4
0.00.044.369 I ggml_metal_init: picking default device: Apple M4
0.00.045.147 I ggml_metal_init: using embedded metal library
0.00.049.368 I ggml_metal_init: GPU name:   Apple M4
0.00.049.371 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.049.372 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.049.373 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.049.373 I ggml_metal_init: simdgroup reduction   = true
0.00.049.373 I ggml_metal_init: simdgroup matrix mul. = true
0.00.049.373 I ggml_metal_init: has residency sets    = true
0.00.049.374 I ggml_metal_init: has bfloat            = true
0.00.049.374 I ggml_metal_init: use bfloat            = true
0.00.049.374 I ggml_metal_init: hasUnifiedMemory      = true
0.00.049.375 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.061.493 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.062.190 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.062.193 I llama_init_from_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.062.194 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.063.421 I llama_init_from_model:      Metal compute buffer size =    16.00 MiB
0.00.063.423 I llama_init_from_model:        CPU compute buffer size =     2.51 MiB
0.00.063.423 I llama_init_from_model: graph nodes  = 429
0.00.063.424 I llama_init_from_model: graph splits = 2
0.00.063.425 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.063.425 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.069.185 I 
0.00.069.213 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.069.859 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.043984 -0.019895  0.007670 -0.000838  0.001372 -0.037036  0.109436  0.042589  0.092046 -0.015918  0.006773 -0.035692 -0.017889  0.015040  0.018126  0.015860 -0.011308  0.010431 -0.085223 -0.008454  0.091368 -0.017067 -0.060328 -0.024484  0.027524  0.076059  0.027984 -0.014564  0.017645 -0.033268 -0.037874 -0.019012  0.068670 -0.009845 -0.025030  0.072346 -0.046570  0.011026 -0.050252  0.047693  0.032387 -0.011761  0.022055  0.049645  0.010465  0.005795 -0.028860  0.008933 -0.018507 -0.051466 -0.046036  0.030497 -0.035418  0.054204 -0.069650  0.044238  0.029786  0.046305  0.073410 -0.042580  0.076109  0.038851 -0.181182  0.082503  0.042250 -0.064549 -0.060097 -0.017853  0.006472  0.005873  0.017196 -0.026615  0.064578  0.112612  0.035159 -0.067416  0.027073 -0.067295 -0.033493 -0.033219  0.033252  0.013523 -0.003346 -0.037473 -0.052055  0.055155 -0.001982 -0.038276  0.064447  0.028824 -0.043334 -0.029236 -0.039462  0.036315  0.008379 -0.015481 -0.036573  0.018130  0.028600  0.342831 -0.044484  0.056114  0.017624 -0.020867 -0.066808  0.000146 -0.037885 -0.030063 -0.008541 -0.021595  0.000548 -0.003219  0.004020  0.018908 -0.008548  0.025835  0.049433  0.000091  0.050920 -0.042479 -0.031884  0.023606  0.030691 -0.023148 -0.046271 -0.079278  0.115198  0.046757  0.027829 -0.040710  0.067777 -0.022955  0.010332 -0.032937 -0.018305  0.043844  0.024268  0.052413  0.007469  0.008905  0.011244 -0.074655 -0.065561 -0.026750 -0.041192 -0.023884  0.026710  0.006912  0.027741  0.052873 -0.036673  0.057707 -0.000170  0.031764 -0.019760 -0.022083  0.041053 -0.058903  0.019619  0.043144  0.043582  0.041593 -0.022527  0.027037 -0.021833  0.005430 -0.041317 -0.001239  0.024452  0.002088  0.044333 -0.022745  0.043662  0.064763  0.055430  0.037060 -0.000919  0.046111  0.045790 -0.008487  0.063057 -0.073235 -0.011935  0.032107  0.023957  0.014725 -0.033688  0.001083 -0.015827 -0.019002  0.047872  0.110842  0.028428  0.031366 -0.013288 -0.057499  0.006644  0.005140 -0.012262 -0.051443 -0.000965 -0.017642 -0.019440 -0.040937  0.009209 -0.057957  0.050961  0.052340 -0.009617 -0.040259 -0.014075 -0.024877 -0.017251  0.006302  0.006588 -0.026919  0.015600  0.030760  0.002578  0.023206 -0.022199 -0.098557 -0.051105 -0.278031 -0.014994 -0.061553 -0.027218  0.017666 -0.010939 -0.017077  0.035055  0.046994 -0.015432  0.015232 -0.025477  0.047844 -0.005949 -0.000749 -0.061008 -0.068946 -0.060377 -0.035942  0.043319 -0.055047  0.015075  0.000537 -0.058175 -0.010439  0.012643  0.151495  0.127111 -0.013622  0.041993 -0.025672  0.014026 -0.001037 -0.150469  0.044857  0.005320 -0.036274 -0.029801 -0.020192 -0.034891  0.010235  0.033548 -0.048178 -0.051795 -0.017463 -0.023492  0.047349  0.052076 -0.016783 -0.055452  0.025836 -0.005707  0.010710  0.038705  0.008191 -0.009764 -0.105779 -0.027430 -0.096101  0.025068 -0.011260  0.092355  0.056101  0.003758  0.027791  0.002092 -0.051085 -0.039890 -0.013542 -0.044956 -0.015335  0.002910 -0.043498 -0.077951  0.065217 -0.006840 -0.001630 -0.014642  0.071560  0.023714 -0.037180  0.009175  0.001562 -0.032268  0.015468  0.037880  0.000337 -0.053207  0.021321 -0.039829  0.000031  0.013400  0.019798 -0.057879  0.006473 -0.049531 -0.267858  0.039156 -0.067972  0.038248 -0.012333  0.041485 -0.016117  0.052383 -0.071363  0.011369  0.024719 -0.007232  0.082087  0.028535 -0.021504  0.040494 -0.004570 -0.074594 -0.014749  0.020037  0.002304  0.023148  0.197212 -0.043222 -0.025983 -0.004959 -0.019278  0.074258  0.001716 -0.031979 -0.036599 -0.045082  0.000547 -0.011560  0.018126 -0.029461 -0.008463  0.006419  0.050799 -0.014959  0.006176  0.026096 -0.030809  0.048055  0.114093 -0.040826 -0.011457  0.005419 -0.003599  0.025154 -0.059142  0.013753 -0.010399  0.038709  0.051449  0.035413  0.035031 -0.017027  0.026360 -0.014490 -0.050032  0.003219  0.054121  0.039723 -0.039127 

0.00.075.089 I llama_perf_context_print:        load time =      47.19 ms
0.00.075.090 I llama_perf_context_print: prompt eval time =       5.07 ms /     9 tokens (    0.56 ms per token,  1774.45 tokens per second)
0.00.075.091 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.075.092 I llama_perf_context_print:       total time =       5.90 ms /    10 tokens
0.00.075.233 I ggml_metal_free: deallocating

real	0m0.257s
user	0m0.052s
sys	0m0.036s
```
- q8_0:
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.048 I build: 4671 (4d3465c5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.422 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.011.986 I llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.011.989 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.011.991 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.011.991 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.011.992 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.011.992 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.011.992 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.011.993 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.011.993 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.011.994 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.011.994 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.011.994 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.011.996 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.011.996 I llama_model_loader: - kv  11:                      bert.attention.causal bool             = false
0.00.011.997 I llama_model_loader: - kv  12:                          bert.pooling_type u32              = 2
0.00.011.997 I llama_model_loader: - kv  13:            tokenizer.ggml.token_type_count u32              = 2
0.00.011.997 I llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = bert
0.00.011.998 I llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.014.368 I llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.014.992 I llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.014.993 I llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.014.994 I llama_model_loader: - kv  19:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.014.994 I llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 0
0.00.014.994 I llama_model_loader: - kv  21:               tokenizer.ggml.mask_token_id u32              = 103
0.00.014.995 I llama_model_loader: - kv  22:               general.quantization_version u32              = 2
0.00.014.995 I llama_model_loader: - kv  23:                          general.file_type u32              = 7
0.00.014.995 I llama_model_loader: - type  f32:  124 tensors
0.00.014.995 I llama_model_loader: - type q8_0:   73 tensors
0.00.014.996 I print_info: file format = GGUF V3 (latest)
0.00.014.996 I print_info: file type   = Q8_0
0.00.014.997 I print_info: file size   = 34.38 MiB (8.68 BPW) 
0.00.017.367 I load: special tokens cache size = 5
0.00.018.664 I load: token to piece cache size = 0.2032 MB
0.00.018.668 I print_info: arch             = bert
0.00.018.668 I print_info: vocab_only       = 0
0.00.018.668 I print_info: n_ctx_train      = 512
0.00.018.668 I print_info: n_embd           = 384
0.00.018.668 I print_info: n_layer          = 12
0.00.018.671 I print_info: n_head           = 12
0.00.018.672 I print_info: n_head_kv        = 12
0.00.018.672 I print_info: n_rot            = 32
0.00.018.672 I print_info: n_swa            = 0
0.00.018.672 I print_info: n_embd_head_k    = 32
0.00.018.673 I print_info: n_embd_head_v    = 32
0.00.018.673 I print_info: n_gqa            = 1
0.00.018.674 I print_info: n_embd_k_gqa     = 384
0.00.018.676 I print_info: n_embd_v_gqa     = 384
0.00.018.677 I print_info: f_norm_eps       = 1.0e-12
0.00.018.678 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.018.678 I print_info: f_clamp_kqv      = 0.0e+00
0.00.018.678 I print_info: f_max_alibi_bias = 0.0e+00
0.00.018.679 I print_info: f_logit_scale    = 0.0e+00
0.00.018.682 I print_info: n_ff             = 1536
0.00.018.682 I print_info: n_expert         = 0
0.00.018.682 I print_info: n_expert_used    = 0
0.00.018.682 I print_info: causal attn      = 0
0.00.018.682 I print_info: pooling type     = 2
0.00.018.682 I print_info: rope type        = 2
0.00.018.683 I print_info: rope scaling     = linear
0.00.018.683 I print_info: freq_base_train  = 10000.0
0.00.018.683 I print_info: freq_scale_train = 1
0.00.018.683 I print_info: n_ctx_orig_yarn  = 512
0.00.018.683 I print_info: rope_finetuned   = unknown
0.00.018.683 I print_info: ssm_d_conv       = 0
0.00.018.684 I print_info: ssm_d_inner      = 0
0.00.018.684 I print_info: ssm_d_state      = 0
0.00.018.684 I print_info: ssm_dt_rank      = 0
0.00.018.684 I print_info: ssm_dt_b_c_rms   = 0
0.00.018.684 I print_info: model type       = 33M
0.00.018.684 I print_info: model params     = 33.21 M
0.00.018.688 I print_info: general.name     = Bge Small
0.00.018.689 I print_info: vocab type       = WPM
0.00.018.689 I print_info: n_vocab          = 30522
0.00.018.690 I print_info: n_merges         = 0
0.00.018.690 I print_info: BOS token        = 101 '[CLS]'
0.00.018.690 I print_info: UNK token        = 100 '[UNK]'
0.00.018.690 I print_info: SEP token        = 102 '[SEP]'
0.00.018.690 I print_info: PAD token        = 0 '[PAD]'
0.00.018.692 I print_info: MASK token       = 103 '[MASK]'
0.00.018.692 I print_info: LF token         = 0 '[PAD]'
0.00.018.692 I print_info: max token length = 21
0.00.018.693 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.020.490 I load_tensors: offloading 12 repeating layers to GPU
0.00.020.491 I load_tensors: offloading output layer to GPU
0.00.020.492 I load_tensors: offloaded 13/13 layers to GPU
0.00.020.498 I load_tensors: Metal_Mapped model buffer size =    21.75 MiB
0.00.020.499 I load_tensors:   CPU_Mapped model buffer size =    12.63 MiB
................................................
0.00.020.740 I llama_init_from_model: n_seq_max     = 1
0.00.020.741 I llama_init_from_model: n_ctx         = 512
0.00.020.741 I llama_init_from_model: n_ctx_per_seq = 512
0.00.020.741 I llama_init_from_model: n_batch       = 2048
0.00.020.741 I llama_init_from_model: n_ubatch      = 2048
0.00.020.741 I llama_init_from_model: flash_attn    = 0
0.00.020.742 I llama_init_from_model: freq_base     = 10000.0
0.00.020.742 I llama_init_from_model: freq_scale    = 1
0.00.020.742 I ggml_metal_init: allocating
0.00.020.746 I ggml_metal_init: found device: Apple M4
0.00.020.752 I ggml_metal_init: picking default device: Apple M4
0.00.021.273 I ggml_metal_init: using embedded metal library
0.00.023.700 I ggml_metal_init: GPU name:   Apple M4
0.00.023.702 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.023.702 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.023.703 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.023.703 I ggml_metal_init: simdgroup reduction   = true
0.00.023.703 I ggml_metal_init: simdgroup matrix mul. = true
0.00.023.703 I ggml_metal_init: has residency sets    = true
0.00.023.703 I ggml_metal_init: has bfloat            = true
0.00.023.704 I ggml_metal_init: use bfloat            = true
0.00.023.704 I ggml_metal_init: hasUnifiedMemory      = true
0.00.023.705 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.034.216 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.034.814 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.034.815 I llama_init_from_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.034.817 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.035.867 I llama_init_from_model:      Metal compute buffer size =    16.00 MiB
0.00.035.868 I llama_init_from_model:        CPU compute buffer size =     2.51 MiB
0.00.035.869 I llama_init_from_model: graph nodes  = 429
0.00.035.869 I llama_init_from_model: graph splits = 2
0.00.035.870 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.035.871 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.040.031 I 
0.00.040.057 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.040.617 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.044994 -0.019858  0.007986 -0.001410  0.001531 -0.036276  0.109404  0.043235  0.092836 -0.015201  0.005971 -0.036330 -0.018159  0.015015  0.017667  0.014510 -0.011792  0.011812 -0.084764 -0.008081  0.091650 -0.017638 -0.061084 -0.025066  0.026879  0.076180  0.028256 -0.014273  0.017420 -0.033553 -0.037520 -0.018280  0.068843 -0.010129 -0.024896  0.071968 -0.046406  0.011471 -0.050333  0.048931  0.032317 -0.012107  0.021812  0.049796  0.010366  0.005264 -0.028665  0.008581 -0.018508 -0.052358 -0.046410  0.029426 -0.034989  0.053289 -0.069849  0.043767  0.029291  0.046759  0.073664 -0.043090  0.075213  0.038641 -0.180496  0.081648  0.043389 -0.065615 -0.059878 -0.017387  0.006913  0.005155  0.017165 -0.026679  0.064347  0.112564  0.034871 -0.067783  0.027124 -0.066989 -0.034296 -0.033344  0.033189  0.014325 -0.004253 -0.036945 -0.052008  0.054151 -0.001715 -0.038112  0.063016  0.028668 -0.041665 -0.028879 -0.039263  0.036879  0.007718 -0.015657 -0.036270  0.018699  0.029792  0.344948 -0.043880  0.056487  0.018022 -0.021448 -0.065281  0.000253 -0.037910 -0.029663 -0.008828 -0.020643  0.001162 -0.003510  0.003091  0.018576 -0.009637  0.025316  0.049298 -0.000937  0.051568 -0.042363 -0.031550  0.023315  0.030044 -0.023267 -0.045483 -0.079644  0.114489  0.047503  0.027208 -0.041477  0.067717 -0.022845  0.010232 -0.033486 -0.017199  0.044319  0.022956  0.052194  0.008122  0.008112  0.009951 -0.074643 -0.065035 -0.025938 -0.040806 -0.024533  0.027409  0.006078  0.027693  0.052243 -0.036587  0.058682  0.001467  0.032191 -0.019964 -0.021840  0.041805 -0.058801  0.019348  0.042626  0.043999  0.040911 -0.021909  0.028025 -0.022801  0.005520 -0.041266 -0.000085  0.024325  0.001532  0.044802 -0.022888  0.043056  0.065030  0.056292  0.037666 -0.001004  0.047521  0.045624 -0.008614  0.062270 -0.073313 -0.011718  0.032708  0.023348  0.014650 -0.033924  0.001158 -0.015872 -0.018974  0.047982  0.110740  0.029177  0.030881 -0.012325 -0.057836  0.006939  0.004378 -0.011864 -0.051754 -0.003018 -0.017766 -0.020051 -0.041018  0.009118 -0.058568  0.050632  0.051795 -0.008968 -0.040668 -0.014765 -0.024320 -0.015980  0.005823  0.007147 -0.027335  0.016275  0.030734  0.002342  0.023167 -0.022169 -0.098059 -0.050901 -0.277240 -0.015100 -0.061566 -0.026219  0.017141 -0.010285 -0.017522  0.034725  0.047897 -0.016594  0.015489 -0.024132  0.048613 -0.004993 -0.000001 -0.060860 -0.068346 -0.059622 -0.036157  0.043605 -0.056353  0.014713  0.000292 -0.058627 -0.010995  0.012274  0.151597  0.127025 -0.012582  0.042765 -0.025227  0.013629 -0.000381 -0.150686  0.043974  0.004714 -0.036955 -0.029586 -0.019853 -0.034063  0.009522  0.034004 -0.048909 -0.051765 -0.017243 -0.025080  0.047588  0.050981 -0.017553 -0.056746  0.024054 -0.006285  0.011003  0.038719  0.008083 -0.008595 -0.106361 -0.027365 -0.096966  0.025294 -0.011096  0.091579  0.056180  0.004494  0.028066  0.001727 -0.051388 -0.039497 -0.013817 -0.045907 -0.015238  0.002920 -0.043938 -0.077716  0.066102 -0.006195 -0.000885 -0.013768  0.071562  0.023826 -0.036363  0.007848  0.001487 -0.032916  0.016336  0.037378  0.000625 -0.052389  0.021549 -0.039695 -0.000026  0.013496  0.020436 -0.057513  0.005900 -0.049906 -0.267546  0.038686 -0.067509  0.037450 -0.011471  0.041547 -0.015904  0.051347 -0.072313  0.012571  0.024808 -0.007420  0.082069  0.028175 -0.021956  0.040736 -0.003945 -0.074192 -0.014923  0.020417  0.002644  0.023207  0.197144 -0.043068 -0.025876 -0.004886 -0.018582  0.073936  0.001341 -0.031663 -0.037140 -0.044262  0.000049 -0.010741  0.018225 -0.028045 -0.008828  0.006047  0.050183 -0.015017  0.007037  0.025894 -0.030628  0.048075  0.112795 -0.039982 -0.011695  0.005039 -0.002800  0.025072 -0.059720  0.014182 -0.010019  0.038100  0.051088  0.034726  0.035947 -0.017035  0.027243 -0.015391 -0.051195  0.003744  0.053928  0.040014 -0.039140 

0.00.045.147 I llama_perf_context_print:        load time =      30.60 ms
0.00.045.148 I llama_perf_context_print: prompt eval time =       4.40 ms /     9 tokens (    0.49 ms per token,  2046.38 tokens per second)
0.00.045.149 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.045.149 I llama_perf_context_print:       total time =       5.12 ms /    10 tokens
0.00.045.337 I ggml_metal_free: deallocating

real	0m0.057s
user	0m0.030s
sys	0m0.016s
```
### rerank_tiny

Rerank Tiny (Jina):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/rerank-tiny/ggml-model-f16.gguf -p 'what is panda?</s></s>hi\nwhat is panda?</s></s>it'\''s a bear\nwhat is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.' -ngl 99 -c 0 --pooling rank --embd-normalize -1 --verbose-prompt
0.00.000.287 I build: 4671 (4d3465c5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.021.496 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.034.042 I llama_model_loader: loaded meta data with 28 key-value pairs and 70 tensors from ../models-mnt/rerank-tiny/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.034.048 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.034.050 I llama_model_loader: - kv   0:                       general.architecture str              = jina-bert-v2
0.00.034.053 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.034.054 I llama_model_loader: - kv   2:                               general.name str              = Jina Bert Implementation
0.00.034.054 I llama_model_loader: - kv   3:                       general.organization str              = Jinaai
0.00.034.055 I llama_model_loader: - kv   4:                         general.size_label str              = 33M
0.00.034.056 I llama_model_loader: - kv   5:                   jina-bert-v2.block_count u32              = 4
0.00.034.057 I llama_model_loader: - kv   6:                jina-bert-v2.context_length u32              = 8192
0.00.034.058 I llama_model_loader: - kv   7:              jina-bert-v2.embedding_length u32              = 384
0.00.034.059 I llama_model_loader: - kv   8:           jina-bert-v2.feed_forward_length u32              = 1536
0.00.034.062 I llama_model_loader: - kv   9:          jina-bert-v2.attention.head_count u32              = 12
0.00.034.065 I llama_model_loader: - kv  10:  jina-bert-v2.attention.layer_norm_epsilon f32              = 0.000000
0.00.034.066 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.034.067 I llama_model_loader: - kv  12:              jina-bert-v2.attention.causal bool             = false
0.00.034.067 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.034.068 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = jina-v1-en
0.00.040.945 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,61056]   = ["<s>", "<pad>", "</s>", "<unk>", "<m...
0.00.043.027 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,61056]   = [3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, ...
0.00.047.421 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,39382]   = ["t h", "i n", "a n", "e r", "th e", ...
0.00.047.423 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.047.423 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
0.00.047.424 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 3
0.00.047.424 I llama_model_loader: - kv  21:          tokenizer.ggml.seperator_token_id u32              = 2
0.00.047.425 I llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 1
0.00.047.425 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 4
0.00.047.425 I llama_model_loader: - kv  24:            tokenizer.ggml.token_type_count u32              = 2
0.00.047.426 I llama_model_loader: - kv  25:               tokenizer.ggml.add_bos_token bool             = true
0.00.047.426 I llama_model_loader: - kv  26:               tokenizer.ggml.add_eos_token bool             = true
0.00.047.426 I llama_model_loader: - kv  27:               general.quantization_version u32              = 2
0.00.047.427 I llama_model_loader: - type  f32:   40 tensors
0.00.047.427 I llama_model_loader: - type  f16:   30 tensors
0.00.047.428 I print_info: file format = GGUF V3 (latest)
0.00.047.428 I print_info: file type   = F16
0.00.047.430 I print_info: file size   = 62.78 MiB (16.01 BPW) 
0.00.051.739 W load: empty token at index 5
0.00.056.855 W load: model vocab missing newline token, using special_pad_id instead
0.00.058.390 W load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
0.00.058.423 I load: special tokens cache size = 5
0.00.321.589 I load: token to piece cache size = 1.5060 MB
0.00.321.615 I print_info: arch             = jina-bert-v2
0.00.321.622 I print_info: vocab_only       = 0
0.00.321.623 I print_info: n_ctx_train      = 8192
0.00.321.623 I print_info: n_embd           = 384
0.00.321.623 I print_info: n_layer          = 4
0.00.321.632 I print_info: n_head           = 12
0.00.321.633 I print_info: n_head_kv        = 12
0.00.321.634 I print_info: n_rot            = 32
0.00.321.634 I print_info: n_swa            = 0
0.00.321.634 I print_info: n_embd_head_k    = 32
0.00.321.634 I print_info: n_embd_head_v    = 32
0.00.321.634 I print_info: n_gqa            = 1
0.00.321.638 I print_info: n_embd_k_gqa     = 384
0.00.321.640 I print_info: n_embd_v_gqa     = 384
0.00.321.643 I print_info: f_norm_eps       = 1.0e-12
0.00.321.644 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.321.645 I print_info: f_clamp_kqv      = 0.0e+00
0.00.321.645 I print_info: f_max_alibi_bias = 8.0e+00
0.00.321.645 I print_info: f_logit_scale    = 0.0e+00
0.00.321.646 I print_info: n_ff             = 1536
0.00.321.646 I print_info: n_expert         = 0
0.00.321.646 I print_info: n_expert_used    = 0
0.00.321.646 I print_info: causal attn      = 0
0.00.321.647 I print_info: pooling type     = -1
0.00.321.647 I print_info: rope type        = -1
0.00.321.648 I print_info: rope scaling     = linear
0.00.321.648 I print_info: freq_base_train  = 10000.0
0.00.321.648 I print_info: freq_scale_train = 1
0.00.321.649 I print_info: n_ctx_orig_yarn  = 8192
0.00.321.649 I print_info: rope_finetuned   = unknown
0.00.321.649 I print_info: ssm_d_conv       = 0
0.00.321.649 I print_info: ssm_d_inner      = 0
0.00.321.649 I print_info: ssm_d_state      = 0
0.00.321.649 I print_info: ssm_dt_rank      = 0
0.00.321.649 I print_info: ssm_dt_b_c_rms   = 0
0.00.321.649 I print_info: model type       = 33M
0.00.321.654 I print_info: model params     = 32.90 M
0.00.321.657 I print_info: general.name     = Jina Bert Implementation
0.00.321.658 I print_info: vocab type       = BPE
0.00.321.658 I print_info: n_vocab          = 61056
0.00.321.659 I print_info: n_merges         = 39382
0.00.321.659 I print_info: BOS token        = 0 '<s>'
0.00.321.660 I print_info: EOS token        = 2 '</s>'
0.00.321.660 I print_info: UNK token        = 3 '<unk>'
0.00.321.660 I print_info: SEP token        = 2 '</s>'
0.00.321.660 I print_info: PAD token        = 1 '<pad>'
0.00.321.661 I print_info: MASK token       = 4 '<mask>'
0.00.321.661 I print_info: EOG token        = 2 '</s>'
0.00.321.661 I print_info: max token length = 45
0.00.321.662 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.323.938 I load_tensors: offloading 4 repeating layers to GPU
0.00.323.939 I load_tensors: offloading output layer to GPU
0.00.323.939 I load_tensors: offloaded 5/5 layers to GPU
0.00.323.966 I load_tensors: Metal_Mapped model buffer size =    18.06 MiB
0.00.323.967 I load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
....................
0.00.324.527 I llama_init_from_model: n_seq_max     = 1
0.00.324.528 I llama_init_from_model: n_ctx         = 8192
0.00.324.528 I llama_init_from_model: n_ctx_per_seq = 8192
0.00.324.528 I llama_init_from_model: n_batch       = 2048
0.00.324.529 I llama_init_from_model: n_ubatch      = 2048
0.00.324.529 I llama_init_from_model: flash_attn    = 0
0.00.324.529 I llama_init_from_model: freq_base     = 10000.0
0.00.324.529 I llama_init_from_model: freq_scale    = 1
0.00.324.530 I ggml_metal_init: allocating
0.00.324.535 I ggml_metal_init: found device: Apple M4
0.00.324.539 I ggml_metal_init: picking default device: Apple M4
0.00.325.376 I ggml_metal_init: using embedded metal library
0.00.327.910 I ggml_metal_init: GPU name:   Apple M4
0.00.327.912 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.327.913 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.327.913 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.327.913 I ggml_metal_init: simdgroup reduction   = true
0.00.327.913 I ggml_metal_init: simdgroup matrix mul. = true
0.00.327.914 I ggml_metal_init: has residency sets    = true
0.00.327.914 I ggml_metal_init: has bfloat            = true
0.00.327.914 I ggml_metal_init: use bfloat            = true
0.00.327.914 I ggml_metal_init: hasUnifiedMemory      = true
0.00.327.915 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.337.497 I llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 4, can_shift = 1
0.00.340.560 I llama_kv_cache_init:      Metal KV buffer size =    48.00 MiB
0.00.340.562 I llama_init_from_model: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB
0.00.340.564 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.347.442 I llama_init_from_model:      Metal compute buffer size =   220.01 MiB
0.00.347.443 I llama_init_from_model:        CPU compute buffer size =    22.02 MiB
0.00.347.444 I llama_init_from_model: graph nodes  = 154
0.00.347.444 I llama_init_from_model: graph splits = 2
0.00.347.445 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 8192
0.00.347.446 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.354.877 I 
0.00.354.908 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.355.294 I main: prompt 0: 'what is panda?</s></s>hi'
0.00.355.294 I main: number of tokens in prompt = 9
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 23233 -> 'hi'
     2 -> '</s>'


0.00.355.307 I main: prompt 1: 'what is panda?</s></s>it's a bear'
0.00.355.307 I main: number of tokens in prompt = 13
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21153 -> 'it'
    37 -> '''
    87 -> 's'
    69 -> 'a'
 25706 -> 'bear'
     2 -> '</s>'


0.00.355.318 I main: prompt 2: 'what is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'
0.00.355.318 I main: number of tokens in prompt = 40
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21215 -> 'he'
 28390 -> 'giant'
 49990 -> 'panda'
    38 -> '('
 21163 -> 'il'
 26237 -> 'uro'
 21223 -> 'po'
 23179 -> 'da'
 36906 -> 'melan'
 26791 -> 'ole'
    89 -> 'u'
 21402 -> 'ca'
 21686 -> '),'
 23314 -> 'sometimes'
 22517 -> 'called'
    69 -> 'a'
 49990 -> 'panda'
 25706 -> 'bear'
 21142 -> 'or'
 22810 -> 'simply'
 49990 -> 'panda'
    42 -> ','
 21152 -> 'is'
    69 -> 'a'
 25706 -> 'bear'
 25677 -> 'species'
 28930 -> 'ende'
 22024 -> 'mic'
 21148 -> 'to'
    76 -> 'h'
 22344 -> 'ina'
    44 -> '.'
     2 -> '</s>'


0.00.355.851 I batch_decode: n_tokens = 62, n_seq = 3

rerank score 0:    0.023
rerank score 1:    0.024
rerank score 2:    0.199

0.00.359.515 I llama_perf_context_print:        load time =     333.38 ms
0.00.359.516 I llama_perf_context_print: prompt eval time =       3.66 ms /    62 tokens (    0.06 ms per token, 16953.79 tokens per second)
0.00.359.517 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.359.520 I llama_perf_context_print:       total time =       4.64 ms /    63 tokens
0.00.359.734 I ggml_metal_free: deallocating

real	0m1.070s
user	0m0.330s
sys	0m0.049s
  - rerank score 0 @ 0.023 OK
  - rerank score 1 @ 0.024 OK
  - rerank score 2 @ 0.199 OK
```
### pythia_1_4b

Pythia 1.4B:
- status: 0
- perplexity:
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
- imatrix:
```
Final estimate: PPL = 10.1498 +/- 3.22650
```
- f16: 
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.215 I build: 4671 (4d3465c5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.379 I main: llama backend init
0.00.000.385 I main: load the model and apply lora adapter, if any
0.00.039.143 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.051.414 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.051.433 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.051.438 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.051.439 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.051.439 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.051.440 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.051.441 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.051.443 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.051.444 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.051.445 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.051.445 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.051.446 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.051.446 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.051.447 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.051.453 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.051.453 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.051.454 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.059.481 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.061.983 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.069.729 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.069.732 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.069.732 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.069.733 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.069.733 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.069.734 I llama_model_loader: - type  f32:  194 tensors
0.00.069.735 I llama_model_loader: - type  f16:   98 tensors
0.00.069.736 I print_info: file format = GGUF V3 (latest)
0.00.069.737 I print_info: file type   = all F32 (guessed)
0.00.069.740 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.083.800 I load: special tokens cache size = 25
0.00.092.891 I load: token to piece cache size = 0.2984 MB
0.00.092.896 I print_info: arch             = gptneox
0.00.092.896 I print_info: vocab_only       = 0
0.00.092.897 I print_info: n_ctx_train      = 2048
0.00.092.897 I print_info: n_embd           = 2048
0.00.092.897 I print_info: n_layer          = 24
0.00.092.904 I print_info: n_head           = 16
0.00.092.907 I print_info: n_head_kv        = 16
0.00.092.907 I print_info: n_rot            = 32
0.00.092.907 I print_info: n_swa            = 0
0.00.092.908 I print_info: n_embd_head_k    = 128
0.00.092.908 I print_info: n_embd_head_v    = 128
0.00.092.909 I print_info: n_gqa            = 1
0.00.092.909 I print_info: n_embd_k_gqa     = 2048
0.00.092.910 I print_info: n_embd_v_gqa     = 2048
0.00.092.911 I print_info: f_norm_eps       = 1.0e-05
0.00.092.912 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.092.912 I print_info: f_clamp_kqv      = 0.0e+00
0.00.092.912 I print_info: f_max_alibi_bias = 0.0e+00
0.00.092.915 I print_info: f_logit_scale    = 0.0e+00
0.00.092.916 I print_info: n_ff             = 8192
0.00.092.916 I print_info: n_expert         = 0
0.00.092.916 I print_info: n_expert_used    = 0
0.00.092.916 I print_info: causal attn      = 1
0.00.092.917 I print_info: pooling type     = 0
0.00.092.917 I print_info: rope type        = 2
0.00.092.917 I print_info: rope scaling     = linear
0.00.092.917 I print_info: freq_base_train  = 10000.0
0.00.092.918 I print_info: freq_scale_train = 1
0.00.092.918 I print_info: n_ctx_orig_yarn  = 2048
0.00.092.919 I print_info: rope_finetuned   = unknown
0.00.092.919 I print_info: ssm_d_conv       = 0
0.00.092.919 I print_info: ssm_d_inner      = 0
0.00.092.920 I print_info: ssm_d_state      = 0
0.00.092.920 I print_info: ssm_dt_rank      = 0
0.00.092.920 I print_info: ssm_dt_b_c_rms   = 0
0.00.092.920 I print_info: model type       = 1.4B
0.00.092.921 I print_info: model params     = 1.41 B
0.00.092.921 I print_info: general.name     = 1.4B
0.00.092.922 I print_info: vocab type       = BPE
0.00.092.923 I print_info: n_vocab          = 50304
0.00.092.923 I print_info: n_merges         = 50009
0.00.092.923 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.092.923 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.092.923 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.092.924 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.092.924 I print_info: LF token         = 187 ''
0.00.092.924 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.092.925 I print_info: max token length = 1024
0.00.092.925 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.141.235 I load_tensors: offloading 24 repeating layers to GPU
0.00.141.240 I load_tensors: offloading output layer to GPU
0.00.141.240 I load_tensors: offloaded 25/25 layers to GPU
0.00.141.266 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.141.268 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.141.634 I llama_init_from_model: n_seq_max     = 1
0.00.141.635 I llama_init_from_model: n_ctx         = 2048
0.00.141.636 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.141.636 I llama_init_from_model: n_batch       = 2048
0.00.141.636 I llama_init_from_model: n_ubatch      = 512
0.00.141.636 I llama_init_from_model: flash_attn    = 0
0.00.141.637 I llama_init_from_model: freq_base     = 10000.0
0.00.141.637 I llama_init_from_model: freq_scale    = 1
0.00.141.638 I ggml_metal_init: allocating
0.00.141.658 I ggml_metal_init: found device: Apple M4
0.00.141.663 I ggml_metal_init: picking default device: Apple M4
0.00.142.326 I ggml_metal_init: using embedded metal library
0.00.151.679 I ggml_metal_init: GPU name:   Apple M4
0.00.151.681 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.151.681 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.151.682 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.151.682 I ggml_metal_init: simdgroup reduction   = true
0.00.151.682 I ggml_metal_init: simdgroup matrix mul. = true
0.00.151.682 I ggml_metal_init: has residency sets    = true
0.00.151.682 I ggml_metal_init: has bfloat            = true
0.00.151.682 I ggml_metal_init: use bfloat            = true
0.00.151.683 I ggml_metal_init: hasUnifiedMemory      = true
0.00.151.684 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.177.530 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.207.516 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.207.521 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.207.545 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.211.225 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.211.228 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.211.228 I llama_init_from_model: graph nodes  = 967
0.00.211.228 I llama_init_from_model: graph splits = 2
0.00.211.231 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.211.360 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.211.361 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.278.312 I main: llama threadpool init, n_threads = 4
0.00.278.353 I 
0.00.278.385 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.278.387 I 
0.00.278.430 I sampler seed: 1234
0.00.278.434 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.278.459 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.278.461 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.278.461 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.107.043 I llama_perf_sampler_print:    sampling time =       1.17 ms /    71 runs   (    0.02 ms per token, 60580.20 tokens per second)
0.02.107.044 I llama_perf_context_print:        load time =     238.31 ms
0.02.107.045 I llama_perf_context_print: prompt eval time =      43.63 ms /     7 tokens (    6.23 ms per token,   160.44 tokens per second)
0.02.107.046 I llama_perf_context_print:        eval time =    1782.15 ms /    63 runs   (   28.29 ms per token,    35.35 tokens per second)
0.02.107.046 I llama_perf_context_print:       total time =    1829.59 ms /    70 tokens
0.02.107.252 I ggml_metal_free: deallocating

real	0m2.415s
user	0m0.132s
sys	0m0.140s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.575 I build: 4671 (4d3465c5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.021.881 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.040.391 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.040.396 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.040.397 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.040.398 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.040.401 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.040.401 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.040.402 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.040.403 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.040.404 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.040.404 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.040.404 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.040.405 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.040.405 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.040.406 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.040.408 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.040.410 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.040.410 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.048.733 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.050.787 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.057.983 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.057.985 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.057.986 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.057.986 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.057.987 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.057.988 I llama_model_loader: - type  f32:  194 tensors
0.00.057.988 I llama_model_loader: - type  f16:   98 tensors
0.00.057.989 I print_info: file format = GGUF V3 (latest)
0.00.057.990 I print_info: file type   = all F32 (guessed)
0.00.057.991 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.069.879 I load: special tokens cache size = 25
0.00.078.287 I load: token to piece cache size = 0.2984 MB
0.00.078.290 I print_info: arch             = gptneox
0.00.078.290 I print_info: vocab_only       = 0
0.00.078.290 I print_info: n_ctx_train      = 2048
0.00.078.291 I print_info: n_embd           = 2048
0.00.078.291 I print_info: n_layer          = 24
0.00.078.294 I print_info: n_head           = 16
0.00.078.295 I print_info: n_head_kv        = 16
0.00.078.295 I print_info: n_rot            = 32
0.00.078.296 I print_info: n_swa            = 0
0.00.078.296 I print_info: n_embd_head_k    = 128
0.00.078.296 I print_info: n_embd_head_v    = 128
0.00.078.297 I print_info: n_gqa            = 1
0.00.078.298 I print_info: n_embd_k_gqa     = 2048
0.00.078.299 I print_info: n_embd_v_gqa     = 2048
0.00.078.299 I print_info: f_norm_eps       = 1.0e-05
0.00.078.300 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.078.300 I print_info: f_clamp_kqv      = 0.0e+00
0.00.078.300 I print_info: f_max_alibi_bias = 0.0e+00
0.00.078.300 I print_info: f_logit_scale    = 0.0e+00
0.00.078.301 I print_info: n_ff             = 8192
0.00.078.301 I print_info: n_expert         = 0
0.00.078.301 I print_info: n_expert_used    = 0
0.00.078.301 I print_info: causal attn      = 1
0.00.078.302 I print_info: pooling type     = 0
0.00.078.302 I print_info: rope type        = 2
0.00.078.303 I print_info: rope scaling     = linear
0.00.078.304 I print_info: freq_base_train  = 10000.0
0.00.078.304 I print_info: freq_scale_train = 1
0.00.078.304 I print_info: n_ctx_orig_yarn  = 2048
0.00.078.304 I print_info: rope_finetuned   = unknown
0.00.078.304 I print_info: ssm_d_conv       = 0
0.00.078.305 I print_info: ssm_d_inner      = 0
0.00.078.305 I print_info: ssm_d_state      = 0
0.00.078.307 I print_info: ssm_dt_rank      = 0
0.00.078.307 I print_info: ssm_dt_b_c_rms   = 0
0.00.078.307 I print_info: model type       = 1.4B
0.00.078.307 I print_info: model params     = 1.41 B
0.00.078.307 I print_info: general.name     = 1.4B
0.00.078.308 I print_info: vocab type       = BPE
0.00.078.308 I print_info: n_vocab          = 50304
0.00.078.308 I print_info: n_merges         = 50009
0.00.078.309 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.078.309 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.078.309 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.078.309 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.078.310 I print_info: LF token         = 187 ''
0.00.078.310 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.078.310 I print_info: max token length = 1024
0.00.078.311 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.416.000 I load_tensors: offloading 24 repeating layers to GPU
0.01.416.003 I load_tensors: offloading output layer to GPU
0.01.416.004 I load_tensors: offloaded 25/25 layers to GPU
0.01.416.029 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.416.030 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.01.417.150 I llama_init_from_model: n_seq_max     = 1
0.01.417.151 I llama_init_from_model: n_ctx         = 128
0.01.417.151 I llama_init_from_model: n_ctx_per_seq = 128
0.01.417.152 I llama_init_from_model: n_batch       = 128
0.01.417.152 I llama_init_from_model: n_ubatch      = 128
0.01.417.152 I llama_init_from_model: flash_attn    = 0
0.01.417.153 I llama_init_from_model: freq_base     = 10000.0
0.01.417.153 I llama_init_from_model: freq_scale    = 1
0.01.417.154 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.417.154 I ggml_metal_init: allocating
0.01.417.230 I ggml_metal_init: found device: Apple M4
0.01.417.236 I ggml_metal_init: picking default device: Apple M4
0.01.418.383 I ggml_metal_init: using embedded metal library
0.01.422.160 I ggml_metal_init: GPU name:   Apple M4
0.01.422.162 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.422.163 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.422.163 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.422.163 I ggml_metal_init: simdgroup reduction   = true
0.01.422.164 I ggml_metal_init: simdgroup matrix mul. = true
0.01.422.164 I ggml_metal_init: has residency sets    = true
0.01.422.164 I ggml_metal_init: has bfloat            = true
0.01.422.164 I ggml_metal_init: use bfloat            = true
0.01.422.164 I ggml_metal_init: hasUnifiedMemory      = true
0.01.422.167 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.432.867 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.434.600 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.434.603 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.434.616 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.436.302 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.436.304 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.436.304 I llama_init_from_model: graph nodes  = 967
0.01.436.304 I llama_init_from_model: graph splits = 2
0.01.436.305 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.436.306 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.472.195 I 
0.01.472.230 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.472.234 I perplexity: tokenizing the input ..
0.01.477.374 I perplexity: tokenization took 5.138 ms
0.01.477.379 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.596.009 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.597.352 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.597.386 I llama_perf_context_print:        load time =    1450.31 ms
0.01.597.387 I llama_perf_context_print: prompt eval time =     118.32 ms /   128 tokens (    0.92 ms per token,  1081.83 tokens per second)
0.01.597.388 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.597.388 I llama_perf_context_print:       total time =     125.19 ms /   129 tokens
0.01.597.775 I ggml_metal_free: deallocating

real	0m1.805s
user	0m0.099s
sys	0m0.267s
```
- q8_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.058 I build: 4671 (4d3465c5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.094 I main: llama backend init
0.00.000.096 I main: load the model and apply lora adapter, if any
0.00.010.183 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.028.869 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.028.875 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.028.882 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.028.883 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.028.883 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.028.883 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.028.884 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.028.884 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.028.885 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.028.885 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.028.885 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.028.886 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.028.886 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.028.886 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.028.888 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.028.889 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.028.889 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.032.823 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.033.856 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.037.650 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.037.652 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.037.652 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.037.653 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.037.653 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.037.653 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.037.654 I llama_model_loader: - type  f32:  194 tensors
0.00.037.654 I llama_model_loader: - type q8_0:   98 tensors
0.00.037.655 I print_info: file format = GGUF V3 (latest)
0.00.037.656 I print_info: file type   = Q8_0
0.00.037.658 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.046.880 I load: special tokens cache size = 25
0.00.054.110 I load: token to piece cache size = 0.2984 MB
0.00.054.114 I print_info: arch             = gptneox
0.00.054.115 I print_info: vocab_only       = 0
0.00.054.115 I print_info: n_ctx_train      = 2048
0.00.054.115 I print_info: n_embd           = 2048
0.00.054.115 I print_info: n_layer          = 24
0.00.054.121 I print_info: n_head           = 16
0.00.054.122 I print_info: n_head_kv        = 16
0.00.054.123 I print_info: n_rot            = 32
0.00.054.123 I print_info: n_swa            = 0
0.00.054.123 I print_info: n_embd_head_k    = 128
0.00.054.123 I print_info: n_embd_head_v    = 128
0.00.054.126 I print_info: n_gqa            = 1
0.00.054.127 I print_info: n_embd_k_gqa     = 2048
0.00.054.128 I print_info: n_embd_v_gqa     = 2048
0.00.054.129 I print_info: f_norm_eps       = 1.0e-05
0.00.054.129 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.054.129 I print_info: f_clamp_kqv      = 0.0e+00
0.00.054.129 I print_info: f_max_alibi_bias = 0.0e+00
0.00.054.130 I print_info: f_logit_scale    = 0.0e+00
0.00.054.130 I print_info: n_ff             = 8192
0.00.054.131 I print_info: n_expert         = 0
0.00.054.131 I print_info: n_expert_used    = 0
0.00.054.131 I print_info: causal attn      = 1
0.00.054.131 I print_info: pooling type     = 0
0.00.054.131 I print_info: rope type        = 2
0.00.054.132 I print_info: rope scaling     = linear
0.00.054.135 I print_info: freq_base_train  = 10000.0
0.00.054.135 I print_info: freq_scale_train = 1
0.00.054.135 I print_info: n_ctx_orig_yarn  = 2048
0.00.054.136 I print_info: rope_finetuned   = unknown
0.00.054.136 I print_info: ssm_d_conv       = 0
0.00.054.136 I print_info: ssm_d_inner      = 0
0.00.054.136 I print_info: ssm_d_state      = 0
0.00.054.136 I print_info: ssm_dt_rank      = 0
0.00.054.137 I print_info: ssm_dt_b_c_rms   = 0
0.00.054.137 I print_info: model type       = 1.4B
0.00.054.137 I print_info: model params     = 1.41 B
0.00.054.137 I print_info: general.name     = 1.4B
0.00.054.138 I print_info: vocab type       = BPE
0.00.054.138 I print_info: n_vocab          = 50304
0.00.054.138 I print_info: n_merges         = 50009
0.00.054.139 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.054.139 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.054.144 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.054.144 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.054.145 I print_info: LF token         = 187 ''
0.00.054.145 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.054.145 I print_info: max token length = 1024
0.00.054.146 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.269.650 I load_tensors: offloading 24 repeating layers to GPU
0.01.269.653 I load_tensors: offloading output layer to GPU
0.01.269.654 I load_tensors: offloaded 25/25 layers to GPU
0.01.269.674 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.01.269.677 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.01.270.528 I llama_init_from_model: n_seq_max     = 1
0.01.270.530 I llama_init_from_model: n_ctx         = 2048
0.01.270.530 I llama_init_from_model: n_ctx_per_seq = 2048
0.01.270.530 I llama_init_from_model: n_batch       = 2048
0.01.270.531 I llama_init_from_model: n_ubatch      = 512
0.01.270.531 I llama_init_from_model: flash_attn    = 0
0.01.270.532 I llama_init_from_model: freq_base     = 10000.0
0.01.270.533 I llama_init_from_model: freq_scale    = 1
0.01.270.534 I ggml_metal_init: allocating
0.01.270.551 I ggml_metal_init: found device: Apple M4
0.01.270.559 I ggml_metal_init: picking default device: Apple M4
0.01.271.823 I ggml_metal_init: using embedded metal library
0.01.277.325 I ggml_metal_init: GPU name:   Apple M4
0.01.277.328 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.277.329 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.277.330 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.277.330 I ggml_metal_init: simdgroup reduction   = true
0.01.277.330 I ggml_metal_init: simdgroup matrix mul. = true
0.01.277.330 I ggml_metal_init: has residency sets    = true
0.01.277.331 I ggml_metal_init: has bfloat            = true
0.01.277.331 I ggml_metal_init: use bfloat            = true
0.01.277.332 I ggml_metal_init: hasUnifiedMemory      = true
0.01.277.333 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.294.074 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.348.798 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.01.348.804 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.01.348.872 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.353.148 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.01.353.150 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.01.353.151 I llama_init_from_model: graph nodes  = 967
0.01.353.151 I llama_init_from_model: graph splits = 2
0.01.353.156 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.01.353.284 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.353.285 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.408.447 I main: llama threadpool init, n_threads = 4
0.01.408.490 I 
0.01.408.512 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.408.512 I 
0.01.408.664 I sampler seed: 1234
0.01.408.669 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.408.679 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.408.680 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.408.680 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.506.898 I llama_perf_sampler_print:    sampling time =       1.24 ms /    71 runs   (    0.02 ms per token, 57304.28 tokens per second)
0.02.506.899 I llama_perf_context_print:        load time =    1397.55 ms
0.02.506.899 I llama_perf_context_print: prompt eval time =      49.16 ms /     7 tokens (    7.02 ms per token,   142.39 tokens per second)
0.02.506.900 I llama_perf_context_print:        eval time =    1046.24 ms /    63 runs   (   16.61 ms per token,    60.22 tokens per second)
0.02.506.900 I llama_perf_context_print:       total time =    1099.16 ms /    70 tokens
0.02.507.180 I ggml_metal_free: deallocating

real	0m2.525s
user	0m0.111s
sys	0m0.281s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.348 I build: 4671 (4d3465c5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.626 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.019.920 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.019.927 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.929 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.935 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.935 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.936 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.936 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.937 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.937 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.937 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.938 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.938 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.938 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.939 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.940 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.941 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.941 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.859 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.921 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.028.851 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.028.852 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.028.853 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.028.853 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.028.853 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.028.854 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.028.854 I llama_model_loader: - type  f32:  194 tensors
0.00.028.855 I llama_model_loader: - type q8_0:   98 tensors
0.00.028.856 I print_info: file format = GGUF V3 (latest)
0.00.028.856 I print_info: file type   = Q8_0
0.00.028.857 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.037.187 I load: special tokens cache size = 25
0.00.043.052 I load: token to piece cache size = 0.2984 MB
0.00.043.057 I print_info: arch             = gptneox
0.00.043.057 I print_info: vocab_only       = 0
0.00.043.058 I print_info: n_ctx_train      = 2048
0.00.043.058 I print_info: n_embd           = 2048
0.00.043.058 I print_info: n_layer          = 24
0.00.043.063 I print_info: n_head           = 16
0.00.043.066 I print_info: n_head_kv        = 16
0.00.043.066 I print_info: n_rot            = 32
0.00.043.066 I print_info: n_swa            = 0
0.00.043.067 I print_info: n_embd_head_k    = 128
0.00.043.068 I print_info: n_embd_head_v    = 128
0.00.043.068 I print_info: n_gqa            = 1
0.00.043.069 I print_info: n_embd_k_gqa     = 2048
0.00.043.071 I print_info: n_embd_v_gqa     = 2048
0.00.043.071 I print_info: f_norm_eps       = 1.0e-05
0.00.043.072 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.043.072 I print_info: f_clamp_kqv      = 0.0e+00
0.00.043.072 I print_info: f_max_alibi_bias = 0.0e+00
0.00.043.072 I print_info: f_logit_scale    = 0.0e+00
0.00.043.073 I print_info: n_ff             = 8192
0.00.043.073 I print_info: n_expert         = 0
0.00.043.073 I print_info: n_expert_used    = 0
0.00.043.073 I print_info: causal attn      = 1
0.00.043.074 I print_info: pooling type     = 0
0.00.043.074 I print_info: rope type        = 2
0.00.043.074 I print_info: rope scaling     = linear
0.00.043.076 I print_info: freq_base_train  = 10000.0
0.00.043.076 I print_info: freq_scale_train = 1
0.00.043.076 I print_info: n_ctx_orig_yarn  = 2048
0.00.043.076 I print_info: rope_finetuned   = unknown
0.00.043.076 I print_info: ssm_d_conv       = 0
0.00.043.076 I print_info: ssm_d_inner      = 0
0.00.043.077 I print_info: ssm_d_state      = 0
0.00.043.077 I print_info: ssm_dt_rank      = 0
0.00.043.077 I print_info: ssm_dt_b_c_rms   = 0
0.00.043.077 I print_info: model type       = 1.4B
0.00.043.077 I print_info: model params     = 1.41 B
0.00.043.078 I print_info: general.name     = 1.4B
0.00.043.078 I print_info: vocab type       = BPE
0.00.043.078 I print_info: n_vocab          = 50304
0.00.043.078 I print_info: n_merges         = 50009
0.00.043.078 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.043.079 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.043.079 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.043.079 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.043.080 I print_info: LF token         = 187 ''
0.00.043.080 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.043.080 I print_info: max token length = 1024
0.00.043.081 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.872.832 I load_tensors: offloading 24 repeating layers to GPU
0.00.872.837 I load_tensors: offloading output layer to GPU
0.00.872.838 I load_tensors: offloaded 25/25 layers to GPU
0.00.872.865 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.872.868 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.874.098 I llama_init_from_model: n_seq_max     = 1
0.00.874.100 I llama_init_from_model: n_ctx         = 128
0.00.874.100 I llama_init_from_model: n_ctx_per_seq = 128
0.00.874.100 I llama_init_from_model: n_batch       = 128
0.00.874.101 I llama_init_from_model: n_ubatch      = 128
0.00.874.101 I llama_init_from_model: flash_attn    = 0
0.00.874.102 I llama_init_from_model: freq_base     = 10000.0
0.00.874.103 I llama_init_from_model: freq_scale    = 1
0.00.874.103 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.874.104 I ggml_metal_init: allocating
0.00.874.171 I ggml_metal_init: found device: Apple M4
0.00.874.180 I ggml_metal_init: picking default device: Apple M4
0.00.875.525 I ggml_metal_init: using embedded metal library
0.00.880.777 I ggml_metal_init: GPU name:   Apple M4
0.00.880.780 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.880.781 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.880.782 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.880.782 I ggml_metal_init: simdgroup reduction   = true
0.00.880.782 I ggml_metal_init: simdgroup matrix mul. = true
0.00.880.782 I ggml_metal_init: has residency sets    = true
0.00.880.783 I ggml_metal_init: has bfloat            = true
0.00.880.783 I ggml_metal_init: use bfloat            = true
0.00.880.784 I ggml_metal_init: hasUnifiedMemory      = true
0.00.880.786 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.895.654 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.899.041 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.899.045 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.899.071 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.902.223 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.902.224 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.902.225 I llama_init_from_model: graph nodes  = 967
0.00.902.225 I llama_init_from_model: graph splits = 2
0.00.902.228 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.902.228 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.929.240 I 
0.00.929.307 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.929.315 I perplexity: tokenizing the input ..
0.00.936.429 I perplexity: tokenization took 7.111 ms
0.00.936.436 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.073.914 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.075.242 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.075.270 I llama_perf_context_print:        load time =     917.61 ms
0.01.075.271 I llama_perf_context_print: prompt eval time =     136.76 ms /   128 tokens (    1.07 ms per token,   935.93 tokens per second)
0.01.075.271 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.075.271 I llama_perf_context_print:       total time =     146.03 ms /   129 tokens
0.01.075.646 I ggml_metal_free: deallocating

real	0m1.094s
user	0m0.075s
sys	0m0.174s
```
- q4_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.059 I build: 4671 (4d3465c5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.094 I main: llama backend init
0.00.000.096 I main: load the model and apply lora adapter, if any
0.00.025.006 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.044.291 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.044.298 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.044.300 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.044.301 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.044.307 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.044.307 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.044.308 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.044.312 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.044.313 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.044.313 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.044.314 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.044.314 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.044.318 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.044.319 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.044.322 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.044.323 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.044.323 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.050.472 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.052.135 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.058.538 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.058.540 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.058.540 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.058.541 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.058.541 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.058.542 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.058.543 I llama_model_loader: - type  f32:  194 tensors
0.00.058.543 I llama_model_loader: - type q4_0:   97 tensors
0.00.058.543 I llama_model_loader: - type q6_K:    1 tensors
0.00.058.544 I print_info: file format = GGUF V3 (latest)
0.00.058.545 I print_info: file type   = Q4_0
0.00.058.546 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.077.120 I load: special tokens cache size = 25
0.00.089.120 I load: token to piece cache size = 0.2984 MB
0.00.089.124 I print_info: arch             = gptneox
0.00.089.124 I print_info: vocab_only       = 0
0.00.089.124 I print_info: n_ctx_train      = 2048
0.00.089.125 I print_info: n_embd           = 2048
0.00.089.125 I print_info: n_layer          = 24
0.00.089.129 I print_info: n_head           = 16
0.00.089.130 I print_info: n_head_kv        = 16
0.00.089.130 I print_info: n_rot            = 32
0.00.089.130 I print_info: n_swa            = 0
0.00.089.131 I print_info: n_embd_head_k    = 128
0.00.089.131 I print_info: n_embd_head_v    = 128
0.00.089.134 I print_info: n_gqa            = 1
0.00.089.135 I print_info: n_embd_k_gqa     = 2048
0.00.089.136 I print_info: n_embd_v_gqa     = 2048
0.00.089.137 I print_info: f_norm_eps       = 1.0e-05
0.00.089.140 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.089.140 I print_info: f_clamp_kqv      = 0.0e+00
0.00.089.140 I print_info: f_max_alibi_bias = 0.0e+00
0.00.089.142 I print_info: f_logit_scale    = 0.0e+00
0.00.089.143 I print_info: n_ff             = 8192
0.00.089.144 I print_info: n_expert         = 0
0.00.089.144 I print_info: n_expert_used    = 0
0.00.089.144 I print_info: causal attn      = 1
0.00.089.144 I print_info: pooling type     = 0
0.00.089.144 I print_info: rope type        = 2
0.00.089.145 I print_info: rope scaling     = linear
0.00.089.145 I print_info: freq_base_train  = 10000.0
0.00.089.146 I print_info: freq_scale_train = 1
0.00.089.146 I print_info: n_ctx_orig_yarn  = 2048
0.00.089.146 I print_info: rope_finetuned   = unknown
0.00.089.150 I print_info: ssm_d_conv       = 0
0.00.089.151 I print_info: ssm_d_inner      = 0
0.00.089.151 I print_info: ssm_d_state      = 0
0.00.089.151 I print_info: ssm_dt_rank      = 0
0.00.089.151 I print_info: ssm_dt_b_c_rms   = 0
0.00.089.152 I print_info: model type       = 1.4B
0.00.089.152 I print_info: model params     = 1.41 B
0.00.089.153 I print_info: general.name     = 1.4B
0.00.089.157 I print_info: vocab type       = BPE
0.00.089.157 I print_info: n_vocab          = 50304
0.00.089.157 I print_info: n_merges         = 50009
0.00.089.158 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.089.158 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.089.158 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.089.158 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.089.159 I print_info: LF token         = 187 ''
0.00.089.159 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.089.159 I print_info: max token length = 1024
0.00.089.160 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.684.084 I load_tensors: offloading 24 repeating layers to GPU
0.00.684.099 I load_tensors: offloading output layer to GPU
0.00.684.100 I load_tensors: offloaded 25/25 layers to GPU
0.00.684.134 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.684.136 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.685.620 I llama_init_from_model: n_seq_max     = 1
0.00.685.623 I llama_init_from_model: n_ctx         = 2048
0.00.685.624 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.685.624 I llama_init_from_model: n_batch       = 2048
0.00.685.625 I llama_init_from_model: n_ubatch      = 512
0.00.685.625 I llama_init_from_model: flash_attn    = 0
0.00.685.627 I llama_init_from_model: freq_base     = 10000.0
0.00.685.628 I llama_init_from_model: freq_scale    = 1
0.00.685.630 I ggml_metal_init: allocating
0.00.685.706 I ggml_metal_init: found device: Apple M4
0.00.685.719 I ggml_metal_init: picking default device: Apple M4
0.00.687.666 I ggml_metal_init: using embedded metal library
0.00.693.354 I ggml_metal_init: GPU name:   Apple M4
0.00.693.359 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.693.360 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.693.360 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.693.361 I ggml_metal_init: simdgroup reduction   = true
0.00.693.361 I ggml_metal_init: simdgroup matrix mul. = true
0.00.693.362 I ggml_metal_init: has residency sets    = true
0.00.693.362 I ggml_metal_init: has bfloat            = true
0.00.693.362 I ggml_metal_init: use bfloat            = true
0.00.693.363 I ggml_metal_init: hasUnifiedMemory      = true
0.00.693.368 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.712.537 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.768.112 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.768.119 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.768.150 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.772.759 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.772.761 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.772.761 I llama_init_from_model: graph nodes  = 967
0.00.772.762 I llama_init_from_model: graph splits = 2
0.00.772.767 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.772.901 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.772.902 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.827.206 I main: llama threadpool init, n_threads = 4
0.00.827.250 I 
0.00.827.273 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.827.275 I 
0.00.827.445 I sampler seed: 1234
0.00.827.450 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.827.492 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.827.496 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.827.496 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.510.729 I llama_perf_sampler_print:    sampling time =       1.37 ms /    71 runs   (    0.02 ms per token, 51938.55 tokens per second)
0.01.510.729 I llama_perf_context_print:        load time =     801.49 ms
0.01.510.730 I llama_perf_context_print: prompt eval time =      49.35 ms /     7 tokens (    7.05 ms per token,   141.85 tokens per second)
0.01.510.731 I llama_perf_context_print:        eval time =     630.96 ms /    63 runs   (   10.02 ms per token,    99.85 tokens per second)
0.01.510.731 I llama_perf_context_print:       total time =     684.23 ms /    70 tokens
0.01.510.932 I ggml_metal_free: deallocating

real	0m1.551s
user	0m0.135s
sys	0m0.208s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.283 I build: 4671 (4d3465c5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.789 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.075 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.017.080 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.086 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.087 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.087 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.088 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.088 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.089 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.089 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.090 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.090 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.090 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.090 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.091 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.093 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.093 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.093 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.879 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.911 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.766 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.767 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.768 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.768 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.768 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.769 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.025.769 I llama_model_loader: - type  f32:  194 tensors
0.00.025.770 I llama_model_loader: - type q4_0:   97 tensors
0.00.025.770 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.771 I print_info: file format = GGUF V3 (latest)
0.00.025.771 I print_info: file type   = Q4_0
0.00.025.772 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.033.968 I load: special tokens cache size = 25
0.00.040.090 I load: token to piece cache size = 0.2984 MB
0.00.040.095 I print_info: arch             = gptneox
0.00.040.095 I print_info: vocab_only       = 0
0.00.040.095 I print_info: n_ctx_train      = 2048
0.00.040.095 I print_info: n_embd           = 2048
0.00.040.095 I print_info: n_layer          = 24
0.00.040.100 I print_info: n_head           = 16
0.00.040.100 I print_info: n_head_kv        = 16
0.00.040.101 I print_info: n_rot            = 32
0.00.040.101 I print_info: n_swa            = 0
0.00.040.101 I print_info: n_embd_head_k    = 128
0.00.040.101 I print_info: n_embd_head_v    = 128
0.00.040.102 I print_info: n_gqa            = 1
0.00.040.103 I print_info: n_embd_k_gqa     = 2048
0.00.040.103 I print_info: n_embd_v_gqa     = 2048
0.00.040.104 I print_info: f_norm_eps       = 1.0e-05
0.00.040.104 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.105 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.105 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.105 I print_info: f_logit_scale    = 0.0e+00
0.00.040.106 I print_info: n_ff             = 8192
0.00.040.106 I print_info: n_expert         = 0
0.00.040.106 I print_info: n_expert_used    = 0
0.00.040.106 I print_info: causal attn      = 1
0.00.040.106 I print_info: pooling type     = 0
0.00.040.108 I print_info: rope type        = 2
0.00.040.108 I print_info: rope scaling     = linear
0.00.040.108 I print_info: freq_base_train  = 10000.0
0.00.040.109 I print_info: freq_scale_train = 1
0.00.040.109 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.109 I print_info: rope_finetuned   = unknown
0.00.040.109 I print_info: ssm_d_conv       = 0
0.00.040.109 I print_info: ssm_d_inner      = 0
0.00.040.112 I print_info: ssm_d_state      = 0
0.00.040.112 I print_info: ssm_dt_rank      = 0
0.00.040.112 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.112 I print_info: model type       = 1.4B
0.00.040.112 I print_info: model params     = 1.41 B
0.00.040.113 I print_info: general.name     = 1.4B
0.00.040.114 I print_info: vocab type       = BPE
0.00.040.114 I print_info: n_vocab          = 50304
0.00.040.114 I print_info: n_merges         = 50009
0.00.040.114 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.115 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.115 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.115 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.116 I print_info: LF token         = 187 ''
0.00.040.117 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.117 I print_info: max token length = 1024
0.00.040.117 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.639.579 I load_tensors: offloading 24 repeating layers to GPU
0.00.639.587 I load_tensors: offloading output layer to GPU
0.00.639.588 I load_tensors: offloaded 25/25 layers to GPU
0.00.639.620 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.639.621 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.641.289 I llama_init_from_model: n_seq_max     = 1
0.00.641.293 I llama_init_from_model: n_ctx         = 128
0.00.641.293 I llama_init_from_model: n_ctx_per_seq = 128
0.00.641.294 I llama_init_from_model: n_batch       = 128
0.00.641.294 I llama_init_from_model: n_ubatch      = 128
0.00.641.294 I llama_init_from_model: flash_attn    = 0
0.00.641.296 I llama_init_from_model: freq_base     = 10000.0
0.00.641.296 I llama_init_from_model: freq_scale    = 1
0.00.641.297 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.641.302 I ggml_metal_init: allocating
0.00.641.361 I ggml_metal_init: found device: Apple M4
0.00.641.373 I ggml_metal_init: picking default device: Apple M4
0.00.643.414 I ggml_metal_init: using embedded metal library
0.00.648.825 I ggml_metal_init: GPU name:   Apple M4
0.00.648.847 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.648.847 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.648.848 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.648.849 I ggml_metal_init: simdgroup reduction   = true
0.00.648.849 I ggml_metal_init: simdgroup matrix mul. = true
0.00.648.849 I ggml_metal_init: has residency sets    = true
0.00.648.849 I ggml_metal_init: has bfloat            = true
0.00.648.850 I ggml_metal_init: use bfloat            = true
0.00.648.852 I ggml_metal_init: hasUnifiedMemory      = true
0.00.648.857 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.668.652 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.672.156 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.672.162 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.672.204 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.675.531 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.675.533 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.675.533 I llama_init_from_model: graph nodes  = 967
0.00.675.534 I llama_init_from_model: graph splits = 2
0.00.675.537 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.675.539 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.703.339 I 
0.00.703.420 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.703.429 I perplexity: tokenizing the input ..
0.00.710.297 I perplexity: tokenization took 6.864 ms
0.00.710.303 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.839.828 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.841.176 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.841.197 I llama_perf_context_print:        load time =     693.54 ms
0.00.841.198 I llama_perf_context_print: prompt eval time =     128.63 ms /   128 tokens (    1.00 ms per token,   995.07 tokens per second)
0.00.841.198 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.841.199 I llama_perf_context_print:       total time =     137.86 ms /   129 tokens
0.00.841.561 I ggml_metal_free: deallocating

real	0m0.858s
user	0m0.079s
sys	0m0.145s
```
- q4_1:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.052 I build: 4671 (4d3465c5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.088 I main: llama backend init
0.00.000.090 I main: load the model and apply lora adapter, if any
0.00.009.564 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.514 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.017.526 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.528 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.529 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.529 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.530 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.530 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.531 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.531 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.532 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.536 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.536 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.536 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.537 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.540 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.540 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.540 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.359 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.372 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.126 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.127 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.127 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.128 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.128 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.128 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.026.129 I llama_model_loader: - type  f32:  194 tensors
0.00.026.129 I llama_model_loader: - type q4_1:   97 tensors
0.00.026.129 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.130 I print_info: file format = GGUF V3 (latest)
0.00.026.130 I print_info: file type   = Q4_1
0.00.026.134 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.034.052 I load: special tokens cache size = 25
0.00.040.259 I load: token to piece cache size = 0.2984 MB
0.00.040.262 I print_info: arch             = gptneox
0.00.040.262 I print_info: vocab_only       = 0
0.00.040.263 I print_info: n_ctx_train      = 2048
0.00.040.263 I print_info: n_embd           = 2048
0.00.040.263 I print_info: n_layer          = 24
0.00.040.266 I print_info: n_head           = 16
0.00.040.266 I print_info: n_head_kv        = 16
0.00.040.267 I print_info: n_rot            = 32
0.00.040.267 I print_info: n_swa            = 0
0.00.040.267 I print_info: n_embd_head_k    = 128
0.00.040.267 I print_info: n_embd_head_v    = 128
0.00.040.268 I print_info: n_gqa            = 1
0.00.040.269 I print_info: n_embd_k_gqa     = 2048
0.00.040.270 I print_info: n_embd_v_gqa     = 2048
0.00.040.270 I print_info: f_norm_eps       = 1.0e-05
0.00.040.271 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.271 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.271 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.271 I print_info: f_logit_scale    = 0.0e+00
0.00.040.272 I print_info: n_ff             = 8192
0.00.040.272 I print_info: n_expert         = 0
0.00.040.273 I print_info: n_expert_used    = 0
0.00.040.273 I print_info: causal attn      = 1
0.00.040.273 I print_info: pooling type     = 0
0.00.040.273 I print_info: rope type        = 2
0.00.040.275 I print_info: rope scaling     = linear
0.00.040.275 I print_info: freq_base_train  = 10000.0
0.00.040.275 I print_info: freq_scale_train = 1
0.00.040.276 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.276 I print_info: rope_finetuned   = unknown
0.00.040.276 I print_info: ssm_d_conv       = 0
0.00.040.277 I print_info: ssm_d_inner      = 0
0.00.040.277 I print_info: ssm_d_state      = 0
0.00.040.277 I print_info: ssm_dt_rank      = 0
0.00.040.278 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.278 I print_info: model type       = 1.4B
0.00.040.278 I print_info: model params     = 1.41 B
0.00.040.278 I print_info: general.name     = 1.4B
0.00.040.279 I print_info: vocab type       = BPE
0.00.040.279 I print_info: n_vocab          = 50304
0.00.040.279 I print_info: n_merges         = 50009
0.00.040.279 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.280 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.280 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.280 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.280 I print_info: LF token         = 187 ''
0.00.040.280 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.281 I print_info: max token length = 1024
0.00.040.281 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.686.983 I load_tensors: offloading 24 repeating layers to GPU
0.00.686.993 I load_tensors: offloading output layer to GPU
0.00.686.994 I load_tensors: offloaded 25/25 layers to GPU
0.00.687.015 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.687.016 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.687.938 I llama_init_from_model: n_seq_max     = 1
0.00.687.940 I llama_init_from_model: n_ctx         = 2048
0.00.687.940 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.687.941 I llama_init_from_model: n_batch       = 2048
0.00.687.941 I llama_init_from_model: n_ubatch      = 512
0.00.687.942 I llama_init_from_model: flash_attn    = 0
0.00.687.943 I llama_init_from_model: freq_base     = 10000.0
0.00.687.944 I llama_init_from_model: freq_scale    = 1
0.00.687.947 I ggml_metal_init: allocating
0.00.688.009 I ggml_metal_init: found device: Apple M4
0.00.688.021 I ggml_metal_init: picking default device: Apple M4
0.00.689.231 I ggml_metal_init: using embedded metal library
0.00.693.407 I ggml_metal_init: GPU name:   Apple M4
0.00.693.414 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.693.415 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.693.415 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.693.416 I ggml_metal_init: simdgroup reduction   = true
0.00.693.416 I ggml_metal_init: simdgroup matrix mul. = true
0.00.693.416 I ggml_metal_init: has residency sets    = true
0.00.693.417 I ggml_metal_init: has bfloat            = true
0.00.693.417 I ggml_metal_init: use bfloat            = true
0.00.693.418 I ggml_metal_init: hasUnifiedMemory      = true
0.00.693.420 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.709.921 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.744.413 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.744.420 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.744.443 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.748.937 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.748.940 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.748.941 I llama_init_from_model: graph nodes  = 967
0.00.748.941 I llama_init_from_model: graph splits = 2
0.00.748.947 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.749.077 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.749.077 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.807.247 I main: llama threadpool init, n_threads = 4
0.00.807.293 I 
0.00.807.316 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.807.316 I 
0.00.807.469 I sampler seed: 1234
0.00.807.474 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.807.484 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.807.485 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.807.485 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.535.567 I llama_perf_sampler_print:    sampling time =       1.20 ms /    71 runs   (    0.02 ms per token, 59019.12 tokens per second)
0.01.535.568 I llama_perf_context_print:        load time =     796.97 ms
0.01.535.569 I llama_perf_context_print: prompt eval time =      49.55 ms /     7 tokens (    7.08 ms per token,   141.29 tokens per second)
0.01.535.570 I llama_perf_context_print:        eval time =     675.81 ms /    63 runs   (   10.73 ms per token,    93.22 tokens per second)
0.01.535.571 I llama_perf_context_print:       total time =     729.03 ms /    70 tokens
0.01.535.824 I ggml_metal_free: deallocating

real	0m1.554s
user	0m0.105s
sys	0m0.188s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.096 I build: 4671 (4d3465c5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.927 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.549 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.015.554 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.556 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.557 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.557 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.558 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.558 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.559 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.559 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.560 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.560 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.560 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.561 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.563 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.565 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.565 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.565 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.423 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.421 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.265 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.266 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.267 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.267 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.267 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.268 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.024.268 I llama_model_loader: - type  f32:  194 tensors
0.00.024.269 I llama_model_loader: - type q4_1:   97 tensors
0.00.024.269 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.270 I print_info: file format = GGUF V3 (latest)
0.00.024.276 I print_info: file type   = Q4_1
0.00.024.277 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.032.858 I load: special tokens cache size = 25
0.00.039.016 I load: token to piece cache size = 0.2984 MB
0.00.039.021 I print_info: arch             = gptneox
0.00.039.021 I print_info: vocab_only       = 0
0.00.039.021 I print_info: n_ctx_train      = 2048
0.00.039.021 I print_info: n_embd           = 2048
0.00.039.021 I print_info: n_layer          = 24
0.00.039.026 I print_info: n_head           = 16
0.00.039.027 I print_info: n_head_kv        = 16
0.00.039.027 I print_info: n_rot            = 32
0.00.039.027 I print_info: n_swa            = 0
0.00.039.027 I print_info: n_embd_head_k    = 128
0.00.039.027 I print_info: n_embd_head_v    = 128
0.00.039.028 I print_info: n_gqa            = 1
0.00.039.029 I print_info: n_embd_k_gqa     = 2048
0.00.039.030 I print_info: n_embd_v_gqa     = 2048
0.00.039.030 I print_info: f_norm_eps       = 1.0e-05
0.00.039.033 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.033 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.033 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.033 I print_info: f_logit_scale    = 0.0e+00
0.00.039.037 I print_info: n_ff             = 8192
0.00.039.038 I print_info: n_expert         = 0
0.00.039.038 I print_info: n_expert_used    = 0
0.00.039.038 I print_info: causal attn      = 1
0.00.039.038 I print_info: pooling type     = 0
0.00.039.040 I print_info: rope type        = 2
0.00.039.040 I print_info: rope scaling     = linear
0.00.039.040 I print_info: freq_base_train  = 10000.0
0.00.039.040 I print_info: freq_scale_train = 1
0.00.039.041 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.041 I print_info: rope_finetuned   = unknown
0.00.039.041 I print_info: ssm_d_conv       = 0
0.00.039.041 I print_info: ssm_d_inner      = 0
0.00.039.041 I print_info: ssm_d_state      = 0
0.00.039.041 I print_info: ssm_dt_rank      = 0
0.00.039.041 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.042 I print_info: model type       = 1.4B
0.00.039.042 I print_info: model params     = 1.41 B
0.00.039.042 I print_info: general.name     = 1.4B
0.00.039.043 I print_info: vocab type       = BPE
0.00.039.043 I print_info: n_vocab          = 50304
0.00.039.043 I print_info: n_merges         = 50009
0.00.039.043 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.044 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.046 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.046 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.046 I print_info: LF token         = 187 ''
0.00.039.047 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.047 I print_info: max token length = 1024
0.00.039.047 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.646.550 I load_tensors: offloading 24 repeating layers to GPU
0.00.646.565 I load_tensors: offloading output layer to GPU
0.00.646.566 I load_tensors: offloaded 25/25 layers to GPU
0.00.646.598 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.646.599 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.648.374 I llama_init_from_model: n_seq_max     = 1
0.00.648.377 I llama_init_from_model: n_ctx         = 128
0.00.648.378 I llama_init_from_model: n_ctx_per_seq = 128
0.00.648.378 I llama_init_from_model: n_batch       = 128
0.00.648.378 I llama_init_from_model: n_ubatch      = 128
0.00.648.379 I llama_init_from_model: flash_attn    = 0
0.00.648.381 I llama_init_from_model: freq_base     = 10000.0
0.00.648.382 I llama_init_from_model: freq_scale    = 1
0.00.648.382 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.648.385 I ggml_metal_init: allocating
0.00.648.463 I ggml_metal_init: found device: Apple M4
0.00.648.476 I ggml_metal_init: picking default device: Apple M4
0.00.650.284 I ggml_metal_init: using embedded metal library
0.00.656.777 I ggml_metal_init: GPU name:   Apple M4
0.00.656.785 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.656.786 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.656.787 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.656.788 I ggml_metal_init: simdgroup reduction   = true
0.00.656.788 I ggml_metal_init: simdgroup matrix mul. = true
0.00.656.789 I ggml_metal_init: has residency sets    = true
0.00.656.789 I ggml_metal_init: has bfloat            = true
0.00.656.789 I ggml_metal_init: use bfloat            = true
0.00.656.790 I ggml_metal_init: hasUnifiedMemory      = true
0.00.656.804 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.675.396 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.678.890 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.678.896 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.678.925 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.682.125 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.682.127 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.682.127 I llama_init_from_model: graph nodes  = 967
0.00.682.128 I llama_init_from_model: graph splits = 2
0.00.682.131 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.682.131 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.710.540 I 
0.00.710.629 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.710.654 I perplexity: tokenizing the input ..
0.00.717.569 I perplexity: tokenization took 6.911 ms
0.00.717.576 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.855.307 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.856.642 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.856.666 I llama_perf_context_print:        load time =     701.60 ms
0.00.856.667 I llama_perf_context_print: prompt eval time =     136.83 ms /   128 tokens (    1.07 ms per token,   935.48 tokens per second)
0.00.856.668 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.856.668 I llama_perf_context_print:       total time =     146.13 ms /   129 tokens
0.00.857.081 I ggml_metal_free: deallocating

real	0m0.871s
user	0m0.081s
sys	0m0.134s
```
- q5_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.053 I build: 4671 (4d3465c5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.095 I main: llama backend init
0.00.000.097 I main: load the model and apply lora adapter, if any
0.00.010.196 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.787 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.017.791 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.798 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.798 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.799 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.799 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.799 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.800 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.801 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.801 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.801 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.802 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.802 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.802 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.804 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.804 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.805 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.621 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.621 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.338 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.339 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.339 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.340 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.340 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.340 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.026.341 I llama_model_loader: - type  f32:  194 tensors
0.00.026.341 I llama_model_loader: - type q5_0:   97 tensors
0.00.026.341 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.342 I print_info: file format = GGUF V3 (latest)
0.00.026.342 I print_info: file type   = Q5_0
0.00.026.343 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.034.110 I load: special tokens cache size = 25
0.00.040.200 I load: token to piece cache size = 0.2984 MB
0.00.040.203 I print_info: arch             = gptneox
0.00.040.203 I print_info: vocab_only       = 0
0.00.040.203 I print_info: n_ctx_train      = 2048
0.00.040.203 I print_info: n_embd           = 2048
0.00.040.204 I print_info: n_layer          = 24
0.00.040.206 I print_info: n_head           = 16
0.00.040.207 I print_info: n_head_kv        = 16
0.00.040.207 I print_info: n_rot            = 32
0.00.040.207 I print_info: n_swa            = 0
0.00.040.208 I print_info: n_embd_head_k    = 128
0.00.040.208 I print_info: n_embd_head_v    = 128
0.00.040.208 I print_info: n_gqa            = 1
0.00.040.209 I print_info: n_embd_k_gqa     = 2048
0.00.040.210 I print_info: n_embd_v_gqa     = 2048
0.00.040.211 I print_info: f_norm_eps       = 1.0e-05
0.00.040.211 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.213 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.213 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.214 I print_info: f_logit_scale    = 0.0e+00
0.00.040.214 I print_info: n_ff             = 8192
0.00.040.214 I print_info: n_expert         = 0
0.00.040.215 I print_info: n_expert_used    = 0
0.00.040.215 I print_info: causal attn      = 1
0.00.040.216 I print_info: pooling type     = 0
0.00.040.216 I print_info: rope type        = 2
0.00.040.216 I print_info: rope scaling     = linear
0.00.040.217 I print_info: freq_base_train  = 10000.0
0.00.040.217 I print_info: freq_scale_train = 1
0.00.040.217 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.217 I print_info: rope_finetuned   = unknown
0.00.040.219 I print_info: ssm_d_conv       = 0
0.00.040.219 I print_info: ssm_d_inner      = 0
0.00.040.219 I print_info: ssm_d_state      = 0
0.00.040.219 I print_info: ssm_dt_rank      = 0
0.00.040.220 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.220 I print_info: model type       = 1.4B
0.00.040.220 I print_info: model params     = 1.41 B
0.00.040.220 I print_info: general.name     = 1.4B
0.00.040.221 I print_info: vocab type       = BPE
0.00.040.221 I print_info: n_vocab          = 50304
0.00.040.221 I print_info: n_merges         = 50009
0.00.040.221 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.221 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.222 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.222 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.225 I print_info: LF token         = 187 ''
0.00.040.226 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.227 I print_info: max token length = 1024
0.00.040.228 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.707.221 I load_tensors: offloading 24 repeating layers to GPU
0.00.707.238 I load_tensors: offloading output layer to GPU
0.00.707.239 I load_tensors: offloaded 25/25 layers to GPU
0.00.707.272 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.707.274 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.708.773 I llama_init_from_model: n_seq_max     = 1
0.00.708.776 I llama_init_from_model: n_ctx         = 2048
0.00.708.777 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.708.777 I llama_init_from_model: n_batch       = 2048
0.00.708.778 I llama_init_from_model: n_ubatch      = 512
0.00.708.778 I llama_init_from_model: flash_attn    = 0
0.00.708.781 I llama_init_from_model: freq_base     = 10000.0
0.00.708.781 I llama_init_from_model: freq_scale    = 1
0.00.708.783 I ggml_metal_init: allocating
0.00.708.859 I ggml_metal_init: found device: Apple M4
0.00.708.872 I ggml_metal_init: picking default device: Apple M4
0.00.710.813 I ggml_metal_init: using embedded metal library
0.00.717.337 I ggml_metal_init: GPU name:   Apple M4
0.00.717.341 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.717.341 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.717.342 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.717.343 I ggml_metal_init: simdgroup reduction   = true
0.00.717.343 I ggml_metal_init: simdgroup matrix mul. = true
0.00.717.343 I ggml_metal_init: has residency sets    = true
0.00.717.343 I ggml_metal_init: has bfloat            = true
0.00.717.344 I ggml_metal_init: use bfloat            = true
0.00.717.344 I ggml_metal_init: hasUnifiedMemory      = true
0.00.717.346 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.735.409 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.794.350 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.794.357 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.794.388 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.799.417 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.799.419 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.799.419 I llama_init_from_model: graph nodes  = 967
0.00.799.420 I llama_init_from_model: graph splits = 2
0.00.799.426 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.799.538 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.799.539 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.859.464 I main: llama threadpool init, n_threads = 4
0.00.859.508 I 
0.00.859.532 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.859.533 I 
0.00.859.687 I sampler seed: 1234
0.00.859.691 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.859.712 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.859.712 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.859.712 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.648.494 I llama_perf_sampler_print:    sampling time =       1.36 ms /    71 runs   (    0.02 ms per token, 52129.22 tokens per second)
0.01.648.495 I llama_perf_context_print:        load time =     848.55 ms
0.01.648.495 I llama_perf_context_print: prompt eval time =      52.94 ms /     7 tokens (    7.56 ms per token,   132.23 tokens per second)
0.01.648.497 I llama_perf_context_print:        eval time =     732.96 ms /    63 runs   (   11.63 ms per token,    85.95 tokens per second)
0.01.648.497 I llama_perf_context_print:       total time =     789.75 ms /    70 tokens
0.01.648.725 I ggml_metal_free: deallocating

real	0m1.667s
user	0m0.110s
sys	0m0.218s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.101 I build: 4671 (4d3465c5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.918 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.136 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.017.141 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.145 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.146 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.146 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.146 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.147 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.148 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.148 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.149 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.149 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.149 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.150 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.151 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.153 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.153 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.153 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.098 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.163 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.046 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.048 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.048 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.048 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.049 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.049 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.026.050 I llama_model_loader: - type  f32:  194 tensors
0.00.026.050 I llama_model_loader: - type q5_0:   97 tensors
0.00.026.050 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.051 I print_info: file format = GGUF V3 (latest)
0.00.026.051 I print_info: file type   = Q5_0
0.00.026.053 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.033.914 I load: special tokens cache size = 25
0.00.039.883 I load: token to piece cache size = 0.2984 MB
0.00.039.886 I print_info: arch             = gptneox
0.00.039.886 I print_info: vocab_only       = 0
0.00.039.886 I print_info: n_ctx_train      = 2048
0.00.039.887 I print_info: n_embd           = 2048
0.00.039.887 I print_info: n_layer          = 24
0.00.039.890 I print_info: n_head           = 16
0.00.039.891 I print_info: n_head_kv        = 16
0.00.039.892 I print_info: n_rot            = 32
0.00.039.892 I print_info: n_swa            = 0
0.00.039.892 I print_info: n_embd_head_k    = 128
0.00.039.892 I print_info: n_embd_head_v    = 128
0.00.039.893 I print_info: n_gqa            = 1
0.00.039.894 I print_info: n_embd_k_gqa     = 2048
0.00.039.894 I print_info: n_embd_v_gqa     = 2048
0.00.039.895 I print_info: f_norm_eps       = 1.0e-05
0.00.039.896 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.896 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.896 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.896 I print_info: f_logit_scale    = 0.0e+00
0.00.039.897 I print_info: n_ff             = 8192
0.00.039.897 I print_info: n_expert         = 0
0.00.039.897 I print_info: n_expert_used    = 0
0.00.039.897 I print_info: causal attn      = 1
0.00.039.897 I print_info: pooling type     = 0
0.00.039.898 I print_info: rope type        = 2
0.00.039.898 I print_info: rope scaling     = linear
0.00.039.898 I print_info: freq_base_train  = 10000.0
0.00.039.899 I print_info: freq_scale_train = 1
0.00.039.899 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.899 I print_info: rope_finetuned   = unknown
0.00.039.899 I print_info: ssm_d_conv       = 0
0.00.039.900 I print_info: ssm_d_inner      = 0
0.00.039.901 I print_info: ssm_d_state      = 0
0.00.039.901 I print_info: ssm_dt_rank      = 0
0.00.039.903 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.904 I print_info: model type       = 1.4B
0.00.039.904 I print_info: model params     = 1.41 B
0.00.039.904 I print_info: general.name     = 1.4B
0.00.039.904 I print_info: vocab type       = BPE
0.00.039.905 I print_info: n_vocab          = 50304
0.00.039.905 I print_info: n_merges         = 50009
0.00.039.905 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.905 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.905 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.906 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.906 I print_info: LF token         = 187 ''
0.00.039.906 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.906 I print_info: max token length = 1024
0.00.039.907 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.695.654 I load_tensors: offloading 24 repeating layers to GPU
0.00.695.669 I load_tensors: offloading output layer to GPU
0.00.695.670 I load_tensors: offloaded 25/25 layers to GPU
0.00.695.704 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.695.710 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.697.413 I llama_init_from_model: n_seq_max     = 1
0.00.697.416 I llama_init_from_model: n_ctx         = 128
0.00.697.417 I llama_init_from_model: n_ctx_per_seq = 128
0.00.697.417 I llama_init_from_model: n_batch       = 128
0.00.697.418 I llama_init_from_model: n_ubatch      = 128
0.00.697.419 I llama_init_from_model: flash_attn    = 0
0.00.697.421 I llama_init_from_model: freq_base     = 10000.0
0.00.697.421 I llama_init_from_model: freq_scale    = 1
0.00.697.422 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.697.425 I ggml_metal_init: allocating
0.00.697.494 I ggml_metal_init: found device: Apple M4
0.00.697.508 I ggml_metal_init: picking default device: Apple M4
0.00.699.027 I ggml_metal_init: using embedded metal library
0.00.705.348 I ggml_metal_init: GPU name:   Apple M4
0.00.705.352 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.705.353 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.705.354 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.705.354 I ggml_metal_init: simdgroup reduction   = true
0.00.705.355 I ggml_metal_init: simdgroup matrix mul. = true
0.00.705.355 I ggml_metal_init: has residency sets    = true
0.00.705.355 I ggml_metal_init: has bfloat            = true
0.00.705.355 I ggml_metal_init: use bfloat            = true
0.00.705.356 I ggml_metal_init: hasUnifiedMemory      = true
0.00.705.358 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.722.376 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.725.786 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.725.791 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.725.827 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.729.340 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.729.342 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.729.342 I llama_init_from_model: graph nodes  = 967
0.00.729.343 I llama_init_from_model: graph splits = 2
0.00.729.346 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.729.346 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.759.176 I 
0.00.759.255 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.759.263 I perplexity: tokenizing the input ..
0.00.766.445 I perplexity: tokenization took 7.179 ms
0.00.766.450 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.915.639 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.916.969 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.916.989 I llama_perf_context_print:        load time =     749.25 ms
0.00.916.990 I llama_perf_context_print: prompt eval time =     148.31 ms /   128 tokens (    1.16 ms per token,   863.06 tokens per second)
0.00.916.991 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.916.991 I llama_perf_context_print:       total time =     157.82 ms /   129 tokens
0.00.917.414 I ggml_metal_free: deallocating

real	0m0.932s
user	0m0.079s
sys	0m0.128s
```
- q5_1:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.050 I build: 4671 (4d3465c5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.085 I main: llama backend init
0.00.000.086 I main: load the model and apply lora adapter, if any
0.00.008.641 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.234 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.238 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.239 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.240 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.240 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.241 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.241 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.242 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.242 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.242 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.243 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.243 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.243 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.244 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.246 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.246 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.247 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.003 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.994 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.759 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.760 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.760 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.761 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.761 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.761 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.762 I llama_model_loader: - type  f32:  194 tensors
0.00.024.762 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.762 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.763 I print_info: file format = GGUF V3 (latest)
0.00.024.763 I print_info: file type   = Q5_1
0.00.024.764 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.032.597 I load: special tokens cache size = 25
0.00.038.664 I load: token to piece cache size = 0.2984 MB
0.00.038.667 I print_info: arch             = gptneox
0.00.038.667 I print_info: vocab_only       = 0
0.00.038.667 I print_info: n_ctx_train      = 2048
0.00.038.668 I print_info: n_embd           = 2048
0.00.038.668 I print_info: n_layer          = 24
0.00.038.670 I print_info: n_head           = 16
0.00.038.671 I print_info: n_head_kv        = 16
0.00.038.671 I print_info: n_rot            = 32
0.00.038.671 I print_info: n_swa            = 0
0.00.038.672 I print_info: n_embd_head_k    = 128
0.00.038.672 I print_info: n_embd_head_v    = 128
0.00.038.673 I print_info: n_gqa            = 1
0.00.038.673 I print_info: n_embd_k_gqa     = 2048
0.00.038.674 I print_info: n_embd_v_gqa     = 2048
0.00.038.674 I print_info: f_norm_eps       = 1.0e-05
0.00.038.675 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.676 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.676 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.678 I print_info: f_logit_scale    = 0.0e+00
0.00.038.678 I print_info: n_ff             = 8192
0.00.038.679 I print_info: n_expert         = 0
0.00.038.679 I print_info: n_expert_used    = 0
0.00.038.679 I print_info: causal attn      = 1
0.00.038.679 I print_info: pooling type     = 0
0.00.038.680 I print_info: rope type        = 2
0.00.038.681 I print_info: rope scaling     = linear
0.00.038.681 I print_info: freq_base_train  = 10000.0
0.00.038.681 I print_info: freq_scale_train = 1
0.00.038.682 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.682 I print_info: rope_finetuned   = unknown
0.00.038.682 I print_info: ssm_d_conv       = 0
0.00.038.682 I print_info: ssm_d_inner      = 0
0.00.038.682 I print_info: ssm_d_state      = 0
0.00.038.684 I print_info: ssm_dt_rank      = 0
0.00.038.684 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.684 I print_info: model type       = 1.4B
0.00.038.684 I print_info: model params     = 1.41 B
0.00.038.685 I print_info: general.name     = 1.4B
0.00.038.685 I print_info: vocab type       = BPE
0.00.038.685 I print_info: n_vocab          = 50304
0.00.038.685 I print_info: n_merges         = 50009
0.00.038.686 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.686 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.686 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.686 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.687 I print_info: LF token         = 187 ''
0.00.038.687 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.687 I print_info: max token length = 1024
0.00.038.688 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.650.076 I load_tensors: offloading 24 repeating layers to GPU
0.00.650.080 I load_tensors: offloading output layer to GPU
0.00.650.082 I load_tensors: offloaded 25/25 layers to GPU
0.00.650.105 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.650.106 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.651.511 I llama_init_from_model: n_seq_max     = 1
0.00.651.512 I llama_init_from_model: n_ctx         = 2048
0.00.651.513 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.651.513 I llama_init_from_model: n_batch       = 2048
0.00.651.514 I llama_init_from_model: n_ubatch      = 512
0.00.651.515 I llama_init_from_model: flash_attn    = 0
0.00.651.515 I llama_init_from_model: freq_base     = 10000.0
0.00.651.516 I llama_init_from_model: freq_scale    = 1
0.00.651.517 I ggml_metal_init: allocating
0.00.651.534 I ggml_metal_init: found device: Apple M4
0.00.651.543 I ggml_metal_init: picking default device: Apple M4
0.00.653.047 I ggml_metal_init: using embedded metal library
0.00.659.312 I ggml_metal_init: GPU name:   Apple M4
0.00.659.316 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.659.317 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.659.318 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.659.318 I ggml_metal_init: simdgroup reduction   = true
0.00.659.319 I ggml_metal_init: simdgroup matrix mul. = true
0.00.659.319 I ggml_metal_init: has residency sets    = true
0.00.659.319 I ggml_metal_init: has bfloat            = true
0.00.659.319 I ggml_metal_init: use bfloat            = true
0.00.659.322 I ggml_metal_init: hasUnifiedMemory      = true
0.00.659.324 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.675.699 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.730.429 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.730.434 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.730.458 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.735.402 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.735.403 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.735.404 I llama_init_from_model: graph nodes  = 967
0.00.735.404 I llama_init_from_model: graph splits = 2
0.00.735.410 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.735.534 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.735.534 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.793.679 I main: llama threadpool init, n_threads = 4
0.00.793.722 I 
0.00.793.744 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.793.745 I 
0.00.793.920 I sampler seed: 1234
0.00.793.924 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.793.935 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.793.935 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.793.935 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.631.588 I llama_perf_sampler_print:    sampling time =       1.39 ms /    71 runs   (    0.02 ms per token, 51005.75 tokens per second)
0.01.631.589 I llama_perf_context_print:        load time =     784.34 ms
0.01.631.591 I llama_perf_context_print: prompt eval time =      51.81 ms /     7 tokens (    7.40 ms per token,   135.11 tokens per second)
0.01.631.591 I llama_perf_context_print:        eval time =     782.77 ms /    63 runs   (   12.42 ms per token,    80.48 tokens per second)
0.01.631.592 I llama_perf_context_print:       total time =     838.61 ms /    70 tokens
0.01.631.847 I ggml_metal_free: deallocating

real	0m1.650s
user	0m0.107s
sys	0m0.243s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.096 I build: 4671 (4d3465c5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.477 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.631 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.637 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.639 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.640 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.640 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.640 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.641 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.642 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.642 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.642 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.645 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.645 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.646 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.646 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.648 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.648 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.649 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.587 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.647 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.539 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.540 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.541 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.541 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.541 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.542 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.542 I llama_model_loader: - type  f32:  194 tensors
0.00.025.542 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.543 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.544 I print_info: file format = GGUF V3 (latest)
0.00.025.544 I print_info: file type   = Q5_1
0.00.025.545 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.034.067 I load: special tokens cache size = 25
0.00.040.270 I load: token to piece cache size = 0.2984 MB
0.00.040.275 I print_info: arch             = gptneox
0.00.040.275 I print_info: vocab_only       = 0
0.00.040.275 I print_info: n_ctx_train      = 2048
0.00.040.275 I print_info: n_embd           = 2048
0.00.040.275 I print_info: n_layer          = 24
0.00.040.280 I print_info: n_head           = 16
0.00.040.281 I print_info: n_head_kv        = 16
0.00.040.281 I print_info: n_rot            = 32
0.00.040.281 I print_info: n_swa            = 0
0.00.040.281 I print_info: n_embd_head_k    = 128
0.00.040.282 I print_info: n_embd_head_v    = 128
0.00.040.282 I print_info: n_gqa            = 1
0.00.040.283 I print_info: n_embd_k_gqa     = 2048
0.00.040.284 I print_info: n_embd_v_gqa     = 2048
0.00.040.286 I print_info: f_norm_eps       = 1.0e-05
0.00.040.287 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.287 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.287 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.287 I print_info: f_logit_scale    = 0.0e+00
0.00.040.288 I print_info: n_ff             = 8192
0.00.040.288 I print_info: n_expert         = 0
0.00.040.288 I print_info: n_expert_used    = 0
0.00.040.288 I print_info: causal attn      = 1
0.00.040.288 I print_info: pooling type     = 0
0.00.040.288 I print_info: rope type        = 2
0.00.040.290 I print_info: rope scaling     = linear
0.00.040.290 I print_info: freq_base_train  = 10000.0
0.00.040.290 I print_info: freq_scale_train = 1
0.00.040.290 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.291 I print_info: rope_finetuned   = unknown
0.00.040.291 I print_info: ssm_d_conv       = 0
0.00.040.291 I print_info: ssm_d_inner      = 0
0.00.040.291 I print_info: ssm_d_state      = 0
0.00.040.292 I print_info: ssm_dt_rank      = 0
0.00.040.292 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.292 I print_info: model type       = 1.4B
0.00.040.292 I print_info: model params     = 1.41 B
0.00.040.292 I print_info: general.name     = 1.4B
0.00.040.293 I print_info: vocab type       = BPE
0.00.040.293 I print_info: n_vocab          = 50304
0.00.040.293 I print_info: n_merges         = 50009
0.00.040.295 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.295 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.295 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.296 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.296 I print_info: LF token         = 187 ''
0.00.040.296 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.296 I print_info: max token length = 1024
0.00.040.297 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.753.331 I load_tensors: offloading 24 repeating layers to GPU
0.00.753.351 I load_tensors: offloading output layer to GPU
0.00.753.352 I load_tensors: offloaded 25/25 layers to GPU
0.00.753.386 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.753.387 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.754.773 I llama_init_from_model: n_seq_max     = 1
0.00.754.776 I llama_init_from_model: n_ctx         = 128
0.00.754.776 I llama_init_from_model: n_ctx_per_seq = 128
0.00.754.777 I llama_init_from_model: n_batch       = 128
0.00.754.777 I llama_init_from_model: n_ubatch      = 128
0.00.754.778 I llama_init_from_model: flash_attn    = 0
0.00.754.780 I llama_init_from_model: freq_base     = 10000.0
0.00.754.780 I llama_init_from_model: freq_scale    = 1
0.00.754.781 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.754.783 I ggml_metal_init: allocating
0.00.754.868 I ggml_metal_init: found device: Apple M4
0.00.754.883 I ggml_metal_init: picking default device: Apple M4
0.00.756.708 I ggml_metal_init: using embedded metal library
0.00.763.763 I ggml_metal_init: GPU name:   Apple M4
0.00.763.774 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.763.774 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.763.775 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.763.779 I ggml_metal_init: simdgroup reduction   = true
0.00.763.779 I ggml_metal_init: simdgroup matrix mul. = true
0.00.763.779 I ggml_metal_init: has residency sets    = true
0.00.763.780 I ggml_metal_init: has bfloat            = true
0.00.763.780 I ggml_metal_init: use bfloat            = true
0.00.763.781 I ggml_metal_init: hasUnifiedMemory      = true
0.00.763.789 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.781.418 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.785.162 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.785.168 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.785.207 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.788.481 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.788.483 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.788.484 I llama_init_from_model: graph nodes  = 967
0.00.788.484 I llama_init_from_model: graph splits = 2
0.00.788.488 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.788.488 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.815.028 I 
0.00.815.085 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.815.092 I perplexity: tokenizing the input ..
0.00.822.491 I perplexity: tokenization took 7.396 ms
0.00.822.500 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.958.679 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.960.002 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.960.026 I llama_perf_context_print:        load time =     805.54 ms
0.00.960.027 I llama_perf_context_print: prompt eval time =     135.21 ms /   128 tokens (    1.06 ms per token,   946.70 tokens per second)
0.00.960.027 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.960.028 I llama_perf_context_print:       total time =     145.00 ms /   129 tokens
0.00.960.445 I ggml_metal_free: deallocating

real	0m0.974s
user	0m0.082s
sys	0m0.173s
```
- q2_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.052 I build: 4671 (4d3465c5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.084 I main: llama backend init
0.00.000.086 I main: load the model and apply lora adapter, if any
0.00.009.910 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.459 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.463 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.465 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.465 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.466 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.466 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.466 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.467 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.468 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.468 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.468 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.469 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.469 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.470 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.471 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.471 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.472 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.281 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.324 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.134 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.135 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.136 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.136 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.136 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.137 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.137 I llama_model_loader: - type  f32:  194 tensors
0.00.025.138 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.138 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.138 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.139 I print_info: file format = GGUF V3 (latest)
0.00.025.139 I print_info: file type   = Q2_K - Medium
0.00.025.140 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.032.963 I load: special tokens cache size = 25
0.00.039.074 I load: token to piece cache size = 0.2984 MB
0.00.039.077 I print_info: arch             = gptneox
0.00.039.077 I print_info: vocab_only       = 0
0.00.039.078 I print_info: n_ctx_train      = 2048
0.00.039.078 I print_info: n_embd           = 2048
0.00.039.078 I print_info: n_layer          = 24
0.00.039.081 I print_info: n_head           = 16
0.00.039.081 I print_info: n_head_kv        = 16
0.00.039.082 I print_info: n_rot            = 32
0.00.039.082 I print_info: n_swa            = 0
0.00.039.082 I print_info: n_embd_head_k    = 128
0.00.039.082 I print_info: n_embd_head_v    = 128
0.00.039.083 I print_info: n_gqa            = 1
0.00.039.084 I print_info: n_embd_k_gqa     = 2048
0.00.039.084 I print_info: n_embd_v_gqa     = 2048
0.00.039.085 I print_info: f_norm_eps       = 1.0e-05
0.00.039.085 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.086 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.086 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.086 I print_info: f_logit_scale    = 0.0e+00
0.00.039.089 I print_info: n_ff             = 8192
0.00.039.089 I print_info: n_expert         = 0
0.00.039.089 I print_info: n_expert_used    = 0
0.00.039.089 I print_info: causal attn      = 1
0.00.039.089 I print_info: pooling type     = 0
0.00.039.090 I print_info: rope type        = 2
0.00.039.091 I print_info: rope scaling     = linear
0.00.039.091 I print_info: freq_base_train  = 10000.0
0.00.039.091 I print_info: freq_scale_train = 1
0.00.039.091 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.092 I print_info: rope_finetuned   = unknown
0.00.039.092 I print_info: ssm_d_conv       = 0
0.00.039.093 I print_info: ssm_d_inner      = 0
0.00.039.093 I print_info: ssm_d_state      = 0
0.00.039.094 I print_info: ssm_dt_rank      = 0
0.00.039.094 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.095 I print_info: model type       = 1.4B
0.00.039.095 I print_info: model params     = 1.41 B
0.00.039.095 I print_info: general.name     = 1.4B
0.00.039.096 I print_info: vocab type       = BPE
0.00.039.096 I print_info: n_vocab          = 50304
0.00.039.096 I print_info: n_merges         = 50009
0.00.039.096 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.096 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.097 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.097 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.099 I print_info: LF token         = 187 ''
0.00.039.099 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.099 I print_info: max token length = 1024
0.00.039.100 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.355.649 I load_tensors: offloading 24 repeating layers to GPU
0.00.355.662 I load_tensors: offloading output layer to GPU
0.00.355.663 I load_tensors: offloaded 25/25 layers to GPU
0.00.355.694 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.355.695 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.357.309 I llama_init_from_model: n_seq_max     = 1
0.00.357.316 I llama_init_from_model: n_ctx         = 2048
0.00.357.317 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.357.317 I llama_init_from_model: n_batch       = 2048
0.00.357.318 I llama_init_from_model: n_ubatch      = 512
0.00.357.318 I llama_init_from_model: flash_attn    = 0
0.00.357.319 I llama_init_from_model: freq_base     = 10000.0
0.00.357.320 I llama_init_from_model: freq_scale    = 1
0.00.357.326 I ggml_metal_init: allocating
0.00.357.401 I ggml_metal_init: found device: Apple M4
0.00.357.415 I ggml_metal_init: picking default device: Apple M4
0.00.359.235 I ggml_metal_init: using embedded metal library
0.00.365.190 I ggml_metal_init: GPU name:   Apple M4
0.00.365.204 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.365.204 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.365.205 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.365.206 I ggml_metal_init: simdgroup reduction   = true
0.00.365.206 I ggml_metal_init: simdgroup matrix mul. = true
0.00.365.207 I ggml_metal_init: has residency sets    = true
0.00.365.207 I ggml_metal_init: has bfloat            = true
0.00.365.207 I ggml_metal_init: use bfloat            = true
0.00.365.211 I ggml_metal_init: hasUnifiedMemory      = true
0.00.365.215 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.387.571 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.450.777 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.450.783 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.450.804 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.456.353 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.456.355 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.456.355 I llama_init_from_model: graph nodes  = 967
0.00.456.355 I llama_init_from_model: graph splits = 2
0.00.456.361 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.456.485 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.456.486 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.513.093 I main: llama threadpool init, n_threads = 4
0.00.513.136 I 
0.00.513.159 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.513.159 I 
0.00.513.335 I sampler seed: 1234
0.00.513.340 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.513.373 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.513.377 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.513.377 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.184.262 I llama_perf_sampler_print:    sampling time =       1.36 ms /    71 runs   (    0.02 ms per token, 52052.79 tokens per second)
0.01.184.262 I llama_perf_context_print:        load time =     502.49 ms
0.01.184.263 I llama_perf_context_print: prompt eval time =      35.76 ms /     7 tokens (    5.11 ms per token,   195.77 tokens per second)
0.01.184.265 I llama_perf_context_print:        eval time =     632.23 ms /    63 runs   (   10.04 ms per token,    99.65 tokens per second)
0.01.184.265 I llama_perf_context_print:       total time =     671.86 ms /    70 tokens
0.01.184.483 I ggml_metal_free: deallocating

real	0m1.203s
user	0m0.111s
sys	0m0.186s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.106 I build: 4671 (4d3465c5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.249 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.077 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.017.083 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.090 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.091 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.091 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.093 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.093 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.094 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.094 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.095 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.095 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.095 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.096 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.096 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.098 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.098 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.098 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.001 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.043 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.940 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.942 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.942 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.943 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.943 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.943 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.944 I llama_model_loader: - type  f32:  194 tensors
0.00.025.944 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.944 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.945 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.945 I print_info: file format = GGUF V3 (latest)
0.00.025.946 I print_info: file type   = Q2_K - Medium
0.00.025.947 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.034.468 I load: special tokens cache size = 25
0.00.042.439 I load: token to piece cache size = 0.2984 MB
0.00.042.444 I print_info: arch             = gptneox
0.00.042.444 I print_info: vocab_only       = 0
0.00.042.444 I print_info: n_ctx_train      = 2048
0.00.042.445 I print_info: n_embd           = 2048
0.00.042.445 I print_info: n_layer          = 24
0.00.042.450 I print_info: n_head           = 16
0.00.042.451 I print_info: n_head_kv        = 16
0.00.042.451 I print_info: n_rot            = 32
0.00.042.451 I print_info: n_swa            = 0
0.00.042.451 I print_info: n_embd_head_k    = 128
0.00.042.454 I print_info: n_embd_head_v    = 128
0.00.042.455 I print_info: n_gqa            = 1
0.00.042.456 I print_info: n_embd_k_gqa     = 2048
0.00.042.457 I print_info: n_embd_v_gqa     = 2048
0.00.042.458 I print_info: f_norm_eps       = 1.0e-05
0.00.042.458 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.042.459 I print_info: f_clamp_kqv      = 0.0e+00
0.00.042.459 I print_info: f_max_alibi_bias = 0.0e+00
0.00.042.459 I print_info: f_logit_scale    = 0.0e+00
0.00.042.461 I print_info: n_ff             = 8192
0.00.042.462 I print_info: n_expert         = 0
0.00.042.462 I print_info: n_expert_used    = 0
0.00.042.462 I print_info: causal attn      = 1
0.00.042.462 I print_info: pooling type     = 0
0.00.042.462 I print_info: rope type        = 2
0.00.042.463 I print_info: rope scaling     = linear
0.00.042.463 I print_info: freq_base_train  = 10000.0
0.00.042.463 I print_info: freq_scale_train = 1
0.00.042.464 I print_info: n_ctx_orig_yarn  = 2048
0.00.042.464 I print_info: rope_finetuned   = unknown
0.00.042.464 I print_info: ssm_d_conv       = 0
0.00.042.464 I print_info: ssm_d_inner      = 0
0.00.042.464 I print_info: ssm_d_state      = 0
0.00.042.464 I print_info: ssm_dt_rank      = 0
0.00.042.465 I print_info: ssm_dt_b_c_rms   = 0
0.00.042.465 I print_info: model type       = 1.4B
0.00.042.470 I print_info: model params     = 1.41 B
0.00.042.470 I print_info: general.name     = 1.4B
0.00.042.471 I print_info: vocab type       = BPE
0.00.042.471 I print_info: n_vocab          = 50304
0.00.042.471 I print_info: n_merges         = 50009
0.00.042.472 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.042.472 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.042.472 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.042.473 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.042.473 I print_info: LF token         = 187 ''
0.00.042.473 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.042.474 I print_info: max token length = 1024
0.00.042.474 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.357.698 I load_tensors: offloading 24 repeating layers to GPU
0.00.357.716 I load_tensors: offloading output layer to GPU
0.00.357.716 I load_tensors: offloaded 25/25 layers to GPU
0.00.357.750 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.357.751 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.359.106 I llama_init_from_model: n_seq_max     = 1
0.00.359.110 I llama_init_from_model: n_ctx         = 128
0.00.359.111 I llama_init_from_model: n_ctx_per_seq = 128
0.00.359.111 I llama_init_from_model: n_batch       = 128
0.00.359.112 I llama_init_from_model: n_ubatch      = 128
0.00.359.112 I llama_init_from_model: flash_attn    = 0
0.00.359.114 I llama_init_from_model: freq_base     = 10000.0
0.00.359.115 I llama_init_from_model: freq_scale    = 1
0.00.359.115 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.359.118 I ggml_metal_init: allocating
0.00.359.194 I ggml_metal_init: found device: Apple M4
0.00.359.209 I ggml_metal_init: picking default device: Apple M4
0.00.361.066 I ggml_metal_init: using embedded metal library
0.00.366.878 I ggml_metal_init: GPU name:   Apple M4
0.00.366.896 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.366.897 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.366.898 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.366.899 I ggml_metal_init: simdgroup reduction   = true
0.00.366.899 I ggml_metal_init: simdgroup matrix mul. = true
0.00.366.899 I ggml_metal_init: has residency sets    = true
0.00.366.899 I ggml_metal_init: has bfloat            = true
0.00.366.900 I ggml_metal_init: use bfloat            = true
0.00.366.905 I ggml_metal_init: hasUnifiedMemory      = true
0.00.366.911 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.388.229 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.391.990 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.391.997 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.392.062 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.395.463 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.395.465 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.395.466 I llama_init_from_model: graph nodes  = 967
0.00.395.466 I llama_init_from_model: graph splits = 2
0.00.395.470 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.395.470 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.421.712 I 
0.00.421.772 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.421.778 I perplexity: tokenizing the input ..
0.00.429.072 I perplexity: tokenization took 7.289 ms
0.00.429.078 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.562.962 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.564.273 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.564.300 I llama_perf_context_print:        load time =     411.45 ms
0.00.564.300 I llama_perf_context_print: prompt eval time =     132.86 ms /   128 tokens (    1.04 ms per token,   963.41 tokens per second)
0.00.564.301 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.564.302 I llama_perf_context_print:       total time =     142.59 ms /   129 tokens
0.00.564.732 I ggml_metal_free: deallocating

real	0m0.581s
user	0m0.086s
sys	0m0.100s
```
- q3_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.050 I build: 4671 (4d3465c5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.082 I main: llama backend init
0.00.000.084 I main: load the model and apply lora adapter, if any
0.00.008.698 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.311 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.316 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.318 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.319 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.320 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.322 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.322 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.323 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.325 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.325 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.325 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.328 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.329 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.329 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.332 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.332 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.333 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.123 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.111 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.851 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.852 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.852 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.852 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.853 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.853 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.854 I llama_model_loader: - type  f32:  194 tensors
0.00.024.854 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.854 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.854 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.855 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.855 I print_info: file format = GGUF V3 (latest)
0.00.024.856 I print_info: file type   = Q3_K - Medium
0.00.024.857 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.032.689 I load: special tokens cache size = 25
0.00.038.841 I load: token to piece cache size = 0.2984 MB
0.00.038.843 I print_info: arch             = gptneox
0.00.038.844 I print_info: vocab_only       = 0
0.00.038.844 I print_info: n_ctx_train      = 2048
0.00.038.844 I print_info: n_embd           = 2048
0.00.038.844 I print_info: n_layer          = 24
0.00.038.847 I print_info: n_head           = 16
0.00.038.847 I print_info: n_head_kv        = 16
0.00.038.848 I print_info: n_rot            = 32
0.00.038.848 I print_info: n_swa            = 0
0.00.038.848 I print_info: n_embd_head_k    = 128
0.00.038.848 I print_info: n_embd_head_v    = 128
0.00.038.849 I print_info: n_gqa            = 1
0.00.038.850 I print_info: n_embd_k_gqa     = 2048
0.00.038.851 I print_info: n_embd_v_gqa     = 2048
0.00.038.851 I print_info: f_norm_eps       = 1.0e-05
0.00.038.851 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.852 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.852 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.852 I print_info: f_logit_scale    = 0.0e+00
0.00.038.853 I print_info: n_ff             = 8192
0.00.038.853 I print_info: n_expert         = 0
0.00.038.853 I print_info: n_expert_used    = 0
0.00.038.854 I print_info: causal attn      = 1
0.00.038.856 I print_info: pooling type     = 0
0.00.038.856 I print_info: rope type        = 2
0.00.038.857 I print_info: rope scaling     = linear
0.00.038.857 I print_info: freq_base_train  = 10000.0
0.00.038.857 I print_info: freq_scale_train = 1
0.00.038.857 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.858 I print_info: rope_finetuned   = unknown
0.00.038.858 I print_info: ssm_d_conv       = 0
0.00.038.858 I print_info: ssm_d_inner      = 0
0.00.038.858 I print_info: ssm_d_state      = 0
0.00.038.858 I print_info: ssm_dt_rank      = 0
0.00.038.858 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.858 I print_info: model type       = 1.4B
0.00.038.859 I print_info: model params     = 1.41 B
0.00.038.859 I print_info: general.name     = 1.4B
0.00.038.859 I print_info: vocab type       = BPE
0.00.038.860 I print_info: n_vocab          = 50304
0.00.038.860 I print_info: n_merges         = 50009
0.00.038.860 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.861 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.862 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.862 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.862 I print_info: LF token         = 187 ''
0.00.038.862 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.864 I print_info: max token length = 1024
0.00.038.864 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.435.749 I load_tensors: offloading 24 repeating layers to GPU
0.00.435.762 I load_tensors: offloading output layer to GPU
0.00.435.763 I load_tensors: offloaded 25/25 layers to GPU
0.00.435.796 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.435.807 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.437.422 I llama_init_from_model: n_seq_max     = 1
0.00.437.425 I llama_init_from_model: n_ctx         = 2048
0.00.437.426 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.437.426 I llama_init_from_model: n_batch       = 2048
0.00.437.427 I llama_init_from_model: n_ubatch      = 512
0.00.437.427 I llama_init_from_model: flash_attn    = 0
0.00.437.429 I llama_init_from_model: freq_base     = 10000.0
0.00.437.430 I llama_init_from_model: freq_scale    = 1
0.00.437.432 I ggml_metal_init: allocating
0.00.437.504 I ggml_metal_init: found device: Apple M4
0.00.437.518 I ggml_metal_init: picking default device: Apple M4
0.00.439.416 I ggml_metal_init: using embedded metal library
0.00.445.591 I ggml_metal_init: GPU name:   Apple M4
0.00.445.596 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.445.597 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.445.597 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.445.598 I ggml_metal_init: simdgroup reduction   = true
0.00.445.598 I ggml_metal_init: simdgroup matrix mul. = true
0.00.445.599 I ggml_metal_init: has residency sets    = true
0.00.445.599 I ggml_metal_init: has bfloat            = true
0.00.445.599 I ggml_metal_init: use bfloat            = true
0.00.445.601 I ggml_metal_init: hasUnifiedMemory      = true
0.00.445.605 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.464.544 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.519.682 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.519.689 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.519.711 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.524.105 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.524.107 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.524.107 I llama_init_from_model: graph nodes  = 967
0.00.524.107 I llama_init_from_model: graph splits = 2
0.00.524.114 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.524.229 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.524.230 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.581.691 I main: llama threadpool init, n_threads = 4
0.00.581.730 I 
0.00.581.749 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.581.749 I 
0.00.581.864 I sampler seed: 1234
0.00.581.868 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.581.886 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.581.887 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.581.887 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.333.351 I llama_perf_sampler_print:    sampling time =       1.40 ms /    71 runs   (    0.02 ms per token, 50823.19 tokens per second)
0.01.333.353 I llama_perf_context_print:        load time =     572.28 ms
0.01.333.354 I llama_perf_context_print: prompt eval time =      50.02 ms /     7 tokens (    7.15 ms per token,   139.94 tokens per second)
0.01.333.354 I llama_perf_context_print:        eval time =     698.44 ms /    63 runs   (   11.09 ms per token,    90.20 tokens per second)
0.01.333.355 I llama_perf_context_print:       total time =     752.37 ms /    70 tokens
0.01.333.626 I ggml_metal_free: deallocating

real	0m1.352s
user	0m0.109s
sys	0m0.177s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.106 I build: 4671 (4d3465c5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.149 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.437 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.443 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.445 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.446 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.446 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.446 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.447 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.448 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.448 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.449 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.449 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.449 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.450 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.450 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.452 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.452 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.453 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.372 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.400 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.367 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.369 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.369 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.369 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.370 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.370 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.371 I llama_model_loader: - type  f32:  194 tensors
0.00.025.371 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.371 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.372 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.372 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.372 I print_info: file format = GGUF V3 (latest)
0.00.025.373 I print_info: file type   = Q3_K - Medium
0.00.025.374 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.033.472 I load: special tokens cache size = 25
0.00.039.712 I load: token to piece cache size = 0.2984 MB
0.00.039.717 I print_info: arch             = gptneox
0.00.039.717 I print_info: vocab_only       = 0
0.00.039.717 I print_info: n_ctx_train      = 2048
0.00.039.717 I print_info: n_embd           = 2048
0.00.039.717 I print_info: n_layer          = 24
0.00.039.722 I print_info: n_head           = 16
0.00.039.723 I print_info: n_head_kv        = 16
0.00.039.723 I print_info: n_rot            = 32
0.00.039.723 I print_info: n_swa            = 0
0.00.039.723 I print_info: n_embd_head_k    = 128
0.00.039.723 I print_info: n_embd_head_v    = 128
0.00.039.724 I print_info: n_gqa            = 1
0.00.039.725 I print_info: n_embd_k_gqa     = 2048
0.00.039.726 I print_info: n_embd_v_gqa     = 2048
0.00.039.726 I print_info: f_norm_eps       = 1.0e-05
0.00.039.727 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.727 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.727 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.727 I print_info: f_logit_scale    = 0.0e+00
0.00.039.728 I print_info: n_ff             = 8192
0.00.039.728 I print_info: n_expert         = 0
0.00.039.728 I print_info: n_expert_used    = 0
0.00.039.728 I print_info: causal attn      = 1
0.00.039.728 I print_info: pooling type     = 0
0.00.039.729 I print_info: rope type        = 2
0.00.039.729 I print_info: rope scaling     = linear
0.00.039.729 I print_info: freq_base_train  = 10000.0
0.00.039.730 I print_info: freq_scale_train = 1
0.00.039.730 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.730 I print_info: rope_finetuned   = unknown
0.00.039.733 I print_info: ssm_d_conv       = 0
0.00.039.733 I print_info: ssm_d_inner      = 0
0.00.039.734 I print_info: ssm_d_state      = 0
0.00.039.734 I print_info: ssm_dt_rank      = 0
0.00.039.734 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.734 I print_info: model type       = 1.4B
0.00.039.735 I print_info: model params     = 1.41 B
0.00.039.735 I print_info: general.name     = 1.4B
0.00.039.735 I print_info: vocab type       = BPE
0.00.039.735 I print_info: n_vocab          = 50304
0.00.039.737 I print_info: n_merges         = 50009
0.00.039.737 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.737 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.737 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.737 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.738 I print_info: LF token         = 187 ''
0.00.039.738 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.738 I print_info: max token length = 1024
0.00.039.738 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.447.452 I load_tensors: offloading 24 repeating layers to GPU
0.00.447.466 I load_tensors: offloading output layer to GPU
0.00.447.468 I load_tensors: offloaded 25/25 layers to GPU
0.00.447.498 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.447.500 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.448.877 I llama_init_from_model: n_seq_max     = 1
0.00.448.882 I llama_init_from_model: n_ctx         = 128
0.00.448.883 I llama_init_from_model: n_ctx_per_seq = 128
0.00.448.883 I llama_init_from_model: n_batch       = 128
0.00.448.883 I llama_init_from_model: n_ubatch      = 128
0.00.448.884 I llama_init_from_model: flash_attn    = 0
0.00.448.886 I llama_init_from_model: freq_base     = 10000.0
0.00.448.886 I llama_init_from_model: freq_scale    = 1
0.00.448.887 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.448.889 I ggml_metal_init: allocating
0.00.448.972 I ggml_metal_init: found device: Apple M4
0.00.448.985 I ggml_metal_init: picking default device: Apple M4
0.00.450.885 I ggml_metal_init: using embedded metal library
0.00.456.809 I ggml_metal_init: GPU name:   Apple M4
0.00.456.828 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.456.829 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.456.830 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.456.830 I ggml_metal_init: simdgroup reduction   = true
0.00.456.831 I ggml_metal_init: simdgroup matrix mul. = true
0.00.456.831 I ggml_metal_init: has residency sets    = true
0.00.456.831 I ggml_metal_init: has bfloat            = true
0.00.456.832 I ggml_metal_init: use bfloat            = true
0.00.456.838 I ggml_metal_init: hasUnifiedMemory      = true
0.00.456.845 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.477.850 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.481.581 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.481.588 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.481.658 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.485.027 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.485.028 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.485.029 I llama_init_from_model: graph nodes  = 967
0.00.485.029 I llama_init_from_model: graph splits = 2
0.00.485.034 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.485.034 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.515.213 I 
0.00.515.271 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.515.277 I perplexity: tokenizing the input ..
0.00.521.938 I perplexity: tokenization took 6.658 ms
0.00.521.944 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.662.815 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.664.200 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.664.226 I llama_perf_context_print:        load time =     506.06 ms
0.00.664.227 I llama_perf_context_print: prompt eval time =     140.26 ms /   128 tokens (    1.10 ms per token,   912.56 tokens per second)
0.00.664.228 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.664.228 I llama_perf_context_print:       total time =     149.02 ms /   129 tokens
0.00.664.600 I ggml_metal_free: deallocating

real	0m0.678s
user	0m0.082s
sys	0m0.112s
```
- q4_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.050 I build: 4671 (4d3465c5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.082 I main: llama backend init
0.00.000.084 I main: load the model and apply lora adapter, if any
0.00.009.317 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.758 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.763 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.764 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.765 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.765 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.765 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.769 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.770 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.771 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.771 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.772 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.772 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.772 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.773 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.776 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.777 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.777 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.599 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.606 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.400 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.401 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.401 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.402 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.402 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.402 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.403 I llama_model_loader: - type  f32:  194 tensors
0.00.025.403 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.403 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.404 I llama_model_loader: - type q6_K:   13 tensors
0.00.025.404 I print_info: file format = GGUF V3 (latest)
0.00.025.405 I print_info: file type   = Q4_K - Medium
0.00.025.405 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.033.186 I load: special tokens cache size = 25
0.00.039.404 I load: token to piece cache size = 0.2984 MB
0.00.039.407 I print_info: arch             = gptneox
0.00.039.407 I print_info: vocab_only       = 0
0.00.039.407 I print_info: n_ctx_train      = 2048
0.00.039.407 I print_info: n_embd           = 2048
0.00.039.408 I print_info: n_layer          = 24
0.00.039.410 I print_info: n_head           = 16
0.00.039.411 I print_info: n_head_kv        = 16
0.00.039.411 I print_info: n_rot            = 32
0.00.039.411 I print_info: n_swa            = 0
0.00.039.414 I print_info: n_embd_head_k    = 128
0.00.039.414 I print_info: n_embd_head_v    = 128
0.00.039.414 I print_info: n_gqa            = 1
0.00.039.415 I print_info: n_embd_k_gqa     = 2048
0.00.039.416 I print_info: n_embd_v_gqa     = 2048
0.00.039.417 I print_info: f_norm_eps       = 1.0e-05
0.00.039.417 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.417 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.417 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.418 I print_info: f_logit_scale    = 0.0e+00
0.00.039.418 I print_info: n_ff             = 8192
0.00.039.418 I print_info: n_expert         = 0
0.00.039.418 I print_info: n_expert_used    = 0
0.00.039.419 I print_info: causal attn      = 1
0.00.039.420 I print_info: pooling type     = 0
0.00.039.422 I print_info: rope type        = 2
0.00.039.422 I print_info: rope scaling     = linear
0.00.039.423 I print_info: freq_base_train  = 10000.0
0.00.039.423 I print_info: freq_scale_train = 1
0.00.039.423 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.423 I print_info: rope_finetuned   = unknown
0.00.039.423 I print_info: ssm_d_conv       = 0
0.00.039.424 I print_info: ssm_d_inner      = 0
0.00.039.424 I print_info: ssm_d_state      = 0
0.00.039.424 I print_info: ssm_dt_rank      = 0
0.00.039.424 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.424 I print_info: model type       = 1.4B
0.00.039.425 I print_info: model params     = 1.41 B
0.00.039.425 I print_info: general.name     = 1.4B
0.00.039.425 I print_info: vocab type       = BPE
0.00.039.426 I print_info: n_vocab          = 50304
0.00.039.426 I print_info: n_merges         = 50009
0.00.039.428 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.428 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.428 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.428 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.429 I print_info: LF token         = 187 ''
0.00.039.429 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.429 I print_info: max token length = 1024
0.00.039.430 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.515.425 I load_tensors: offloading 24 repeating layers to GPU
0.00.515.440 I load_tensors: offloading output layer to GPU
0.00.515.441 I load_tensors: offloaded 25/25 layers to GPU
0.00.515.475 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.515.476 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.517.131 I llama_init_from_model: n_seq_max     = 1
0.00.517.134 I llama_init_from_model: n_ctx         = 2048
0.00.517.134 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.517.135 I llama_init_from_model: n_batch       = 2048
0.00.517.135 I llama_init_from_model: n_ubatch      = 512
0.00.517.136 I llama_init_from_model: flash_attn    = 0
0.00.517.138 I llama_init_from_model: freq_base     = 10000.0
0.00.517.138 I llama_init_from_model: freq_scale    = 1
0.00.517.140 I ggml_metal_init: allocating
0.00.517.214 I ggml_metal_init: found device: Apple M4
0.00.517.227 I ggml_metal_init: picking default device: Apple M4
0.00.519.106 I ggml_metal_init: using embedded metal library
0.00.525.883 I ggml_metal_init: GPU name:   Apple M4
0.00.525.888 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.525.888 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.525.889 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.525.890 I ggml_metal_init: simdgroup reduction   = true
0.00.525.890 I ggml_metal_init: simdgroup matrix mul. = true
0.00.525.890 I ggml_metal_init: has residency sets    = true
0.00.525.890 I ggml_metal_init: has bfloat            = true
0.00.525.891 I ggml_metal_init: use bfloat            = true
0.00.525.891 I ggml_metal_init: hasUnifiedMemory      = true
0.00.525.893 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.543.117 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.595.137 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.595.145 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.595.177 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.599.953 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.599.955 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.599.955 I llama_init_from_model: graph nodes  = 967
0.00.599.955 I llama_init_from_model: graph splits = 2
0.00.599.960 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.600.082 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.600.083 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.655.217 I main: llama threadpool init, n_threads = 4
0.00.655.262 I 
0.00.655.287 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.655.287 I 
0.00.655.451 I sampler seed: 1234
0.00.655.456 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.655.504 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.655.508 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.655.508 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.418.339 I llama_perf_sampler_print:    sampling time =       1.45 ms /    71 runs   (    0.02 ms per token, 48931.77 tokens per second)
0.01.418.340 I llama_perf_context_print:        load time =     645.19 ms
0.01.418.340 I llama_perf_context_print: prompt eval time =      47.07 ms /     7 tokens (    6.72 ms per token,   148.71 tokens per second)
0.01.418.341 I llama_perf_context_print:        eval time =     712.89 ms /    63 runs   (   11.32 ms per token,    88.37 tokens per second)
0.01.418.341 I llama_perf_context_print:       total time =     763.83 ms /    70 tokens
0.01.418.589 I ggml_metal_free: deallocating

real	0m1.436s
user	0m0.109s
sys	0m0.186s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.098 I build: 4671 (4d3465c5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.096 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.069 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.018.076 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.077 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.078 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.078 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.078 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.079 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.080 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.080 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.081 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.081 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.081 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.082 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.082 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.084 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.085 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.085 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.939 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.972 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.861 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.862 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.863 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.863 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.864 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.864 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.026.865 I llama_model_loader: - type  f32:  194 tensors
0.00.026.865 I llama_model_loader: - type q4_K:   61 tensors
0.00.026.865 I llama_model_loader: - type q5_K:   24 tensors
0.00.026.866 I llama_model_loader: - type q6_K:   13 tensors
0.00.026.866 I print_info: file format = GGUF V3 (latest)
0.00.026.867 I print_info: file type   = Q4_K - Medium
0.00.026.868 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.034.669 I load: special tokens cache size = 25
0.00.040.708 I load: token to piece cache size = 0.2984 MB
0.00.040.712 I print_info: arch             = gptneox
0.00.040.712 I print_info: vocab_only       = 0
0.00.040.712 I print_info: n_ctx_train      = 2048
0.00.040.712 I print_info: n_embd           = 2048
0.00.040.712 I print_info: n_layer          = 24
0.00.040.717 I print_info: n_head           = 16
0.00.040.718 I print_info: n_head_kv        = 16
0.00.040.718 I print_info: n_rot            = 32
0.00.040.721 I print_info: n_swa            = 0
0.00.040.721 I print_info: n_embd_head_k    = 128
0.00.040.721 I print_info: n_embd_head_v    = 128
0.00.040.722 I print_info: n_gqa            = 1
0.00.040.723 I print_info: n_embd_k_gqa     = 2048
0.00.040.723 I print_info: n_embd_v_gqa     = 2048
0.00.040.724 I print_info: f_norm_eps       = 1.0e-05
0.00.040.724 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.725 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.725 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.725 I print_info: f_logit_scale    = 0.0e+00
0.00.040.726 I print_info: n_ff             = 8192
0.00.040.726 I print_info: n_expert         = 0
0.00.040.726 I print_info: n_expert_used    = 0
0.00.040.726 I print_info: causal attn      = 1
0.00.040.726 I print_info: pooling type     = 0
0.00.040.726 I print_info: rope type        = 2
0.00.040.727 I print_info: rope scaling     = linear
0.00.040.727 I print_info: freq_base_train  = 10000.0
0.00.040.727 I print_info: freq_scale_train = 1
0.00.040.727 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.728 I print_info: rope_finetuned   = unknown
0.00.040.728 I print_info: ssm_d_conv       = 0
0.00.040.728 I print_info: ssm_d_inner      = 0
0.00.040.728 I print_info: ssm_d_state      = 0
0.00.040.728 I print_info: ssm_dt_rank      = 0
0.00.040.728 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.729 I print_info: model type       = 1.4B
0.00.040.729 I print_info: model params     = 1.41 B
0.00.040.729 I print_info: general.name     = 1.4B
0.00.040.729 I print_info: vocab type       = BPE
0.00.040.730 I print_info: n_vocab          = 50304
0.00.040.730 I print_info: n_merges         = 50009
0.00.040.730 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.730 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.730 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.731 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.731 I print_info: LF token         = 187 ''
0.00.040.731 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.731 I print_info: max token length = 1024
0.00.040.732 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.531.202 I load_tensors: offloading 24 repeating layers to GPU
0.00.531.223 I load_tensors: offloading output layer to GPU
0.00.531.224 I load_tensors: offloaded 25/25 layers to GPU
0.00.531.269 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.531.270 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.532.562 I llama_init_from_model: n_seq_max     = 1
0.00.532.566 I llama_init_from_model: n_ctx         = 128
0.00.532.566 I llama_init_from_model: n_ctx_per_seq = 128
0.00.532.567 I llama_init_from_model: n_batch       = 128
0.00.532.567 I llama_init_from_model: n_ubatch      = 128
0.00.532.567 I llama_init_from_model: flash_attn    = 0
0.00.532.569 I llama_init_from_model: freq_base     = 10000.0
0.00.532.570 I llama_init_from_model: freq_scale    = 1
0.00.532.571 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.532.573 I ggml_metal_init: allocating
0.00.532.648 I ggml_metal_init: found device: Apple M4
0.00.532.662 I ggml_metal_init: picking default device: Apple M4
0.00.534.527 I ggml_metal_init: using embedded metal library
0.00.540.562 I ggml_metal_init: GPU name:   Apple M4
0.00.540.567 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.540.568 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.540.569 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.540.570 I ggml_metal_init: simdgroup reduction   = true
0.00.540.570 I ggml_metal_init: simdgroup matrix mul. = true
0.00.540.570 I ggml_metal_init: has residency sets    = true
0.00.540.571 I ggml_metal_init: has bfloat            = true
0.00.540.571 I ggml_metal_init: use bfloat            = true
0.00.540.572 I ggml_metal_init: hasUnifiedMemory      = true
0.00.540.577 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.559.524 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.563.365 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.563.369 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.563.397 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.566.535 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.566.537 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.566.537 I llama_init_from_model: graph nodes  = 967
0.00.566.537 I llama_init_from_model: graph splits = 2
0.00.566.541 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.566.541 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.592.251 I 
0.00.592.312 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.592.324 I perplexity: tokenizing the input ..
0.00.598.830 I perplexity: tokenization took 6.51 ms
0.00.598.835 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.732.587 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.733.931 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.733.951 I llama_perf_context_print:        load time =     581.15 ms
0.00.733.952 I llama_perf_context_print: prompt eval time =     133.49 ms /   128 tokens (    1.04 ms per token,   958.84 tokens per second)
0.00.733.953 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.733.953 I llama_perf_context_print:       total time =     141.70 ms /   129 tokens
0.00.734.351 I ggml_metal_free: deallocating

real	0m0.750s
user	0m0.079s
sys	0m0.130s
```
- q5_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.055 I build: 4671 (4d3465c5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.087 I main: llama backend init
0.00.000.089 I main: load the model and apply lora adapter, if any
0.00.008.743 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.869 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.875 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.876 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.877 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.877 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.877 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.877 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.878 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.878 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.879 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.879 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.879 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.880 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.880 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.881 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.882 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.882 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.897 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.925 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.780 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.781 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.782 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.782 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.782 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.782 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.783 I llama_model_loader: - type  f32:  194 tensors
0.00.025.783 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.783 I llama_model_loader: - type q6_K:   37 tensors
0.00.025.784 I print_info: file format = GGUF V3 (latest)
0.00.025.784 I print_info: file type   = Q5_K - Medium
0.00.025.785 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.034.045 I load: special tokens cache size = 25
0.00.040.171 I load: token to piece cache size = 0.2984 MB
0.00.040.174 I print_info: arch             = gptneox
0.00.040.174 I print_info: vocab_only       = 0
0.00.040.175 I print_info: n_ctx_train      = 2048
0.00.040.175 I print_info: n_embd           = 2048
0.00.040.175 I print_info: n_layer          = 24
0.00.040.179 I print_info: n_head           = 16
0.00.040.180 I print_info: n_head_kv        = 16
0.00.040.180 I print_info: n_rot            = 32
0.00.040.180 I print_info: n_swa            = 0
0.00.040.181 I print_info: n_embd_head_k    = 128
0.00.040.181 I print_info: n_embd_head_v    = 128
0.00.040.181 I print_info: n_gqa            = 1
0.00.040.182 I print_info: n_embd_k_gqa     = 2048
0.00.040.185 I print_info: n_embd_v_gqa     = 2048
0.00.040.186 I print_info: f_norm_eps       = 1.0e-05
0.00.040.186 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.186 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.186 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.187 I print_info: f_logit_scale    = 0.0e+00
0.00.040.187 I print_info: n_ff             = 8192
0.00.040.189 I print_info: n_expert         = 0
0.00.040.189 I print_info: n_expert_used    = 0
0.00.040.189 I print_info: causal attn      = 1
0.00.040.189 I print_info: pooling type     = 0
0.00.040.189 I print_info: rope type        = 2
0.00.040.189 I print_info: rope scaling     = linear
0.00.040.190 I print_info: freq_base_train  = 10000.0
0.00.040.190 I print_info: freq_scale_train = 1
0.00.040.190 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.190 I print_info: rope_finetuned   = unknown
0.00.040.190 I print_info: ssm_d_conv       = 0
0.00.040.191 I print_info: ssm_d_inner      = 0
0.00.040.191 I print_info: ssm_d_state      = 0
0.00.040.191 I print_info: ssm_dt_rank      = 0
0.00.040.191 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.192 I print_info: model type       = 1.4B
0.00.040.192 I print_info: model params     = 1.41 B
0.00.040.192 I print_info: general.name     = 1.4B
0.00.040.193 I print_info: vocab type       = BPE
0.00.040.193 I print_info: n_vocab          = 50304
0.00.040.193 I print_info: n_merges         = 50009
0.00.040.194 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.194 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.194 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.194 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.194 I print_info: LF token         = 187 ''
0.00.040.195 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.195 I print_info: max token length = 1024
0.00.040.195 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.573.542 I load_tensors: offloading 24 repeating layers to GPU
0.00.573.548 I load_tensors: offloading output layer to GPU
0.00.573.549 I load_tensors: offloaded 25/25 layers to GPU
0.00.573.568 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.573.569 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.574.503 I llama_init_from_model: n_seq_max     = 1
0.00.574.507 I llama_init_from_model: n_ctx         = 2048
0.00.574.507 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.574.507 I llama_init_from_model: n_batch       = 2048
0.00.574.508 I llama_init_from_model: n_ubatch      = 512
0.00.574.508 I llama_init_from_model: flash_attn    = 0
0.00.574.509 I llama_init_from_model: freq_base     = 10000.0
0.00.574.510 I llama_init_from_model: freq_scale    = 1
0.00.574.511 I ggml_metal_init: allocating
0.00.574.575 I ggml_metal_init: found device: Apple M4
0.00.574.588 I ggml_metal_init: picking default device: Apple M4
0.00.575.683 I ggml_metal_init: using embedded metal library
0.00.579.826 I ggml_metal_init: GPU name:   Apple M4
0.00.579.832 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.579.833 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.579.834 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.579.834 I ggml_metal_init: simdgroup reduction   = true
0.00.579.834 I ggml_metal_init: simdgroup matrix mul. = true
0.00.579.835 I ggml_metal_init: has residency sets    = true
0.00.579.835 I ggml_metal_init: has bfloat            = true
0.00.579.835 I ggml_metal_init: use bfloat            = true
0.00.579.836 I ggml_metal_init: hasUnifiedMemory      = true
0.00.579.839 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.593.481 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.623.912 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.623.918 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.623.940 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.628.370 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.628.372 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.628.372 I llama_init_from_model: graph nodes  = 967
0.00.628.372 I llama_init_from_model: graph splits = 2
0.00.628.377 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.628.506 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.628.507 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.680.704 I main: llama threadpool init, n_threads = 4
0.00.680.748 I 
0.00.680.767 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.680.769 I 
0.00.680.902 I sampler seed: 1234
0.00.680.906 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.680.917 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.680.918 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.680.918 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.529.004 I llama_perf_sampler_print:    sampling time =       1.37 ms /    71 runs   (    0.02 ms per token, 51673.94 tokens per second)
0.01.529.004 I llama_perf_context_print:        load time =     671.26 ms
0.01.529.006 I llama_perf_context_print: prompt eval time =      51.25 ms /     7 tokens (    7.32 ms per token,   136.59 tokens per second)
0.01.529.007 I llama_perf_context_print:        eval time =     794.46 ms /    63 runs   (   12.61 ms per token,    79.30 tokens per second)
0.01.529.007 I llama_perf_context_print:       total time =     849.00 ms /    70 tokens
0.01.529.263 I ggml_metal_free: deallocating

real	0m1.549s
user	0m0.106s
sys	0m0.149s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.093 I build: 4671 (4d3465c5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.346 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.245 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.251 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.253 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.254 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.254 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.254 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.255 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.256 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.256 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.256 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.257 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.257 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.257 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.258 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.260 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.260 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.260 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.221 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.272 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.261 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.262 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.263 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.263 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.264 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.264 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.265 I llama_model_loader: - type  f32:  194 tensors
0.00.025.265 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.265 I llama_model_loader: - type q6_K:   37 tensors
0.00.025.266 I print_info: file format = GGUF V3 (latest)
0.00.025.267 I print_info: file type   = Q5_K - Medium
0.00.025.268 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.033.647 I load: special tokens cache size = 25
0.00.039.855 I load: token to piece cache size = 0.2984 MB
0.00.039.860 I print_info: arch             = gptneox
0.00.039.860 I print_info: vocab_only       = 0
0.00.039.860 I print_info: n_ctx_train      = 2048
0.00.039.861 I print_info: n_embd           = 2048
0.00.039.861 I print_info: n_layer          = 24
0.00.039.866 I print_info: n_head           = 16
0.00.039.866 I print_info: n_head_kv        = 16
0.00.039.867 I print_info: n_rot            = 32
0.00.039.870 I print_info: n_swa            = 0
0.00.039.870 I print_info: n_embd_head_k    = 128
0.00.039.870 I print_info: n_embd_head_v    = 128
0.00.039.871 I print_info: n_gqa            = 1
0.00.039.872 I print_info: n_embd_k_gqa     = 2048
0.00.039.872 I print_info: n_embd_v_gqa     = 2048
0.00.039.873 I print_info: f_norm_eps       = 1.0e-05
0.00.039.873 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.874 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.874 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.874 I print_info: f_logit_scale    = 0.0e+00
0.00.039.875 I print_info: n_ff             = 8192
0.00.039.876 I print_info: n_expert         = 0
0.00.039.876 I print_info: n_expert_used    = 0
0.00.039.876 I print_info: causal attn      = 1
0.00.039.877 I print_info: pooling type     = 0
0.00.039.877 I print_info: rope type        = 2
0.00.039.877 I print_info: rope scaling     = linear
0.00.039.878 I print_info: freq_base_train  = 10000.0
0.00.039.878 I print_info: freq_scale_train = 1
0.00.039.878 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.879 I print_info: rope_finetuned   = unknown
0.00.039.880 I print_info: ssm_d_conv       = 0
0.00.039.880 I print_info: ssm_d_inner      = 0
0.00.039.880 I print_info: ssm_d_state      = 0
0.00.039.880 I print_info: ssm_dt_rank      = 0
0.00.039.880 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.881 I print_info: model type       = 1.4B
0.00.039.881 I print_info: model params     = 1.41 B
0.00.039.881 I print_info: general.name     = 1.4B
0.00.039.881 I print_info: vocab type       = BPE
0.00.039.882 I print_info: n_vocab          = 50304
0.00.039.882 I print_info: n_merges         = 50009
0.00.039.882 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.882 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.882 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.882 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.883 I print_info: LF token         = 187 ''
0.00.039.883 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.883 I print_info: max token length = 1024
0.00.039.884 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.621.374 I load_tensors: offloading 24 repeating layers to GPU
0.00.621.395 I load_tensors: offloading output layer to GPU
0.00.621.396 I load_tensors: offloaded 25/25 layers to GPU
0.00.621.454 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.621.456 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.622.821 I llama_init_from_model: n_seq_max     = 1
0.00.622.824 I llama_init_from_model: n_ctx         = 128
0.00.622.825 I llama_init_from_model: n_ctx_per_seq = 128
0.00.622.825 I llama_init_from_model: n_batch       = 128
0.00.622.826 I llama_init_from_model: n_ubatch      = 128
0.00.622.826 I llama_init_from_model: flash_attn    = 0
0.00.622.829 I llama_init_from_model: freq_base     = 10000.0
0.00.622.829 I llama_init_from_model: freq_scale    = 1
0.00.622.830 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.622.836 I ggml_metal_init: allocating
0.00.622.925 I ggml_metal_init: found device: Apple M4
0.00.622.940 I ggml_metal_init: picking default device: Apple M4
0.00.624.830 I ggml_metal_init: using embedded metal library
0.00.631.579 I ggml_metal_init: GPU name:   Apple M4
0.00.631.583 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.631.584 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.631.585 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.631.586 I ggml_metal_init: simdgroup reduction   = true
0.00.631.586 I ggml_metal_init: simdgroup matrix mul. = true
0.00.631.587 I ggml_metal_init: has residency sets    = true
0.00.631.587 I ggml_metal_init: has bfloat            = true
0.00.631.587 I ggml_metal_init: use bfloat            = true
0.00.631.588 I ggml_metal_init: hasUnifiedMemory      = true
0.00.631.590 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.649.315 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.652.943 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.652.947 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.652.974 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.656.076 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.656.078 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.656.078 I llama_init_from_model: graph nodes  = 967
0.00.656.079 I llama_init_from_model: graph splits = 2
0.00.656.082 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.656.084 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.691.126 I 
0.00.691.191 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.691.198 I perplexity: tokenizing the input ..
0.00.697.395 I perplexity: tokenization took 6.196 ms
0.00.697.399 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.843.413 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.844.760 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.844.780 I llama_perf_context_print:        load time =     681.77 ms
0.00.844.781 I llama_perf_context_print: prompt eval time =     145.76 ms /   128 tokens (    1.14 ms per token,   878.17 tokens per second)
0.00.844.781 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.844.782 I llama_perf_context_print:       total time =     153.66 ms /   129 tokens
0.00.845.154 I ggml_metal_free: deallocating

real	0m0.858s
user	0m0.079s
sys	0m0.155s
```
- q6_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.059 I build: 4671 (4d3465c5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.096 I main: llama backend init
0.00.000.098 I main: load the model and apply lora adapter, if any
0.00.019.154 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.036.859 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.036.864 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.036.865 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.036.866 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.036.866 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.036.866 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.036.867 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.036.867 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.036.868 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.036.868 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.036.868 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.036.870 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.036.871 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.036.871 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.036.873 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.036.873 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.036.874 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.040.779 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.041.789 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.045.775 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.045.777 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.045.778 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.045.778 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.045.778 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.045.779 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.045.779 I llama_model_loader: - type  f32:  194 tensors
0.00.045.780 I llama_model_loader: - type q6_K:   98 tensors
0.00.045.780 I print_info: file format = GGUF V3 (latest)
0.00.045.781 I print_info: file type   = Q6_K
0.00.045.782 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.053.828 I load: special tokens cache size = 25
0.00.060.428 I load: token to piece cache size = 0.2984 MB
0.00.060.432 I print_info: arch             = gptneox
0.00.060.432 I print_info: vocab_only       = 0
0.00.060.432 I print_info: n_ctx_train      = 2048
0.00.060.432 I print_info: n_embd           = 2048
0.00.060.433 I print_info: n_layer          = 24
0.00.060.436 I print_info: n_head           = 16
0.00.060.437 I print_info: n_head_kv        = 16
0.00.060.437 I print_info: n_rot            = 32
0.00.060.437 I print_info: n_swa            = 0
0.00.060.438 I print_info: n_embd_head_k    = 128
0.00.060.441 I print_info: n_embd_head_v    = 128
0.00.060.442 I print_info: n_gqa            = 1
0.00.060.442 I print_info: n_embd_k_gqa     = 2048
0.00.060.443 I print_info: n_embd_v_gqa     = 2048
0.00.060.443 I print_info: f_norm_eps       = 1.0e-05
0.00.060.444 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.060.444 I print_info: f_clamp_kqv      = 0.0e+00
0.00.060.444 I print_info: f_max_alibi_bias = 0.0e+00
0.00.060.444 I print_info: f_logit_scale    = 0.0e+00
0.00.060.449 I print_info: n_ff             = 8192
0.00.060.449 I print_info: n_expert         = 0
0.00.060.449 I print_info: n_expert_used    = 0
0.00.060.449 I print_info: causal attn      = 1
0.00.060.449 I print_info: pooling type     = 0
0.00.060.449 I print_info: rope type        = 2
0.00.060.450 I print_info: rope scaling     = linear
0.00.060.450 I print_info: freq_base_train  = 10000.0
0.00.060.450 I print_info: freq_scale_train = 1
0.00.060.450 I print_info: n_ctx_orig_yarn  = 2048
0.00.060.451 I print_info: rope_finetuned   = unknown
0.00.060.453 I print_info: ssm_d_conv       = 0
0.00.060.453 I print_info: ssm_d_inner      = 0
0.00.060.453 I print_info: ssm_d_state      = 0
0.00.060.454 I print_info: ssm_dt_rank      = 0
0.00.060.454 I print_info: ssm_dt_b_c_rms   = 0
0.00.060.454 I print_info: model type       = 1.4B
0.00.060.454 I print_info: model params     = 1.41 B
0.00.060.454 I print_info: general.name     = 1.4B
0.00.060.455 I print_info: vocab type       = BPE
0.00.060.455 I print_info: n_vocab          = 50304
0.00.060.455 I print_info: n_merges         = 50009
0.00.060.455 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.060.455 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.060.456 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.060.456 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.060.456 I print_info: LF token         = 187 ''
0.00.060.456 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.060.456 I print_info: max token length = 1024
0.00.060.457 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.706.118 I load_tensors: offloading 24 repeating layers to GPU
0.00.706.123 I load_tensors: offloading output layer to GPU
0.00.706.124 I load_tensors: offloaded 25/25 layers to GPU
0.00.706.147 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.706.149 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.707.322 I llama_init_from_model: n_seq_max     = 1
0.00.707.324 I llama_init_from_model: n_ctx         = 2048
0.00.707.325 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.707.325 I llama_init_from_model: n_batch       = 2048
0.00.707.326 I llama_init_from_model: n_ubatch      = 512
0.00.707.326 I llama_init_from_model: flash_attn    = 0
0.00.707.327 I llama_init_from_model: freq_base     = 10000.0
0.00.707.328 I llama_init_from_model: freq_scale    = 1
0.00.707.329 I ggml_metal_init: allocating
0.00.707.380 I ggml_metal_init: found device: Apple M4
0.00.707.392 I ggml_metal_init: picking default device: Apple M4
0.00.708.841 I ggml_metal_init: using embedded metal library
0.00.714.759 I ggml_metal_init: GPU name:   Apple M4
0.00.714.763 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.714.764 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.714.765 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.714.766 I ggml_metal_init: simdgroup reduction   = true
0.00.714.766 I ggml_metal_init: simdgroup matrix mul. = true
0.00.714.766 I ggml_metal_init: has residency sets    = true
0.00.714.766 I ggml_metal_init: has bfloat            = true
0.00.714.767 I ggml_metal_init: use bfloat            = true
0.00.714.768 I ggml_metal_init: hasUnifiedMemory      = true
0.00.714.770 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.731.901 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.787.109 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.787.121 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.787.150 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.791.413 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.791.415 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.791.415 I llama_init_from_model: graph nodes  = 967
0.00.791.415 I llama_init_from_model: graph splits = 2
0.00.791.420 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.791.550 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.791.550 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.855.379 I main: llama threadpool init, n_threads = 4
0.00.855.417 I 
0.00.855.439 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.855.440 I 
0.00.855.618 I sampler seed: 1234
0.00.855.622 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.855.639 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.855.639 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.855.639 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.

He added: Ive

0.01.737.962 I llama_perf_sampler_print:    sampling time =       1.34 ms /    71 runs   (    0.02 ms per token, 52788.10 tokens per second)
0.01.737.963 I llama_perf_context_print:        load time =     835.53 ms
0.01.737.964 I llama_perf_context_print: prompt eval time =      54.30 ms /     7 tokens (    7.76 ms per token,   128.92 tokens per second)
0.01.737.966 I llama_perf_context_print:        eval time =     825.04 ms /    63 runs   (   13.10 ms per token,    76.36 tokens per second)
0.01.737.968 I llama_perf_context_print:       total time =     883.27 ms /    70 tokens
0.01.738.250 I ggml_metal_free: deallocating

real	0m1.760s
user	0m0.109s
sys	0m0.212s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.105 I build: 4671 (4d3465c5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.128 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.151 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.017.156 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.159 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.160 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.160 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.160 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.161 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.162 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.162 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.162 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.163 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.163 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.164 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.164 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.166 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.166 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.166 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.018 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.030 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.808 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.809 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.809 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.809 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.810 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.810 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.811 I llama_model_loader: - type  f32:  194 tensors
0.00.025.811 I llama_model_loader: - type q6_K:   98 tensors
0.00.025.812 I print_info: file format = GGUF V3 (latest)
0.00.025.812 I print_info: file type   = Q6_K
0.00.025.813 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.034.011 I load: special tokens cache size = 25
0.00.040.180 I load: token to piece cache size = 0.2984 MB
0.00.040.182 I print_info: arch             = gptneox
0.00.040.183 I print_info: vocab_only       = 0
0.00.040.183 I print_info: n_ctx_train      = 2048
0.00.040.183 I print_info: n_embd           = 2048
0.00.040.183 I print_info: n_layer          = 24
0.00.040.187 I print_info: n_head           = 16
0.00.040.188 I print_info: n_head_kv        = 16
0.00.040.188 I print_info: n_rot            = 32
0.00.040.190 I print_info: n_swa            = 0
0.00.040.191 I print_info: n_embd_head_k    = 128
0.00.040.191 I print_info: n_embd_head_v    = 128
0.00.040.191 I print_info: n_gqa            = 1
0.00.040.192 I print_info: n_embd_k_gqa     = 2048
0.00.040.193 I print_info: n_embd_v_gqa     = 2048
0.00.040.194 I print_info: f_norm_eps       = 1.0e-05
0.00.040.194 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.194 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.194 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.195 I print_info: f_logit_scale    = 0.0e+00
0.00.040.195 I print_info: n_ff             = 8192
0.00.040.195 I print_info: n_expert         = 0
0.00.040.197 I print_info: n_expert_used    = 0
0.00.040.198 I print_info: causal attn      = 1
0.00.040.198 I print_info: pooling type     = 0
0.00.040.198 I print_info: rope type        = 2
0.00.040.198 I print_info: rope scaling     = linear
0.00.040.198 I print_info: freq_base_train  = 10000.0
0.00.040.199 I print_info: freq_scale_train = 1
0.00.040.199 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.199 I print_info: rope_finetuned   = unknown
0.00.040.199 I print_info: ssm_d_conv       = 0
0.00.040.199 I print_info: ssm_d_inner      = 0
0.00.040.199 I print_info: ssm_d_state      = 0
0.00.040.200 I print_info: ssm_dt_rank      = 0
0.00.040.200 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.200 I print_info: model type       = 1.4B
0.00.040.200 I print_info: model params     = 1.41 B
0.00.040.200 I print_info: general.name     = 1.4B
0.00.040.201 I print_info: vocab type       = BPE
0.00.040.201 I print_info: n_vocab          = 50304
0.00.040.201 I print_info: n_merges         = 50009
0.00.040.201 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.202 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.210 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.212 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.213 I print_info: LF token         = 187 ''
0.00.040.213 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.213 I print_info: max token length = 1024
0.00.040.214 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.474.523 I load_tensors: offloading 24 repeating layers to GPU
0.00.474.536 I load_tensors: offloading output layer to GPU
0.00.474.537 I load_tensors: offloaded 25/25 layers to GPU
0.00.474.570 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.474.571 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.475.928 I llama_init_from_model: n_seq_max     = 1
0.00.475.931 I llama_init_from_model: n_ctx         = 128
0.00.475.931 I llama_init_from_model: n_ctx_per_seq = 128
0.00.475.932 I llama_init_from_model: n_batch       = 128
0.00.475.932 I llama_init_from_model: n_ubatch      = 128
0.00.475.933 I llama_init_from_model: flash_attn    = 0
0.00.475.936 I llama_init_from_model: freq_base     = 10000.0
0.00.475.936 I llama_init_from_model: freq_scale    = 1
0.00.475.937 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.475.940 I ggml_metal_init: allocating
0.00.476.025 I ggml_metal_init: found device: Apple M4
0.00.476.039 I ggml_metal_init: picking default device: Apple M4
0.00.477.952 I ggml_metal_init: using embedded metal library
0.00.484.616 I ggml_metal_init: GPU name:   Apple M4
0.00.484.622 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.484.623 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.484.624 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.484.624 I ggml_metal_init: simdgroup reduction   = true
0.00.484.624 I ggml_metal_init: simdgroup matrix mul. = true
0.00.484.625 I ggml_metal_init: has residency sets    = true
0.00.484.625 I ggml_metal_init: has bfloat            = true
0.00.484.625 I ggml_metal_init: use bfloat            = true
0.00.484.626 I ggml_metal_init: hasUnifiedMemory      = true
0.00.484.629 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.501.873 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.505.468 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.505.472 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.505.499 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.508.780 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.508.782 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.508.783 I llama_init_from_model: graph nodes  = 967
0.00.508.783 I llama_init_from_model: graph splits = 2
0.00.508.787 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.508.787 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.538.455 I 
0.00.538.509 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.538.515 I perplexity: tokenizing the input ..
0.00.544.168 I perplexity: tokenization took 5.649 ms
0.00.544.175 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.684.102 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.685.432 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.685.456 I llama_perf_context_print:        load time =     528.32 ms
0.00.685.457 I llama_perf_context_print: prompt eval time =     139.24 ms /   128 tokens (    1.09 ms per token,   919.27 tokens per second)
0.00.685.457 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.685.458 I llama_perf_context_print:       total time =     147.00 ms /   129 tokens
0.00.685.894 I ggml_metal_free: deallocating

real	0m0.702s
user	0m0.078s
sys	0m0.125s
```
- save-load-state: 
```
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4671 (4d3465c5)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 ''
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x138704870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x138704e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x1387052b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x138705720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x138705b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x138706000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x138706470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x1387068e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x138706d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x1387071c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x138707630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x138707cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x1387087f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x138708fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x1387097b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x138709ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13870a5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13870ad10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13870b430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13870bc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13870c320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13870ca40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13870d160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13870da00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13870e120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13870e3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13870e6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13870eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13870f1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13870f630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13870faa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x138710030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x1387104a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x138710760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x138710bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x138711480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x138711740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x138711bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x138712020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x138712490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x138712900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x138712d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x1387131e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x138713650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x138713ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x138713f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x1387143a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x138714dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x138715090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x138715500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x138715970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x138715de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x138716250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x1387166c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x138716b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x1387171e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x138717680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x138717940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x138717db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x138718480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x138718880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x138718b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x138719040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x138719540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x138719a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x138719f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13871a440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13871a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13871ae40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13871b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13871b840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13871bd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13871c240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13871c740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13871ccf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13871d2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13871d850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13871de00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13871e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13871e960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13871ef10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13871f4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13871fa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x138720020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x1387205d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x138720b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x138721130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x1387216e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x138721c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x138722240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x1387227f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x138722da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x138723350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x138723900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x138723eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x138724460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x138724a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x1387149c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x138725170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x1387255e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x138725a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x138726000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x1387265b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x138726b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x138727110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x1387276c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x138727c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x138728220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x1387287d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x138728d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x138729330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x1387298e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x138729e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13872a440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13872a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13872ae40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13872b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13872b840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13872bd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13872c240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13872c740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13872cc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13872d140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13872d640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13872db40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13872e040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13872e540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13872ea40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13872ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13872f440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13872f940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13872fe40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x138730340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x138730840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x138730d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x138731240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x138731740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x138731c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x138732140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x138732640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x138732b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x138733040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x138733540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x138733a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x138733f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x138734440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x138734940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x138734e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x138735340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x138735840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x138735d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x138736240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x138736740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x138736c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x138737140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x138737640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x138737b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x138738040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x138738540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x138738a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x138738f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x138739440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x138739940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x138739e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13873a340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13873a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13873ad40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13873b240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13873b740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13873bc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13873c140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13873c640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13873cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13873d040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13873d540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13873da40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13873df40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13873e440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13873e940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13873ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13873f340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13873f840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13873fd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x138740240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x138740740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x138740c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x138741140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x138741640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x138741b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x138742040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x138742540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x138742a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x138742f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x138743440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x1387439f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x138743fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x138744550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x138744b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x138745110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x138745720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x138745d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x138746520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x1387469c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x138746c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x138747290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x1387478a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x138748090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x138748530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x1387489d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x138748e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x138749620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x138749b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13874a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13874a610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13874ab60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13874b0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13874b600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13874bb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13874c0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13874c5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13874cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13874d090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13874d5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13874db30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13874e080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13874e5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13874eb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13874f070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13874f5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13874fb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x138750060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x1387505b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x138750b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x138751050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x1387515a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x138751af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x138752040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x138752590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x138752ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x138753030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x138753580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x138753ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x138754020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x138754570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x138754ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x138755010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x138755560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x138755ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x138756000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x138756550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x138756aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x138756ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x138757540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x138757a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x138757fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x138758530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x138758a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x138758fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x138759520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x138759a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x138759fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13875a510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13875aa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13875afb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13875b500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13875ba50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13875bfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13875c440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13875c8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13875cd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13875d220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13875d6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13875db60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13875e000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13875e4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13875e940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13875ede0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13875f280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13875f720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13875fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x138760060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x138760500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x138760a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x138761170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x138761890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x138761fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1387626d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x138762990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x138763180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x138763440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x138763a50 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.771.276 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.771.281 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x11f104b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x11f104f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x11f105400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x11f105870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x11f105ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x11f106150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x11f1065c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x11f106a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x11f106ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x11f107310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x11f107780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x11f107e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x11f108990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x11f109140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x11f109950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x11f10a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x11f10a790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x11f10aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x11f10b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x11f10bd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x11f10c420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x11f10cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x11f10d260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x11f10d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x11f10e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x11f10e360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x11f10e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x11f10ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x11f10ef00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x11f10f370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x11f10f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x11f10fd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x11f110180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x11f110440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x11f1108b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x11f110d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x11f111190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x11f111600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x11f111a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x11f111ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x11f112350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x11f1127c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x11f112c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x11f1130a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x11f113510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x11f113980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x11f113df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x11f114260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x11f1146d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x11f114b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x11f114fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x11f115420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x11f115890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x11f115d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x11f116170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x11f1165e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x11f116b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x11f117050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x11f1174c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x11f117930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x11f117da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x11f118210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x11f118680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x11f118af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x11f118f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x11f1193d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x11f119840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x11f119cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x11f11a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x11f11a590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x11f11aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x11f11ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x11f11b2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x11f11b750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x11f11bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x11f11c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x11f11c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x11f11c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x11f11cd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x11f11d1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x11f11d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x11f11dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x11f11df40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x11f11e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x11f11e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x11f11ec90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x11f11f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x11f11f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x11f11f9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x11f11fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x11f1202c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x11f120730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x11f120ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x11f121010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x11f121480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x11f1218f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x11f121d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x11f1221d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x11f122640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x11f122ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x11f122f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x11f123390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x11f123800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x11f123c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x11f1240e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x11f124550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x11f1249c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x11f124e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x11f1252a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x11f125710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x11f125b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x11f125ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x11f126460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x11f1268d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x11f126d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x11f1271b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x11f127620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x11f127a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x11f127f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x11f128370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x11f1287e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x11f128c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x11f1290c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x11f129530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x11f1299a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x11f129e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x11f12a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x11f12a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x11f12ab60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x11f12afd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x11f12b440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x11f12b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x11f12bd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x11f12c190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x11f12c600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x11f12ca70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x11f12cee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x11f12d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x11f12d7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x11f12dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x11f12e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x11f12e510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x11f12e980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x11f12edf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x11f12f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x11f12f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x11f12fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x11f12ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x11f130420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x11f130890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x11f130d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x11f131170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x11f1315e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x11f131a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x11f131ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x11f132330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x11f1327a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x11f132c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x11f133080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x11f1334f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x11f133960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x11f133dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x11f134240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x11f1346b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x11f134b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x11f134f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x11f135bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x11f135e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x11f136140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x11f1365b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x11f136a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x11f136e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x11f137300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x11f137770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x11f137be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x11f138050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x11f1384c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x11f138930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x11f138da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x11f139210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x11f139680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x11f139af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x11f139f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x11f13a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x11f13a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x11f13acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x11f13b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x11f13b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x11f13ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x11f13be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x11f13c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x11f13c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x11f13cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x11f13d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x11f13d4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x11f13d910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x11f13dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x11f13e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x11f13e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x11f13ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x11f13ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x11f13f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x11f13f910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x11f13fe20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x11f140290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x11f140700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x11f140b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x11f140fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x11f141500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x11f141a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x11f142580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x11f142840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x11f142e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x11f1433c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x11f143980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x11f143f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x11f144500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x11f144ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x11f145080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x11f145640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x11f145c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x11f1461c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x11f146780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x11f146d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x11f147300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x11f1478c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x11f147e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x11f148440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x11f148a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x11f148fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x11f149580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x11f149b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x11f14a100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x11f14a6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x11f14ac80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x11f14b240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x11f14b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x11f14bdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x11f14c380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x11f14c940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x11f14cf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x11f14d4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x11f14da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x11f14e040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x11f14e600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x11f14ebc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x11f14f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x11f14f740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x11f14fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x11f1502c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x11f150880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x11f150e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x11f151400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x11f1519c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x11f151f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x11f152540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x11f152b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x11f1530c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x11f153680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x11f153c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x11f154200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x11f1547c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x11f154d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x11f155340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x11f155900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x11f155ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x11f156480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x11f156a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x11f156f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x11f157440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x11f157940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x11f157e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x11f158340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x11f158840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x11f158d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x11f159240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x11f159740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x11f159c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x11f15a140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x11f15a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x11f15ab40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x11f15b040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x11f15b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x11f15bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x11f15c670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x11f15cd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x11f15d4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x11f15d770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x11f15df60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x11f15e220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x11f15e830 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x1386044e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x138604950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x138604dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x138605230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x1386056a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x138605b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x138605f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x1386063f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x138606860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x138606cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x138607140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x1386077b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x1386082d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x138608a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x138609290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x1386099b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13860a0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13860a7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13860af10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13860b6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13860be00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13860c520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13860cc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13860d360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13860da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13860dd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13860e000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13860e470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13860e8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13860ed50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13860f1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13860f6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13860fb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13860fe20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x138610290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x138610700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x138610b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x138610fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x138611450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x1386118c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x138611d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x1386121a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x138612610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x138612a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x138612ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x138613360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x1386137d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x138613c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x1386140b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x138614520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x138614990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x138614e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x138615270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x1386156e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x138615b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x138615fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x138616530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x138616a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x138616ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x138617310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x138617780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x138617bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x138618060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x1386184d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x138618940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x138618db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x138619220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x138619690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x138619b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x138619f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13861a3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13861a850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13861acc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13861b130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13861b5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13861ba10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13861be80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13861c2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13861c760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13861cbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13861d040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13861d4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13861d920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13861dd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13861e200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13861e670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13861eae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13861ef50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13861f3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13861f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13861fca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x138620110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x138620580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x1386209f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x138620e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x1386212d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x138621740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x138621bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x138622020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x138622490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x138622900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x138622d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x1386231e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x138623a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x138623d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x1386241a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x138624610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x138624a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x138624ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x138625360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x1386257d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x138625c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x1386260b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x138626520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x138626990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x138626e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x138627270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x1386276e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x138627b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x138627fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x138628430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x1386288a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x138628d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x138629180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x1386295f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x138629a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x138629ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13862a340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13862a7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13862ac20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13862b090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13862b500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13862b970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13862bde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13862c250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13862c6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13862cb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13862cfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13862d410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13862d880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13862dcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13862e160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13862e5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13862ea40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13862eeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13862f320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13862f790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13862fc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x138630070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x1386304e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x138630950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x138630dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x138631230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x1386316a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x138631b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x138631f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x1386323f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x138632860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x138632cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x138633140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x1386335b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x138633a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x138633e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x138634300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x138634770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x138634be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x138635050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x1386354c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x138635930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x138635da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x138636210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x138636680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x138636af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x138636f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x1386373d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x138637840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x138637cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x138638120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x138638590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x138638a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x138638e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x1386392e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x138639750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x138639bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13863a030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13863a4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13863a910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13863ad80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13863b1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13863b660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13863bad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13863bf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13863c3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13863c820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13863cc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13863d100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13863d570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13863d9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13863de50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13863e2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13863e730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13863eba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13863f010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13863f480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13863f8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13863fd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x1386401d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x138640640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x138640ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x138640f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x138641aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x138641d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x138642020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x138642490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x138642900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x138642d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x1386431e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x138643650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x138643ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x138643f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x1386443a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x138644810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x138644c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x1386450f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x138645560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x1386459d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x138645e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x1386462b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x138646720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x138646b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x138647000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x138647470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x1386478e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x138647d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x1386481c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x138648630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x138648aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x138648f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x138649380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x1386497f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x138649c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13864a0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13864a540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13864a9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13864ae20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13864b290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13864b700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13864bb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13864bfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13864c450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13864c8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13864cd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13864d1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13864d610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13864da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13864def0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13864e360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13864e7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13864ec40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13864f0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13864f520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13864f990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13864fe00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x138650270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x1386506e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x138650b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x138650fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x138651430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x1386518a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x138651d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x138652180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x1386525f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x138652a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x138652ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x138653340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1386537b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x138653c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x138654090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x138654500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x138654970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x138654de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x138655250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x1386556c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x138656130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x138656850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x138656f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x138657690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x138657950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x138657dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1386583c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x1386589d0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.822s
user	0m0.283s
sys	0m0.323s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4671 (4d3465c5)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 ''
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x154710580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x154710c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x154711240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x1547117f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x154711da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x154712350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x154712900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x154712eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x154713460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x154713960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x154713e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x154714360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x154714e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x154715630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x154715e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x154716560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x154716c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x1547173a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x154717ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x154718290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x1547189b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x1547190d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x1547197f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x15471a090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x15471a7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x15471aa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x15471b080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x15471bcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x15471c230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x15471c4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x15471c990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x15471cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x15471d4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x15471da20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x15471dce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x15471e180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x15471e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x15471eac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x15471ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x15471f400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x15471f8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x15471fd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x1547201e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x154720680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x154720940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x154720f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x154721560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x154721e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x154722490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x154722aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x1547230b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x1547236c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x154723cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x1547242e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x154724ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x154724f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x154725410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x1547256d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x154725ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x1547264d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x154726790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x154726c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x1547270d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x154727570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x154727a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x154727eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x154728350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x1547287f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x154728c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x154729130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x1547295d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x154729a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x154729f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x15472a460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x15472a9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x15472af00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x15472b450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x15472b9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x15472bef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x15472c440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x15472c990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x15472cee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x15472d430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x15472d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x15472ded0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x15472e420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x15472e970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x15472eec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x15472f410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x15472f960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x15472feb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x154730400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x154730950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x154730ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x1547313f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x154731940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x154731e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x154721b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x154732300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x154732ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x154733000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x154733550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x154733aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x154733ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x154734540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x154734a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x154734fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x154735530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x154735a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x154735fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x154736520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x154736a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x154736fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x154737460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x154737900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x154737da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x154738240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x1547386e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x154738b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x154739020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x1547394c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x154739960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x154739e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x15473a2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x15473a740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x15473abe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x15473b080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x15473b520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x15473b9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x15473be60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x15473c300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x15473c7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x15473cc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x15473d0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x15473d580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x15473da20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x15473dec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x15473e360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x15473e800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x15473eca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x15473f140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x15473f5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x15473fa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x15473ff20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x1547403c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x154740860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x154740d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x1547411a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x154741640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x154741ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x154741f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x154742420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x1547428c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x154742d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x154743200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x1547436a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x154743b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x154743fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x154744480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x154744920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x154744dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x154745260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x154745700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x154745ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x154746040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x1547464e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x154746980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x154746e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x1547472c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x154747760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x154747c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x1547480a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x154748540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x1547489e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x154748e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x154749320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x1547497c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x154749c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x15474a100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x15474a5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x15474aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x15474aee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x15474b380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x15474b820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x15474bcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x15474c160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x15474c600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x15474caa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x15474cf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x15474d3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x15474d880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x15474dd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x15474e1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x15474e710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x15474ec60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x15474f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x15474f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x15474f9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x15474ffd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x1547505e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x154750bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x1547513e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x154751880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x154751b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x154752150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x154752760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x154752f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x1547533f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x154753890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x154753d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x1547544e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x154754a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x154754f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x1547554d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x154755a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x154755f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x1547564c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x154756a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x154756f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x1547574b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x154757a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x154757f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x1547584a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x1547589f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x154758f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x154759490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x1547599e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x154759f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x15475a480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x15475a9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x15475af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x15475b470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x15475b9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x15475bf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x15475c460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x15475c9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x15475cf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x15475d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x15475d9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x15475def0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x15475e440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x15475e990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x15475eee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x15475f430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x15475f980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x15475fed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x154760420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x154760970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x154760ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x154761410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x154761960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x154761eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x154762400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x154762950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x154762ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x1547633f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x154763940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x154763e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x1547643e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x154764930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x154764e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x1547653d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x154765920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x154765e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x1547663c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x154766910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x154766e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x154767300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x1547677a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x154767c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x1547680e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x154768580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x154768a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x154768ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x154769360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x154769800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x154769ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x15476a140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x15476a5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x15476aa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x15476af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x15476b3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x15476b910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x15476c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x15476c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x15476ce70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x15476d590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x15476d850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x15476e040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x15476e300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x15476e910 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.097.615 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.097.620 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x155804ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x155804f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x1558053c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x155805830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x155805ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x155806110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x155806580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x1558069f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x155806e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x1558072d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x155807740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x155807e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x155808940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x1558090f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x155809900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x15580a020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x15580a740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x15580ae60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x15580b580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x15580bd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x15580c470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x15580cb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x15580d2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x15580d9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x15580e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x15580e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x15580e670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x15580eae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x15580ef50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x15580f3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x15580f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x15580fd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x1558101d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x155810490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x155810900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x155810d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x1558111e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x155811650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x155811ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x155811f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x1558123a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x155812810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x155812c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x1558130f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x155813560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x1558139d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x155813e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x1558142b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x155814720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x155814b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x155815000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x155815470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x1558158e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x155815d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x1558161c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x155816630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x155816ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x1558170a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x155817510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x155817980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x155817df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x155818260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x1558186d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x155818b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x155818fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x155819420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x155819890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x155819d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x15581a170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x15581a5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x15581aa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x15581aec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x15581b330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x15581b7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x15581bc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x15581c080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x15581c4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x15581c960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x15581cdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x15581d240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x15581d6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x15581db20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x15581df90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x15581e400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x15581e870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x15581ece0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x15581f150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x15581f5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x15581fa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x15581fea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x155820310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x155820780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x155820bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x155821060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x1558214d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x155821940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x155821db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x155822220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x155822690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x155822b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x155822f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x1558233e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x155823850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x155823cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x155824130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x1558245a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x155824a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x155824e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x1558252f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x155825760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x155825bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x155826040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x1558264b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x155826920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x155826d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x155827200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x155827670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x155827ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x155827f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x1558283c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x155828830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x155828ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x155829110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x155829580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x1558299f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x155829e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x15582a2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x15582a740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x15582abb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x15582b020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x15582b490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x15582b900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x15582bd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x15582c1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x15582c650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x15582cac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x15582cf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x15582d3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x15582d810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x15582dc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x15582e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x15582e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x15582e9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x15582ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x15582f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x15582f720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x15582fb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x155830000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x155830470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x1558308e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x155830d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x1558311c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x155831630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x155831aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x155831f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x155832380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x1558327f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x155832c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x1558330d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x155833540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x1558339b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x155833e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x155834290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x155834700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x155834b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x155834fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x155835c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x155835ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x155836190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x155836600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x155836a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x155836ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x155837350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x1558377c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x155837c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x1558380a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x155838510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x155838980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x155838df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x155839260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x1558396d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x155839b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x155839fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x15583a420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x15583a890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x15583ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x15583b170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x15583b5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x15583ba50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x15583bec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x15583c330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x15583c7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x15583cc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x15583d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x15583d4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x15583d960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x15583ddd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x15583e240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x15583e6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x15583eb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x15583ef90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x15583f400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x15583f960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x15583fe70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x1558402e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x155840750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x155840bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x155841030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x155841550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x155841a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x1558425d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x155842890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x155842e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x155843410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x1558439d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x155843f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x155844550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x155844b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x1558450d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x155845690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x155845c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x155846210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x1558467d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x155846d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x155847350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x155847910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x155847ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x155848490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x155848a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x155849010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x1558495d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x155849b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x15584a150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x15584a710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x15584acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x15584b290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x15584b850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x15584be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x15584c3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x15584c990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x15584cf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x15584d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x15584dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x15584e090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x15584e650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x15584ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x15584f1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x15584f790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x15584fd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x155850310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x1558508d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x155850e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x155851450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x155851a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x155851fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x155852590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x155852b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x155853110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x1558536d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x155853c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x155854250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x155854810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x155854dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x155855390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x155855950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x155855f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x1558564d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x155856a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x155856f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x155857490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x155857990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x155857e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x155858390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x155858890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x155858d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x155859290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x155859790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x155859c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x15585a190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x15585a690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x15585ab90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x15585b090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x15585b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x15585bfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x15585c6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x15585cde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x15585d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x15585d7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x15585dfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x15585e270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x15585e880 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x1509044e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x150904950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x150904dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x150905230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x1509056a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x150905b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x150905f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x1509063f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x150906860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x150906cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x150907140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x150907860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x150908380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x150908b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x150909340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x150909a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x15090a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x15090a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x15090afc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x15090b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x15090be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x15090c530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x15090cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x15090d370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x15090da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x15090dd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x15090e010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x15090e480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x15090e8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x15090ed60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x15090f1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x15090f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x15090fb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x15090fe30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1509102a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x150910710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x150910b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x150910ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x150911460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x1509118d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x150911d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x1509121b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x150912620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x150912a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x150912f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x150913370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x1509137e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x150913c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x1509140c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x150914530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x1509149a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x150914e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x150915280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x1509156f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x150915b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x150915fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x150916540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x150916a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x150916eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x150917320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x150917790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x150917c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x150918070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x1509184e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x150918950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x150918dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x150919230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x1509196a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x150919b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x150919f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x15091a3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x15091a860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x15091acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x15091b140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x15091b5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x15091ba20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x15091be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x15091c300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x15091c770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x15091cbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x15091d050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x15091d4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x15091d930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x15091dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x15091e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x15091e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x15091eaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x15091ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x15091f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x15091f840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x15091fcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x150920120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x150920590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x150920a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x150920e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x1509212e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x150921750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x150921bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x150922030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x1509224a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x150922910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x150922d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x1509231f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x150923a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x150923d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x1509241b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x150924620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x150924a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x150924f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x150925370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x1509257e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x150925c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x1509260c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x150926530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x1509269a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x150926e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x150927280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x1509276f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x150927b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x150927fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x150928440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x1509288b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x150928d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x150929190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x150929600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x150929a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x150929ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x15092a350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x15092a7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x15092ac30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x15092b0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x15092b510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x15092b980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x15092bdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x15092c260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x15092c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x15092cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x15092cfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x15092d420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x15092d890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x15092dd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x15092e170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x15092e5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x15092ea50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x15092eec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x15092f330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x15092f7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x15092fc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x150930080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x1509304f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x150930960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x150930dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x150931240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x1509316b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x150931b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x150931f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x150932400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x150932870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x150932ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x150933150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x1509335c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x150933a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x150933ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x150934310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x150934780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x150934bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x150935060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x1509354d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x150935940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x150935db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x150936220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x150936690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x150936b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x150936f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x1509373e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x150937850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x150937cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x150938130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x1509385a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x150938a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x150938e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x1509392f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x150939760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x150939bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x15093a040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x15093a4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x15093a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x15093ad90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x15093b200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x15093b670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x15093bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x15093bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x15093c3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x15093c830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x15093cca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x15093d110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x15093d580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x15093d9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x15093de60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x15093e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x15093e740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x15093ebb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x15093f020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x15093f490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x15093f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x15093fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x1509401e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x150940650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x150940ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x150940f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x150941ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x150941d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x150942030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x1509424a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x150942910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x150942d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x1509431f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x150943660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x150943ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x150943f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x1509443b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x150944820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x150944c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x150945100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x150945570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x1509459e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x150945e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x1509462c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x150946730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x150946ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x150947010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x150947480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x1509478f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x150947d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x1509481d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x150948640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x150948ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x150948f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x150949390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x150949800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x150949c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x15094a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x15094a550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x15094a9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x15094ae30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x15094b2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x15094b710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x15094bb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x15094bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x15094c460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x15094c8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x15094cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x15094d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x15094d620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x15094da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x15094df00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x15094e370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x15094e7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x15094ec50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x15094f0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x15094f530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x15094f9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x15094fe10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x150950280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x1509506f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x150950b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x150950fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x150951440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x1509518b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x150951d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x150952190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x150952600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x150952a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x150952ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x150953350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1509537c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x150953c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x1509540a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x150954510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x150954980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x150954df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x150955260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x1509556d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x150956140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x150956860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x150956f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1509576a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x150957960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x150957dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1509583d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x1509589e0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.944s
user	0m0.230s
sys	0m0.183s
```
### ctest_with_model_debug

Runs ctest with model files in debug mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-debug
    Start 26: test-model-load-cancel
1/2 Test #26: test-model-load-cancel ...........   Passed    0.44 sec
    Start 27: test-autorelease
2/2 Test #27: test-autorelease .................   Passed    1.56 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   2.00 sec*proc (2 tests)

Total Test time (real) =   2.01 sec
        2.03 real         0.52 user         0.24 sys
```
### ctest_with_model_release

Runs ctest with model files in release mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-release
    Start 26: test-model-load-cancel
1/2 Test #26: test-model-load-cancel ...........   Passed    0.23 sec
    Start 27: test-autorelease
2/2 Test #27: test-autorelease .................   Passed    0.29 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   0.53 sec*proc (2 tests)

Total Test time (real) =   0.54 sec
        0.54 real         0.12 user         0.09 sys
```
