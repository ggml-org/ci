Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:301 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- ARM -mcpu not found, -mcpu=native will be used
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod - Success
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sve
-- Performing Test GGML_MACHINE_SUPPORTS_sve - Failed
-- ARM feature DOTPROD enabled
-- ARM feature MATMUL_INT8 enabled
-- ARM feature FMA enabled
-- ARM feature FP16_VECTOR_ARITHMETIC enabled
-- Adding CPU backend variant ggml-cpu: -mcpu=native+dotprod+i8mm+nosve 
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (2.3s)
-- Generating done (0.2s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m2.513s
user	0m0.853s
sys	0m1.203s
++ nproc
+ make -j10
[  0%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  0%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  0%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  1%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  1%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  3%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  3%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  4%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  4%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  5%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  5%] Built target build_info
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o
[  5%] Built target xxhash
[  5%] Built target sha1
[  5%] Built target sha256
[  6%] Linking CXX shared library libggml-base.dylib
[  6%] Built target ggml-base
[  6%] Generate assembly for embedded Metal library
Embedding Metal library
[  6%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[  6%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[  6%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[  9%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 11%] Linking CXX shared library libggml-blas.dylib
[ 12%] Linking CXX shared library libggml-cpu.dylib
[ 12%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 13%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 13%] Built target ggml-blas
[ 13%] Built target ggml-cpu
[ 14%] Linking C shared library libggml-metal.dylib
[ 14%] Built target ggml-metal
[ 14%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 15%] Linking CXX shared library libggml.dylib
[ 15%] Built target ggml
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 15%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o
[ 17%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o
[ 19%] Linking CXX executable ../../bin/llama-gguf-hash
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 24%] Linking CXX executable ../../bin/llama-gguf
[ 24%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 25%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 26%] Linking CXX shared library libllama.dylib
[ 26%] Built target llama-gguf-hash
[ 26%] Built target llama-gguf
[ 26%] Built target llama
[ 26%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 26%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 26%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 26%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 29%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 29%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 30%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 30%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 30%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 31%] Linking CXX executable ../../bin/llama-simple
[ 31%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 32%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 33%] Linking C executable ../bin/test-c
[ 33%] Linking CXX executable ../../bin/llama-quantize-stats
[ 34%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 34%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 35%] Linking CXX executable ../../bin/llama-simple-chat
[ 35%] Built target llava
[ 36%] Linking CXX static library libcommon.a
[ 37%] Linking CXX static library libllava_static.a
[ 37%] Linking CXX shared library libllava_shared.dylib
[ 37%] Built target llama-simple
[ 37%] Built target llama-simple-chat
[ 37%] Built target test-c
[ 37%] Built target llama-quantize-stats
[ 37%] Built target common
[ 37%] Built target llava_static
[ 38%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 41%] Built target llava_shared
[ 42%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 45%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 46%] Linking CXX executable ../bin/test-tokenizer-0
[ 46%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 46%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 46%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 46%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 46%] Linking CXX executable ../bin/test-sampling
[ 46%] Linking CXX executable ../bin/test-grammar-parser
[ 46%] Linking CXX executable ../bin/test-llama-grammar
[ 47%] Linking CXX executable ../bin/test-grammar-integration
[ 48%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 49%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 50%] Linking CXX executable ../bin/test-log
[ 50%] Built target test-tokenizer-1-spm
[ 50%] Built target test-tokenizer-0
[ 50%] Built target test-tokenizer-1-bpe
[ 50%] Linking CXX executable ../bin/test-arg-parser
[ 50%] Built target test-sampling
[ 50%] Built target test-grammar-parser
[ 50%] Built target test-grammar-integration
[ 50%] Built target test-json-schema-to-grammar
[ 50%] Built target test-llama-grammar
[ 51%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 51%] Built target test-log
[ 52%] Building CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 56%] Built target test-arg-parser
[ 57%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 57%] Linking CXX executable ../bin/test-chat-template
[ 57%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 57%] Linking CXX executable ../bin/test-gguf
[ 57%] Linking CXX executable ../bin/test-backend-ops
[ 57%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 57%] Linking CXX executable ../bin/test-model-load-cancel
[ 57%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 58%] Linking CXX executable ../bin/test-barrier
[ 59%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 59%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 60%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 61%] Linking CXX executable ../bin/test-autorelease
[ 62%] Linking CXX executable ../bin/test-quantize-fns
[ 62%] Built target test-chat-template
[ 62%] Built target test-gguf
[ 62%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 62%] Linking CXX executable ../bin/test-quantize-perf
[ 62%] Built target test-model-load-cancel
[ 62%] Built target test-backend-ops
[ 62%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 63%] Linking CXX executable ../bin/test-rope
[ 63%] Built target test-barrier
[ 63%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 63%] Built target test-autorelease
[ 63%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 63%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 63%] Built target test-quantize-fns
[ 63%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 63%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 64%] Linking CXX executable ../../bin/llama-batched
[ 65%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 65%] Built target test-quantize-perf
[ 66%] Linking CXX executable ../../bin/llama-embedding
[ 67%] Linking CXX executable ../../bin/llama-batched-bench
[ 68%] Linking CXX executable ../../bin/llama-eval-callback
[ 69%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 69%] Built target test-rope
[ 69%] Linking CXX executable ../../bin/llama-gguf-split
[ 70%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 70%] Linking CXX executable ../../bin/llama-gritlm
[ 71%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 71%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 71%] Built target llama-batched-bench
[ 71%] Linking CXX executable ../../bin/llama-imatrix
[ 71%] Built target llama-batched
[ 71%] Built target llama-embedding
[ 71%] Linking CXX executable ../../bin/llama-infill
[ 71%] Built target llama-eval-callback
[ 71%] Built target llama-gguf-split
[ 71%] Built target llama-gbnf-validator
[ 72%] Linking CXX executable ../../bin/llama-bench
[ 72%] Built target llama-gritlm
[ 73%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 74%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 75%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 77%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 77%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 77%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 77%] Built target llama-imatrix
[ 77%] Linking CXX executable ../../bin/llama-lookahead
[ 77%] Linking CXX executable ../../bin/llama-lookup
[ 77%] Built target llama-infill
[ 77%] Linking CXX executable ../../bin/llama-lookup-create
[ 78%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 78%] Linking CXX executable ../../bin/llama-lookup-stats
[ 78%] Linking CXX executable ../../bin/llama-lookup-merge
[ 79%] Linking CXX executable ../../bin/llama-cli
[ 79%] Built target llama-bench
[ 80%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 81%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 81%] Linking CXX executable ../../bin/llama-parallel
[ 82%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 82%] Built target llama-lookahead
[ 82%] Linking CXX executable ../../bin/llama-passkey
[ 82%] Built target llama-lookup
[ 82%] Built target llama-lookup-create
[ 82%] Linking CXX executable ../../bin/llama-perplexity
[ 82%] Built target llama-cli
[ 82%] Built target llama-lookup-merge
[ 83%] Generating loading.html.hpp
[ 83%] Built target llama-lookup-stats
[ 83%] Linking CXX executable ../../bin/llama-quantize
[ 84%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 85%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 85%] Built target llama-parallel
[ 85%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 86%] Generating index.html.gz.hpp
[ 86%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 86%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 86%] Built target llama-passkey
[ 86%] Linking CXX executable ../../bin/llama-retrieval
[ 86%] Built target llama-perplexity
[ 87%] Linking CXX executable ../../bin/llama-speculative
[ 87%] Linking CXX executable ../../bin/llama-save-load-state
[ 87%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 87%] Linking CXX executable ../../bin/llama-run
[ 88%] Linking CXX executable ../../bin/llama-speculative-simple
[ 88%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 88%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 88%] Built target llama-quantize
[ 89%] Linking CXX executable ../../bin/llama-tokenize
[ 90%] Linking CXX executable ../../bin/llama-gen-docs
[ 90%] Built target llama-retrieval
[ 91%] Linking CXX executable ../../bin/llama-tts
[ 91%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 91%] Built target llama-speculative
[ 91%] Built target llama-save-load-state
[ 91%] Built target llama-run
[ 91%] Built target llama-speculative-simple
[ 91%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 91%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 92%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 93%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 93%] Built target llama-tokenize
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 94%] Built target llama-gen-docs
[ 95%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 95%] Built target llama-tts
[ 96%] Linking CXX executable ../../bin/llama-cvector-generator
[ 97%] Linking CXX executable ../../bin/llama-export-lora
[ 97%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 97%] Linking CXX executable ../../bin/llama-llava-cli
[ 98%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 98%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 98%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 98%] Built target llama-convert-llama2c-to-ggml
[ 98%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Linking CXX executable ../../bin/llama-vdot
[ 99%] Built target llama-cvector-generator
[ 99%] Built target llama-export-lora
[ 99%] Built target llama-minicpmv-cli
[ 99%] Built target llama-llava-cli
[ 99%] Built target llama-qwen2vl-cli
[ 99%] Built target llama-vdot
[ 99%] Built target llama-q8dot
[ 99%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m3.047s
user	0m6.119s
sys	0m9.887s

main: quantize time =  3816.95 ms
main:    total time =  3816.95 ms

main: quantize time =  1557.02 ms
main:    total time =  1557.02 ms

main: quantize time =  1574.25 ms
main:    total time =  1574.25 ms

main: quantize time =  1744.72 ms
main:    total time =  1744.72 ms

main: quantize time =  2280.47 ms
main:    total time =  2280.47 ms

main: quantize time =  4952.25 ms
main:    total time =  4952.25 ms

main: quantize time =  5655.93 ms
main:    total time =  5655.93 ms

main: quantize time =  6826.68 ms
main:    total time =  6826.68 ms

main: quantize time =  6105.62 ms
main:    total time =  6105.62 ms

main: quantize time =  4724.07 ms
main:    total time =  4724.07 ms
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.121 I build: 4507 (4dd34ff8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.211 I main: llama backend init
0.00.000.217 I main: load the model and apply lora adapter, if any
0.00.033.220 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.045.452 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.045.461 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.045.464 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.045.465 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.045.465 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.045.466 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.045.466 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.045.476 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.045.476 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.045.477 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.045.478 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.045.478 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.045.479 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.045.479 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.045.482 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.045.485 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.045.486 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.052.481 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.054.831 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.059.958 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.059.962 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.059.963 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.059.963 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.059.963 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.059.964 I llama_model_loader: - type  f32:  194 tensors
0.00.059.964 I llama_model_loader: - type  f16:   98 tensors
0.00.059.965 I print_info: file format = GGUF V3 (latest)
0.00.059.970 I print_info: file type   = all F32 (guessed)
0.00.059.971 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.078.496 I load: special tokens cache size = 25
0.00.084.493 I load: token to piece cache size = 0.2984 MB
0.00.084.496 I print_info: arch             = gptneox
0.00.084.497 I print_info: vocab_only       = 0
0.00.084.497 I print_info: n_ctx_train      = 2048
0.00.084.497 I print_info: n_embd           = 2048
0.00.084.497 I print_info: n_layer          = 24
0.00.084.501 I print_info: n_head           = 16
0.00.084.501 I print_info: n_head_kv        = 16
0.00.084.501 I print_info: n_rot            = 32
0.00.084.502 I print_info: n_swa            = 0
0.00.084.502 I print_info: n_embd_head_k    = 128
0.00.084.502 I print_info: n_embd_head_v    = 128
0.00.084.503 I print_info: n_gqa            = 1
0.00.084.504 I print_info: n_embd_k_gqa     = 2048
0.00.084.504 I print_info: n_embd_v_gqa     = 2048
0.00.084.505 I print_info: f_norm_eps       = 1.0e-05
0.00.084.505 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.084.505 I print_info: f_clamp_kqv      = 0.0e+00
0.00.084.505 I print_info: f_max_alibi_bias = 0.0e+00
0.00.084.506 I print_info: f_logit_scale    = 0.0e+00
0.00.084.506 I print_info: n_ff             = 8192
0.00.084.507 I print_info: n_expert         = 0
0.00.084.507 I print_info: n_expert_used    = 0
0.00.084.507 I print_info: causal attn      = 1
0.00.084.507 I print_info: pooling type     = 0
0.00.084.510 I print_info: rope type        = 2
0.00.084.534 I print_info: rope scaling     = linear
0.00.084.536 I print_info: freq_base_train  = 10000.0
0.00.084.536 I print_info: freq_scale_train = 1
0.00.084.536 I print_info: n_ctx_orig_yarn  = 2048
0.00.084.536 I print_info: rope_finetuned   = unknown
0.00.084.536 I print_info: ssm_d_conv       = 0
0.00.084.537 I print_info: ssm_d_inner      = 0
0.00.084.537 I print_info: ssm_d_state      = 0
0.00.084.537 I print_info: ssm_dt_rank      = 0
0.00.084.537 I print_info: ssm_dt_b_c_rms   = 0
0.00.084.539 I print_info: model type       = 1.4B
0.00.084.539 I print_info: model params     = 1.41 B
0.00.084.539 I print_info: general.name     = 1.4B
0.00.084.540 I print_info: vocab type       = BPE
0.00.084.540 I print_info: n_vocab          = 50304
0.00.084.540 I print_info: n_merges         = 50009
0.00.084.541 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.084.541 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.084.542 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.084.542 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.084.542 I print_info: LF token         = 128 'Ä'
0.00.084.542 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.084.542 I print_info: max token length = 1024
0.00.086.727 I load_tensors: offloading 24 repeating layers to GPU
0.00.086.728 I load_tensors: offloading output layer to GPU
0.00.086.728 I load_tensors: offloaded 25/25 layers to GPU
0.00.086.748 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.086.749 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.087.023 I llama_init_from_model: n_seq_max     = 1
0.00.087.024 I llama_init_from_model: n_ctx         = 2048
0.00.087.024 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.087.025 I llama_init_from_model: n_batch       = 2048
0.00.087.025 I llama_init_from_model: n_ubatch      = 512
0.00.087.025 I llama_init_from_model: flash_attn    = 0
0.00.087.025 I llama_init_from_model: freq_base     = 10000.0
0.00.087.026 I llama_init_from_model: freq_scale    = 1
0.00.087.026 I ggml_metal_init: allocating
0.00.087.030 I ggml_metal_init: found device: Apple M4
0.00.087.032 I ggml_metal_init: picking default device: Apple M4
0.00.087.709 I ggml_metal_init: using embedded metal library
0.00.129.982 I ggml_metal_init: GPU name:   Apple M4
0.00.129.986 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.129.987 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.129.987 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.129.987 I ggml_metal_init: simdgroup reduction   = true
0.00.129.987 I ggml_metal_init: simdgroup matrix mul. = true
0.00.129.988 I ggml_metal_init: has bfloat            = true
0.00.129.988 I ggml_metal_init: use bfloat            = true
0.00.129.989 I ggml_metal_init: hasUnifiedMemory      = true
0.00.129.990 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.154.109 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.173.482 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.173.507 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.173.528 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.174.470 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.174.471 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.174.472 I llama_init_from_model: graph nodes  = 967
0.00.174.472 I llama_init_from_model: graph splits = 2
0.00.174.475 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.174.608 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.174.609 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.252.787 I main: llama threadpool init, n_threads = 4
0.00.252.825 I 
0.00.252.860 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.252.862 I 
0.00.252.923 I sampler seed: 1234
0.00.252.927 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.252.951 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.252.952 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.252.952 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.103.653 I llama_perf_sampler_print:    sampling time =       1.28 ms /    71 runs   (    0.02 ms per token, 55686.27 tokens per second)
0.02.103.654 I llama_perf_context_print:        load time =     219.56 ms
0.02.103.655 I llama_perf_context_print: prompt eval time =      43.50 ms /     7 tokens (    6.21 ms per token,   160.92 tokens per second)
0.02.103.656 I llama_perf_context_print:        eval time =    1804.37 ms /    63 runs   (   28.64 ms per token,    34.92 tokens per second)
0.02.103.656 I llama_perf_context_print:       total time =    1850.87 ms /    70 tokens
0.02.103.904 I ggml_metal_free: deallocating

real	0m2.421s
user	0m0.126s
sys	0m0.089s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.049 I build: 4507 (4dd34ff8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.078 I main: llama backend init
0.00.000.080 I main: load the model and apply lora adapter, if any
0.00.009.850 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.019.160 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.019.165 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.171 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.172 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.172 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.173 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.173 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.174 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.174 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.175 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.175 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.175 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.175 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.176 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.178 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.180 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.180 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.992 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.033 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.945 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.947 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.948 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.948 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.948 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.949 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.027.949 I llama_model_loader: - type  f32:  194 tensors
0.00.027.950 I llama_model_loader: - type q8_0:   98 tensors
0.00.027.951 I print_info: file format = GGUF V3 (latest)
0.00.027.951 I print_info: file type   = Q8_0
0.00.027.952 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.047.903 I load: special tokens cache size = 25
0.00.053.730 I load: token to piece cache size = 0.2984 MB
0.00.053.735 I print_info: arch             = gptneox
0.00.053.735 I print_info: vocab_only       = 0
0.00.053.738 I print_info: n_ctx_train      = 2048
0.00.053.739 I print_info: n_embd           = 2048
0.00.053.739 I print_info: n_layer          = 24
0.00.053.745 I print_info: n_head           = 16
0.00.053.746 I print_info: n_head_kv        = 16
0.00.053.746 I print_info: n_rot            = 32
0.00.053.746 I print_info: n_swa            = 0
0.00.053.746 I print_info: n_embd_head_k    = 128
0.00.053.746 I print_info: n_embd_head_v    = 128
0.00.053.747 I print_info: n_gqa            = 1
0.00.053.748 I print_info: n_embd_k_gqa     = 2048
0.00.053.748 I print_info: n_embd_v_gqa     = 2048
0.00.053.749 I print_info: f_norm_eps       = 1.0e-05
0.00.053.749 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.053.749 I print_info: f_clamp_kqv      = 0.0e+00
0.00.053.750 I print_info: f_max_alibi_bias = 0.0e+00
0.00.053.750 I print_info: f_logit_scale    = 0.0e+00
0.00.053.751 I print_info: n_ff             = 8192
0.00.053.751 I print_info: n_expert         = 0
0.00.053.751 I print_info: n_expert_used    = 0
0.00.053.751 I print_info: causal attn      = 1
0.00.053.752 I print_info: pooling type     = 0
0.00.053.752 I print_info: rope type        = 2
0.00.053.752 I print_info: rope scaling     = linear
0.00.053.753 I print_info: freq_base_train  = 10000.0
0.00.053.753 I print_info: freq_scale_train = 1
0.00.053.753 I print_info: n_ctx_orig_yarn  = 2048
0.00.053.753 I print_info: rope_finetuned   = unknown
0.00.053.754 I print_info: ssm_d_conv       = 0
0.00.053.754 I print_info: ssm_d_inner      = 0
0.00.053.754 I print_info: ssm_d_state      = 0
0.00.053.754 I print_info: ssm_dt_rank      = 0
0.00.053.754 I print_info: ssm_dt_b_c_rms   = 0
0.00.053.754 I print_info: model type       = 1.4B
0.00.053.755 I print_info: model params     = 1.41 B
0.00.053.755 I print_info: general.name     = 1.4B
0.00.053.756 I print_info: vocab type       = BPE
0.00.053.756 I print_info: n_vocab          = 50304
0.00.053.756 I print_info: n_merges         = 50009
0.00.053.756 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.053.757 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.053.757 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.053.757 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.053.758 I print_info: LF token         = 128 'Ä'
0.00.053.758 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.053.758 I print_info: max token length = 1024
0.00.056.173 I load_tensors: offloading 24 repeating layers to GPU
0.00.056.173 I load_tensors: offloading output layer to GPU
0.00.056.173 I load_tensors: offloaded 25/25 layers to GPU
0.00.056.185 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.056.186 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.00.056.518 I llama_init_from_model: n_seq_max     = 1
0.00.056.519 I llama_init_from_model: n_ctx         = 2048
0.00.056.519 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.056.519 I llama_init_from_model: n_batch       = 2048
0.00.056.519 I llama_init_from_model: n_ubatch      = 512
0.00.056.519 I llama_init_from_model: flash_attn    = 0
0.00.056.520 I llama_init_from_model: freq_base     = 10000.0
0.00.056.520 I llama_init_from_model: freq_scale    = 1
0.00.056.520 I ggml_metal_init: allocating
0.00.056.525 I ggml_metal_init: found device: Apple M4
0.00.056.527 I ggml_metal_init: picking default device: Apple M4
0.00.057.290 I ggml_metal_init: using embedded metal library
0.00.059.864 I ggml_metal_init: GPU name:   Apple M4
0.00.059.866 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.059.866 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.059.867 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.059.867 I ggml_metal_init: simdgroup reduction   = true
0.00.059.867 I ggml_metal_init: simdgroup matrix mul. = true
0.00.059.867 I ggml_metal_init: has bfloat            = true
0.00.059.867 I ggml_metal_init: use bfloat            = true
0.00.059.868 I ggml_metal_init: hasUnifiedMemory      = true
0.00.059.869 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.070.367 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.096.684 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.096.707 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.096.736 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.098.017 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.098.019 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.098.020 I llama_init_from_model: graph nodes  = 967
0.00.098.020 I llama_init_from_model: graph splits = 2
0.00.098.024 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.098.156 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.098.156 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.293.857 I main: llama threadpool init, n_threads = 4
0.01.293.895 I 
0.01.293.926 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.293.928 I 
0.01.294.145 I sampler seed: 1234
0.01.294.150 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.294.193 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.294.196 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.294.196 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.373.885 I llama_perf_sampler_print:    sampling time =       1.23 ms /    71 runs   (    0.02 ms per token, 57959.18 tokens per second)
0.02.373.886 I llama_perf_context_print:        load time =    1284.00 ms
0.02.373.887 I llama_perf_context_print: prompt eval time =      43.87 ms /     7 tokens (    6.27 ms per token,   159.56 tokens per second)
0.02.373.888 I llama_perf_context_print:        eval time =    1032.89 ms /    63 runs   (   16.40 ms per token,    60.99 tokens per second)
0.02.373.888 I llama_perf_context_print:       total time =    1080.03 ms /    70 tokens
0.02.374.123 I ggml_metal_free: deallocating

real	0m2.392s
user	0m0.112s
sys	0m0.215s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.055 I build: 4507 (4dd34ff8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.086 I main: llama backend init
0.00.000.088 I main: load the model and apply lora adapter, if any
0.00.013.035 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.033.176 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.033.183 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.033.185 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.033.185 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.033.185 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.033.185 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.033.186 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.033.190 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.033.190 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.033.193 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.033.194 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.033.194 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.033.194 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.033.195 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.033.200 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.033.200 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.033.200 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.037.327 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.038.403 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.042.421 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.042.422 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.042.423 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.042.423 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.042.423 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.042.424 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.042.424 I llama_model_loader: - type  f32:  194 tensors
0.00.042.424 I llama_model_loader: - type q4_0:   97 tensors
0.00.042.425 I llama_model_loader: - type q6_K:    1 tensors
0.00.042.425 I print_info: file format = GGUF V3 (latest)
0.00.042.429 I print_info: file type   = Q4_0
0.00.042.430 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.065.274 I load: special tokens cache size = 25
0.00.073.704 I load: token to piece cache size = 0.2984 MB
0.00.073.708 I print_info: arch             = gptneox
0.00.073.708 I print_info: vocab_only       = 0
0.00.073.708 I print_info: n_ctx_train      = 2048
0.00.073.709 I print_info: n_embd           = 2048
0.00.073.709 I print_info: n_layer          = 24
0.00.073.713 I print_info: n_head           = 16
0.00.073.714 I print_info: n_head_kv        = 16
0.00.073.715 I print_info: n_rot            = 32
0.00.073.715 I print_info: n_swa            = 0
0.00.073.715 I print_info: n_embd_head_k    = 128
0.00.073.715 I print_info: n_embd_head_v    = 128
0.00.073.716 I print_info: n_gqa            = 1
0.00.073.717 I print_info: n_embd_k_gqa     = 2048
0.00.073.718 I print_info: n_embd_v_gqa     = 2048
0.00.073.718 I print_info: f_norm_eps       = 1.0e-05
0.00.073.719 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.073.719 I print_info: f_clamp_kqv      = 0.0e+00
0.00.073.719 I print_info: f_max_alibi_bias = 0.0e+00
0.00.073.719 I print_info: f_logit_scale    = 0.0e+00
0.00.073.720 I print_info: n_ff             = 8192
0.00.073.721 I print_info: n_expert         = 0
0.00.073.721 I print_info: n_expert_used    = 0
0.00.073.721 I print_info: causal attn      = 1
0.00.073.721 I print_info: pooling type     = 0
0.00.073.723 I print_info: rope type        = 2
0.00.073.725 I print_info: rope scaling     = linear
0.00.073.726 I print_info: freq_base_train  = 10000.0
0.00.073.726 I print_info: freq_scale_train = 1
0.00.073.726 I print_info: n_ctx_orig_yarn  = 2048
0.00.073.727 I print_info: rope_finetuned   = unknown
0.00.073.727 I print_info: ssm_d_conv       = 0
0.00.073.727 I print_info: ssm_d_inner      = 0
0.00.073.727 I print_info: ssm_d_state      = 0
0.00.073.727 I print_info: ssm_dt_rank      = 0
0.00.073.727 I print_info: ssm_dt_b_c_rms   = 0
0.00.073.728 I print_info: model type       = 1.4B
0.00.073.728 I print_info: model params     = 1.41 B
0.00.073.728 I print_info: general.name     = 1.4B
0.00.073.734 I print_info: vocab type       = BPE
0.00.073.735 I print_info: n_vocab          = 50304
0.00.073.736 I print_info: n_merges         = 50009
0.00.073.736 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.073.736 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.073.736 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.073.737 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.073.737 I print_info: LF token         = 128 'Ä'
0.00.073.738 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.073.739 I print_info: max token length = 1024
0.00.076.433 I load_tensors: offloading 24 repeating layers to GPU
0.00.076.433 I load_tensors: offloading output layer to GPU
0.00.076.434 I load_tensors: offloaded 25/25 layers to GPU
0.00.076.445 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.076.447 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.076.801 I llama_init_from_model: n_seq_max     = 1
0.00.076.801 I llama_init_from_model: n_ctx         = 2048
0.00.076.802 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.076.802 I llama_init_from_model: n_batch       = 2048
0.00.076.802 I llama_init_from_model: n_ubatch      = 512
0.00.076.802 I llama_init_from_model: flash_attn    = 0
0.00.076.803 I llama_init_from_model: freq_base     = 10000.0
0.00.076.803 I llama_init_from_model: freq_scale    = 1
0.00.076.803 I ggml_metal_init: allocating
0.00.076.807 I ggml_metal_init: found device: Apple M4
0.00.076.809 I ggml_metal_init: picking default device: Apple M4
0.00.077.667 I ggml_metal_init: using embedded metal library
0.00.080.857 I ggml_metal_init: GPU name:   Apple M4
0.00.080.859 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.080.859 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.080.860 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.080.860 I ggml_metal_init: simdgroup reduction   = true
0.00.080.860 I ggml_metal_init: simdgroup matrix mul. = true
0.00.080.860 I ggml_metal_init: has bfloat            = true
0.00.080.860 I ggml_metal_init: use bfloat            = true
0.00.080.861 I ggml_metal_init: hasUnifiedMemory      = true
0.00.080.862 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.094.950 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.121.040 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.121.063 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.121.085 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.122.382 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.122.383 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.122.384 I llama_init_from_model: graph nodes  = 967
0.00.122.384 I llama_init_from_model: graph splits = 2
0.00.122.387 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.122.518 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.122.519 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.949.010 I main: llama threadpool init, n_threads = 4
0.00.949.051 I 
0.00.949.084 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.949.085 I 
0.00.949.308 I sampler seed: 1234
0.00.949.313 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.949.352 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.949.352 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.949.353 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.625.372 I llama_perf_sampler_print:    sampling time =       1.17 ms /    71 runs   (    0.02 ms per token, 60944.21 tokens per second)
0.01.625.372 I llama_perf_context_print:        load time =     935.97 ms
0.01.625.373 I llama_perf_context_print: prompt eval time =      39.71 ms /     7 tokens (    5.67 ms per token,   176.28 tokens per second)
0.01.625.374 I llama_perf_context_print:        eval time =     633.41 ms /    63 runs   (   10.05 ms per token,    99.46 tokens per second)
0.01.625.374 I llama_perf_context_print:       total time =     676.36 ms /    70 tokens
0.01.625.615 I ggml_metal_free: deallocating

real	0m1.648s
user	0m0.122s
sys	0m0.159s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.043 I build: 4507 (4dd34ff8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.071 I main: llama backend init
0.00.000.073 I main: load the model and apply lora adapter, if any
0.00.013.295 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.023.973 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.023.977 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.023.983 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.023.983 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.023.984 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.023.984 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.023.985 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.023.985 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.023.986 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.023.986 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.023.986 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.023.987 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.023.987 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.023.987 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.023.989 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.023.989 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.023.989 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.028.424 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.029.743 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.034.596 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.034.598 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.034.598 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.034.598 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.034.599 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.034.599 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.034.600 I llama_model_loader: - type  f32:  194 tensors
0.00.034.600 I llama_model_loader: - type q4_1:   97 tensors
0.00.034.600 I llama_model_loader: - type q6_K:    1 tensors
0.00.034.601 I print_info: file format = GGUF V3 (latest)
0.00.034.601 I print_info: file type   = Q4_1
0.00.034.604 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.059.784 I load: special tokens cache size = 25
0.00.066.720 I load: token to piece cache size = 0.2984 MB
0.00.066.723 I print_info: arch             = gptneox
0.00.066.723 I print_info: vocab_only       = 0
0.00.066.723 I print_info: n_ctx_train      = 2048
0.00.066.723 I print_info: n_embd           = 2048
0.00.066.724 I print_info: n_layer          = 24
0.00.066.726 I print_info: n_head           = 16
0.00.066.727 I print_info: n_head_kv        = 16
0.00.066.727 I print_info: n_rot            = 32
0.00.066.727 I print_info: n_swa            = 0
0.00.066.727 I print_info: n_embd_head_k    = 128
0.00.066.728 I print_info: n_embd_head_v    = 128
0.00.066.728 I print_info: n_gqa            = 1
0.00.066.729 I print_info: n_embd_k_gqa     = 2048
0.00.066.730 I print_info: n_embd_v_gqa     = 2048
0.00.066.730 I print_info: f_norm_eps       = 1.0e-05
0.00.066.730 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.066.731 I print_info: f_clamp_kqv      = 0.0e+00
0.00.066.731 I print_info: f_max_alibi_bias = 0.0e+00
0.00.066.731 I print_info: f_logit_scale    = 0.0e+00
0.00.066.732 I print_info: n_ff             = 8192
0.00.066.732 I print_info: n_expert         = 0
0.00.066.732 I print_info: n_expert_used    = 0
0.00.066.732 I print_info: causal attn      = 1
0.00.066.732 I print_info: pooling type     = 0
0.00.066.732 I print_info: rope type        = 2
0.00.066.733 I print_info: rope scaling     = linear
0.00.066.733 I print_info: freq_base_train  = 10000.0
0.00.066.733 I print_info: freq_scale_train = 1
0.00.066.733 I print_info: n_ctx_orig_yarn  = 2048
0.00.066.734 I print_info: rope_finetuned   = unknown
0.00.066.734 I print_info: ssm_d_conv       = 0
0.00.066.734 I print_info: ssm_d_inner      = 0
0.00.066.734 I print_info: ssm_d_state      = 0
0.00.066.734 I print_info: ssm_dt_rank      = 0
0.00.066.736 I print_info: ssm_dt_b_c_rms   = 0
0.00.066.736 I print_info: model type       = 1.4B
0.00.066.737 I print_info: model params     = 1.41 B
0.00.066.737 I print_info: general.name     = 1.4B
0.00.066.737 I print_info: vocab type       = BPE
0.00.066.737 I print_info: n_vocab          = 50304
0.00.066.738 I print_info: n_merges         = 50009
0.00.066.738 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.066.738 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.066.738 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.066.738 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.066.739 I print_info: LF token         = 128 'Ä'
0.00.066.739 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.066.739 I print_info: max token length = 1024
0.00.068.978 I load_tensors: offloading 24 repeating layers to GPU
0.00.068.978 I load_tensors: offloading output layer to GPU
0.00.068.979 I load_tensors: offloaded 25/25 layers to GPU
0.00.068.989 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.068.990 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.069.353 I llama_init_from_model: n_seq_max     = 1
0.00.069.354 I llama_init_from_model: n_ctx         = 2048
0.00.069.355 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.069.355 I llama_init_from_model: n_batch       = 2048
0.00.069.355 I llama_init_from_model: n_ubatch      = 512
0.00.069.355 I llama_init_from_model: flash_attn    = 0
0.00.069.356 I llama_init_from_model: freq_base     = 10000.0
0.00.069.356 I llama_init_from_model: freq_scale    = 1
0.00.069.356 I ggml_metal_init: allocating
0.00.069.359 I ggml_metal_init: found device: Apple M4
0.00.069.361 I ggml_metal_init: picking default device: Apple M4
0.00.070.031 I ggml_metal_init: using embedded metal library
0.00.073.693 I ggml_metal_init: GPU name:   Apple M4
0.00.073.696 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.073.697 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.073.697 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.073.698 I ggml_metal_init: simdgroup reduction   = true
0.00.073.698 I ggml_metal_init: simdgroup matrix mul. = true
0.00.073.698 I ggml_metal_init: has bfloat            = true
0.00.073.698 I ggml_metal_init: use bfloat            = true
0.00.073.699 I ggml_metal_init: hasUnifiedMemory      = true
0.00.073.700 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.086.494 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.108.875 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.108.893 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.108.913 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.109.905 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.109.906 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.109.906 I llama_init_from_model: graph nodes  = 967
0.00.109.907 I llama_init_from_model: graph splits = 2
0.00.109.910 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.110.041 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.110.042 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.746.520 I main: llama threadpool init, n_threads = 4
0.00.746.562 I 
0.00.746.593 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.746.595 I 
0.00.746.813 I sampler seed: 1234
0.00.746.817 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.746.837 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.746.837 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.746.837 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.473.924 I llama_perf_sampler_print:    sampling time =       1.15 ms /    71 runs   (    0.02 ms per token, 61792.86 tokens per second)
0.01.473.925 I llama_perf_context_print:        load time =     733.22 ms
0.01.473.926 I llama_perf_context_print: prompt eval time =      43.49 ms /     7 tokens (    6.21 ms per token,   160.95 tokens per second)
0.01.473.927 I llama_perf_context_print:        eval time =     680.66 ms /    63 runs   (   10.80 ms per token,    92.56 tokens per second)
0.01.473.927 I llama_perf_context_print:       total time =     727.41 ms /    70 tokens
0.01.474.132 I ggml_metal_free: deallocating

real	0m1.500s
user	0m0.123s
sys	0m0.147s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4507 (4dd34ff8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.073 I main: llama backend init
0.00.000.075 I main: load the model and apply lora adapter, if any
0.00.011.311 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.988 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.018.993 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.994 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.995 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.995 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.995 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.996 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.996 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.998 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.999 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.999 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.999 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.000 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.000 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.002 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.002 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.002 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.792 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.817 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.558 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.559 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.559 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.560 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.560 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.560 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.027.561 I llama_model_loader: - type  f32:  194 tensors
0.00.027.561 I llama_model_loader: - type q5_0:   97 tensors
0.00.027.561 I llama_model_loader: - type q6_K:    1 tensors
0.00.027.562 I print_info: file format = GGUF V3 (latest)
0.00.027.563 I print_info: file type   = Q5_0
0.00.027.564 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.045.945 I load: special tokens cache size = 25
0.00.051.897 I load: token to piece cache size = 0.2984 MB
0.00.051.900 I print_info: arch             = gptneox
0.00.051.901 I print_info: vocab_only       = 0
0.00.051.901 I print_info: n_ctx_train      = 2048
0.00.051.901 I print_info: n_embd           = 2048
0.00.051.901 I print_info: n_layer          = 24
0.00.051.904 I print_info: n_head           = 16
0.00.051.905 I print_info: n_head_kv        = 16
0.00.051.905 I print_info: n_rot            = 32
0.00.051.906 I print_info: n_swa            = 0
0.00.051.906 I print_info: n_embd_head_k    = 128
0.00.051.906 I print_info: n_embd_head_v    = 128
0.00.051.907 I print_info: n_gqa            = 1
0.00.051.908 I print_info: n_embd_k_gqa     = 2048
0.00.051.910 I print_info: n_embd_v_gqa     = 2048
0.00.051.911 I print_info: f_norm_eps       = 1.0e-05
0.00.051.911 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.911 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.912 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.912 I print_info: f_logit_scale    = 0.0e+00
0.00.051.912 I print_info: n_ff             = 8192
0.00.051.912 I print_info: n_expert         = 0
0.00.051.913 I print_info: n_expert_used    = 0
0.00.051.913 I print_info: causal attn      = 1
0.00.051.914 I print_info: pooling type     = 0
0.00.051.914 I print_info: rope type        = 2
0.00.051.914 I print_info: rope scaling     = linear
0.00.051.914 I print_info: freq_base_train  = 10000.0
0.00.051.915 I print_info: freq_scale_train = 1
0.00.051.915 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.915 I print_info: rope_finetuned   = unknown
0.00.051.915 I print_info: ssm_d_conv       = 0
0.00.051.915 I print_info: ssm_d_inner      = 0
0.00.051.915 I print_info: ssm_d_state      = 0
0.00.051.915 I print_info: ssm_dt_rank      = 0
0.00.051.915 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.916 I print_info: model type       = 1.4B
0.00.051.916 I print_info: model params     = 1.41 B
0.00.051.916 I print_info: general.name     = 1.4B
0.00.051.917 I print_info: vocab type       = BPE
0.00.051.917 I print_info: n_vocab          = 50304
0.00.051.917 I print_info: n_merges         = 50009
0.00.051.917 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.917 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.917 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.918 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.918 I print_info: LF token         = 128 'Ä'
0.00.051.918 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.919 I print_info: max token length = 1024
0.00.053.677 I load_tensors: offloading 24 repeating layers to GPU
0.00.053.677 I load_tensors: offloading output layer to GPU
0.00.053.678 I load_tensors: offloaded 25/25 layers to GPU
0.00.053.683 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.053.684 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.053.968 I llama_init_from_model: n_seq_max     = 1
0.00.053.969 I llama_init_from_model: n_ctx         = 2048
0.00.053.969 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.053.969 I llama_init_from_model: n_batch       = 2048
0.00.053.969 I llama_init_from_model: n_ubatch      = 512
0.00.053.969 I llama_init_from_model: flash_attn    = 0
0.00.053.970 I llama_init_from_model: freq_base     = 10000.0
0.00.053.970 I llama_init_from_model: freq_scale    = 1
0.00.053.970 I ggml_metal_init: allocating
0.00.053.973 I ggml_metal_init: found device: Apple M4
0.00.053.975 I ggml_metal_init: picking default device: Apple M4
0.00.054.559 I ggml_metal_init: using embedded metal library
0.00.056.905 I ggml_metal_init: GPU name:   Apple M4
0.00.056.907 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.907 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.908 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.908 I ggml_metal_init: simdgroup reduction   = true
0.00.056.908 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.908 I ggml_metal_init: has bfloat            = true
0.00.056.908 I ggml_metal_init: use bfloat            = true
0.00.056.909 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.910 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.539 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.085.645 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.667 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.685 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.086.757 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.086.759 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.086.759 I llama_init_from_model: graph nodes  = 967
0.00.086.759 I llama_init_from_model: graph splits = 2
0.00.086.762 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.086.897 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.086.898 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.786.441 I main: llama threadpool init, n_threads = 4
0.00.786.494 I 
0.00.786.524 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.786.525 I 
0.00.786.764 I sampler seed: 1234
0.00.786.770 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.786.781 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.786.781 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.786.781 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.578.075 I llama_perf_sampler_print:    sampling time =       1.27 ms /    71 runs   (    0.02 ms per token, 55949.57 tokens per second)
0.01.578.080 I llama_perf_context_print:        load time =     775.12 ms
0.01.578.081 I llama_perf_context_print: prompt eval time =      43.13 ms /     7 tokens (    6.16 ms per token,   162.32 tokens per second)
0.01.578.082 I llama_perf_context_print:        eval time =     745.21 ms /    63 runs   (   11.83 ms per token,    84.54 tokens per second)
0.01.578.082 I llama_perf_context_print:       total time =     791.64 ms /    70 tokens
0.01.578.383 I ggml_metal_free: deallocating

real	0m1.598s
user	0m0.107s
sys	0m0.151s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.055 I build: 4507 (4dd34ff8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.087 I main: llama backend init
0.00.000.090 I main: load the model and apply lora adapter, if any
0.00.018.266 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.027.902 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.027.908 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.027.909 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.027.910 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.027.910 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.027.910 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.027.911 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.027.912 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.027.912 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.027.912 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.027.912 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.027.913 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.027.913 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.027.913 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.027.916 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.027.916 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.027.917 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.031.814 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.032.856 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.036.712 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.036.713 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.036.713 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.036.714 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.036.714 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.036.714 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.036.715 I llama_model_loader: - type  f32:  194 tensors
0.00.036.715 I llama_model_loader: - type q5_1:   97 tensors
0.00.036.716 I llama_model_loader: - type q6_K:    1 tensors
0.00.036.716 I print_info: file format = GGUF V3 (latest)
0.00.036.717 I print_info: file type   = Q5_1
0.00.036.718 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.056.761 I load: special tokens cache size = 25
0.00.062.877 I load: token to piece cache size = 0.2984 MB
0.00.062.880 I print_info: arch             = gptneox
0.00.062.880 I print_info: vocab_only       = 0
0.00.062.881 I print_info: n_ctx_train      = 2048
0.00.062.881 I print_info: n_embd           = 2048
0.00.062.881 I print_info: n_layer          = 24
0.00.062.884 I print_info: n_head           = 16
0.00.062.885 I print_info: n_head_kv        = 16
0.00.062.885 I print_info: n_rot            = 32
0.00.062.885 I print_info: n_swa            = 0
0.00.062.886 I print_info: n_embd_head_k    = 128
0.00.062.886 I print_info: n_embd_head_v    = 128
0.00.062.888 I print_info: n_gqa            = 1
0.00.062.889 I print_info: n_embd_k_gqa     = 2048
0.00.062.889 I print_info: n_embd_v_gqa     = 2048
0.00.062.890 I print_info: f_norm_eps       = 1.0e-05
0.00.062.890 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.062.891 I print_info: f_clamp_kqv      = 0.0e+00
0.00.062.893 I print_info: f_max_alibi_bias = 0.0e+00
0.00.062.894 I print_info: f_logit_scale    = 0.0e+00
0.00.062.894 I print_info: n_ff             = 8192
0.00.062.895 I print_info: n_expert         = 0
0.00.062.895 I print_info: n_expert_used    = 0
0.00.062.896 I print_info: causal attn      = 1
0.00.062.896 I print_info: pooling type     = 0
0.00.062.896 I print_info: rope type        = 2
0.00.062.896 I print_info: rope scaling     = linear
0.00.062.897 I print_info: freq_base_train  = 10000.0
0.00.062.897 I print_info: freq_scale_train = 1
0.00.062.897 I print_info: n_ctx_orig_yarn  = 2048
0.00.062.897 I print_info: rope_finetuned   = unknown
0.00.062.897 I print_info: ssm_d_conv       = 0
0.00.062.897 I print_info: ssm_d_inner      = 0
0.00.062.897 I print_info: ssm_d_state      = 0
0.00.062.897 I print_info: ssm_dt_rank      = 0
0.00.062.898 I print_info: ssm_dt_b_c_rms   = 0
0.00.062.898 I print_info: model type       = 1.4B
0.00.062.898 I print_info: model params     = 1.41 B
0.00.062.898 I print_info: general.name     = 1.4B
0.00.062.899 I print_info: vocab type       = BPE
0.00.062.899 I print_info: n_vocab          = 50304
0.00.062.899 I print_info: n_merges         = 50009
0.00.062.899 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.062.899 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.062.899 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.062.900 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.062.900 I print_info: LF token         = 128 'Ä'
0.00.062.900 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.062.900 I print_info: max token length = 1024
0.00.064.993 I load_tensors: offloading 24 repeating layers to GPU
0.00.064.993 I load_tensors: offloading output layer to GPU
0.00.064.994 I load_tensors: offloaded 25/25 layers to GPU
0.00.065.004 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.065.006 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.065.372 I llama_init_from_model: n_seq_max     = 1
0.00.065.373 I llama_init_from_model: n_ctx         = 2048
0.00.065.373 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.065.373 I llama_init_from_model: n_batch       = 2048
0.00.065.373 I llama_init_from_model: n_ubatch      = 512
0.00.065.374 I llama_init_from_model: flash_attn    = 0
0.00.065.374 I llama_init_from_model: freq_base     = 10000.0
0.00.065.374 I llama_init_from_model: freq_scale    = 1
0.00.065.375 I ggml_metal_init: allocating
0.00.065.379 I ggml_metal_init: found device: Apple M4
0.00.065.381 I ggml_metal_init: picking default device: Apple M4
0.00.066.044 I ggml_metal_init: using embedded metal library
0.00.068.456 I ggml_metal_init: GPU name:   Apple M4
0.00.068.457 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.068.458 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.068.458 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.068.458 I ggml_metal_init: simdgroup reduction   = true
0.00.068.459 I ggml_metal_init: simdgroup matrix mul. = true
0.00.068.459 I ggml_metal_init: has bfloat            = true
0.00.068.459 I ggml_metal_init: use bfloat            = true
0.00.068.459 I ggml_metal_init: hasUnifiedMemory      = true
0.00.068.460 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.079.281 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.099.373 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.099.398 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.099.421 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.100.496 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.100.498 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.100.498 I llama_init_from_model: graph nodes  = 967
0.00.100.499 I llama_init_from_model: graph splits = 2
0.00.100.502 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.100.630 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.100.631 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.718.773 I main: llama threadpool init, n_threads = 4
0.00.718.815 I 
0.00.718.844 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.718.845 I 
0.00.719.062 I sampler seed: 1234
0.00.719.067 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.719.119 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.719.124 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.719.124 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.593.824 I llama_perf_sampler_print:    sampling time =       1.22 ms /    71 runs   (    0.02 ms per token, 58388.16 tokens per second)
0.01.593.825 I llama_perf_context_print:        load time =     700.50 ms
0.01.593.826 I llama_perf_context_print: prompt eval time =      42.20 ms /     7 tokens (    6.03 ms per token,   165.86 tokens per second)
0.01.593.826 I llama_perf_context_print:        eval time =     829.60 ms /    63 runs   (   13.17 ms per token,    75.94 tokens per second)
0.01.593.827 I llama_perf_context_print:       total time =     875.05 ms /    70 tokens
0.01.594.089 I ggml_metal_free: deallocating

real	0m1.616s
user	0m0.113s
sys	0m0.155s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.044 I build: 4507 (4dd34ff8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.074 I main: llama backend init
0.00.000.078 I main: load the model and apply lora adapter, if any
0.00.010.029 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.491 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.017.496 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.497 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.498 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.498 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.498 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.499 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.500 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.500 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.500 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.501 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.501 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.502 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.502 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.504 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.504 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.504 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.252 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.456 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.711 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.712 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.713 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.713 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.713 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.714 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.026.714 I llama_model_loader: - type  f32:  194 tensors
0.00.026.715 I llama_model_loader: - type q2_K:   49 tensors
0.00.026.715 I llama_model_loader: - type q3_K:   48 tensors
0.00.026.715 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.716 I print_info: file format = GGUF V3 (latest)
0.00.026.716 I print_info: file type   = Q2_K - Medium
0.00.026.717 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.049.764 I load: special tokens cache size = 25
0.00.056.269 I load: token to piece cache size = 0.2984 MB
0.00.056.272 I print_info: arch             = gptneox
0.00.056.272 I print_info: vocab_only       = 0
0.00.056.272 I print_info: n_ctx_train      = 2048
0.00.056.273 I print_info: n_embd           = 2048
0.00.056.273 I print_info: n_layer          = 24
0.00.056.276 I print_info: n_head           = 16
0.00.056.277 I print_info: n_head_kv        = 16
0.00.056.277 I print_info: n_rot            = 32
0.00.056.277 I print_info: n_swa            = 0
0.00.056.277 I print_info: n_embd_head_k    = 128
0.00.056.277 I print_info: n_embd_head_v    = 128
0.00.056.279 I print_info: n_gqa            = 1
0.00.056.279 I print_info: n_embd_k_gqa     = 2048
0.00.056.280 I print_info: n_embd_v_gqa     = 2048
0.00.056.281 I print_info: f_norm_eps       = 1.0e-05
0.00.056.281 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.056.281 I print_info: f_clamp_kqv      = 0.0e+00
0.00.056.282 I print_info: f_max_alibi_bias = 0.0e+00
0.00.056.282 I print_info: f_logit_scale    = 0.0e+00
0.00.056.282 I print_info: n_ff             = 8192
0.00.056.282 I print_info: n_expert         = 0
0.00.056.283 I print_info: n_expert_used    = 0
0.00.056.283 I print_info: causal attn      = 1
0.00.056.283 I print_info: pooling type     = 0
0.00.056.283 I print_info: rope type        = 2
0.00.056.283 I print_info: rope scaling     = linear
0.00.056.284 I print_info: freq_base_train  = 10000.0
0.00.056.284 I print_info: freq_scale_train = 1
0.00.056.284 I print_info: n_ctx_orig_yarn  = 2048
0.00.056.284 I print_info: rope_finetuned   = unknown
0.00.056.285 I print_info: ssm_d_conv       = 0
0.00.056.285 I print_info: ssm_d_inner      = 0
0.00.056.285 I print_info: ssm_d_state      = 0
0.00.056.285 I print_info: ssm_dt_rank      = 0
0.00.056.285 I print_info: ssm_dt_b_c_rms   = 0
0.00.056.285 I print_info: model type       = 1.4B
0.00.056.286 I print_info: model params     = 1.41 B
0.00.056.287 I print_info: general.name     = 1.4B
0.00.056.288 I print_info: vocab type       = BPE
0.00.056.288 I print_info: n_vocab          = 50304
0.00.056.288 I print_info: n_merges         = 50009
0.00.056.288 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.056.289 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.056.289 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.056.289 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.056.289 I print_info: LF token         = 128 'Ä'
0.00.056.289 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.056.290 I print_info: max token length = 1024
0.00.058.071 I load_tensors: offloading 24 repeating layers to GPU
0.00.058.071 I load_tensors: offloading output layer to GPU
0.00.058.071 I load_tensors: offloaded 25/25 layers to GPU
0.00.058.082 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.058.083 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.058.382 I llama_init_from_model: n_seq_max     = 1
0.00.058.382 I llama_init_from_model: n_ctx         = 2048
0.00.058.383 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.058.383 I llama_init_from_model: n_batch       = 2048
0.00.058.383 I llama_init_from_model: n_ubatch      = 512
0.00.058.383 I llama_init_from_model: flash_attn    = 0
0.00.058.384 I llama_init_from_model: freq_base     = 10000.0
0.00.058.384 I llama_init_from_model: freq_scale    = 1
0.00.058.384 I ggml_metal_init: allocating
0.00.058.387 I ggml_metal_init: found device: Apple M4
0.00.058.389 I ggml_metal_init: picking default device: Apple M4
0.00.059.050 I ggml_metal_init: using embedded metal library
0.00.061.668 I ggml_metal_init: GPU name:   Apple M4
0.00.061.669 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.061.670 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.061.670 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.061.670 I ggml_metal_init: simdgroup reduction   = true
0.00.061.671 I ggml_metal_init: simdgroup matrix mul. = true
0.00.061.671 I ggml_metal_init: has bfloat            = true
0.00.061.671 I ggml_metal_init: use bfloat            = true
0.00.061.671 I ggml_metal_init: hasUnifiedMemory      = true
0.00.061.672 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.072.514 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.092.846 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.092.875 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.092.911 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.093.827 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.093.829 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.093.829 I llama_init_from_model: graph nodes  = 967
0.00.093.830 I llama_init_from_model: graph splits = 2
0.00.093.833 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.093.957 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.093.957 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.453.041 I main: llama threadpool init, n_threads = 4
0.00.453.110 I 
0.00.453.140 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.453.141 I 
0.00.453.304 I sampler seed: 1234
0.00.453.309 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.453.319 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.453.320 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.453.320 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.145.287 I llama_perf_sampler_print:    sampling time =       1.19 ms /    71 runs   (    0.02 ms per token, 59463.99 tokens per second)
0.01.145.288 I llama_perf_context_print:        load time =     443.00 ms
0.01.145.288 I llama_perf_context_print: prompt eval time =      38.79 ms /     7 tokens (    5.54 ms per token,   180.45 tokens per second)
0.01.145.289 I llama_perf_context_print:        eval time =     650.10 ms /    63 runs   (   10.32 ms per token,    96.91 tokens per second)
0.01.145.290 I llama_perf_context_print:       total time =     692.25 ms /    70 tokens
0.01.145.484 I ggml_metal_free: deallocating

real	0m1.175s
user	0m0.116s
sys	0m0.106s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.044 I build: 4507 (4dd34ff8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.074 I main: llama backend init
0.00.000.076 I main: load the model and apply lora adapter, if any
0.00.010.009 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.441 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.017.446 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.448 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.448 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.449 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.449 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.449 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.451 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.451 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.452 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.452 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.452 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.453 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.453 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.455 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.456 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.456 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.239 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.293 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.013 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.014 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.014 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.014 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.015 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.015 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.026.015 I llama_model_loader: - type  f32:  194 tensors
0.00.026.016 I llama_model_loader: - type q3_K:   25 tensors
0.00.026.016 I llama_model_loader: - type q4_K:   71 tensors
0.00.026.016 I llama_model_loader: - type q5_K:    1 tensors
0.00.026.016 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.017 I print_info: file format = GGUF V3 (latest)
0.00.026.018 I print_info: file type   = Q3_K - Medium
0.00.026.019 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.044.565 I load: special tokens cache size = 25
0.00.050.233 I load: token to piece cache size = 0.2984 MB
0.00.050.236 I print_info: arch             = gptneox
0.00.050.236 I print_info: vocab_only       = 0
0.00.050.236 I print_info: n_ctx_train      = 2048
0.00.050.236 I print_info: n_embd           = 2048
0.00.050.237 I print_info: n_layer          = 24
0.00.050.240 I print_info: n_head           = 16
0.00.050.241 I print_info: n_head_kv        = 16
0.00.050.241 I print_info: n_rot            = 32
0.00.050.241 I print_info: n_swa            = 0
0.00.050.241 I print_info: n_embd_head_k    = 128
0.00.050.244 I print_info: n_embd_head_v    = 128
0.00.050.245 I print_info: n_gqa            = 1
0.00.050.246 I print_info: n_embd_k_gqa     = 2048
0.00.050.246 I print_info: n_embd_v_gqa     = 2048
0.00.050.247 I print_info: f_norm_eps       = 1.0e-05
0.00.050.247 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.248 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.248 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.248 I print_info: f_logit_scale    = 0.0e+00
0.00.050.249 I print_info: n_ff             = 8192
0.00.050.249 I print_info: n_expert         = 0
0.00.050.249 I print_info: n_expert_used    = 0
0.00.050.250 I print_info: causal attn      = 1
0.00.050.251 I print_info: pooling type     = 0
0.00.050.251 I print_info: rope type        = 2
0.00.050.251 I print_info: rope scaling     = linear
0.00.050.252 I print_info: freq_base_train  = 10000.0
0.00.050.252 I print_info: freq_scale_train = 1
0.00.050.252 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.252 I print_info: rope_finetuned   = unknown
0.00.050.252 I print_info: ssm_d_conv       = 0
0.00.050.253 I print_info: ssm_d_inner      = 0
0.00.050.253 I print_info: ssm_d_state      = 0
0.00.050.253 I print_info: ssm_dt_rank      = 0
0.00.050.253 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.253 I print_info: model type       = 1.4B
0.00.050.254 I print_info: model params     = 1.41 B
0.00.050.254 I print_info: general.name     = 1.4B
0.00.050.255 I print_info: vocab type       = BPE
0.00.050.257 I print_info: n_vocab          = 50304
0.00.050.257 I print_info: n_merges         = 50009
0.00.050.257 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.257 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.257 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.257 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.258 I print_info: LF token         = 128 'Ä'
0.00.050.261 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.262 I print_info: max token length = 1024
0.00.051.843 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.844 I load_tensors: offloading output layer to GPU
0.00.051.844 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.854 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.051.855 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.052.139 I llama_init_from_model: n_seq_max     = 1
0.00.052.140 I llama_init_from_model: n_ctx         = 2048
0.00.052.140 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.052.140 I llama_init_from_model: n_batch       = 2048
0.00.052.140 I llama_init_from_model: n_ubatch      = 512
0.00.052.140 I llama_init_from_model: flash_attn    = 0
0.00.052.141 I llama_init_from_model: freq_base     = 10000.0
0.00.052.141 I llama_init_from_model: freq_scale    = 1
0.00.052.141 I ggml_metal_init: allocating
0.00.052.145 I ggml_metal_init: found device: Apple M4
0.00.052.147 I ggml_metal_init: picking default device: Apple M4
0.00.052.742 I ggml_metal_init: using embedded metal library
0.00.055.045 I ggml_metal_init: GPU name:   Apple M4
0.00.055.047 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.047 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.048 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.048 I ggml_metal_init: simdgroup reduction   = true
0.00.055.048 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.048 I ggml_metal_init: has bfloat            = true
0.00.055.048 I ggml_metal_init: use bfloat            = true
0.00.055.049 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.049 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.725 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.083.690 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.083.712 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.083.745 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.084.695 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.084.697 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.084.697 I llama_init_from_model: graph nodes  = 967
0.00.084.697 I llama_init_from_model: graph splits = 2
0.00.084.700 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.084.850 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.084.850 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.527.195 I main: llama threadpool init, n_threads = 4
0.00.527.241 I 
0.00.527.272 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.527.273 I 
0.00.527.438 I sampler seed: 1234
0.00.527.443 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.527.455 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.527.456 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.527.456 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.296.250 I llama_perf_sampler_print:    sampling time =       1.23 ms /    71 runs   (    0.02 ms per token, 57676.69 tokens per second)
0.01.296.251 I llama_perf_context_print:        load time =     517.18 ms
0.01.296.251 I llama_perf_context_print: prompt eval time =      40.52 ms /     7 tokens (    5.79 ms per token,   172.74 tokens per second)
0.01.296.252 I llama_perf_context_print:        eval time =     725.22 ms /    63 runs   (   11.51 ms per token,    86.87 tokens per second)
0.01.296.252 I llama_perf_context_print:       total time =     769.06 ms /    70 tokens
0.01.296.516 I ggml_metal_free: deallocating

real	0m1.315s
user	0m0.109s
sys	0m0.118s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.044 I build: 4507 (4dd34ff8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.075 I main: llama backend init
0.00.000.077 I main: load the model and apply lora adapter, if any
0.00.012.044 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.019.518 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.019.523 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.525 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.525 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.526 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.526 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.526 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.529 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.530 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.530 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.530 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.531 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.531 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.532 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.534 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.534 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.535 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.292 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.282 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.984 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.985 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.985 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.985 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.986 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.986 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.027.986 I llama_model_loader: - type  f32:  194 tensors
0.00.027.986 I llama_model_loader: - type q4_K:   61 tensors
0.00.027.987 I llama_model_loader: - type q5_K:   24 tensors
0.00.027.987 I llama_model_loader: - type q6_K:   13 tensors
0.00.027.987 I print_info: file format = GGUF V3 (latest)
0.00.027.988 I print_info: file type   = Q4_K - Medium
0.00.027.989 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.046.519 I load: special tokens cache size = 25
0.00.052.401 I load: token to piece cache size = 0.2984 MB
0.00.052.404 I print_info: arch             = gptneox
0.00.052.405 I print_info: vocab_only       = 0
0.00.052.405 I print_info: n_ctx_train      = 2048
0.00.052.405 I print_info: n_embd           = 2048
0.00.052.405 I print_info: n_layer          = 24
0.00.052.408 I print_info: n_head           = 16
0.00.052.409 I print_info: n_head_kv        = 16
0.00.052.409 I print_info: n_rot            = 32
0.00.052.409 I print_info: n_swa            = 0
0.00.052.410 I print_info: n_embd_head_k    = 128
0.00.052.410 I print_info: n_embd_head_v    = 128
0.00.052.410 I print_info: n_gqa            = 1
0.00.052.411 I print_info: n_embd_k_gqa     = 2048
0.00.052.414 I print_info: n_embd_v_gqa     = 2048
0.00.052.414 I print_info: f_norm_eps       = 1.0e-05
0.00.052.414 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.052.415 I print_info: f_clamp_kqv      = 0.0e+00
0.00.052.415 I print_info: f_max_alibi_bias = 0.0e+00
0.00.052.415 I print_info: f_logit_scale    = 0.0e+00
0.00.052.416 I print_info: n_ff             = 8192
0.00.052.416 I print_info: n_expert         = 0
0.00.052.416 I print_info: n_expert_used    = 0
0.00.052.417 I print_info: causal attn      = 1
0.00.052.418 I print_info: pooling type     = 0
0.00.052.419 I print_info: rope type        = 2
0.00.052.419 I print_info: rope scaling     = linear
0.00.052.419 I print_info: freq_base_train  = 10000.0
0.00.052.420 I print_info: freq_scale_train = 1
0.00.052.420 I print_info: n_ctx_orig_yarn  = 2048
0.00.052.420 I print_info: rope_finetuned   = unknown
0.00.052.420 I print_info: ssm_d_conv       = 0
0.00.052.420 I print_info: ssm_d_inner      = 0
0.00.052.420 I print_info: ssm_d_state      = 0
0.00.052.421 I print_info: ssm_dt_rank      = 0
0.00.052.421 I print_info: ssm_dt_b_c_rms   = 0
0.00.052.421 I print_info: model type       = 1.4B
0.00.052.421 I print_info: model params     = 1.41 B
0.00.052.421 I print_info: general.name     = 1.4B
0.00.052.422 I print_info: vocab type       = BPE
0.00.052.422 I print_info: n_vocab          = 50304
0.00.052.422 I print_info: n_merges         = 50009
0.00.052.422 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.052.423 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.052.427 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.052.427 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.052.427 I print_info: LF token         = 128 'Ä'
0.00.052.427 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.052.427 I print_info: max token length = 1024
0.00.054.015 I load_tensors: offloading 24 repeating layers to GPU
0.00.054.016 I load_tensors: offloading output layer to GPU
0.00.054.016 I load_tensors: offloaded 25/25 layers to GPU
0.00.054.026 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.054.027 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.054.301 I llama_init_from_model: n_seq_max     = 1
0.00.054.302 I llama_init_from_model: n_ctx         = 2048
0.00.054.302 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.054.302 I llama_init_from_model: n_batch       = 2048
0.00.054.302 I llama_init_from_model: n_ubatch      = 512
0.00.054.302 I llama_init_from_model: flash_attn    = 0
0.00.054.303 I llama_init_from_model: freq_base     = 10000.0
0.00.054.303 I llama_init_from_model: freq_scale    = 1
0.00.054.304 I ggml_metal_init: allocating
0.00.054.307 I ggml_metal_init: found device: Apple M4
0.00.054.308 I ggml_metal_init: picking default device: Apple M4
0.00.054.900 I ggml_metal_init: using embedded metal library
0.00.057.218 I ggml_metal_init: GPU name:   Apple M4
0.00.057.219 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.220 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.220 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.220 I ggml_metal_init: simdgroup reduction   = true
0.00.057.220 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.221 I ggml_metal_init: has bfloat            = true
0.00.057.221 I ggml_metal_init: use bfloat            = true
0.00.057.221 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.222 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.929 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.085.592 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.609 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.628 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.086.754 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.086.756 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.086.756 I llama_init_from_model: graph nodes  = 967
0.00.086.756 I llama_init_from_model: graph splits = 2
0.00.086.759 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.086.912 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.086.913 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.625.737 I main: llama threadpool init, n_threads = 4
0.00.625.781 I 
0.00.625.817 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.625.819 I 
0.00.625.973 I sampler seed: 1234
0.00.625.979 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.625.990 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.625.990 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.625.990 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.385.320 I llama_perf_sampler_print:    sampling time =       1.25 ms /    71 runs   (    0.02 ms per token, 56664.01 tokens per second)
0.01.385.321 I llama_perf_context_print:        load time =     613.69 ms
0.01.385.322 I llama_perf_context_print: prompt eval time =      46.97 ms /     7 tokens (    6.71 ms per token,   149.02 tokens per second)
0.01.385.322 I llama_perf_context_print:        eval time =     709.27 ms /    63 runs   (   11.26 ms per token,    88.82 tokens per second)
0.01.385.323 I llama_perf_context_print:       total time =     759.59 ms /    70 tokens
0.01.385.554 I ggml_metal_free: deallocating

real	0m1.405s
user	0m0.108s
sys	0m0.149s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4507 (4dd34ff8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.075 I main: llama backend init
0.00.000.077 I main: load the model and apply lora adapter, if any
0.00.010.988 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.476 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.018.481 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.482 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.483 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.483 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.483 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.484 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.486 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.486 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.486 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.487 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.487 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.487 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.488 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.491 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.492 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.492 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.309 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.345 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.105 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.106 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.106 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.106 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.107 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.107 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.027.107 I llama_model_loader: - type  f32:  194 tensors
0.00.027.108 I llama_model_loader: - type q5_K:   61 tensors
0.00.027.108 I llama_model_loader: - type q6_K:   37 tensors
0.00.027.109 I print_info: file format = GGUF V3 (latest)
0.00.027.109 I print_info: file type   = Q5_K - Medium
0.00.027.110 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.045.789 I load: special tokens cache size = 25
0.00.051.718 I load: token to piece cache size = 0.2984 MB
0.00.051.722 I print_info: arch             = gptneox
0.00.051.722 I print_info: vocab_only       = 0
0.00.051.722 I print_info: n_ctx_train      = 2048
0.00.051.722 I print_info: n_embd           = 2048
0.00.051.722 I print_info: n_layer          = 24
0.00.051.725 I print_info: n_head           = 16
0.00.051.726 I print_info: n_head_kv        = 16
0.00.051.727 I print_info: n_rot            = 32
0.00.051.727 I print_info: n_swa            = 0
0.00.051.727 I print_info: n_embd_head_k    = 128
0.00.051.727 I print_info: n_embd_head_v    = 128
0.00.051.728 I print_info: n_gqa            = 1
0.00.051.728 I print_info: n_embd_k_gqa     = 2048
0.00.051.729 I print_info: n_embd_v_gqa     = 2048
0.00.051.730 I print_info: f_norm_eps       = 1.0e-05
0.00.051.730 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.730 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.730 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.731 I print_info: f_logit_scale    = 0.0e+00
0.00.051.731 I print_info: n_ff             = 8192
0.00.051.734 I print_info: n_expert         = 0
0.00.051.734 I print_info: n_expert_used    = 0
0.00.051.734 I print_info: causal attn      = 1
0.00.051.734 I print_info: pooling type     = 0
0.00.051.736 I print_info: rope type        = 2
0.00.051.737 I print_info: rope scaling     = linear
0.00.051.738 I print_info: freq_base_train  = 10000.0
0.00.051.738 I print_info: freq_scale_train = 1
0.00.051.738 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.738 I print_info: rope_finetuned   = unknown
0.00.051.738 I print_info: ssm_d_conv       = 0
0.00.051.738 I print_info: ssm_d_inner      = 0
0.00.051.739 I print_info: ssm_d_state      = 0
0.00.051.739 I print_info: ssm_dt_rank      = 0
0.00.051.739 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.739 I print_info: model type       = 1.4B
0.00.051.739 I print_info: model params     = 1.41 B
0.00.051.740 I print_info: general.name     = 1.4B
0.00.051.740 I print_info: vocab type       = BPE
0.00.051.740 I print_info: n_vocab          = 50304
0.00.051.740 I print_info: n_merges         = 50009
0.00.051.741 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.742 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.743 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.743 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.743 I print_info: LF token         = 128 'Ä'
0.00.051.743 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.743 I print_info: max token length = 1024
0.00.053.732 I load_tensors: offloading 24 repeating layers to GPU
0.00.053.732 I load_tensors: offloading output layer to GPU
0.00.053.732 I load_tensors: offloaded 25/25 layers to GPU
0.00.053.743 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.053.744 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.054.016 I llama_init_from_model: n_seq_max     = 1
0.00.054.016 I llama_init_from_model: n_ctx         = 2048
0.00.054.017 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.054.017 I llama_init_from_model: n_batch       = 2048
0.00.054.017 I llama_init_from_model: n_ubatch      = 512
0.00.054.017 I llama_init_from_model: flash_attn    = 0
0.00.054.017 I llama_init_from_model: freq_base     = 10000.0
0.00.054.018 I llama_init_from_model: freq_scale    = 1
0.00.054.018 I ggml_metal_init: allocating
0.00.054.021 I ggml_metal_init: found device: Apple M4
0.00.054.023 I ggml_metal_init: picking default device: Apple M4
0.00.054.606 I ggml_metal_init: using embedded metal library
0.00.057.071 I ggml_metal_init: GPU name:   Apple M4
0.00.057.072 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.073 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.073 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.073 I ggml_metal_init: simdgroup reduction   = true
0.00.057.074 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.074 I ggml_metal_init: has bfloat            = true
0.00.057.074 I ggml_metal_init: use bfloat            = true
0.00.057.074 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.075 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.624 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.087.026 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.087.048 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.087.070 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.088.112 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.088.113 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.088.113 I llama_init_from_model: graph nodes  = 967
0.00.088.114 I llama_init_from_model: graph splits = 2
0.00.088.117 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.088.243 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.088.243 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.723.049 I main: llama threadpool init, n_threads = 4
0.00.723.088 I 
0.00.723.117 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.723.118 I 
0.00.723.343 I sampler seed: 1234
0.00.723.348 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.723.387 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.723.388 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.723.388 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.574.188 I llama_perf_sampler_print:    sampling time =       1.15 ms /    71 runs   (    0.02 ms per token, 61578.49 tokens per second)
0.01.574.188 I llama_perf_context_print:        load time =     712.06 ms
0.01.574.190 I llama_perf_context_print: prompt eval time =      55.59 ms /     7 tokens (    7.94 ms per token,   125.91 tokens per second)
0.01.574.191 I llama_perf_context_print:        eval time =     792.20 ms /    63 runs   (   12.57 ms per token,    79.53 tokens per second)
0.01.574.191 I llama_perf_context_print:       total time =     851.14 ms /    70 tokens
0.01.574.395 I ggml_metal_free: deallocating

real	0m1.595s
user	0m0.108s
sys	0m0.151s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4507 (4dd34ff8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.076 I main: llama backend init
0.00.000.078 I main: load the model and apply lora adapter, if any
0.00.010.580 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.158 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.018.163 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.169 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.169 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.170 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.170 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.170 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.173 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.174 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.174 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.174 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.175 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.175 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.176 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.177 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.178 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.178 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.998 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.054 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.870 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.872 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.872 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.872 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.873 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.873 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.026.873 I llama_model_loader: - type  f32:  194 tensors
0.00.026.874 I llama_model_loader: - type q6_K:   98 tensors
0.00.026.874 I print_info: file format = GGUF V3 (latest)
0.00.026.874 I print_info: file type   = Q6_K
0.00.026.875 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.045.371 I load: special tokens cache size = 25
0.00.051.384 I load: token to piece cache size = 0.2984 MB
0.00.051.387 I print_info: arch             = gptneox
0.00.051.387 I print_info: vocab_only       = 0
0.00.051.387 I print_info: n_ctx_train      = 2048
0.00.051.387 I print_info: n_embd           = 2048
0.00.051.388 I print_info: n_layer          = 24
0.00.051.390 I print_info: n_head           = 16
0.00.051.391 I print_info: n_head_kv        = 16
0.00.051.391 I print_info: n_rot            = 32
0.00.051.391 I print_info: n_swa            = 0
0.00.051.392 I print_info: n_embd_head_k    = 128
0.00.051.392 I print_info: n_embd_head_v    = 128
0.00.051.393 I print_info: n_gqa            = 1
0.00.051.393 I print_info: n_embd_k_gqa     = 2048
0.00.051.394 I print_info: n_embd_v_gqa     = 2048
0.00.051.395 I print_info: f_norm_eps       = 1.0e-05
0.00.051.395 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.395 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.395 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.396 I print_info: f_logit_scale    = 0.0e+00
0.00.051.396 I print_info: n_ff             = 8192
0.00.051.397 I print_info: n_expert         = 0
0.00.051.397 I print_info: n_expert_used    = 0
0.00.051.397 I print_info: causal attn      = 1
0.00.051.397 I print_info: pooling type     = 0
0.00.051.397 I print_info: rope type        = 2
0.00.051.397 I print_info: rope scaling     = linear
0.00.051.398 I print_info: freq_base_train  = 10000.0
0.00.051.398 I print_info: freq_scale_train = 1
0.00.051.398 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.399 I print_info: rope_finetuned   = unknown
0.00.051.399 I print_info: ssm_d_conv       = 0
0.00.051.399 I print_info: ssm_d_inner      = 0
0.00.051.401 I print_info: ssm_d_state      = 0
0.00.051.401 I print_info: ssm_dt_rank      = 0
0.00.051.401 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.401 I print_info: model type       = 1.4B
0.00.051.402 I print_info: model params     = 1.41 B
0.00.051.402 I print_info: general.name     = 1.4B
0.00.051.403 I print_info: vocab type       = BPE
0.00.051.403 I print_info: n_vocab          = 50304
0.00.051.403 I print_info: n_merges         = 50009
0.00.051.403 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.403 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.404 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.404 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.404 I print_info: LF token         = 128 'Ä'
0.00.051.404 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.404 I print_info: max token length = 1024
0.00.053.354 I load_tensors: offloading 24 repeating layers to GPU
0.00.053.354 I load_tensors: offloading output layer to GPU
0.00.053.354 I load_tensors: offloaded 25/25 layers to GPU
0.00.053.365 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.053.366 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.053.637 I llama_init_from_model: n_seq_max     = 1
0.00.053.638 I llama_init_from_model: n_ctx         = 2048
0.00.053.638 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.053.638 I llama_init_from_model: n_batch       = 2048
0.00.053.638 I llama_init_from_model: n_ubatch      = 512
0.00.053.638 I llama_init_from_model: flash_attn    = 0
0.00.053.639 I llama_init_from_model: freq_base     = 10000.0
0.00.053.639 I llama_init_from_model: freq_scale    = 1
0.00.053.639 I ggml_metal_init: allocating
0.00.053.647 I ggml_metal_init: found device: Apple M4
0.00.053.649 I ggml_metal_init: picking default device: Apple M4
0.00.054.230 I ggml_metal_init: using embedded metal library
0.00.056.569 I ggml_metal_init: GPU name:   Apple M4
0.00.056.571 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.571 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.571 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.572 I ggml_metal_init: simdgroup reduction   = true
0.00.056.572 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.572 I ggml_metal_init: has bfloat            = true
0.00.056.572 I ggml_metal_init: use bfloat            = true
0.00.056.573 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.573 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.217 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.085.638 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.659 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.679 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.086.765 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.086.766 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.086.766 I llama_init_from_model: graph nodes  = 967
0.00.086.767 I llama_init_from_model: graph splits = 2
0.00.086.769 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.086.886 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.086.887 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.754.091 I main: llama threadpool init, n_threads = 4
0.00.754.130 I 
0.00.754.162 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.754.163 I 
0.00.754.385 I sampler seed: 1234
0.00.754.390 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.754.432 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.754.450 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.754.450 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.626.542 I llama_perf_sampler_print:    sampling time =       1.21 ms /    71 runs   (    0.02 ms per token, 58872.31 tokens per second)
0.01.626.543 I llama_perf_context_print:        load time =     743.51 ms
0.01.626.544 I llama_perf_context_print: prompt eval time =      54.41 ms /     7 tokens (    7.77 ms per token,   128.66 tokens per second)
0.01.626.544 I llama_perf_context_print:        eval time =     814.76 ms /    63 runs   (   12.93 ms per token,    77.32 tokens per second)
0.01.626.545 I llama_perf_context_print:       total time =     872.45 ms /    70 tokens
0.01.626.750 I ggml_metal_free: deallocating

real	0m1.646s
user	0m0.108s
sys	0m0.168s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.586 I build: 4507 (4dd34ff8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.026.448 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.039.825 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.039.847 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.039.851 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.039.851 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.039.852 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.039.853 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.039.853 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.039.856 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.039.856 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.039.861 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.039.861 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.039.862 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.039.863 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.039.864 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.039.868 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.039.868 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.039.869 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.049.393 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.051.614 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.059.159 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.059.162 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.059.162 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.059.163 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.059.163 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.059.165 I llama_model_loader: - type  f32:  194 tensors
0.00.059.165 I llama_model_loader: - type  f16:   98 tensors
0.00.059.166 I print_info: file format = GGUF V3 (latest)
0.00.059.170 I print_info: file type   = all F32 (guessed)
0.00.059.172 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.086.810 I load: special tokens cache size = 25
0.00.093.636 I load: token to piece cache size = 0.2984 MB
0.00.093.639 I print_info: arch             = gptneox
0.00.093.639 I print_info: vocab_only       = 0
0.00.093.639 I print_info: n_ctx_train      = 2048
0.00.093.639 I print_info: n_embd           = 2048
0.00.093.639 I print_info: n_layer          = 24
0.00.093.642 I print_info: n_head           = 16
0.00.093.643 I print_info: n_head_kv        = 16
0.00.093.643 I print_info: n_rot            = 32
0.00.093.643 I print_info: n_swa            = 0
0.00.093.644 I print_info: n_embd_head_k    = 128
0.00.093.644 I print_info: n_embd_head_v    = 128
0.00.093.645 I print_info: n_gqa            = 1
0.00.093.645 I print_info: n_embd_k_gqa     = 2048
0.00.093.646 I print_info: n_embd_v_gqa     = 2048
0.00.093.647 I print_info: f_norm_eps       = 1.0e-05
0.00.093.647 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.093.649 I print_info: f_clamp_kqv      = 0.0e+00
0.00.093.649 I print_info: f_max_alibi_bias = 0.0e+00
0.00.093.649 I print_info: f_logit_scale    = 0.0e+00
0.00.093.650 I print_info: n_ff             = 8192
0.00.093.650 I print_info: n_expert         = 0
0.00.093.650 I print_info: n_expert_used    = 0
0.00.093.650 I print_info: causal attn      = 1
0.00.093.650 I print_info: pooling type     = 0
0.00.093.650 I print_info: rope type        = 2
0.00.093.651 I print_info: rope scaling     = linear
0.00.093.651 I print_info: freq_base_train  = 10000.0
0.00.093.651 I print_info: freq_scale_train = 1
0.00.093.651 I print_info: n_ctx_orig_yarn  = 2048
0.00.093.652 I print_info: rope_finetuned   = unknown
0.00.093.652 I print_info: ssm_d_conv       = 0
0.00.093.652 I print_info: ssm_d_inner      = 0
0.00.093.652 I print_info: ssm_d_state      = 0
0.00.093.652 I print_info: ssm_dt_rank      = 0
0.00.093.652 I print_info: ssm_dt_b_c_rms   = 0
0.00.093.652 I print_info: model type       = 1.4B
0.00.093.653 I print_info: model params     = 1.41 B
0.00.093.653 I print_info: general.name     = 1.4B
0.00.093.653 I print_info: vocab type       = BPE
0.00.093.653 I print_info: n_vocab          = 50304
0.00.093.654 I print_info: n_merges         = 50009
0.00.093.654 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.093.654 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.093.654 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.093.654 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.093.655 I print_info: LF token         = 128 'Ä'
0.00.093.655 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.093.655 I print_info: max token length = 1024
0.00.095.740 I load_tensors: offloading 24 repeating layers to GPU
0.00.095.740 I load_tensors: offloading output layer to GPU
0.00.095.740 I load_tensors: offloaded 25/25 layers to GPU
0.00.095.746 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.095.746 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.096.033 I llama_init_from_model: n_seq_max     = 1
0.00.096.034 I llama_init_from_model: n_ctx         = 128
0.00.096.034 I llama_init_from_model: n_ctx_per_seq = 128
0.00.096.034 I llama_init_from_model: n_batch       = 128
0.00.096.034 I llama_init_from_model: n_ubatch      = 128
0.00.096.035 I llama_init_from_model: flash_attn    = 0
0.00.096.035 I llama_init_from_model: freq_base     = 10000.0
0.00.096.035 I llama_init_from_model: freq_scale    = 1
0.00.096.036 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.096.036 I ggml_metal_init: allocating
0.00.096.039 I ggml_metal_init: found device: Apple M4
0.00.096.042 I ggml_metal_init: picking default device: Apple M4
0.00.096.691 I ggml_metal_init: using embedded metal library
0.00.099.279 I ggml_metal_init: GPU name:   Apple M4
0.00.099.281 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.099.282 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.099.282 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.099.282 I ggml_metal_init: simdgroup reduction   = true
0.00.099.282 I ggml_metal_init: simdgroup matrix mul. = true
0.00.099.282 I ggml_metal_init: has bfloat            = true
0.00.099.282 I ggml_metal_init: use bfloat            = true
0.00.099.283 I ggml_metal_init: hasUnifiedMemory      = true
0.00.099.283 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.109.039 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.110.310 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.110.322 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.110.337 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.111.195 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.111.196 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.111.197 I llama_init_from_model: graph nodes  = 967
0.00.111.197 I llama_init_from_model: graph splits = 2
0.00.111.198 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.111.198 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.844.720 I 
0.00.844.816 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.844.824 I perplexity: tokenizing the input ..
0.00.856.956 I perplexity: tokenization took 12.13 ms
0.00.856.969 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.976.625 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.00.979.539 I Final estimate: PPL = 10.1498 +/- 3.22650

0.00.979.584 I llama_perf_context_print:        load time =     818.25 ms
0.00.979.586 I llama_perf_context_print: prompt eval time =     119.27 ms /   128 tokens (    0.93 ms per token,  1073.18 tokens per second)
0.00.979.587 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.979.588 I llama_perf_context_print:       total time =     134.87 ms /   129 tokens
0.00.980.190 I ggml_metal_free: deallocating

real	0m1.173s
user	0m0.126s
sys	0m0.176s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.139 I build: 4507 (4dd34ff8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.831 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.022.552 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.022.560 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.022.562 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.022.569 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.022.570 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.022.570 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.022.571 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.022.572 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.022.572 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.022.573 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.022.573 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.022.574 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.022.574 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.022.575 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.022.577 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.022.577 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.022.578 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.028.446 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.030.012 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.035.863 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.035.864 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.035.865 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.035.865 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.035.866 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.035.866 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.035.867 I llama_model_loader: - type  f32:  194 tensors
0.00.035.867 I llama_model_loader: - type q8_0:   98 tensors
0.00.035.868 I print_info: file format = GGUF V3 (latest)
0.00.035.868 I print_info: file type   = Q8_0
0.00.035.869 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.060.796 I load: special tokens cache size = 25
0.00.066.997 I load: token to piece cache size = 0.2984 MB
0.00.067.000 I print_info: arch             = gptneox
0.00.067.000 I print_info: vocab_only       = 0
0.00.067.000 I print_info: n_ctx_train      = 2048
0.00.067.000 I print_info: n_embd           = 2048
0.00.067.001 I print_info: n_layer          = 24
0.00.067.004 I print_info: n_head           = 16
0.00.067.005 I print_info: n_head_kv        = 16
0.00.067.005 I print_info: n_rot            = 32
0.00.067.005 I print_info: n_swa            = 0
0.00.067.005 I print_info: n_embd_head_k    = 128
0.00.067.005 I print_info: n_embd_head_v    = 128
0.00.067.006 I print_info: n_gqa            = 1
0.00.067.007 I print_info: n_embd_k_gqa     = 2048
0.00.067.007 I print_info: n_embd_v_gqa     = 2048
0.00.067.008 I print_info: f_norm_eps       = 1.0e-05
0.00.067.016 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.067.018 I print_info: f_clamp_kqv      = 0.0e+00
0.00.067.018 I print_info: f_max_alibi_bias = 0.0e+00
0.00.067.020 I print_info: f_logit_scale    = 0.0e+00
0.00.067.026 I print_info: n_ff             = 8192
0.00.067.028 I print_info: n_expert         = 0
0.00.067.028 I print_info: n_expert_used    = 0
0.00.067.028 I print_info: causal attn      = 1
0.00.067.028 I print_info: pooling type     = 0
0.00.067.028 I print_info: rope type        = 2
0.00.067.029 I print_info: rope scaling     = linear
0.00.067.029 I print_info: freq_base_train  = 10000.0
0.00.067.029 I print_info: freq_scale_train = 1
0.00.067.029 I print_info: n_ctx_orig_yarn  = 2048
0.00.067.030 I print_info: rope_finetuned   = unknown
0.00.067.030 I print_info: ssm_d_conv       = 0
0.00.067.030 I print_info: ssm_d_inner      = 0
0.00.067.030 I print_info: ssm_d_state      = 0
0.00.067.030 I print_info: ssm_dt_rank      = 0
0.00.067.031 I print_info: ssm_dt_b_c_rms   = 0
0.00.067.031 I print_info: model type       = 1.4B
0.00.067.031 I print_info: model params     = 1.41 B
0.00.067.031 I print_info: general.name     = 1.4B
0.00.067.032 I print_info: vocab type       = BPE
0.00.067.032 I print_info: n_vocab          = 50304
0.00.067.033 I print_info: n_merges         = 50009
0.00.067.033 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.067.034 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.067.034 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.067.035 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.067.035 I print_info: LF token         = 128 'Ä'
0.00.067.035 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.067.035 I print_info: max token length = 1024
0.00.069.423 I load_tensors: offloading 24 repeating layers to GPU
0.00.069.424 I load_tensors: offloading output layer to GPU
0.00.069.424 I load_tensors: offloaded 25/25 layers to GPU
0.00.069.435 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.069.436 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.00.069.739 I llama_init_from_model: n_seq_max     = 1
0.00.069.739 I llama_init_from_model: n_ctx         = 128
0.00.069.740 I llama_init_from_model: n_ctx_per_seq = 128
0.00.069.740 I llama_init_from_model: n_batch       = 128
0.00.069.740 I llama_init_from_model: n_ubatch      = 128
0.00.069.740 I llama_init_from_model: flash_attn    = 0
0.00.069.741 I llama_init_from_model: freq_base     = 10000.0
0.00.069.741 I llama_init_from_model: freq_scale    = 1
0.00.069.741 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.069.741 I ggml_metal_init: allocating
0.00.069.744 I ggml_metal_init: found device: Apple M4
0.00.069.746 I ggml_metal_init: picking default device: Apple M4
0.00.070.414 I ggml_metal_init: using embedded metal library
0.00.073.056 I ggml_metal_init: GPU name:   Apple M4
0.00.073.058 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.073.059 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.073.059 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.073.059 I ggml_metal_init: simdgroup reduction   = true
0.00.073.059 I ggml_metal_init: simdgroup matrix mul. = true
0.00.073.060 I ggml_metal_init: has bfloat            = true
0.00.073.060 I ggml_metal_init: use bfloat            = true
0.00.073.060 I ggml_metal_init: hasUnifiedMemory      = true
0.00.073.061 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.083.484 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.084.873 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.084.889 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.084.908 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.085.879 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.085.880 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.085.881 I llama_init_from_model: graph nodes  = 967
0.00.085.881 I llama_init_from_model: graph splits = 2
0.00.085.883 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.085.883 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.930.015 I 
0.00.930.046 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.930.050 I perplexity: tokenizing the input ..
0.00.938.119 I perplexity: tokenization took 8.068 ms
0.00.938.130 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.062.639 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.063.796 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.063.822 I llama_perf_context_print:        load time =     918.18 ms
0.01.063.823 I llama_perf_context_print: prompt eval time =     124.25 ms /   128 tokens (    0.97 ms per token,  1030.17 tokens per second)
0.01.063.824 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.063.824 I llama_perf_context_print:       total time =     133.81 ms /   129 tokens
0.01.064.187 I ggml_metal_free: deallocating

real	0m1.083s
user	0m0.097s
sys	0m0.154s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.083 I build: 4507 (4dd34ff8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.278 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.385 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.017.389 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.390 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.394 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.395 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.395 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.395 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.396 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.397 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.397 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.397 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.398 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.398 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.399 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.400 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.401 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.401 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.095 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.158 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.867 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.868 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.869 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.869 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.869 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.869 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.025.870 I llama_model_loader: - type  f32:  194 tensors
0.00.025.870 I llama_model_loader: - type q4_0:   97 tensors
0.00.025.870 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.871 I print_info: file format = GGUF V3 (latest)
0.00.025.871 I print_info: file type   = Q4_0
0.00.025.872 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.044.109 I load: special tokens cache size = 25
0.00.050.049 I load: token to piece cache size = 0.2984 MB
0.00.050.052 I print_info: arch             = gptneox
0.00.050.053 I print_info: vocab_only       = 0
0.00.050.053 I print_info: n_ctx_train      = 2048
0.00.050.053 I print_info: n_embd           = 2048
0.00.050.053 I print_info: n_layer          = 24
0.00.050.056 I print_info: n_head           = 16
0.00.050.057 I print_info: n_head_kv        = 16
0.00.050.057 I print_info: n_rot            = 32
0.00.050.058 I print_info: n_swa            = 0
0.00.050.058 I print_info: n_embd_head_k    = 128
0.00.050.058 I print_info: n_embd_head_v    = 128
0.00.050.059 I print_info: n_gqa            = 1
0.00.050.059 I print_info: n_embd_k_gqa     = 2048
0.00.050.060 I print_info: n_embd_v_gqa     = 2048
0.00.050.061 I print_info: f_norm_eps       = 1.0e-05
0.00.050.061 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.062 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.062 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.063 I print_info: f_logit_scale    = 0.0e+00
0.00.050.064 I print_info: n_ff             = 8192
0.00.050.064 I print_info: n_expert         = 0
0.00.050.064 I print_info: n_expert_used    = 0
0.00.050.064 I print_info: causal attn      = 1
0.00.050.065 I print_info: pooling type     = 0
0.00.050.066 I print_info: rope type        = 2
0.00.050.067 I print_info: rope scaling     = linear
0.00.050.067 I print_info: freq_base_train  = 10000.0
0.00.050.067 I print_info: freq_scale_train = 1
0.00.050.068 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.068 I print_info: rope_finetuned   = unknown
0.00.050.068 I print_info: ssm_d_conv       = 0
0.00.050.068 I print_info: ssm_d_inner      = 0
0.00.050.068 I print_info: ssm_d_state      = 0
0.00.050.068 I print_info: ssm_dt_rank      = 0
0.00.050.068 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.069 I print_info: model type       = 1.4B
0.00.050.069 I print_info: model params     = 1.41 B
0.00.050.069 I print_info: general.name     = 1.4B
0.00.050.070 I print_info: vocab type       = BPE
0.00.050.070 I print_info: n_vocab          = 50304
0.00.050.070 I print_info: n_merges         = 50009
0.00.050.074 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.074 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.074 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.076 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.076 I print_info: LF token         = 128 'Ä'
0.00.050.076 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.076 I print_info: max token length = 1024
0.00.051.989 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.989 I load_tensors: offloading output layer to GPU
0.00.051.990 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.001 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.052.002 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.052.301 I llama_init_from_model: n_seq_max     = 1
0.00.052.302 I llama_init_from_model: n_ctx         = 128
0.00.052.302 I llama_init_from_model: n_ctx_per_seq = 128
0.00.052.302 I llama_init_from_model: n_batch       = 128
0.00.052.302 I llama_init_from_model: n_ubatch      = 128
0.00.052.303 I llama_init_from_model: flash_attn    = 0
0.00.052.303 I llama_init_from_model: freq_base     = 10000.0
0.00.052.303 I llama_init_from_model: freq_scale    = 1
0.00.052.304 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.304 I ggml_metal_init: allocating
0.00.052.306 I ggml_metal_init: found device: Apple M4
0.00.052.308 I ggml_metal_init: picking default device: Apple M4
0.00.052.874 I ggml_metal_init: using embedded metal library
0.00.055.233 I ggml_metal_init: GPU name:   Apple M4
0.00.055.234 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.235 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.235 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.235 I ggml_metal_init: simdgroup reduction   = true
0.00.055.235 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.236 I ggml_metal_init: has bfloat            = true
0.00.055.236 I ggml_metal_init: use bfloat            = true
0.00.055.236 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.236 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.334 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.065.570 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.585 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.600 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.066.563 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.066.564 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.066.564 I llama_init_from_model: graph nodes  = 967
0.00.066.564 I llama_init_from_model: graph splits = 2
0.00.066.565 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.566 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.609.545 I 
0.00.609.660 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.609.694 I perplexity: tokenizing the input ..
0.00.617.635 I perplexity: tokenization took 7.94 ms
0.00.617.639 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.740.212 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.741.357 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.741.388 I llama_perf_context_print:        load time =     599.26 ms
0.00.741.389 I llama_perf_context_print: prompt eval time =     122.35 ms /   128 tokens (    0.96 ms per token,  1046.20 tokens per second)
0.00.741.390 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.741.390 I llama_perf_context_print:       total time =     131.85 ms /   129 tokens
0.00.741.870 I ggml_metal_free: deallocating

real	0m0.758s
user	0m0.075s
sys	0m0.098s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.085 I build: 4507 (4dd34ff8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.866 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.119 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.123 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.126 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.127 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.127 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.128 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.128 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.130 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.131 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.131 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.131 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.132 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.132 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.133 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.134 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.134 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.135 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.915 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.910 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.682 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.683 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.684 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.684 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.684 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.685 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.024.685 I llama_model_loader: - type  f32:  194 tensors
0.00.024.685 I llama_model_loader: - type q4_1:   97 tensors
0.00.024.686 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.686 I print_info: file format = GGUF V3 (latest)
0.00.024.687 I print_info: file type   = Q4_1
0.00.024.687 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.043.745 I load: special tokens cache size = 25
0.00.049.494 I load: token to piece cache size = 0.2984 MB
0.00.049.498 I print_info: arch             = gptneox
0.00.049.498 I print_info: vocab_only       = 0
0.00.049.498 I print_info: n_ctx_train      = 2048
0.00.049.498 I print_info: n_embd           = 2048
0.00.049.499 I print_info: n_layer          = 24
0.00.049.502 I print_info: n_head           = 16
0.00.049.503 I print_info: n_head_kv        = 16
0.00.049.503 I print_info: n_rot            = 32
0.00.049.503 I print_info: n_swa            = 0
0.00.049.503 I print_info: n_embd_head_k    = 128
0.00.049.503 I print_info: n_embd_head_v    = 128
0.00.049.504 I print_info: n_gqa            = 1
0.00.049.505 I print_info: n_embd_k_gqa     = 2048
0.00.049.505 I print_info: n_embd_v_gqa     = 2048
0.00.049.506 I print_info: f_norm_eps       = 1.0e-05
0.00.049.506 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.506 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.506 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.507 I print_info: f_logit_scale    = 0.0e+00
0.00.049.508 I print_info: n_ff             = 8192
0.00.049.509 I print_info: n_expert         = 0
0.00.049.509 I print_info: n_expert_used    = 0
0.00.049.509 I print_info: causal attn      = 1
0.00.049.509 I print_info: pooling type     = 0
0.00.049.509 I print_info: rope type        = 2
0.00.049.512 I print_info: rope scaling     = linear
0.00.049.512 I print_info: freq_base_train  = 10000.0
0.00.049.512 I print_info: freq_scale_train = 1
0.00.049.513 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.513 I print_info: rope_finetuned   = unknown
0.00.049.513 I print_info: ssm_d_conv       = 0
0.00.049.513 I print_info: ssm_d_inner      = 0
0.00.049.513 I print_info: ssm_d_state      = 0
0.00.049.513 I print_info: ssm_dt_rank      = 0
0.00.049.514 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.514 I print_info: model type       = 1.4B
0.00.049.514 I print_info: model params     = 1.41 B
0.00.049.514 I print_info: general.name     = 1.4B
0.00.049.515 I print_info: vocab type       = BPE
0.00.049.515 I print_info: n_vocab          = 50304
0.00.049.516 I print_info: n_merges         = 50009
0.00.049.516 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.518 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.518 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.518 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.518 I print_info: LF token         = 128 'Ä'
0.00.049.519 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.519 I print_info: max token length = 1024
0.00.051.472 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.473 I load_tensors: offloading output layer to GPU
0.00.051.473 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.483 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.051.484 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.051.761 I llama_init_from_model: n_seq_max     = 1
0.00.051.762 I llama_init_from_model: n_ctx         = 128
0.00.051.762 I llama_init_from_model: n_ctx_per_seq = 128
0.00.051.762 I llama_init_from_model: n_batch       = 128
0.00.051.762 I llama_init_from_model: n_ubatch      = 128
0.00.051.762 I llama_init_from_model: flash_attn    = 0
0.00.051.763 I llama_init_from_model: freq_base     = 10000.0
0.00.051.763 I llama_init_from_model: freq_scale    = 1
0.00.051.763 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.764 I ggml_metal_init: allocating
0.00.051.767 I ggml_metal_init: found device: Apple M4
0.00.051.769 I ggml_metal_init: picking default device: Apple M4
0.00.052.339 I ggml_metal_init: using embedded metal library
0.00.054.661 I ggml_metal_init: GPU name:   Apple M4
0.00.054.662 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.663 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.663 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.663 I ggml_metal_init: simdgroup reduction   = true
0.00.054.664 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.664 I ggml_metal_init: has bfloat            = true
0.00.054.664 I ggml_metal_init: use bfloat            = true
0.00.054.664 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.665 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.206 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.065.471 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.487 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.504 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.066.356 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.066.357 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.066.357 I llama_init_from_model: graph nodes  = 967
0.00.066.358 I llama_init_from_model: graph splits = 2
0.00.066.359 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.359 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.679.275 I 
0.00.679.315 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.679.318 I perplexity: tokenizing the input ..
0.00.687.046 I perplexity: tokenization took 7.726 ms
0.00.687.051 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.810.058 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.811.212 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.811.239 I llama_perf_context_print:        load time =     670.41 ms
0.00.811.240 I llama_perf_context_print: prompt eval time =     122.77 ms /   128 tokens (    0.96 ms per token,  1042.61 tokens per second)
0.00.811.241 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.811.241 I llama_perf_context_print:       total time =     131.96 ms /   129 tokens
0.00.811.702 I ggml_metal_free: deallocating

real	0m0.825s
user	0m0.077s
sys	0m0.113s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.084 I build: 4507 (4dd34ff8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.906 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.503 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.016.507 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.508 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.509 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.509 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.510 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.510 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.511 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.511 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.512 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.512 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.514 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.514 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.515 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.516 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.517 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.517 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.531 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.610 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.570 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.571 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.571 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.571 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.572 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.572 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.025.572 I llama_model_loader: - type  f32:  194 tensors
0.00.025.573 I llama_model_loader: - type q5_0:   97 tensors
0.00.025.573 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.573 I print_info: file format = GGUF V3 (latest)
0.00.025.574 I print_info: file type   = Q5_0
0.00.025.574 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.043.834 I load: special tokens cache size = 25
0.00.049.528 I load: token to piece cache size = 0.2984 MB
0.00.049.531 I print_info: arch             = gptneox
0.00.049.532 I print_info: vocab_only       = 0
0.00.049.532 I print_info: n_ctx_train      = 2048
0.00.049.532 I print_info: n_embd           = 2048
0.00.049.532 I print_info: n_layer          = 24
0.00.049.535 I print_info: n_head           = 16
0.00.049.536 I print_info: n_head_kv        = 16
0.00.049.536 I print_info: n_rot            = 32
0.00.049.536 I print_info: n_swa            = 0
0.00.049.537 I print_info: n_embd_head_k    = 128
0.00.049.537 I print_info: n_embd_head_v    = 128
0.00.049.537 I print_info: n_gqa            = 1
0.00.049.538 I print_info: n_embd_k_gqa     = 2048
0.00.049.540 I print_info: n_embd_v_gqa     = 2048
0.00.049.540 I print_info: f_norm_eps       = 1.0e-05
0.00.049.540 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.541 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.541 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.541 I print_info: f_logit_scale    = 0.0e+00
0.00.049.542 I print_info: n_ff             = 8192
0.00.049.542 I print_info: n_expert         = 0
0.00.049.542 I print_info: n_expert_used    = 0
0.00.049.542 I print_info: causal attn      = 1
0.00.049.542 I print_info: pooling type     = 0
0.00.049.542 I print_info: rope type        = 2
0.00.049.543 I print_info: rope scaling     = linear
0.00.049.543 I print_info: freq_base_train  = 10000.0
0.00.049.544 I print_info: freq_scale_train = 1
0.00.049.545 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.545 I print_info: rope_finetuned   = unknown
0.00.049.545 I print_info: ssm_d_conv       = 0
0.00.049.545 I print_info: ssm_d_inner      = 0
0.00.049.545 I print_info: ssm_d_state      = 0
0.00.049.545 I print_info: ssm_dt_rank      = 0
0.00.049.545 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.546 I print_info: model type       = 1.4B
0.00.049.546 I print_info: model params     = 1.41 B
0.00.049.546 I print_info: general.name     = 1.4B
0.00.049.547 I print_info: vocab type       = BPE
0.00.049.547 I print_info: n_vocab          = 50304
0.00.049.547 I print_info: n_merges         = 50009
0.00.049.547 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.548 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.548 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.548 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.548 I print_info: LF token         = 128 'Ä'
0.00.049.549 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.549 I print_info: max token length = 1024
0.00.051.518 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.518 I load_tensors: offloading output layer to GPU
0.00.051.519 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.529 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.051.530 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.051.805 I llama_init_from_model: n_seq_max     = 1
0.00.051.805 I llama_init_from_model: n_ctx         = 128
0.00.051.805 I llama_init_from_model: n_ctx_per_seq = 128
0.00.051.806 I llama_init_from_model: n_batch       = 128
0.00.051.806 I llama_init_from_model: n_ubatch      = 128
0.00.051.806 I llama_init_from_model: flash_attn    = 0
0.00.051.806 I llama_init_from_model: freq_base     = 10000.0
0.00.051.806 I llama_init_from_model: freq_scale    = 1
0.00.051.807 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.807 I ggml_metal_init: allocating
0.00.051.809 I ggml_metal_init: found device: Apple M4
0.00.051.811 I ggml_metal_init: picking default device: Apple M4
0.00.052.376 I ggml_metal_init: using embedded metal library
0.00.054.730 I ggml_metal_init: GPU name:   Apple M4
0.00.054.731 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.731 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.732 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.732 I ggml_metal_init: simdgroup reduction   = true
0.00.054.732 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.732 I ggml_metal_init: has bfloat            = true
0.00.054.732 I ggml_metal_init: use bfloat            = true
0.00.054.733 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.733 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.323 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.064.589 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.064.605 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.064.630 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.065.493 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.065.494 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.065.494 I llama_init_from_model: graph nodes  = 967
0.00.065.495 I llama_init_from_model: graph splits = 2
0.00.065.496 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.065.496 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.701.804 I 
0.00.701.842 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.701.845 I perplexity: tokenizing the input ..
0.00.709.674 I perplexity: tokenization took 7.826 ms
0.00.709.677 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.844.364 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.845.624 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.845.649 I llama_perf_context_print:        load time =     691.89 ms
0.00.845.650 I llama_perf_context_print: prompt eval time =     134.46 ms /   128 tokens (    1.05 ms per token,   951.96 tokens per second)
0.00.845.651 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.845.651 I llama_perf_context_print:       total time =     143.85 ms /   129 tokens
0.00.846.087 I ggml_metal_free: deallocating

real	0m0.861s
user	0m0.075s
sys	0m0.106s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.083 I build: 4507 (4dd34ff8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.806 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.829 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.015.834 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.839 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.840 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.840 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.841 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.841 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.842 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.842 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.843 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.843 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.844 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.844 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.844 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.847 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.847 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.848 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.677 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.711 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.542 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.544 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.544 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.544 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.545 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.545 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.546 I llama_model_loader: - type  f32:  194 tensors
0.00.024.546 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.546 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.547 I print_info: file format = GGUF V3 (latest)
0.00.024.547 I print_info: file type   = Q5_1
0.00.024.548 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.043.574 I load: special tokens cache size = 25
0.00.049.512 I load: token to piece cache size = 0.2984 MB
0.00.049.516 I print_info: arch             = gptneox
0.00.049.516 I print_info: vocab_only       = 0
0.00.049.516 I print_info: n_ctx_train      = 2048
0.00.049.516 I print_info: n_embd           = 2048
0.00.049.516 I print_info: n_layer          = 24
0.00.049.519 I print_info: n_head           = 16
0.00.049.520 I print_info: n_head_kv        = 16
0.00.049.520 I print_info: n_rot            = 32
0.00.049.521 I print_info: n_swa            = 0
0.00.049.521 I print_info: n_embd_head_k    = 128
0.00.049.521 I print_info: n_embd_head_v    = 128
0.00.049.522 I print_info: n_gqa            = 1
0.00.049.523 I print_info: n_embd_k_gqa     = 2048
0.00.049.523 I print_info: n_embd_v_gqa     = 2048
0.00.049.524 I print_info: f_norm_eps       = 1.0e-05
0.00.049.524 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.524 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.524 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.525 I print_info: f_logit_scale    = 0.0e+00
0.00.049.525 I print_info: n_ff             = 8192
0.00.049.525 I print_info: n_expert         = 0
0.00.049.526 I print_info: n_expert_used    = 0
0.00.049.526 I print_info: causal attn      = 1
0.00.049.526 I print_info: pooling type     = 0
0.00.049.526 I print_info: rope type        = 2
0.00.049.534 I print_info: rope scaling     = linear
0.00.049.538 I print_info: freq_base_train  = 10000.0
0.00.049.539 I print_info: freq_scale_train = 1
0.00.049.539 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.539 I print_info: rope_finetuned   = unknown
0.00.049.539 I print_info: ssm_d_conv       = 0
0.00.049.540 I print_info: ssm_d_inner      = 0
0.00.049.540 I print_info: ssm_d_state      = 0
0.00.049.540 I print_info: ssm_dt_rank      = 0
0.00.049.540 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.543 I print_info: model type       = 1.4B
0.00.049.543 I print_info: model params     = 1.41 B
0.00.049.543 I print_info: general.name     = 1.4B
0.00.049.544 I print_info: vocab type       = BPE
0.00.049.544 I print_info: n_vocab          = 50304
0.00.049.544 I print_info: n_merges         = 50009
0.00.049.544 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.545 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.546 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.546 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.546 I print_info: LF token         = 128 'Ä'
0.00.049.548 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.548 I print_info: max token length = 1024
0.00.051.531 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.531 I load_tensors: offloading output layer to GPU
0.00.051.531 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.541 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.051.543 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.051.829 I llama_init_from_model: n_seq_max     = 1
0.00.051.830 I llama_init_from_model: n_ctx         = 128
0.00.051.830 I llama_init_from_model: n_ctx_per_seq = 128
0.00.051.830 I llama_init_from_model: n_batch       = 128
0.00.051.830 I llama_init_from_model: n_ubatch      = 128
0.00.051.830 I llama_init_from_model: flash_attn    = 0
0.00.051.831 I llama_init_from_model: freq_base     = 10000.0
0.00.051.831 I llama_init_from_model: freq_scale    = 1
0.00.051.831 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.832 I ggml_metal_init: allocating
0.00.051.834 I ggml_metal_init: found device: Apple M4
0.00.051.836 I ggml_metal_init: picking default device: Apple M4
0.00.052.401 I ggml_metal_init: using embedded metal library
0.00.054.715 I ggml_metal_init: GPU name:   Apple M4
0.00.054.716 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.716 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.717 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.717 I ggml_metal_init: simdgroup reduction   = true
0.00.054.717 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.717 I ggml_metal_init: has bfloat            = true
0.00.054.717 I ggml_metal_init: use bfloat            = true
0.00.054.718 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.718 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.087 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.065.321 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.327 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.340 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.066.184 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.066.185 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.066.185 I llama_init_from_model: graph nodes  = 967
0.00.066.185 I llama_init_from_model: graph splits = 2
0.00.066.187 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.187 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.657.510 I 
0.00.657.566 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.657.571 I perplexity: tokenizing the input ..
0.00.665.548 I perplexity: tokenization took 7.975 ms
0.00.665.557 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.800.597 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.801.823 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.801.850 I llama_perf_context_print:        load time =     648.70 ms
0.00.801.851 I llama_perf_context_print: prompt eval time =     134.81 ms /   128 tokens (    1.05 ms per token,   949.46 tokens per second)
0.00.801.851 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.801.852 I llama_perf_context_print:       total time =     144.34 ms /   129 tokens
0.00.802.258 I ggml_metal_free: deallocating

real	0m0.816s
user	0m0.077s
sys	0m0.118s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.088 I build: 4507 (4dd34ff8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.182 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.781 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.787 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.788 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.789 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.789 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.790 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.790 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.791 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.791 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.792 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.792 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.793 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.793 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.793 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.797 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.798 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.798 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.630 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.638 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.437 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.439 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.439 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.439 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.439 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.440 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.440 I llama_model_loader: - type  f32:  194 tensors
0.00.025.441 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.441 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.441 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.442 I print_info: file format = GGUF V3 (latest)
0.00.025.442 I print_info: file type   = Q2_K - Medium
0.00.025.445 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.044.548 I load: special tokens cache size = 25
0.00.050.448 I load: token to piece cache size = 0.2984 MB
0.00.050.451 I print_info: arch             = gptneox
0.00.050.451 I print_info: vocab_only       = 0
0.00.050.451 I print_info: n_ctx_train      = 2048
0.00.050.451 I print_info: n_embd           = 2048
0.00.050.452 I print_info: n_layer          = 24
0.00.050.454 I print_info: n_head           = 16
0.00.050.455 I print_info: n_head_kv        = 16
0.00.050.455 I print_info: n_rot            = 32
0.00.050.456 I print_info: n_swa            = 0
0.00.050.456 I print_info: n_embd_head_k    = 128
0.00.050.456 I print_info: n_embd_head_v    = 128
0.00.050.457 I print_info: n_gqa            = 1
0.00.050.458 I print_info: n_embd_k_gqa     = 2048
0.00.050.458 I print_info: n_embd_v_gqa     = 2048
0.00.050.459 I print_info: f_norm_eps       = 1.0e-05
0.00.050.459 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.460 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.460 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.460 I print_info: f_logit_scale    = 0.0e+00
0.00.050.461 I print_info: n_ff             = 8192
0.00.050.461 I print_info: n_expert         = 0
0.00.050.461 I print_info: n_expert_used    = 0
0.00.050.461 I print_info: causal attn      = 1
0.00.050.461 I print_info: pooling type     = 0
0.00.050.462 I print_info: rope type        = 2
0.00.050.462 I print_info: rope scaling     = linear
0.00.050.463 I print_info: freq_base_train  = 10000.0
0.00.050.463 I print_info: freq_scale_train = 1
0.00.050.463 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.464 I print_info: rope_finetuned   = unknown
0.00.050.464 I print_info: ssm_d_conv       = 0
0.00.050.464 I print_info: ssm_d_inner      = 0
0.00.050.464 I print_info: ssm_d_state      = 0
0.00.050.464 I print_info: ssm_dt_rank      = 0
0.00.050.464 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.466 I print_info: model type       = 1.4B
0.00.050.466 I print_info: model params     = 1.41 B
0.00.050.467 I print_info: general.name     = 1.4B
0.00.050.467 I print_info: vocab type       = BPE
0.00.050.467 I print_info: n_vocab          = 50304
0.00.050.467 I print_info: n_merges         = 50009
0.00.050.468 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.468 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.468 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.468 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.468 I print_info: LF token         = 128 'Ä'
0.00.050.469 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.471 I print_info: max token length = 1024
0.00.052.328 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.329 I load_tensors: offloading output layer to GPU
0.00.052.329 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.340 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.052.341 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.052.622 I llama_init_from_model: n_seq_max     = 1
0.00.052.622 I llama_init_from_model: n_ctx         = 128
0.00.052.622 I llama_init_from_model: n_ctx_per_seq = 128
0.00.052.623 I llama_init_from_model: n_batch       = 128
0.00.052.623 I llama_init_from_model: n_ubatch      = 128
0.00.052.623 I llama_init_from_model: flash_attn    = 0
0.00.052.623 I llama_init_from_model: freq_base     = 10000.0
0.00.052.624 I llama_init_from_model: freq_scale    = 1
0.00.052.624 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.624 I ggml_metal_init: allocating
0.00.052.627 I ggml_metal_init: found device: Apple M4
0.00.052.629 I ggml_metal_init: picking default device: Apple M4
0.00.053.229 I ggml_metal_init: using embedded metal library
0.00.055.582 I ggml_metal_init: GPU name:   Apple M4
0.00.055.583 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.584 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.584 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.584 I ggml_metal_init: simdgroup reduction   = true
0.00.055.585 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.585 I ggml_metal_init: has bfloat            = true
0.00.055.585 I ggml_metal_init: use bfloat            = true
0.00.055.585 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.586 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.129 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.361 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.376 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.394 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.067.265 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.067.266 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.067.267 I llama_init_from_model: graph nodes  = 967
0.00.067.267 I llama_init_from_model: graph splits = 2
0.00.067.268 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.268 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.374.760 I 
0.00.374.795 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.374.798 I perplexity: tokenizing the input ..
0.00.382.231 I perplexity: tokenization took 7.43 ms
0.00.382.235 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.514.848 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.516.016 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.516.042 I llama_perf_context_print:        load time =     364.57 ms
0.00.516.043 I llama_perf_context_print: prompt eval time =     132.39 ms /   128 tokens (    1.03 ms per token,   966.85 tokens per second)
0.00.516.044 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.516.044 I llama_perf_context_print:       total time =     141.29 ms /   129 tokens
0.00.516.521 I ggml_metal_free: deallocating

real	0m0.532s
user	0m0.077s
sys	0m0.068s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.082 I build: 4507 (4dd34ff8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.694 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.549 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.015.555 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.557 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.557 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.558 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.558 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.558 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.559 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.559 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.560 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.560 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.560 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.561 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.561 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.562 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.563 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.563 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.271 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.243 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.919 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.921 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.921 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.921 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.921 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.922 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.023.922 I llama_model_loader: - type  f32:  194 tensors
0.00.023.922 I llama_model_loader: - type q3_K:   25 tensors
0.00.023.922 I llama_model_loader: - type q4_K:   71 tensors
0.00.023.923 I llama_model_loader: - type q5_K:    1 tensors
0.00.023.923 I llama_model_loader: - type q6_K:    1 tensors
0.00.023.923 I print_info: file format = GGUF V3 (latest)
0.00.023.924 I print_info: file type   = Q3_K - Medium
0.00.023.925 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.042.262 I load: special tokens cache size = 25
0.00.048.109 I load: token to piece cache size = 0.2984 MB
0.00.048.112 I print_info: arch             = gptneox
0.00.048.113 I print_info: vocab_only       = 0
0.00.048.113 I print_info: n_ctx_train      = 2048
0.00.048.113 I print_info: n_embd           = 2048
0.00.048.113 I print_info: n_layer          = 24
0.00.048.116 I print_info: n_head           = 16
0.00.048.117 I print_info: n_head_kv        = 16
0.00.048.117 I print_info: n_rot            = 32
0.00.048.118 I print_info: n_swa            = 0
0.00.048.118 I print_info: n_embd_head_k    = 128
0.00.048.120 I print_info: n_embd_head_v    = 128
0.00.048.120 I print_info: n_gqa            = 1
0.00.048.121 I print_info: n_embd_k_gqa     = 2048
0.00.048.122 I print_info: n_embd_v_gqa     = 2048
0.00.048.122 I print_info: f_norm_eps       = 1.0e-05
0.00.048.123 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.048.127 I print_info: f_clamp_kqv      = 0.0e+00
0.00.048.128 I print_info: f_max_alibi_bias = 0.0e+00
0.00.048.128 I print_info: f_logit_scale    = 0.0e+00
0.00.048.129 I print_info: n_ff             = 8192
0.00.048.129 I print_info: n_expert         = 0
0.00.048.129 I print_info: n_expert_used    = 0
0.00.048.129 I print_info: causal attn      = 1
0.00.048.129 I print_info: pooling type     = 0
0.00.048.129 I print_info: rope type        = 2
0.00.048.130 I print_info: rope scaling     = linear
0.00.048.130 I print_info: freq_base_train  = 10000.0
0.00.048.131 I print_info: freq_scale_train = 1
0.00.048.131 I print_info: n_ctx_orig_yarn  = 2048
0.00.048.131 I print_info: rope_finetuned   = unknown
0.00.048.131 I print_info: ssm_d_conv       = 0
0.00.048.131 I print_info: ssm_d_inner      = 0
0.00.048.131 I print_info: ssm_d_state      = 0
0.00.048.132 I print_info: ssm_dt_rank      = 0
0.00.048.134 I print_info: ssm_dt_b_c_rms   = 0
0.00.048.134 I print_info: model type       = 1.4B
0.00.048.134 I print_info: model params     = 1.41 B
0.00.048.134 I print_info: general.name     = 1.4B
0.00.048.135 I print_info: vocab type       = BPE
0.00.048.135 I print_info: n_vocab          = 50304
0.00.048.136 I print_info: n_merges         = 50009
0.00.048.136 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.048.137 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.048.137 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.048.137 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.048.137 I print_info: LF token         = 128 'Ä'
0.00.048.137 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.048.137 I print_info: max token length = 1024
0.00.050.006 I load_tensors: offloading 24 repeating layers to GPU
0.00.050.006 I load_tensors: offloading output layer to GPU
0.00.050.006 I load_tensors: offloaded 25/25 layers to GPU
0.00.050.017 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.050.018 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.050.296 I llama_init_from_model: n_seq_max     = 1
0.00.050.296 I llama_init_from_model: n_ctx         = 128
0.00.050.297 I llama_init_from_model: n_ctx_per_seq = 128
0.00.050.297 I llama_init_from_model: n_batch       = 128
0.00.050.297 I llama_init_from_model: n_ubatch      = 128
0.00.050.297 I llama_init_from_model: flash_attn    = 0
0.00.050.297 I llama_init_from_model: freq_base     = 10000.0
0.00.050.298 I llama_init_from_model: freq_scale    = 1
0.00.050.298 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.050.298 I ggml_metal_init: allocating
0.00.050.301 I ggml_metal_init: found device: Apple M4
0.00.050.302 I ggml_metal_init: picking default device: Apple M4
0.00.050.871 I ggml_metal_init: using embedded metal library
0.00.053.201 I ggml_metal_init: GPU name:   Apple M4
0.00.053.203 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.053.203 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.053.203 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.053.204 I ggml_metal_init: simdgroup reduction   = true
0.00.053.204 I ggml_metal_init: simdgroup matrix mul. = true
0.00.053.204 I ggml_metal_init: has bfloat            = true
0.00.053.204 I ggml_metal_init: use bfloat            = true
0.00.053.204 I ggml_metal_init: hasUnifiedMemory      = true
0.00.053.205 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.061.774 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.062.975 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.062.989 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.063.013 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.063.933 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.063.934 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.063.935 I llama_init_from_model: graph nodes  = 967
0.00.063.935 I llama_init_from_model: graph splits = 2
0.00.063.936 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.063.936 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.472.053 I 
0.00.472.089 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.472.092 I perplexity: tokenizing the input ..
0.00.480.246 I perplexity: tokenization took 8.152 ms
0.00.480.249 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.612.397 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.613.568 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.613.592 I llama_perf_context_print:        load time =     463.36 ms
0.00.613.593 I llama_perf_context_print: prompt eval time =     131.92 ms /   128 tokens (    1.03 ms per token,   970.30 tokens per second)
0.00.613.594 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.613.594 I llama_perf_context_print:       total time =     141.54 ms /   129 tokens
0.00.613.939 I ggml_metal_free: deallocating

real	0m0.627s
user	0m0.075s
sys	0m0.080s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.084 I build: 4507 (4dd34ff8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.035 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.752 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.758 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.759 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.760 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.760 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.761 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.761 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.763 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.763 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.765 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.766 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.766 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.767 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.767 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.771 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.771 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.771 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.530 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.525 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.250 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.251 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.251 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.252 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.252 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.252 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.253 I llama_model_loader: - type  f32:  194 tensors
0.00.025.253 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.253 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.253 I llama_model_loader: - type q6_K:   13 tensors
0.00.025.254 I print_info: file format = GGUF V3 (latest)
0.00.025.254 I print_info: file type   = Q4_K - Medium
0.00.025.255 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.043.610 I load: special tokens cache size = 25
0.00.049.517 I load: token to piece cache size = 0.2984 MB
0.00.049.520 I print_info: arch             = gptneox
0.00.049.520 I print_info: vocab_only       = 0
0.00.049.520 I print_info: n_ctx_train      = 2048
0.00.049.520 I print_info: n_embd           = 2048
0.00.049.521 I print_info: n_layer          = 24
0.00.049.523 I print_info: n_head           = 16
0.00.049.524 I print_info: n_head_kv        = 16
0.00.049.524 I print_info: n_rot            = 32
0.00.049.525 I print_info: n_swa            = 0
0.00.049.525 I print_info: n_embd_head_k    = 128
0.00.049.525 I print_info: n_embd_head_v    = 128
0.00.049.526 I print_info: n_gqa            = 1
0.00.049.526 I print_info: n_embd_k_gqa     = 2048
0.00.049.527 I print_info: n_embd_v_gqa     = 2048
0.00.049.528 I print_info: f_norm_eps       = 1.0e-05
0.00.049.528 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.528 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.530 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.530 I print_info: f_logit_scale    = 0.0e+00
0.00.049.531 I print_info: n_ff             = 8192
0.00.049.532 I print_info: n_expert         = 0
0.00.049.533 I print_info: n_expert_used    = 0
0.00.049.533 I print_info: causal attn      = 1
0.00.049.533 I print_info: pooling type     = 0
0.00.049.533 I print_info: rope type        = 2
0.00.049.533 I print_info: rope scaling     = linear
0.00.049.534 I print_info: freq_base_train  = 10000.0
0.00.049.534 I print_info: freq_scale_train = 1
0.00.049.534 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.535 I print_info: rope_finetuned   = unknown
0.00.049.535 I print_info: ssm_d_conv       = 0
0.00.049.535 I print_info: ssm_d_inner      = 0
0.00.049.535 I print_info: ssm_d_state      = 0
0.00.049.535 I print_info: ssm_dt_rank      = 0
0.00.049.535 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.535 I print_info: model type       = 1.4B
0.00.049.536 I print_info: model params     = 1.41 B
0.00.049.536 I print_info: general.name     = 1.4B
0.00.049.536 I print_info: vocab type       = BPE
0.00.049.536 I print_info: n_vocab          = 50304
0.00.049.537 I print_info: n_merges         = 50009
0.00.049.537 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.541 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.542 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.542 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.542 I print_info: LF token         = 128 'Ä'
0.00.049.542 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.542 I print_info: max token length = 1024
0.00.051.469 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.469 I load_tensors: offloading output layer to GPU
0.00.051.469 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.480 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.051.481 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.051.749 I llama_init_from_model: n_seq_max     = 1
0.00.051.750 I llama_init_from_model: n_ctx         = 128
0.00.051.750 I llama_init_from_model: n_ctx_per_seq = 128
0.00.051.750 I llama_init_from_model: n_batch       = 128
0.00.051.750 I llama_init_from_model: n_ubatch      = 128
0.00.051.751 I llama_init_from_model: flash_attn    = 0
0.00.051.751 I llama_init_from_model: freq_base     = 10000.0
0.00.051.751 I llama_init_from_model: freq_scale    = 1
0.00.051.752 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.752 I ggml_metal_init: allocating
0.00.051.754 I ggml_metal_init: found device: Apple M4
0.00.051.756 I ggml_metal_init: picking default device: Apple M4
0.00.052.324 I ggml_metal_init: using embedded metal library
0.00.054.692 I ggml_metal_init: GPU name:   Apple M4
0.00.054.693 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.694 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.694 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.694 I ggml_metal_init: simdgroup reduction   = true
0.00.054.694 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.695 I ggml_metal_init: has bfloat            = true
0.00.054.695 I ggml_metal_init: use bfloat            = true
0.00.054.695 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.695 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.216 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.064.455 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.064.468 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.064.485 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.065.442 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.065.443 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.065.443 I llama_init_from_model: graph nodes  = 967
0.00.065.444 I llama_init_from_model: graph splits = 2
0.00.065.444 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.065.445 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.491.858 I 
0.00.491.893 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.491.897 I perplexity: tokenizing the input ..
0.00.500.099 I perplexity: tokenization took 8.2 ms
0.00.500.110 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.634.535 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.635.704 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.635.734 I llama_perf_context_print:        load time =     481.82 ms
0.00.635.735 I llama_perf_context_print: prompt eval time =     134.20 ms /   128 tokens (    1.05 ms per token,   953.81 tokens per second)
0.00.635.736 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.635.736 I llama_perf_context_print:       total time =     143.88 ms /   129 tokens
0.00.636.167 I ggml_metal_free: deallocating

real	0m0.652s
user	0m0.075s
sys	0m0.095s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.083 I build: 4507 (4dd34ff8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.792 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.679 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.685 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.686 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.687 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.687 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.688 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.688 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.689 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.689 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.690 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.690 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.690 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.691 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.691 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.693 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.693 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.693 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.435 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.433 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.189 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.190 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.190 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.191 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.191 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.191 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.192 I llama_model_loader: - type  f32:  194 tensors
0.00.024.192 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.192 I llama_model_loader: - type q6_K:   37 tensors
0.00.024.193 I print_info: file format = GGUF V3 (latest)
0.00.024.193 I print_info: file type   = Q5_K - Medium
0.00.024.194 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.042.468 I load: special tokens cache size = 25
0.00.048.459 I load: token to piece cache size = 0.2984 MB
0.00.048.462 I print_info: arch             = gptneox
0.00.048.462 I print_info: vocab_only       = 0
0.00.048.462 I print_info: n_ctx_train      = 2048
0.00.048.463 I print_info: n_embd           = 2048
0.00.048.463 I print_info: n_layer          = 24
0.00.048.465 I print_info: n_head           = 16
0.00.048.466 I print_info: n_head_kv        = 16
0.00.048.467 I print_info: n_rot            = 32
0.00.048.467 I print_info: n_swa            = 0
0.00.048.467 I print_info: n_embd_head_k    = 128
0.00.048.467 I print_info: n_embd_head_v    = 128
0.00.048.468 I print_info: n_gqa            = 1
0.00.048.469 I print_info: n_embd_k_gqa     = 2048
0.00.048.469 I print_info: n_embd_v_gqa     = 2048
0.00.048.470 I print_info: f_norm_eps       = 1.0e-05
0.00.048.470 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.048.470 I print_info: f_clamp_kqv      = 0.0e+00
0.00.048.471 I print_info: f_max_alibi_bias = 0.0e+00
0.00.048.471 I print_info: f_logit_scale    = 0.0e+00
0.00.048.472 I print_info: n_ff             = 8192
0.00.048.472 I print_info: n_expert         = 0
0.00.048.472 I print_info: n_expert_used    = 0
0.00.048.472 I print_info: causal attn      = 1
0.00.048.472 I print_info: pooling type     = 0
0.00.048.472 I print_info: rope type        = 2
0.00.048.473 I print_info: rope scaling     = linear
0.00.048.475 I print_info: freq_base_train  = 10000.0
0.00.048.476 I print_info: freq_scale_train = 1
0.00.048.476 I print_info: n_ctx_orig_yarn  = 2048
0.00.048.476 I print_info: rope_finetuned   = unknown
0.00.048.476 I print_info: ssm_d_conv       = 0
0.00.048.476 I print_info: ssm_d_inner      = 0
0.00.048.476 I print_info: ssm_d_state      = 0
0.00.048.477 I print_info: ssm_dt_rank      = 0
0.00.048.477 I print_info: ssm_dt_b_c_rms   = 0
0.00.048.477 I print_info: model type       = 1.4B
0.00.048.477 I print_info: model params     = 1.41 B
0.00.048.478 I print_info: general.name     = 1.4B
0.00.048.478 I print_info: vocab type       = BPE
0.00.048.478 I print_info: n_vocab          = 50304
0.00.048.478 I print_info: n_merges         = 50009
0.00.048.479 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.048.479 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.048.479 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.048.483 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.048.483 I print_info: LF token         = 128 'Ä'
0.00.048.483 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.048.483 I print_info: max token length = 1024
0.00.050.026 I load_tensors: offloading 24 repeating layers to GPU
0.00.050.026 I load_tensors: offloading output layer to GPU
0.00.050.026 I load_tensors: offloaded 25/25 layers to GPU
0.00.050.036 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.050.038 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.050.307 I llama_init_from_model: n_seq_max     = 1
0.00.050.308 I llama_init_from_model: n_ctx         = 128
0.00.050.308 I llama_init_from_model: n_ctx_per_seq = 128
0.00.050.308 I llama_init_from_model: n_batch       = 128
0.00.050.308 I llama_init_from_model: n_ubatch      = 128
0.00.050.308 I llama_init_from_model: flash_attn    = 0
0.00.050.309 I llama_init_from_model: freq_base     = 10000.0
0.00.050.309 I llama_init_from_model: freq_scale    = 1
0.00.050.309 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.050.310 I ggml_metal_init: allocating
0.00.050.312 I ggml_metal_init: found device: Apple M4
0.00.050.314 I ggml_metal_init: picking default device: Apple M4
0.00.050.884 I ggml_metal_init: using embedded metal library
0.00.053.239 I ggml_metal_init: GPU name:   Apple M4
0.00.053.240 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.053.241 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.053.241 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.053.241 I ggml_metal_init: simdgroup reduction   = true
0.00.053.241 I ggml_metal_init: simdgroup matrix mul. = true
0.00.053.242 I ggml_metal_init: has bfloat            = true
0.00.053.242 I ggml_metal_init: use bfloat            = true
0.00.053.242 I ggml_metal_init: hasUnifiedMemory      = true
0.00.053.243 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.061.599 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.062.945 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.062.958 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.062.974 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.063.832 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.063.833 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.063.834 I llama_init_from_model: graph nodes  = 967
0.00.063.834 I llama_init_from_model: graph splits = 2
0.00.063.835 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.063.835 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.630.887 I 
0.00.630.929 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.630.932 I perplexity: tokenizing the input ..
0.00.638.938 I perplexity: tokenization took 8.004 ms
0.00.638.941 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.779.563 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.780.742 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.780.771 I llama_perf_context_print:        load time =     622.09 ms
0.00.780.772 I llama_perf_context_print: prompt eval time =     140.40 ms /   128 tokens (    1.10 ms per token,   911.71 tokens per second)
0.00.780.773 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.780.773 I llama_perf_context_print:       total time =     149.89 ms /   129 tokens
0.00.781.257 I ggml_metal_free: deallocating

real	0m0.795s
user	0m0.075s
sys	0m0.110s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.082 I build: 4507 (4dd34ff8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.014 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.732 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.737 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.738 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.739 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.739 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.739 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.740 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.741 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.741 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.741 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.742 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.742 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.743 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.743 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.745 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.746 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.746 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.434 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.451 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.103 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.105 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.105 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.105 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.105 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.106 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.106 I llama_model_loader: - type  f32:  194 tensors
0.00.025.106 I llama_model_loader: - type q6_K:   98 tensors
0.00.025.107 I print_info: file format = GGUF V3 (latest)
0.00.025.107 I print_info: file type   = Q6_K
0.00.025.108 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.043.350 I load: special tokens cache size = 25
0.00.049.247 I load: token to piece cache size = 0.2984 MB
0.00.049.250 I print_info: arch             = gptneox
0.00.049.250 I print_info: vocab_only       = 0
0.00.049.251 I print_info: n_ctx_train      = 2048
0.00.049.251 I print_info: n_embd           = 2048
0.00.049.251 I print_info: n_layer          = 24
0.00.049.254 I print_info: n_head           = 16
0.00.049.255 I print_info: n_head_kv        = 16
0.00.049.255 I print_info: n_rot            = 32
0.00.049.255 I print_info: n_swa            = 0
0.00.049.255 I print_info: n_embd_head_k    = 128
0.00.049.255 I print_info: n_embd_head_v    = 128
0.00.049.256 I print_info: n_gqa            = 1
0.00.049.257 I print_info: n_embd_k_gqa     = 2048
0.00.049.259 I print_info: n_embd_v_gqa     = 2048
0.00.049.259 I print_info: f_norm_eps       = 1.0e-05
0.00.049.260 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.260 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.262 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.262 I print_info: f_logit_scale    = 0.0e+00
0.00.049.263 I print_info: n_ff             = 8192
0.00.049.263 I print_info: n_expert         = 0
0.00.049.263 I print_info: n_expert_used    = 0
0.00.049.263 I print_info: causal attn      = 1
0.00.049.263 I print_info: pooling type     = 0
0.00.049.263 I print_info: rope type        = 2
0.00.049.264 I print_info: rope scaling     = linear
0.00.049.269 I print_info: freq_base_train  = 10000.0
0.00.049.269 I print_info: freq_scale_train = 1
0.00.049.269 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.270 I print_info: rope_finetuned   = unknown
0.00.049.270 I print_info: ssm_d_conv       = 0
0.00.049.270 I print_info: ssm_d_inner      = 0
0.00.049.270 I print_info: ssm_d_state      = 0
0.00.049.270 I print_info: ssm_dt_rank      = 0
0.00.049.270 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.271 I print_info: model type       = 1.4B
0.00.049.271 I print_info: model params     = 1.41 B
0.00.049.271 I print_info: general.name     = 1.4B
0.00.049.272 I print_info: vocab type       = BPE
0.00.049.272 I print_info: n_vocab          = 50304
0.00.049.272 I print_info: n_merges         = 50009
0.00.049.272 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.273 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.273 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.273 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.273 I print_info: LF token         = 128 'Ä'
0.00.049.273 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.274 I print_info: max token length = 1024
0.00.051.209 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.210 I load_tensors: offloading output layer to GPU
0.00.051.210 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.220 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.051.222 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.051.492 I llama_init_from_model: n_seq_max     = 1
0.00.051.493 I llama_init_from_model: n_ctx         = 128
0.00.051.493 I llama_init_from_model: n_ctx_per_seq = 128
0.00.051.493 I llama_init_from_model: n_batch       = 128
0.00.051.493 I llama_init_from_model: n_ubatch      = 128
0.00.051.493 I llama_init_from_model: flash_attn    = 0
0.00.051.494 I llama_init_from_model: freq_base     = 10000.0
0.00.051.494 I llama_init_from_model: freq_scale    = 1
0.00.051.494 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.495 I ggml_metal_init: allocating
0.00.051.497 I ggml_metal_init: found device: Apple M4
0.00.051.499 I ggml_metal_init: picking default device: Apple M4
0.00.052.057 I ggml_metal_init: using embedded metal library
0.00.054.425 I ggml_metal_init: GPU name:   Apple M4
0.00.054.426 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.426 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.427 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.427 I ggml_metal_init: simdgroup reduction   = true
0.00.054.427 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.427 I ggml_metal_init: has bfloat            = true
0.00.054.427 I ggml_metal_init: use bfloat            = true
0.00.054.427 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.428 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.062.959 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.064.334 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.064.349 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.064.368 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.065.245 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.065.246 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.065.246 I llama_init_from_model: graph nodes  = 967
0.00.065.247 I llama_init_from_model: graph splits = 2
0.00.065.248 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.065.248 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.138.036 I 
0.00.138.082 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.138.087 I perplexity: tokenizing the input ..
0.00.146.116 I perplexity: tokenization took 8.027 ms
0.00.146.121 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.284.441 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.285.579 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.285.601 I llama_perf_context_print:        load time =     128.01 ms
0.00.285.602 I llama_perf_context_print: prompt eval time =     138.09 ms /   128 tokens (    1.08 ms per token,   926.96 tokens per second)
0.00.285.603 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.285.603 I llama_perf_context_print:       total time =     147.57 ms /   129 tokens
0.00.286.020 I ggml_metal_free: deallocating

real	0m0.301s
user	0m0.075s
sys	0m0.041s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.239 I build: 4507 (4dd34ff8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.022.449 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.036.738 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.036.747 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.036.753 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.036.754 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.036.755 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.036.755 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.036.756 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.036.758 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.036.758 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.036.759 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.036.760 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.036.760 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.036.761 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.036.763 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.036.766 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.036.767 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.036.767 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.044.998 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.046.969 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.053.804 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.053.806 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.053.806 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.053.807 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.053.807 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.053.808 I llama_model_loader: - type  f32:  194 tensors
0.00.053.808 I llama_model_loader: - type  f16:   98 tensors
0.00.053.809 I print_info: file format = GGUF V3 (latest)
0.00.053.810 I print_info: file type   = all F32 (guessed)
0.00.053.811 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.080.042 I load: special tokens cache size = 25
0.00.086.341 I load: token to piece cache size = 0.2984 MB
0.00.086.344 I print_info: arch             = gptneox
0.00.086.344 I print_info: vocab_only       = 0
0.00.086.345 I print_info: n_ctx_train      = 2048
0.00.086.345 I print_info: n_embd           = 2048
0.00.086.345 I print_info: n_layer          = 24
0.00.086.348 I print_info: n_head           = 16
0.00.086.349 I print_info: n_head_kv        = 16
0.00.086.349 I print_info: n_rot            = 32
0.00.086.349 I print_info: n_swa            = 0
0.00.086.351 I print_info: n_embd_head_k    = 128
0.00.086.352 I print_info: n_embd_head_v    = 128
0.00.086.352 I print_info: n_gqa            = 1
0.00.086.353 I print_info: n_embd_k_gqa     = 2048
0.00.086.354 I print_info: n_embd_v_gqa     = 2048
0.00.086.355 I print_info: f_norm_eps       = 1.0e-05
0.00.086.355 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.086.356 I print_info: f_clamp_kqv      = 0.0e+00
0.00.086.356 I print_info: f_max_alibi_bias = 0.0e+00
0.00.086.356 I print_info: f_logit_scale    = 0.0e+00
0.00.086.357 I print_info: n_ff             = 8192
0.00.086.357 I print_info: n_expert         = 0
0.00.086.357 I print_info: n_expert_used    = 0
0.00.086.358 I print_info: causal attn      = 1
0.00.086.358 I print_info: pooling type     = 0
0.00.086.358 I print_info: rope type        = 2
0.00.086.359 I print_info: rope scaling     = linear
0.00.086.359 I print_info: freq_base_train  = 10000.0
0.00.086.359 I print_info: freq_scale_train = 1
0.00.086.359 I print_info: n_ctx_orig_yarn  = 2048
0.00.086.360 I print_info: rope_finetuned   = unknown
0.00.086.360 I print_info: ssm_d_conv       = 0
0.00.086.360 I print_info: ssm_d_inner      = 0
0.00.086.360 I print_info: ssm_d_state      = 0
0.00.086.360 I print_info: ssm_dt_rank      = 0
0.00.086.360 I print_info: ssm_dt_b_c_rms   = 0
0.00.086.360 I print_info: model type       = 1.4B
0.00.086.361 I print_info: model params     = 1.41 B
0.00.086.361 I print_info: general.name     = 1.4B
0.00.086.361 I print_info: vocab type       = BPE
0.00.086.361 I print_info: n_vocab          = 50304
0.00.086.361 I print_info: n_merges         = 50009
0.00.086.362 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.086.362 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.086.362 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.086.362 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.086.362 I print_info: LF token         = 128 'Ä'
0.00.086.363 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.086.363 I print_info: max token length = 1024
0.00.088.875 I load_tensors: offloading 24 repeating layers to GPU
0.00.088.875 I load_tensors: offloading output layer to GPU
0.00.088.875 I load_tensors: offloaded 25/25 layers to GPU
0.00.088.886 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.088.888 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.089.169 I llama_init_from_model: n_seq_max     = 1
0.00.089.170 I llama_init_from_model: n_ctx         = 128
0.00.089.170 I llama_init_from_model: n_ctx_per_seq = 128
0.00.089.171 I llama_init_from_model: n_batch       = 128
0.00.089.171 I llama_init_from_model: n_ubatch      = 128
0.00.089.171 I llama_init_from_model: flash_attn    = 0
0.00.089.171 I llama_init_from_model: freq_base     = 10000.0
0.00.089.172 I llama_init_from_model: freq_scale    = 1
0.00.089.172 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.089.172 I ggml_metal_init: allocating
0.00.089.175 I ggml_metal_init: found device: Apple M4
0.00.089.177 I ggml_metal_init: picking default device: Apple M4
0.00.089.766 I ggml_metal_init: using embedded metal library
0.00.092.341 I ggml_metal_init: GPU name:   Apple M4
0.00.092.343 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.092.343 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.092.343 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.092.344 I ggml_metal_init: simdgroup reduction   = true
0.00.092.344 I ggml_metal_init: simdgroup matrix mul. = true
0.00.092.344 I ggml_metal_init: has bfloat            = true
0.00.092.344 I ggml_metal_init: use bfloat            = true
0.00.092.344 I ggml_metal_init: hasUnifiedMemory      = true
0.00.092.345 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.101.428 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.102.643 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.102.656 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.102.671 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.103.521 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.103.522 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.103.522 I llama_init_from_model: graph nodes  = 967
0.00.103.523 I llama_init_from_model: graph splits = 2
0.00.103.524 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.103.525 I 
0.00.103.560 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.103.561 I compute_imatrix: tokenizing the input ..
0.00.110.198 I compute_imatrix: tokenization took 6.636 ms
0.00.110.199 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.503.886 I compute_imatrix: 1.39 seconds per pass - ETA 0.02 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.506.919 I llama_perf_context_print:        load time =    1481.43 ms
0.01.506.920 I llama_perf_context_print: prompt eval time =    1393.08 ms /   128 tokens (   10.88 ms per token,    91.88 tokens per second)
0.01.506.921 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.506.922 I llama_perf_context_print:       total time =    1484.45 ms /   129 tokens
0.01.507.924 I ggml_metal_free: deallocating

real	0m1.693s
user	0m0.170s
sys	0m0.233s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4507 (4dd34ff8)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 'Ä'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x123e0a2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x123e0a9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x123e0afa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x123e0b550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x123e0bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x123e0c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x123e0c660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x123e0cc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x123e0d1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x123e0d6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x123e0dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x123e0e0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x123e0ebe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x123e0f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x123e0fba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x123e102c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x123e109e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x123e11100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x123e11820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x123e11ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x123e12710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x123e12e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x123e13550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x123e13df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x123e14510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x123e147d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x123e14de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x123e15a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x123e15f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x123e16250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x123e166f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x123e169b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x123e17240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x123e17780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x123e17a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x123e17ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x123e18380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x123e18820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x123e18cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x123e19160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x123e19600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x123e19aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x123e19f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x123e1a3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x123e1a6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x123e1acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x123e1b2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x123e1bbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x123e1c1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x123e1c800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x123e1ce10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x123e1d420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x123e1da30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x123e1e040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x123e1e830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x123e1ecd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x123e1f170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x123e1f430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x123e1fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x123e20230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x123e204f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x123e20990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x123e20e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x123e212d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x123e21770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x123e21c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x123e220b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x123e22550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x123e229f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x123e22e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x123e23330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x123e237d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x123e23c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x123e241c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x123e24710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x123e24c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x123e251b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x123e25700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x123e25c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x123e261a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x123e266f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x123e26c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x123e27190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x123e276e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x123e27c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x123e28180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x123e286d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x123e28c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x123e29170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x123e296c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x123e29c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x123e2a160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x123e2a6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x123e2ac00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x123e2b150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x123e2b6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x123e2bbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x123e1b8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x123e2c060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x123e2c810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x123e2cd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x123e2d2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x123e2d800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x123e2dd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x123e2e2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x123e2e7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x123e2ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x123e2f290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x123e2f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x123e2fd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x123e30280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x123e307d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x123e30d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x123e311c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x123e31660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x123e31b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x123e31fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x123e32440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x123e328e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x123e32d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x123e33220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x123e336c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x123e33b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x123e34000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x123e344a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x123e34940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x123e34de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x123e35280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x123e35720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x123e35bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x123e36060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x123e36500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x123e369a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x123e36e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x123e372e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x123e37780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x123e37c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x123e380c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x123e38560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x123e38a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x123e38ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x123e39340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x123e397e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x123e39c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x123e3a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x123e3a5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x123e3aa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x123e3af00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x123e3b3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x123e3b840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x123e3bce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x123e3c180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x123e3c620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x123e3cac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x123e3cf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x123e3d400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x123e3d8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x123e3dd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x123e3e1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x123e3e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x123e3eb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x123e3efc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x123e3f460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x123e3f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x123e3fda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x123e40240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x123e406e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x123e40b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x123e41020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x123e414c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x123e41960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x123e41e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x123e422a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x123e42740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x123e42be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x123e43080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x123e43520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x123e439c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x123e43e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x123e44300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x123e447a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x123e44c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x123e450e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x123e45580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x123e45a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x123e45ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x123e46360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x123e46800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x123e46ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x123e47140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x123e475e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x123e47a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x123e47f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x123e48470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x123e489c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x123e48f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x123e49460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x123e49720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x123e49d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x123e4a340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x123e4a950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x123e4b140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x123e4b5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x123e4b8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x123e4beb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x123e4c4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x123e4ccb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x123e4d150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x123e4d5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x123e4da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x123e4e240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x123e4e790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x123e4ece0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x123e4f230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x123e4f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x123e4fcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x123e50220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x123e50770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x123e50cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x123e51210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x123e51760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x123e51cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x123e52200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x123e52750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x123e52ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x123e531f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x123e53740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x123e53c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x123e541e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x123e54730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x123e54c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x123e551d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x123e55720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x123e55c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x123e561c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x123e56710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x123e56c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x123e571b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x123e57700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x123e57c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x123e581a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x123e586f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x123e58c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x123e59190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x123e596e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x123e59c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x123e5a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x123e5a6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x123e5ac20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x123e5b170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x123e5b6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x123e5bc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x123e5c160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x123e5c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x123e5cc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x123e5d150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x123e5d6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x123e5dbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x123e5e140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x123e5e690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x123e5ebe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x123e5f130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x123e5f680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x123e5fbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x123e60120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x123e60670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x123e60bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x123e61060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x123e61500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x123e619a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x123e61e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x123e622e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x123e62780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x123e62c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x123e630c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x123e63560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x123e63a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x123e63ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x123e64340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x123e647e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x123e64c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x123e65120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x123e65670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x123e65d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x123e664b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x123e66bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x123e672f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x123e675b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x123e67da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x123e68060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x123e68670 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.144.594 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.144.598 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x133e04c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x133e05070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x133e054e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x133e05950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x133e05dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x133e06230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x133e066a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x133e06b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x133e06f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x133e073f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x133e07860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x133e07f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x133e08a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x133e091f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x133e09a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x133e0a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x133e0a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x133e0af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x133e0b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x133e0be50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x133e0c570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x133e0cc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x133e0d3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x133e0dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x133e0e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x133e0e4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x133e0e770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x133e0ebe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x133e0f050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x133e0f4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x133e0f930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x133e0fe60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x133e102d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x133e10590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x133e10a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x133e10e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x133e112e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x133e11750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x133e11bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x133e12030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x133e124a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x133e12910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x133e12d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x133e131f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x133e13660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x133e13ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x133e13f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x133e143b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x133e14820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x133e14c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x133e15100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x133e15570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x133e159e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x133e15e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x133e162c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x133e16730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x133e16ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x133e171a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x133e17610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x133e17a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x133e17ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x133e18360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x133e187d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x133e18c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x133e190b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x133e19520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x133e19990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x133e19e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x133e1a270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x133e1a6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x133e1ab50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x133e1afc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x133e1b430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x133e1b8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x133e1bd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x133e1c180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x133e1c5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x133e1ca60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x133e1ced0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x133e1d340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x133e1d7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x133e1dc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x133e1e090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x133e1e500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x133e1e970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x133e1ede0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x133e1f250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x133e1f6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x133e1fb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x133e1ffa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x133e20410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x133e20880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x133e20cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x133e21160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x133e215d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x133e21a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x133e21eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x133e22320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x133e22790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x133e22c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x133e23070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x133e234e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x133e23950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x133e23dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x133e24230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x133e246a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x133e24b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x133e24f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x133e253f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x133e25860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x133e25cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x133e26140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x133e265b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x133e26a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x133e26e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x133e27300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x133e27770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x133e27be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x133e28050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x133e284c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x133e28930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x133e28da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x133e29210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x133e29680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x133e29af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x133e29f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x133e2a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x133e2a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x133e2acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x133e2b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x133e2b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x133e2ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x133e2be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x133e2c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x133e2c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x133e2cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x133e2d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x133e2d4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x133e2d910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x133e2dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x133e2e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x133e2e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x133e2ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x133e2ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x133e2f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x133e2f820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x133e2fc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x133e30100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x133e30570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x133e309e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x133e30e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x133e312c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x133e31730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x133e31ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x133e32010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x133e32480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x133e328f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x133e32d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x133e331d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x133e33640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x133e33ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x133e33f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x133e34390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x133e34800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x133e34c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x133e350e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x133e35d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x133e35fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x133e36290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x133e36700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x133e36b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x133e36fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x133e37450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x133e378c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x133e37d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x133e381a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x133e38610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x133e38a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x133e38ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x133e39360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x133e397d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x133e39c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x133e3a0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x133e3a520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x133e3a990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x133e3ae00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x133e3b270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x133e3b6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x133e3bb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x133e3bfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x133e3c430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x133e3c8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x133e3cd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x133e3d180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x133e3d5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x133e3da60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x133e3ded0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x133e3e340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x133e3e7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x133e3ec20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x133e3f090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x133e3f500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x133e3fa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x133e3ff70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x133e403e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x133e40850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x133e40cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x133e41130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x133e41650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x133e41b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x133e426d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x133e42990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x133e42f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x133e43510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x133e43ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x133e44090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x133e44650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x133e44c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x133e451d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x133e45790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x133e45d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x133e46310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x133e468d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x133e46e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x133e47450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x133e47a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x133e47fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x133e48590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x133e48b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x133e49110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x133e496d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x133e49c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x133e4a250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x133e4a810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x133e4add0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x133e4b390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x133e4b950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x133e4bf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x133e4c4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x133e4ca90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x133e4d050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x133e4d610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x133e4dbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x133e4e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x133e4e750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x133e4ed10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x133e4f2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x133e4f890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x133e4fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x133e50410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x133e509d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x133e50f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x133e51550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x133e51b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x133e520d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x133e52690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x133e52c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x133e53210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x133e537d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x133e53d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x133e54350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x133e54910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x133e54ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x133e55490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x133e55a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x133e56010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x133e565d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x133e56b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x133e57090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x133e57590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x133e57a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x133e57f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x133e58490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x133e58990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x133e58e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x133e59390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x133e59890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x133e59d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x133e5a290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x133e5a790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x133e5ac90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x133e5b190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x133e5b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x133e5c0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x133e5c7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x133e5cee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x133e5d600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x133e5d8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x133e5e0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x133e5e370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x133e5e980 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x1232048d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x123204d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x1232051b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x123205620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x123205a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x123205f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x123206370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x1232067e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x123206c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x1232070c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x123207530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x123207c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x123208730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x123208ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x1232096f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x123209e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12320a530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12320ac50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12320b370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12320bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12320c260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12320c980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12320d0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12320d7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12320dee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12320e1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12320e460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12320e8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12320ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12320f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12320f620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12320fb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12320ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x123210280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1232106f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x123210b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x123210fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x123211440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x1232118b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x123211d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x123212190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x123212600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x123212a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x123212ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x123213350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x1232137c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x123213c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x1232140a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x123214510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x123214980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x123214df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x123215260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x1232156d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x123215b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x123215fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x123216420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x123216990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x123216e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x123217300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x123217770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x123217be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x123218050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x1232184c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x123218930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x123218da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x123219210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x123219680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x123219af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x123219f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12321a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12321a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12321acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12321b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12321b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12321ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12321be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12321c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12321c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12321cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12321d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12321d4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12321d910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12321dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12321e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12321e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12321ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12321ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12321f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12321f820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12321fc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x123220100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x123220570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x1232209e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x123220e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x1232212c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x123221730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x123221ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x123222010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x123222480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x1232228f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x123222d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x1232231d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x123223640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x123223ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x123224190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x123224600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x123224a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x123224ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x123225350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x1232257c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x123225c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x1232260a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x123226510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x123226980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x123226df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x123227260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x1232276d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x123227b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x123227fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x123228420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x123228890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x123228d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x123229170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x1232295e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x123229a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x123229ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12322a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12322a7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12322ac10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12322b080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12322b4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12322b960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12322bdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12322c240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12322c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12322cb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12322cf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12322d400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12322d870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12322dce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12322e150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12322e5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12322ea30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12322eea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12322f310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12322f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12322fbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x123230060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x1232304d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x123230940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x123230db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x123231220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x123231690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x123231b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x123231f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x1232323e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x123232850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x123232cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x123233130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x1232335a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x123233a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x123233e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x1232342f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x123234760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x123234bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x123235040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x1232354b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x123235920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x123235d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x123236200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x123236670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x123236ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x123236f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x1232373c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x123237830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x123237ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x123238110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x123238580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x1232389f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x123238e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x1232392d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x123239740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x123239bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12323a020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12323a490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12323a900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12323ad70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12323b1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12323b650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12323bac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12323bf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12323c3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12323c810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12323cc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12323d0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12323d560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12323d9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12323de40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12323e2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12323e720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12323eb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12323f000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12323f470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12323f8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12323fd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x1232401c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x123240630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x123240aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x123240f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x123241380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x123241f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x1232421c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x123242480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x1232428f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x123242d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x1232431d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x123243640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x123243ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x123243f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x123244390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x123244800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x123244c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x1232450e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x123245550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x1232459c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x123245e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x1232462a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x123246710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x123246b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x123246ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x123247460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x1232478d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x123247d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x1232481b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x123248620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x123248a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x123248f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x123249370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x1232497e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x123249c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12324a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12324a530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12324a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12324ae10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12324b280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12324b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12324bb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12324bfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12324c440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12324c8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12324cd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12324d190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12324d600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12324da70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12324dee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12324e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12324e7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12324ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12324f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12324f510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12324f980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12324fdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x123250260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x1232506d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x123250b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x123250fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x123251420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x123251890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x123251d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x123252170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x1232525e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x123252a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x123252ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x123253330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x1232537a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x123253c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x123254080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x1232544f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x123254960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x123254dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x123255240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x1232556b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x123255b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x123256590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x123256cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x1232573d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x123257af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x123257db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x123258220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x123258820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x123258e30 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.833s
user	0m0.297s
sys	0m0.327s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4507 (4dd34ff8)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 'Ä'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x156f10410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x156f10b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x156f110d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x156f11680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x156f11c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x156f121e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x156f12790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x156f12d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x156f132f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x156f137f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x156f13cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x156f141f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x156f14d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x156f154c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x156f15cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x156f163f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x156f16b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x156f17230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x156f17950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x156f18120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x156f18840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x156f18f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x156f19680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x156f19f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x156f1a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x156f1a900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x156f1af10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x156f1bb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x156f1c0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x156f1c380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x156f1c820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x156f1cae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x156f1d370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x156f1d8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x156f1db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x156f1e010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x156f1e4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x156f1e950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x156f1edf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x156f1f290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x156f1f730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x156f1fbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x156f20070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x156f20510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x156f207d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x156f20de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x156f213f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x156f21d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x156f22320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x156f22930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x156f22f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x156f23550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x156f23b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x156f24170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x156f24960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x156f24e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x156f252a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x156f25560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x156f25b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x156f26360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x156f26620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x156f26ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x156f26f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x156f27400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x156f278a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x156f27d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x156f281e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x156f28680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x156f28b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x156f28fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x156f29460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x156f29900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x156f29da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x156f2a2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x156f2a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x156f2ad90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x156f2b2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x156f2b830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x156f2bd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x156f2c2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x156f2c820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x156f2cd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x156f2d2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x156f2d810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x156f2dd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x156f2e2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x156f2e800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x156f2ed50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x156f2f2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x156f2f7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x156f2fd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x156f30290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x156f307e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x156f30d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x156f31280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x156f317d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x156f31d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x156f21a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x156f32190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x156f32940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x156f32e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x156f333e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x156f33930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x156f33e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x156f343d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x156f34920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x156f34e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x156f353c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x156f35910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x156f35e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x156f363b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x156f36900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x156f36e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x156f372f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x156f37790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x156f37c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x156f380d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x156f38570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x156f38a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x156f38eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x156f39350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x156f397f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x156f39c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x156f3a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x156f3a5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x156f3aa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x156f3af10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x156f3b3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x156f3b850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x156f3bcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x156f3c190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x156f3c630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x156f3cad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x156f3cf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x156f3d410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x156f3d8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x156f3dd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x156f3e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x156f3e690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x156f3eb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x156f3efd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x156f3f470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x156f3f910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x156f3fdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x156f40250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x156f406f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x156f40b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x156f41030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x156f414d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x156f41970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x156f41e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x156f422b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x156f42750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x156f42bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x156f43090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x156f43530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x156f439d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x156f43e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x156f44310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x156f447b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x156f44c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x156f450f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x156f45590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x156f45a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x156f45ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x156f46370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x156f46810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x156f46cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x156f47150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x156f475f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x156f47a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x156f47f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x156f483d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x156f48870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x156f48d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x156f491b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x156f49650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x156f49af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x156f49f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x156f4a430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x156f4a8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x156f4ad70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x156f4b210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x156f4b6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x156f4bb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x156f4bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x156f4c490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x156f4c930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x156f4cdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x156f4d270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x156f4d710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x156f4dbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x156f4e050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x156f4e5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x156f4eaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x156f4f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x156f4f590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x156f4f850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x156f4fe60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x156f50470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x156f50a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x156f51270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x156f51710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x156f519d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x156f51fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x156f525f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x156f52de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x156f53280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x156f53720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x156f53bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x156f54370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x156f548c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x156f54e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x156f55360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x156f558b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x156f55e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x156f56350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x156f568a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x156f56df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x156f57340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x156f57890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x156f57de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x156f58330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x156f58880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x156f58dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x156f59320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x156f59870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x156f59dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x156f5a310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x156f5a860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x156f5adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x156f5b300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x156f5b850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x156f5bda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x156f5c2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x156f5c840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x156f5cd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x156f5d2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x156f5d830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x156f5dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x156f5e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x156f5e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x156f5ed70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x156f5f2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x156f5f810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x156f5fd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x156f602b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x156f60800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x156f60d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x156f612a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x156f617f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x156f61d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x156f62290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x156f627e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x156f62d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x156f63280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x156f637d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x156f63d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x156f64270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x156f647c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x156f64d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x156f65260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x156f657b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x156f65d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x156f66250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x156f667a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x156f66cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x156f67190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x156f67630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x156f67ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x156f67f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x156f68410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x156f688b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x156f68d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x156f691f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x156f69690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x156f69b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x156f69fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x156f6a470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x156f6a910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x156f6adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x156f6b250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x156f6b7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x156f6bec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x156f6c5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x156f6cd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x156f6d420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x156f6d6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x156f6ded0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x156f6e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x156f6e7a0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.086.553 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.086.557 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x156f6e450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x156f51c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x156f4fb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x156f50730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x156f23810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x156f23200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x156f25820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x156f522a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x156f1abc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x156f216b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x156f21fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x156f225e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x156f20a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x156f22bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x156f19bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x156f25e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x156f32450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x156f6d9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x156f1cda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x156f1d060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x156f528b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x156f50d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x156f1b1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x156f1b490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x156f1b750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x156f6ec00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x156f6eec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x156f6f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x156f6f440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x156f6f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x156f6f9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x156f6fc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x156f6ff40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x156f70200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x156f704c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x156f70780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x156f70a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x156f70d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x156f70fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x156f71280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x156f71540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x156f71800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x156f71ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x156f71d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x156f72040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x156f72300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x156f725c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x156f72880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x156f72b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x156f72e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x156f730c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x156f73380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x156f73640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x156f73900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x156f73bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x156f73e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x156f74140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x156f74400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x156f746c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x156f74980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x156f74c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x156f74f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x156f751c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x156f75480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x156f75740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x156f75a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x156f75cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x156f75f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x156f76240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x156f76500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x156f767c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x156f76a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x156f76d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x156f77000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x156f772c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x156f77580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x156f77840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x156f77b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x156f77dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x156f78080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x156f78340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x156f78600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x156f788c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x156f78b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x156f78e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x156f79100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x156f793c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x156f79680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x156f79940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x156f79c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x156f79ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x156f7a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x156f7a440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x156f7a700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x156f7a9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x156f7ac80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x156f7af40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x156f7b200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x156f7b4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x156f7b780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x156f7ba40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x156f7bd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x156f7bfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x156f7c280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x156f7c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x156f7c800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x156f7cac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x156f7cd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x156f7d040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x156f7d300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x156f7d5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x156f7d880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x156f7db40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x156f7de00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x156f7e0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x156f7e380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x156f7e640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x156f7e900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x156f7ebc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x156f7ee80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x156f7f140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x156f7f400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x156f7f6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x156f7f980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x156f7fc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x156f7ff00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x156f801c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x156f80480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x156f80740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x156f80a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x156f80cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x156f80f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x156f81240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x156f81500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x156f817c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x156f81a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x156f81d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x156f82000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x156f822c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x156f82580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x156f82840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x156f82b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x156f82dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x156f83080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x156f83340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x156f83600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x156f838c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x156f83b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x156f83e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x156f84100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x156f843c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x156f84680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x156f84940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x156f84c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x156f84ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x156f85180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x156f85440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x156f85700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x156f859c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x156f85c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x156f85f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x156f86200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x156f864c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x156f86780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x156f86a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x156f86d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x156f86fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x156f87280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x156f87540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x156f87800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x156f87ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x156f87d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x156f88040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x156f88300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x156f885c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x156f88880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x156f88b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x156f88e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x156f890c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x156f89380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x156f89640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x156f89900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x156f89bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x156f89e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x156f8a140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x156f8a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x156f8a6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x156f8a980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x156f8ac40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x156f8af00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x156f8b1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x156f8b480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x156f8b740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x156f8ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x156f8bcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x156f8bf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x156f8c240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x156f8c500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x156f8c7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x156f8ca80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x156f8cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x156f8d000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x156f8d2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x156f8d580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x156f8d840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x156f8db00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x156f8ddc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x156f8e080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x156f8e340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x156f8e600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x156f8ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x156f8ee90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x156f8f150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x156f8f5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x156f8fa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x156f8fea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x156f90310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x156f90780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x156f90bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x156f91060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x156f914d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x156f91940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x156f91db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x156f92220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x156f92690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x156f92b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x156f92f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x156f933e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x156f93850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x156f93cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x156f94130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x156f945a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x156f94a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x156f94e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x156f952f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x156f95760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x156f95bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x156f96040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x156f964b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x156f96920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x156f96d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x156f97200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x156f97670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x156f97ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x156f97f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x156f983c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x156f98830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x156f98ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x156f99110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x156f99580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x156f999f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x156f99e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x156f9a2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x156f9a740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x156f9abb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x156f9b020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x156f9b490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x156f9b900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x156f9bd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x156f9c1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x156f9c650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x156f9cac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x156f9cf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x156f9d3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x156f9d810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x156f9dc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x156f9e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x156f9e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x156f9e9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x156f9ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x156f9f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x156f9f720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x156f9fb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x156fa0000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x156fa0470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x156fa08e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x156fa0d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x156fa11c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x156fa1630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x156fa1aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x156fa1f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x156fa2380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x156fa27f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x156fa3260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x156fa3980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x156fa40a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x156fa47c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x156fa4a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x156fa5270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x156fa5530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x156fa5b40 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x1477044e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x147704950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x147704dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x147705230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x1477056a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x147705b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x147705f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x1477063f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x147706860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x147706dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x147707240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x1477078c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x1477083e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x147708b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x1477093a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x147709ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14770a1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14770a900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14770b020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14770b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14770bf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14770c630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14770cd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14770d470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14770db90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14770de50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14770e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14770e580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14770e9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14770ee60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14770f2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14770f800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14770fc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14770ff30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1477103a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x147710810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x147710c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x1477110f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x147711560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x1477119d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x147711e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x1477122b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x147712720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x147712b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x147713000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x147713470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x1477138e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x147713d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x1477141c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x147714630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x147714aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x147714f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x147715380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x1477157f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x147715c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x1477160d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x147716640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x147716b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x147716fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x147717420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x147717890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x147717d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x147718170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x1477185e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x147718a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x147718ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x147719330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x1477197a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x147719c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14771a080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14771a4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14771a960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14771add0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14771b240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14771b6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14771bb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14771bf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14771c400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14771c870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14771cce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14771d150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x14771d5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x14771da30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x14771dea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x14771e310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x14771e780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x14771ebf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x14771f060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x14771f4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x14771f940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x14771fdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x147720220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x147720690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x147720b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x147720f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x1477213e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x147721850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x147721cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x147722130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x1477225a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x147722a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x147722e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x1477232f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x147723b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x147723e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x1477242b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x147724720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x147724b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x147725000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x147725470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x1477258e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x147725d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x1477261c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x147726630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x147726aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x147726f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x147727380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x1477277f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x147727c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x1477280d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x147728540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x1477289b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x147728e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x147729290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x147729700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x147729b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x147729fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14772a450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14772a8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14772ad30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14772b1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14772b610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14772ba80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14772bef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14772c360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14772c7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14772cc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14772d0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14772d520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14772d990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14772de00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14772e270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14772e6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14772eb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14772efc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14772f430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14772f8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14772fd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x147730180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x1477305f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x147730a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x147730ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x147731340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x1477317b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x147731c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x147732090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x147732500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x147732970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x147732de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x147733250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x1477336c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x147733b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x147733fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x147734410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x147734880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x147734cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x147735160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x1477355d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x147735a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x147735eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x147736320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x147736790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x147736c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x147737070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x1477374e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x147737950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x147737dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x147738230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x1477386a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x147738b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x147738f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x1477393f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x147739860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x147739cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14773a140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14773a5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14773aa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14773ae90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14773b300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14773b770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14773bbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14773c050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14773c4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14773c930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14773cda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14773d210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14773d680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14773daf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14773df60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14773e3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14773e840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14773ecb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14773f120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14773f590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14773fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x14773fe70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x1477402e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x147740750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x147740bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x147741030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x147741bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x147741e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x147742130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x1477425a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x147742a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x147742e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x1477432f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x147743760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x147743bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x147744040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x1477444b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x147744920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x147744d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x147745200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x147745670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x147745ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x147745f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x1477463c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x147746830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x147746ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x147747110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x147747580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x1477479f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x147747e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x1477482d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x147748740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x147748bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x147749020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x147749490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x147749900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x147749d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14774a1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14774a650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14774aac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14774af30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14774b3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14774b810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14774bc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14774c0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14774c560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14774c9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14774ce40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14774d2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14774d720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14774db90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14774e000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14774e470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14774e8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14774ed50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14774f1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14774f630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14774faa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14774ff10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x147750380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x1477507f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x147750c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x1477510d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x147751540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x1477519b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x147751e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x147752290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x147752700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x147752b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x147752fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x147753450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1477538c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x147753d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x1477541a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x147754610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x147754a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x147754ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x147755360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x1477557d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x147756240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x147756960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x147757080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1477577a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x147757a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x147757ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1477584d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x147758ae0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.917s
user	0m0.242s
sys	0m0.136s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
