Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:301 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- ARM -mcpu not found, -mcpu=native will be used
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod - Success
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sve
-- Performing Test GGML_MACHINE_SUPPORTS_sve - Failed
-- ARM feature DOTPROD enabled
-- ARM feature MATMUL_INT8 enabled
-- ARM feature FMA enabled
-- ARM feature FP16_VECTOR_ARITHMETIC enabled
-- Adding CPU backend variant ggml-cpu: -mcpu=native+dotprod+i8mm+nosve 
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (2.3s)
-- Generating done (0.2s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m2.530s
user	0m0.868s
sys	0m1.225s
++ nproc
+ make -j10
[  0%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  0%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  1%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  3%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  3%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  3%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  3%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  3%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  5%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  5%] Built target sha256
[  5%] Built target build_info
[  5%] Built target sha1
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o
[  5%] Built target xxhash
[  6%] Linking CXX shared library libggml-base.dylib
[  6%] Built target ggml-base
[  6%] Generate assembly for embedded Metal library
Embedding Metal library
[  6%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[  6%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[  7%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[  7%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 11%] Linking CXX shared library libggml-blas.dylib
[ 12%] Linking CXX shared library libggml-cpu.dylib
[ 12%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 13%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 13%] Built target ggml-blas
[ 13%] Built target ggml-cpu
[ 14%] Linking C shared library libggml-metal.dylib
[ 14%] Built target ggml-metal
[ 14%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 15%] Linking CXX shared library libggml.dylib
[ 15%] Built target ggml
[ 15%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 15%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o
[ 21%] Linking CXX executable ../../bin/llama-gguf-hash
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 22%] Linking CXX executable ../../bin/llama-gguf
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 25%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 25%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 26%] Linking CXX shared library libllama.dylib
[ 26%] Built target llama-gguf-hash
[ 26%] Built target llama-gguf
[ 26%] Built target llama
[ 27%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 28%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 28%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 28%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 28%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 28%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 29%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 29%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 29%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 30%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 30%] Linking CXX executable ../../bin/llama-quantize-stats
[ 31%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 32%] Linking CXX executable ../../bin/llama-simple
[ 33%] Linking C executable ../bin/test-c
[ 34%] Linking CXX executable ../../bin/llama-simple-chat
[ 34%] Built target llava
[ 34%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 35%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 35%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 35%] Linking CXX shared library libllava_shared.dylib
[ 36%] Linking CXX static library libllava_static.a
[ 37%] Linking CXX static library libcommon.a
[ 37%] Built target llama-simple-chat
[ 37%] Built target llama-quantize-stats
[ 37%] Built target test-c
[ 37%] Built target llama-simple
[ 37%] Built target llava_static
[ 37%] Built target common
[ 37%] Built target llava_shared
[ 37%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 44%] Linking CXX executable ../bin/test-tokenizer-0
[ 44%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 44%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 44%] Linking CXX executable ../bin/test-sampling
[ 44%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 45%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 45%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 45%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 47%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 48%] Linking CXX executable ../bin/test-grammar-integration
[ 48%] Linking CXX executable ../bin/test-llama-grammar
[ 49%] Linking CXX executable ../bin/test-log
[ 49%] Linking CXX executable ../bin/test-grammar-parser
[ 50%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 50%] Built target test-tokenizer-1-bpe
[ 50%] Built target test-sampling
[ 50%] Built target test-tokenizer-1-spm
[ 50%] Built target test-tokenizer-0
[ 50%] Linking CXX executable ../bin/test-arg-parser
[ 50%] Built target test-grammar-integration
[ 50%] Built target test-log
[ 50%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 51%] Built target test-json-schema-to-grammar
[ 51%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 51%] Built target test-llama-grammar
[ 52%] Building CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 53%] Built target test-grammar-parser
[ 53%] Linking CXX executable ../bin/test-chat-template
[ 54%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 55%] Linking CXX executable ../bin/test-gguf
[ 55%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 56%] Linking CXX executable ../bin/test-model-load-cancel
[ 57%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 57%] Built target test-arg-parser
[ 57%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 57%] Linking CXX executable ../bin/test-backend-ops
[ 57%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 58%] Linking CXX executable ../bin/test-autorelease
[ 59%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 59%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 59%] Built target test-gguf
[ 60%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 61%] Linking CXX executable ../bin/test-barrier
[ 61%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 61%] Built target test-chat-template
[ 61%] Built target test-model-load-cancel
[ 61%] Linking CXX executable ../bin/test-quantize-perf
[ 62%] Linking CXX executable ../bin/test-quantize-fns
[ 62%] Built target test-backend-ops
[ 62%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 62%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 63%] Linking CXX executable ../bin/test-rope
[ 63%] Built target test-autorelease
[ 63%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 63%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 63%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 64%] Linking CXX executable ../../bin/llama-batched-bench
[ 64%] Built target test-quantize-perf
[ 64%] Built target test-barrier
[ 65%] Linking CXX executable ../../bin/llama-batched
[ 65%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 65%] Built target test-quantize-fns
[ 66%] Linking CXX executable ../../bin/llama-embedding
[ 67%] Linking CXX executable ../../bin/llama-eval-callback
[ 68%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 69%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 69%] Built target test-rope
[ 70%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 70%] Linking CXX executable ../../bin/llama-gguf-split
[ 71%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 71%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 71%] Built target llama-batched
[ 71%] Built target llama-batched-bench
[ 71%] Linking CXX executable ../../bin/llama-gritlm
[ 71%] Linking CXX executable ../../bin/llama-imatrix
[ 71%] Linking CXX executable ../../bin/llama-infill
[ 71%] Built target llama-embedding
[ 71%] Built target llama-eval-callback
[ 71%] Built target llama-gbnf-validator
[ 72%] Linking CXX executable ../../bin/llama-bench
[ 73%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 74%] Built target llama-gguf-split
[ 75%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 76%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 77%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 77%] Built target llama-gritlm
[ 77%] Built target llama-imatrix
[ 77%] Linking CXX executable ../../bin/llama-lookup
[ 77%] Linking CXX executable ../../bin/llama-lookahead
[ 77%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 77%] Built target llama-infill
[ 77%] Linking CXX executable ../../bin/llama-lookup-merge
[ 77%] Linking CXX executable ../../bin/llama-lookup-create
[ 78%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 78%] Linking CXX executable ../../bin/llama-lookup-stats
[ 79%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 79%] Built target llama-bench
[ 80%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 81%] Linking CXX executable ../../bin/llama-cli
[ 81%] Linking CXX executable ../../bin/llama-passkey
[ 81%] Linking CXX executable ../../bin/llama-parallel
[ 81%] Built target llama-lookahead
[ 82%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 82%] Built target llama-lookup-merge
[ 82%] Linking CXX executable ../../bin/llama-perplexity
[ 82%] Built target llama-lookup-create
[ 82%] Built target llama-lookup
[ 82%] Built target llama-lookup-stats
[ 83%] Generating loading.html.hpp
[ 83%] Built target llama-cli
[ 84%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 84%] Generating index.html.gz.hpp
[ 84%] Linking CXX executable ../../bin/llama-quantize
[ 84%] Built target llama-passkey
[ 85%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 86%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 86%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 86%] Built target llama-perplexity
[ 86%] Built target llama-parallel
[ 86%] Linking CXX executable ../../bin/llama-retrieval
[ 86%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 86%] Linking CXX executable ../../bin/llama-run
[ 86%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 86%] Linking CXX executable ../../bin/llama-save-load-state
[ 87%] Linking CXX executable ../../bin/llama-speculative
[ 87%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 87%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 87%] Built target llama-quantize
[ 88%] Linking CXX executable ../../bin/llama-speculative-simple
[ 89%] Linking CXX executable ../../bin/llama-tokenize
[ 89%] Built target llama-retrieval
[ 90%] Linking CXX executable ../../bin/llama-tts
[ 90%] Built target llama-run
[ 91%] Linking CXX executable ../../bin/llama-gen-docs
[ 91%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 91%] Built target llama-save-load-state
[ 91%] Built target llama-speculative
[ 91%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 91%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 92%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 92%] Built target llama-speculative-simple
[ 93%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 93%] Built target llama-tokenize
[ 93%] Built target llama-tts
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 95%] Linking CXX executable ../../bin/llama-cvector-generator
[ 96%] Linking CXX executable ../../bin/llama-export-lora
[ 96%] Built target llama-gen-docs
[ 97%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 97%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 98%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 98%] Linking CXX executable ../../bin/llama-llava-cli
[ 98%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 99%] Linking CXX executable ../../bin/llama-vdot
[ 99%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 99%] Built target llama-convert-llama2c-to-ggml
[ 99%] Built target llama-cvector-generator
[ 99%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Built target llama-export-lora
[ 99%] Built target llama-minicpmv-cli
[ 99%] Built target llama-llava-cli
[ 99%] Built target llama-vdot
[ 99%] Built target llama-qwen2vl-cli
[ 99%] Built target llama-q8dot
[ 99%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m3.051s
user	0m6.005s
sys	0m9.100s

main: quantize time =  5357.45 ms
main:    total time =  5357.45 ms

main: quantize time =  1806.75 ms
main:    total time =  1806.75 ms

main: quantize time =  2317.75 ms
main:    total time =  2317.75 ms

main: quantize time =  1432.09 ms
main:    total time =  1432.09 ms

main: quantize time =  2231.34 ms
main:    total time =  2231.34 ms

main: quantize time =  5115.24 ms
main:    total time =  5115.24 ms

main: quantize time =  5851.52 ms
main:    total time =  5851.52 ms

main: quantize time =  6806.14 ms
main:    total time =  6806.14 ms

main: quantize time =  5702.81 ms
main:    total time =  5702.81 ms

main: quantize time =  4442.28 ms
main:    total time =  4442.28 ms
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.124 I build: 4477 (28f12723) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.211 I main: llama backend init
0.00.000.216 I main: load the model and apply lora adapter, if any
0.00.023.322 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.035.336 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.035.348 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.035.351 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.035.352 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.035.353 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.035.353 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.035.355 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.035.356 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.035.357 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.035.358 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.035.358 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.035.359 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.035.359 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.035.360 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.035.363 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.035.364 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.035.364 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.039.638 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.040.645 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.044.386 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.044.388 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.044.388 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.044.389 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.044.389 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.044.390 I llama_model_loader: - type  f32:  194 tensors
0.00.044.390 I llama_model_loader: - type  f16:   98 tensors
0.00.044.390 I print_info: file format = GGUF V3 (latest)
0.00.044.391 I print_info: file type   = all F32 (guessed)
0.00.044.393 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.064.101 I load: special tokens cache size = 25
0.00.070.326 I load: token to piece cache size = 0.2984 MB
0.00.070.331 I print_info: arch             = gptneox
0.00.070.332 I print_info: vocab_only       = 0
0.00.070.332 I print_info: n_ctx_train      = 2048
0.00.070.332 I print_info: n_embd           = 2048
0.00.070.332 I print_info: n_layer          = 24
0.00.070.337 I print_info: n_head           = 16
0.00.070.338 I print_info: n_head_kv        = 16
0.00.070.338 I print_info: n_rot            = 32
0.00.070.338 I print_info: n_swa            = 0
0.00.070.338 I print_info: n_embd_head_k    = 128
0.00.070.339 I print_info: n_embd_head_v    = 128
0.00.070.339 I print_info: n_gqa            = 1
0.00.070.340 I print_info: n_embd_k_gqa     = 2048
0.00.070.344 I print_info: n_embd_v_gqa     = 2048
0.00.070.344 I print_info: f_norm_eps       = 1.0e-05
0.00.070.348 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.070.349 I print_info: f_clamp_kqv      = 0.0e+00
0.00.070.350 I print_info: f_max_alibi_bias = 0.0e+00
0.00.070.350 I print_info: f_logit_scale    = 0.0e+00
0.00.070.350 I print_info: n_ff             = 8192
0.00.070.351 I print_info: n_expert         = 0
0.00.070.351 I print_info: n_expert_used    = 0
0.00.070.351 I print_info: causal attn      = 1
0.00.070.351 I print_info: pooling type     = 0
0.00.070.351 I print_info: rope type        = 2
0.00.070.351 I print_info: rope scaling     = linear
0.00.070.352 I print_info: freq_base_train  = 10000.0
0.00.070.352 I print_info: freq_scale_train = 1
0.00.070.352 I print_info: n_ctx_orig_yarn  = 2048
0.00.070.352 I print_info: rope_finetuned   = unknown
0.00.070.352 I print_info: ssm_d_conv       = 0
0.00.070.353 I print_info: ssm_d_inner      = 0
0.00.070.353 I print_info: ssm_d_state      = 0
0.00.070.353 I print_info: ssm_dt_rank      = 0
0.00.070.353 I print_info: ssm_dt_b_c_rms   = 0
0.00.070.353 I print_info: model type       = 1.4B
0.00.070.354 I print_info: model params     = 1.41 B
0.00.070.354 I print_info: general.name     = 1.4B
0.00.070.354 I print_info: vocab type       = BPE
0.00.070.355 I print_info: n_vocab          = 50304
0.00.070.355 I print_info: n_merges         = 50009
0.00.070.355 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.070.355 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.070.355 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.070.356 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.070.356 I print_info: LF token         = 128 'Ä'
0.00.070.356 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.070.356 I print_info: max token length = 1024
0.00.072.236 I load_tensors: offloading 24 repeating layers to GPU
0.00.072.237 I load_tensors: offloading output layer to GPU
0.00.072.237 I load_tensors: offloaded 25/25 layers to GPU
0.00.072.256 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.072.257 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.072.546 I llama_init_from_model: n_seq_max     = 1
0.00.072.547 I llama_init_from_model: n_ctx         = 2048
0.00.072.547 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.072.548 I llama_init_from_model: n_batch       = 2048
0.00.072.548 I llama_init_from_model: n_ubatch      = 512
0.00.072.548 I llama_init_from_model: flash_attn    = 0
0.00.072.548 I llama_init_from_model: freq_base     = 10000.0
0.00.072.549 I llama_init_from_model: freq_scale    = 1
0.00.072.549 I ggml_metal_init: allocating
0.00.072.553 I ggml_metal_init: found device: Apple M4
0.00.072.555 I ggml_metal_init: picking default device: Apple M4
0.00.073.234 I ggml_metal_init: using embedded metal library
0.00.094.370 I ggml_metal_init: GPU name:   Apple M4
0.00.094.374 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.094.374 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.094.375 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.094.375 I ggml_metal_init: simdgroup reduction   = true
0.00.094.375 I ggml_metal_init: simdgroup matrix mul. = true
0.00.094.375 I ggml_metal_init: has bfloat            = true
0.00.094.375 I ggml_metal_init: use bfloat            = true
0.00.094.376 I ggml_metal_init: hasUnifiedMemory      = true
0.00.094.378 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.235.101 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.265.474 I init:      Metal KV buffer size =   384.00 MiB
0.00.265.480 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.265.498 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.266.414 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.266.416 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.266.416 I llama_init_from_model: graph nodes  = 967
0.00.266.416 I llama_init_from_model: graph splits = 2
0.00.266.419 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.266.550 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.266.551 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.350.884 I main: llama threadpool init, n_threads = 4
0.00.350.931 I 
0.00.350.955 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.350.956 I 
0.00.351.027 I sampler seed: 1234
0.00.351.031 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.351.057 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.351.059 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.351.059 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.188.808 I llama_perf_sampler_print:    sampling time =       1.26 ms /    71 runs   (    0.02 ms per token, 56573.71 tokens per second)
0.02.188.809 I llama_perf_context_print:        load time =     327.56 ms
0.02.188.810 I llama_perf_context_print: prompt eval time =      43.72 ms /     7 tokens (    6.25 ms per token,   160.12 tokens per second)
0.02.188.810 I llama_perf_context_print:        eval time =    1791.18 ms /    63 runs   (   28.43 ms per token,    35.17 tokens per second)
0.02.188.812 I llama_perf_context_print:       total time =    1837.93 ms /    70 tokens
0.02.192.625 I ggml_metal_free: deallocating

real	0m2.494s
user	0m0.125s
sys	0m0.110s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.055 I build: 4477 (28f12723) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.086 I main: llama backend init
0.00.000.088 I main: load the model and apply lora adapter, if any
0.00.009.891 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.024.095 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.024.102 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.024.104 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.024.104 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.024.105 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.024.105 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.024.105 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.024.106 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.024.107 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.024.107 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.024.107 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.024.108 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.024.108 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.024.109 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.024.111 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.024.111 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.024.111 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.027.982 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.029.044 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.032.922 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.032.924 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.032.925 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.032.925 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.032.925 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.032.926 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.032.926 I llama_model_loader: - type  f32:  194 tensors
0.00.032.927 I llama_model_loader: - type q8_0:   98 tensors
0.00.032.927 I print_info: file format = GGUF V3 (latest)
0.00.032.928 I print_info: file type   = Q8_0
0.00.032.929 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.053.201 I load: special tokens cache size = 25
0.00.059.259 I load: token to piece cache size = 0.2984 MB
0.00.059.264 I print_info: arch             = gptneox
0.00.059.264 I print_info: vocab_only       = 0
0.00.059.264 I print_info: n_ctx_train      = 2048
0.00.059.264 I print_info: n_embd           = 2048
0.00.059.265 I print_info: n_layer          = 24
0.00.059.270 I print_info: n_head           = 16
0.00.059.271 I print_info: n_head_kv        = 16
0.00.059.271 I print_info: n_rot            = 32
0.00.059.271 I print_info: n_swa            = 0
0.00.059.271 I print_info: n_embd_head_k    = 128
0.00.059.272 I print_info: n_embd_head_v    = 128
0.00.059.272 I print_info: n_gqa            = 1
0.00.059.273 I print_info: n_embd_k_gqa     = 2048
0.00.059.274 I print_info: n_embd_v_gqa     = 2048
0.00.059.274 I print_info: f_norm_eps       = 1.0e-05
0.00.059.275 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.059.275 I print_info: f_clamp_kqv      = 0.0e+00
0.00.059.275 I print_info: f_max_alibi_bias = 0.0e+00
0.00.059.275 I print_info: f_logit_scale    = 0.0e+00
0.00.059.276 I print_info: n_ff             = 8192
0.00.059.276 I print_info: n_expert         = 0
0.00.059.277 I print_info: n_expert_used    = 0
0.00.059.277 I print_info: causal attn      = 1
0.00.059.277 I print_info: pooling type     = 0
0.00.059.277 I print_info: rope type        = 2
0.00.059.277 I print_info: rope scaling     = linear
0.00.059.278 I print_info: freq_base_train  = 10000.0
0.00.059.278 I print_info: freq_scale_train = 1
0.00.059.278 I print_info: n_ctx_orig_yarn  = 2048
0.00.059.279 I print_info: rope_finetuned   = unknown
0.00.059.279 I print_info: ssm_d_conv       = 0
0.00.059.279 I print_info: ssm_d_inner      = 0
0.00.059.279 I print_info: ssm_d_state      = 0
0.00.059.279 I print_info: ssm_dt_rank      = 0
0.00.059.280 I print_info: ssm_dt_b_c_rms   = 0
0.00.059.280 I print_info: model type       = 1.4B
0.00.059.284 I print_info: model params     = 1.41 B
0.00.059.284 I print_info: general.name     = 1.4B
0.00.059.285 I print_info: vocab type       = BPE
0.00.059.285 I print_info: n_vocab          = 50304
0.00.059.285 I print_info: n_merges         = 50009
0.00.059.285 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.059.285 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.059.286 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.059.287 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.059.287 I print_info: LF token         = 128 'Ä'
0.00.059.287 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.059.287 I print_info: max token length = 1024
0.00.061.893 I load_tensors: offloading 24 repeating layers to GPU
0.00.061.894 I load_tensors: offloading output layer to GPU
0.00.061.894 I load_tensors: offloaded 25/25 layers to GPU
0.00.061.906 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.061.907 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.00.062.267 I llama_init_from_model: n_seq_max     = 1
0.00.062.268 I llama_init_from_model: n_ctx         = 2048
0.00.062.268 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.062.268 I llama_init_from_model: n_batch       = 2048
0.00.062.268 I llama_init_from_model: n_ubatch      = 512
0.00.062.269 I llama_init_from_model: flash_attn    = 0
0.00.062.269 I llama_init_from_model: freq_base     = 10000.0
0.00.062.269 I llama_init_from_model: freq_scale    = 1
0.00.062.270 I ggml_metal_init: allocating
0.00.062.274 I ggml_metal_init: found device: Apple M4
0.00.062.276 I ggml_metal_init: picking default device: Apple M4
0.00.063.058 I ggml_metal_init: using embedded metal library
0.00.065.637 I ggml_metal_init: GPU name:   Apple M4
0.00.065.639 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.065.639 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.065.640 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.065.640 I ggml_metal_init: simdgroup reduction   = true
0.00.065.640 I ggml_metal_init: simdgroup matrix mul. = true
0.00.065.640 I ggml_metal_init: has bfloat            = true
0.00.065.641 I ggml_metal_init: use bfloat            = true
0.00.065.641 I ggml_metal_init: hasUnifiedMemory      = true
0.00.065.642 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.076.103 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.100.067 I init:      Metal KV buffer size =   384.00 MiB
0.00.100.082 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.100.108 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.101.272 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.101.274 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.101.274 I llama_init_from_model: graph nodes  = 967
0.00.101.274 I llama_init_from_model: graph splits = 2
0.00.101.279 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.101.412 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.101.413 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.215.760 I main: llama threadpool init, n_threads = 4
0.01.215.792 I 
0.01.215.815 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.215.815 I 
0.01.216.056 I sampler seed: 1234
0.01.216.061 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.216.073 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.216.074 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.216.074 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.308.319 I llama_perf_sampler_print:    sampling time =       1.23 ms /    71 runs   (    0.02 ms per token, 57676.69 tokens per second)
0.02.308.320 I llama_perf_context_print:        load time =    1205.87 ms
0.02.308.321 I llama_perf_context_print: prompt eval time =      45.77 ms /     7 tokens (    6.54 ms per token,   152.96 tokens per second)
0.02.308.322 I llama_perf_context_print:        eval time =    1043.42 ms /    63 runs   (   16.56 ms per token,    60.38 tokens per second)
0.02.308.322 I llama_perf_context_print:       total time =    1092.56 ms /    70 tokens
0.02.311.473 I ggml_metal_free: deallocating

real	0m2.330s
user	0m0.112s
sys	0m0.224s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.051 I build: 4477 (28f12723) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.082 I main: llama backend init
0.00.000.084 I main: load the model and apply lora adapter, if any
0.00.012.302 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.021.759 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.021.763 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.021.765 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.021.766 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.021.766 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.021.767 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.021.767 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.021.768 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.021.768 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.021.769 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.021.769 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.021.769 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.021.770 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.021.770 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.021.772 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.021.772 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.021.773 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.025.586 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.026.591 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.030.354 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.030.355 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.030.355 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.030.356 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.030.356 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.030.356 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.030.357 I llama_model_loader: - type  f32:  194 tensors
0.00.030.357 I llama_model_loader: - type q4_0:   97 tensors
0.00.030.357 I llama_model_loader: - type q6_K:    1 tensors
0.00.030.358 I print_info: file format = GGUF V3 (latest)
0.00.030.359 I print_info: file type   = Q4_0
0.00.030.360 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.049.565 I load: special tokens cache size = 25
0.00.055.590 I load: token to piece cache size = 0.2984 MB
0.00.055.594 I print_info: arch             = gptneox
0.00.055.594 I print_info: vocab_only       = 0
0.00.055.594 I print_info: n_ctx_train      = 2048
0.00.055.594 I print_info: n_embd           = 2048
0.00.055.595 I print_info: n_layer          = 24
0.00.055.598 I print_info: n_head           = 16
0.00.055.599 I print_info: n_head_kv        = 16
0.00.055.599 I print_info: n_rot            = 32
0.00.055.600 I print_info: n_swa            = 0
0.00.055.600 I print_info: n_embd_head_k    = 128
0.00.055.600 I print_info: n_embd_head_v    = 128
0.00.055.601 I print_info: n_gqa            = 1
0.00.055.602 I print_info: n_embd_k_gqa     = 2048
0.00.055.602 I print_info: n_embd_v_gqa     = 2048
0.00.055.603 I print_info: f_norm_eps       = 1.0e-05
0.00.055.603 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.055.603 I print_info: f_clamp_kqv      = 0.0e+00
0.00.055.604 I print_info: f_max_alibi_bias = 0.0e+00
0.00.055.604 I print_info: f_logit_scale    = 0.0e+00
0.00.055.607 I print_info: n_ff             = 8192
0.00.055.607 I print_info: n_expert         = 0
0.00.055.607 I print_info: n_expert_used    = 0
0.00.055.608 I print_info: causal attn      = 1
0.00.055.609 I print_info: pooling type     = 0
0.00.055.609 I print_info: rope type        = 2
0.00.055.610 I print_info: rope scaling     = linear
0.00.055.610 I print_info: freq_base_train  = 10000.0
0.00.055.610 I print_info: freq_scale_train = 1
0.00.055.610 I print_info: n_ctx_orig_yarn  = 2048
0.00.055.611 I print_info: rope_finetuned   = unknown
0.00.055.611 I print_info: ssm_d_conv       = 0
0.00.055.611 I print_info: ssm_d_inner      = 0
0.00.055.611 I print_info: ssm_d_state      = 0
0.00.055.611 I print_info: ssm_dt_rank      = 0
0.00.055.611 I print_info: ssm_dt_b_c_rms   = 0
0.00.055.611 I print_info: model type       = 1.4B
0.00.055.612 I print_info: model params     = 1.41 B
0.00.055.612 I print_info: general.name     = 1.4B
0.00.055.617 I print_info: vocab type       = BPE
0.00.055.617 I print_info: n_vocab          = 50304
0.00.055.617 I print_info: n_merges         = 50009
0.00.055.622 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.055.622 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.055.622 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.055.623 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.055.623 I print_info: LF token         = 128 'Ä'
0.00.055.623 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.055.623 I print_info: max token length = 1024
0.00.057.792 I load_tensors: offloading 24 repeating layers to GPU
0.00.057.792 I load_tensors: offloading output layer to GPU
0.00.057.792 I load_tensors: offloaded 25/25 layers to GPU
0.00.057.803 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.057.804 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.058.132 I llama_init_from_model: n_seq_max     = 1
0.00.058.132 I llama_init_from_model: n_ctx         = 2048
0.00.058.133 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.058.133 I llama_init_from_model: n_batch       = 2048
0.00.058.133 I llama_init_from_model: n_ubatch      = 512
0.00.058.133 I llama_init_from_model: flash_attn    = 0
0.00.058.134 I llama_init_from_model: freq_base     = 10000.0
0.00.058.134 I llama_init_from_model: freq_scale    = 1
0.00.058.134 I ggml_metal_init: allocating
0.00.058.137 I ggml_metal_init: found device: Apple M4
0.00.058.139 I ggml_metal_init: picking default device: Apple M4
0.00.058.832 I ggml_metal_init: using embedded metal library
0.00.061.345 I ggml_metal_init: GPU name:   Apple M4
0.00.061.347 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.061.347 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.061.347 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.061.348 I ggml_metal_init: simdgroup reduction   = true
0.00.061.348 I ggml_metal_init: simdgroup matrix mul. = true
0.00.061.348 I ggml_metal_init: has bfloat            = true
0.00.061.348 I ggml_metal_init: use bfloat            = true
0.00.061.349 I ggml_metal_init: hasUnifiedMemory      = true
0.00.061.349 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.071.476 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.095.201 I init:      Metal KV buffer size =   384.00 MiB
0.00.095.213 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.095.242 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.096.368 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.096.370 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.096.370 I llama_init_from_model: graph nodes  = 967
0.00.096.371 I llama_init_from_model: graph splits = 2
0.00.096.374 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.096.496 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.096.497 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.679.027 I main: llama threadpool init, n_threads = 4
0.00.679.071 I 
0.00.679.096 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.679.098 I 
0.00.679.333 I sampler seed: 1234
0.00.679.338 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.679.349 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.679.349 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.679.350 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.362.953 I llama_perf_sampler_print:    sampling time =       1.24 ms /    71 runs   (    0.02 ms per token, 57165.86 tokens per second)
0.01.362.954 I llama_perf_context_print:        load time =     666.72 ms
0.01.362.954 I llama_perf_context_print: prompt eval time =      42.69 ms /     7 tokens (    6.10 ms per token,   163.98 tokens per second)
0.01.362.955 I llama_perf_context_print:        eval time =     637.85 ms /    63 runs   (   10.12 ms per token,    98.77 tokens per second)
0.01.362.956 I llama_perf_context_print:       total time =     683.93 ms /    70 tokens
0.01.366.010 I ggml_metal_free: deallocating

real	0m1.386s
user	0m0.110s
sys	0m0.152s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4477 (28f12723) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.074 I main: llama backend init
0.00.000.076 I main: load the model and apply lora adapter, if any
0.00.009.097 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.025.466 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.025.471 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.025.473 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.025.473 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.025.473 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.025.474 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.025.478 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.025.481 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.025.482 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.025.482 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.025.483 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.025.486 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.025.486 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.025.487 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.025.492 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.025.492 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.025.492 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.029.302 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.030.324 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.034.193 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.034.194 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.034.195 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.034.195 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.034.195 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.034.196 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.034.196 I llama_model_loader: - type  f32:  194 tensors
0.00.034.196 I llama_model_loader: - type q4_1:   97 tensors
0.00.034.197 I llama_model_loader: - type q6_K:    1 tensors
0.00.034.197 I print_info: file format = GGUF V3 (latest)
0.00.034.198 I print_info: file type   = Q4_1
0.00.034.199 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.055.526 I load: special tokens cache size = 25
0.00.062.111 I load: token to piece cache size = 0.2984 MB
0.00.062.114 I print_info: arch             = gptneox
0.00.062.114 I print_info: vocab_only       = 0
0.00.062.114 I print_info: n_ctx_train      = 2048
0.00.062.114 I print_info: n_embd           = 2048
0.00.062.115 I print_info: n_layer          = 24
0.00.062.117 I print_info: n_head           = 16
0.00.062.118 I print_info: n_head_kv        = 16
0.00.062.118 I print_info: n_rot            = 32
0.00.062.119 I print_info: n_swa            = 0
0.00.062.119 I print_info: n_embd_head_k    = 128
0.00.062.119 I print_info: n_embd_head_v    = 128
0.00.062.120 I print_info: n_gqa            = 1
0.00.062.121 I print_info: n_embd_k_gqa     = 2048
0.00.062.121 I print_info: n_embd_v_gqa     = 2048
0.00.062.122 I print_info: f_norm_eps       = 1.0e-05
0.00.062.124 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.062.124 I print_info: f_clamp_kqv      = 0.0e+00
0.00.062.124 I print_info: f_max_alibi_bias = 0.0e+00
0.00.062.124 I print_info: f_logit_scale    = 0.0e+00
0.00.062.125 I print_info: n_ff             = 8192
0.00.062.125 I print_info: n_expert         = 0
0.00.062.125 I print_info: n_expert_used    = 0
0.00.062.127 I print_info: causal attn      = 1
0.00.062.128 I print_info: pooling type     = 0
0.00.062.128 I print_info: rope type        = 2
0.00.062.129 I print_info: rope scaling     = linear
0.00.062.129 I print_info: freq_base_train  = 10000.0
0.00.062.130 I print_info: freq_scale_train = 1
0.00.062.130 I print_info: n_ctx_orig_yarn  = 2048
0.00.062.130 I print_info: rope_finetuned   = unknown
0.00.062.130 I print_info: ssm_d_conv       = 0
0.00.062.130 I print_info: ssm_d_inner      = 0
0.00.062.130 I print_info: ssm_d_state      = 0
0.00.062.130 I print_info: ssm_dt_rank      = 0
0.00.062.130 I print_info: ssm_dt_b_c_rms   = 0
0.00.062.131 I print_info: model type       = 1.4B
0.00.062.131 I print_info: model params     = 1.41 B
0.00.062.131 I print_info: general.name     = 1.4B
0.00.062.132 I print_info: vocab type       = BPE
0.00.062.132 I print_info: n_vocab          = 50304
0.00.062.132 I print_info: n_merges         = 50009
0.00.062.132 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.062.132 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.062.133 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.062.133 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.062.133 I print_info: LF token         = 128 'Ä'
0.00.062.137 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.062.137 I print_info: max token length = 1024
0.00.064.119 I load_tensors: offloading 24 repeating layers to GPU
0.00.064.119 I load_tensors: offloading output layer to GPU
0.00.064.120 I load_tensors: offloaded 25/25 layers to GPU
0.00.064.130 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.064.132 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.064.414 I llama_init_from_model: n_seq_max     = 1
0.00.064.415 I llama_init_from_model: n_ctx         = 2048
0.00.064.415 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.064.415 I llama_init_from_model: n_batch       = 2048
0.00.064.415 I llama_init_from_model: n_ubatch      = 512
0.00.064.415 I llama_init_from_model: flash_attn    = 0
0.00.064.416 I llama_init_from_model: freq_base     = 10000.0
0.00.064.416 I llama_init_from_model: freq_scale    = 1
0.00.064.416 I ggml_metal_init: allocating
0.00.064.420 I ggml_metal_init: found device: Apple M4
0.00.064.422 I ggml_metal_init: picking default device: Apple M4
0.00.065.029 I ggml_metal_init: using embedded metal library
0.00.067.516 I ggml_metal_init: GPU name:   Apple M4
0.00.067.518 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.067.518 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.067.518 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.067.519 I ggml_metal_init: simdgroup reduction   = true
0.00.067.519 I ggml_metal_init: simdgroup matrix mul. = true
0.00.067.519 I ggml_metal_init: has bfloat            = true
0.00.067.519 I ggml_metal_init: use bfloat            = true
0.00.067.520 I ggml_metal_init: hasUnifiedMemory      = true
0.00.067.520 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.077.698 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.098.130 I init:      Metal KV buffer size =   384.00 MiB
0.00.098.141 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.098.161 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.099.208 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.099.209 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.099.210 I llama_init_from_model: graph nodes  = 967
0.00.099.210 I llama_init_from_model: graph splits = 2
0.00.099.213 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.099.343 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.099.344 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.779.555 I main: llama threadpool init, n_threads = 4
0.00.779.591 I 
0.00.779.616 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.779.616 I 
0.00.779.848 I sampler seed: 1234
0.00.779.853 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.779.874 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.779.874 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.779.874 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.511.615 I llama_perf_sampler_print:    sampling time =       1.16 ms /    71 runs   (    0.02 ms per token, 61049.01 tokens per second)
0.01.511.616 I llama_perf_context_print:        load time =     770.45 ms
0.01.511.617 I llama_perf_context_print: prompt eval time =      42.18 ms /     7 tokens (    6.03 ms per token,   165.97 tokens per second)
0.01.511.620 I llama_perf_context_print:        eval time =     686.58 ms /    63 runs   (   10.90 ms per token,    91.76 tokens per second)
0.01.511.621 I llama_perf_context_print:       total time =     732.06 ms /    70 tokens
0.01.514.306 I ggml_metal_free: deallocating

real	0m1.529s
user	0m0.112s
sys	0m0.152s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4477 (28f12723) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.071 I main: llama backend init
0.00.000.073 I main: load the model and apply lora adapter, if any
0.00.013.741 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.035.404 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.035.410 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.035.417 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.035.418 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.035.419 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.035.419 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.035.419 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.035.420 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.035.421 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.035.421 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.035.422 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.035.422 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.035.423 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.035.423 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.035.426 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.035.426 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.035.427 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.041.303 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.042.838 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.048.627 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.048.629 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.048.629 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.048.630 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.048.630 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.048.631 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.048.631 I llama_model_loader: - type  f32:  194 tensors
0.00.048.632 I llama_model_loader: - type q5_0:   97 tensors
0.00.048.632 I llama_model_loader: - type q6_K:    1 tensors
0.00.048.633 I print_info: file format = GGUF V3 (latest)
0.00.048.633 I print_info: file type   = Q5_0
0.00.048.634 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.085.171 I load: special tokens cache size = 25
0.00.095.173 I load: token to piece cache size = 0.2984 MB
0.00.095.177 I print_info: arch             = gptneox
0.00.095.178 I print_info: vocab_only       = 0
0.00.095.178 I print_info: n_ctx_train      = 2048
0.00.095.178 I print_info: n_embd           = 2048
0.00.095.179 I print_info: n_layer          = 24
0.00.095.182 I print_info: n_head           = 16
0.00.095.183 I print_info: n_head_kv        = 16
0.00.095.183 I print_info: n_rot            = 32
0.00.095.183 I print_info: n_swa            = 0
0.00.095.184 I print_info: n_embd_head_k    = 128
0.00.095.184 I print_info: n_embd_head_v    = 128
0.00.095.185 I print_info: n_gqa            = 1
0.00.095.186 I print_info: n_embd_k_gqa     = 2048
0.00.095.186 I print_info: n_embd_v_gqa     = 2048
0.00.095.187 I print_info: f_norm_eps       = 1.0e-05
0.00.095.188 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.095.188 I print_info: f_clamp_kqv      = 0.0e+00
0.00.095.188 I print_info: f_max_alibi_bias = 0.0e+00
0.00.095.188 I print_info: f_logit_scale    = 0.0e+00
0.00.095.189 I print_info: n_ff             = 8192
0.00.095.189 I print_info: n_expert         = 0
0.00.095.189 I print_info: n_expert_used    = 0
0.00.095.190 I print_info: causal attn      = 1
0.00.095.190 I print_info: pooling type     = 0
0.00.095.190 I print_info: rope type        = 2
0.00.095.190 I print_info: rope scaling     = linear
0.00.095.192 I print_info: freq_base_train  = 10000.0
0.00.095.194 I print_info: freq_scale_train = 1
0.00.095.194 I print_info: n_ctx_orig_yarn  = 2048
0.00.095.194 I print_info: rope_finetuned   = unknown
0.00.095.195 I print_info: ssm_d_conv       = 0
0.00.095.195 I print_info: ssm_d_inner      = 0
0.00.095.195 I print_info: ssm_d_state      = 0
0.00.095.195 I print_info: ssm_dt_rank      = 0
0.00.095.195 I print_info: ssm_dt_b_c_rms   = 0
0.00.095.196 I print_info: model type       = 1.4B
0.00.095.196 I print_info: model params     = 1.41 B
0.00.095.196 I print_info: general.name     = 1.4B
0.00.095.197 I print_info: vocab type       = BPE
0.00.095.197 I print_info: n_vocab          = 50304
0.00.095.197 I print_info: n_merges         = 50009
0.00.095.197 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.095.198 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.095.198 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.095.198 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.095.198 I print_info: LF token         = 128 'Ä'
0.00.095.199 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.095.203 I print_info: max token length = 1024
0.00.097.817 I load_tensors: offloading 24 repeating layers to GPU
0.00.097.817 I load_tensors: offloading output layer to GPU
0.00.097.817 I load_tensors: offloaded 25/25 layers to GPU
0.00.097.828 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.097.830 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.098.232 I llama_init_from_model: n_seq_max     = 1
0.00.098.233 I llama_init_from_model: n_ctx         = 2048
0.00.098.233 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.098.234 I llama_init_from_model: n_batch       = 2048
0.00.098.234 I llama_init_from_model: n_ubatch      = 512
0.00.098.234 I llama_init_from_model: flash_attn    = 0
0.00.098.235 I llama_init_from_model: freq_base     = 10000.0
0.00.098.235 I llama_init_from_model: freq_scale    = 1
0.00.098.235 I ggml_metal_init: allocating
0.00.098.239 I ggml_metal_init: found device: Apple M4
0.00.098.242 I ggml_metal_init: picking default device: Apple M4
0.00.099.030 I ggml_metal_init: using embedded metal library
0.00.102.352 I ggml_metal_init: GPU name:   Apple M4
0.00.102.354 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.102.354 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.102.355 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.102.355 I ggml_metal_init: simdgroup reduction   = true
0.00.102.355 I ggml_metal_init: simdgroup matrix mul. = true
0.00.102.356 I ggml_metal_init: has bfloat            = true
0.00.102.356 I ggml_metal_init: use bfloat            = true
0.00.102.356 I ggml_metal_init: hasUnifiedMemory      = true
0.00.102.357 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.113.163 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.135.707 I init:      Metal KV buffer size =   384.00 MiB
0.00.135.715 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.135.737 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.136.866 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.136.867 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.136.867 I llama_init_from_model: graph nodes  = 967
0.00.136.867 I llama_init_from_model: graph splits = 2
0.00.136.871 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.137.001 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.137.002 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.908.489 I main: llama threadpool init, n_threads = 4
0.00.908.572 I 
0.00.908.626 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.908.626 I 
0.00.909.117 I sampler seed: 1234
0.00.909.125 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.909.209 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.909.216 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.909.217 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.719.984 I llama_perf_sampler_print:    sampling time =       1.38 ms /    71 runs   (    0.02 ms per token, 51337.67 tokens per second)
0.01.719.984 I llama_perf_context_print:        load time =     894.74 ms
0.01.719.985 I llama_perf_context_print: prompt eval time =      54.76 ms /     7 tokens (    7.82 ms per token,   127.84 tokens per second)
0.01.719.986 I llama_perf_context_print:        eval time =     753.35 ms /    63 runs   (   11.96 ms per token,    83.63 tokens per second)
0.01.719.986 I llama_perf_context_print:       total time =     811.50 ms /    70 tokens
0.01.723.106 I ggml_metal_free: deallocating

real	0m1.760s
user	0m0.150s
sys	0m0.189s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.046 I build: 4477 (28f12723) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.076 I main: llama backend init
0.00.000.078 I main: load the model and apply lora adapter, if any
0.00.008.716 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.023.650 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.023.655 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.023.657 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.023.657 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.023.658 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.023.658 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.023.658 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.023.659 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.023.660 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.023.661 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.023.661 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.023.661 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.023.662 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.023.662 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.023.663 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.023.664 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.023.664 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.027.517 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.028.492 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.032.369 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.032.370 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.032.371 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.032.371 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.032.371 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.032.372 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.032.372 I llama_model_loader: - type  f32:  194 tensors
0.00.032.373 I llama_model_loader: - type q5_1:   97 tensors
0.00.032.373 I llama_model_loader: - type q6_K:    1 tensors
0.00.032.373 I print_info: file format = GGUF V3 (latest)
0.00.032.374 I print_info: file type   = Q5_1
0.00.032.375 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.053.007 I load: special tokens cache size = 25
0.00.059.271 I load: token to piece cache size = 0.2984 MB
0.00.059.274 I print_info: arch             = gptneox
0.00.059.274 I print_info: vocab_only       = 0
0.00.059.275 I print_info: n_ctx_train      = 2048
0.00.059.275 I print_info: n_embd           = 2048
0.00.059.275 I print_info: n_layer          = 24
0.00.059.278 I print_info: n_head           = 16
0.00.059.279 I print_info: n_head_kv        = 16
0.00.059.279 I print_info: n_rot            = 32
0.00.059.279 I print_info: n_swa            = 0
0.00.059.280 I print_info: n_embd_head_k    = 128
0.00.059.281 I print_info: n_embd_head_v    = 128
0.00.059.283 I print_info: n_gqa            = 1
0.00.059.284 I print_info: n_embd_k_gqa     = 2048
0.00.059.285 I print_info: n_embd_v_gqa     = 2048
0.00.059.285 I print_info: f_norm_eps       = 1.0e-05
0.00.059.286 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.059.286 I print_info: f_clamp_kqv      = 0.0e+00
0.00.059.286 I print_info: f_max_alibi_bias = 0.0e+00
0.00.059.286 I print_info: f_logit_scale    = 0.0e+00
0.00.059.287 I print_info: n_ff             = 8192
0.00.059.287 I print_info: n_expert         = 0
0.00.059.287 I print_info: n_expert_used    = 0
0.00.059.288 I print_info: causal attn      = 1
0.00.059.288 I print_info: pooling type     = 0
0.00.059.288 I print_info: rope type        = 2
0.00.059.288 I print_info: rope scaling     = linear
0.00.059.288 I print_info: freq_base_train  = 10000.0
0.00.059.290 I print_info: freq_scale_train = 1
0.00.059.290 I print_info: n_ctx_orig_yarn  = 2048
0.00.059.291 I print_info: rope_finetuned   = unknown
0.00.059.291 I print_info: ssm_d_conv       = 0
0.00.059.291 I print_info: ssm_d_inner      = 0
0.00.059.291 I print_info: ssm_d_state      = 0
0.00.059.291 I print_info: ssm_dt_rank      = 0
0.00.059.291 I print_info: ssm_dt_b_c_rms   = 0
0.00.059.291 I print_info: model type       = 1.4B
0.00.059.292 I print_info: model params     = 1.41 B
0.00.059.292 I print_info: general.name     = 1.4B
0.00.059.293 I print_info: vocab type       = BPE
0.00.059.293 I print_info: n_vocab          = 50304
0.00.059.293 I print_info: n_merges         = 50009
0.00.059.293 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.059.293 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.059.297 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.059.297 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.059.297 I print_info: LF token         = 128 'Ä'
0.00.059.298 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.059.298 I print_info: max token length = 1024
0.00.061.348 I load_tensors: offloading 24 repeating layers to GPU
0.00.061.348 I load_tensors: offloading output layer to GPU
0.00.061.349 I load_tensors: offloaded 25/25 layers to GPU
0.00.061.359 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.061.360 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.061.649 I llama_init_from_model: n_seq_max     = 1
0.00.061.650 I llama_init_from_model: n_ctx         = 2048
0.00.061.650 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.061.650 I llama_init_from_model: n_batch       = 2048
0.00.061.651 I llama_init_from_model: n_ubatch      = 512
0.00.061.651 I llama_init_from_model: flash_attn    = 0
0.00.061.651 I llama_init_from_model: freq_base     = 10000.0
0.00.061.651 I llama_init_from_model: freq_scale    = 1
0.00.061.652 I ggml_metal_init: allocating
0.00.061.655 I ggml_metal_init: found device: Apple M4
0.00.061.657 I ggml_metal_init: picking default device: Apple M4
0.00.062.272 I ggml_metal_init: using embedded metal library
0.00.064.751 I ggml_metal_init: GPU name:   Apple M4
0.00.064.753 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.064.753 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.064.754 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.064.754 I ggml_metal_init: simdgroup reduction   = true
0.00.064.754 I ggml_metal_init: simdgroup matrix mul. = true
0.00.064.754 I ggml_metal_init: has bfloat            = true
0.00.064.754 I ggml_metal_init: use bfloat            = true
0.00.064.755 I ggml_metal_init: hasUnifiedMemory      = true
0.00.064.755 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.075.153 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.095.169 I init:      Metal KV buffer size =   384.00 MiB
0.00.095.176 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.095.197 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.096.268 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.096.270 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.096.270 I llama_init_from_model: graph nodes  = 967
0.00.096.270 I llama_init_from_model: graph splits = 2
0.00.096.273 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.096.402 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.096.402 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.909.752 I main: llama threadpool init, n_threads = 4
0.00.909.789 I 
0.00.909.818 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.909.818 I 
0.00.910.047 I sampler seed: 1234
0.00.910.052 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.910.063 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.910.063 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.910.063 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.743.091 I llama_perf_sampler_print:    sampling time =       1.22 ms /    71 runs   (    0.02 ms per token, 58436.21 tokens per second)
0.01.743.092 I llama_perf_context_print:        load time =     901.03 ms
0.01.743.093 I llama_perf_context_print: prompt eval time =      42.36 ms /     7 tokens (    6.05 ms per token,   165.26 tokens per second)
0.01.743.094 I llama_perf_context_print:        eval time =     787.60 ms /    63 runs   (   12.50 ms per token,    79.99 tokens per second)
0.01.743.094 I llama_perf_context_print:       total time =     833.34 ms /    70 tokens
0.01.746.128 I ggml_metal_free: deallocating

real	0m1.761s
user	0m0.111s
sys	0m0.160s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.043 I build: 4477 (28f12723) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.070 I main: llama backend init
0.00.000.072 I main: load the model and apply lora adapter, if any
0.00.013.588 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.023.391 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.023.396 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.023.398 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.023.398 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.023.399 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.023.399 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.023.399 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.023.400 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.023.400 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.023.401 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.023.401 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.023.401 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.023.402 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.023.402 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.023.404 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.023.404 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.023.405 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.027.741 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.028.899 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.033.754 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.033.756 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.033.756 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.033.757 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.033.757 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.033.758 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.033.758 I llama_model_loader: - type  f32:  194 tensors
0.00.033.758 I llama_model_loader: - type q2_K:   49 tensors
0.00.033.759 I llama_model_loader: - type q3_K:   48 tensors
0.00.033.759 I llama_model_loader: - type q6_K:    1 tensors
0.00.033.760 I print_info: file format = GGUF V3 (latest)
0.00.033.762 I print_info: file type   = Q2_K - Medium
0.00.033.763 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.064.284 I load: special tokens cache size = 25
0.00.074.967 I load: token to piece cache size = 0.2984 MB
0.00.074.972 I print_info: arch             = gptneox
0.00.074.973 I print_info: vocab_only       = 0
0.00.074.973 I print_info: n_ctx_train      = 2048
0.00.074.973 I print_info: n_embd           = 2048
0.00.074.973 I print_info: n_layer          = 24
0.00.074.977 I print_info: n_head           = 16
0.00.074.981 I print_info: n_head_kv        = 16
0.00.074.981 I print_info: n_rot            = 32
0.00.074.981 I print_info: n_swa            = 0
0.00.074.981 I print_info: n_embd_head_k    = 128
0.00.074.982 I print_info: n_embd_head_v    = 128
0.00.074.983 I print_info: n_gqa            = 1
0.00.074.984 I print_info: n_embd_k_gqa     = 2048
0.00.074.985 I print_info: n_embd_v_gqa     = 2048
0.00.074.985 I print_info: f_norm_eps       = 1.0e-05
0.00.074.986 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.074.986 I print_info: f_clamp_kqv      = 0.0e+00
0.00.074.986 I print_info: f_max_alibi_bias = 0.0e+00
0.00.074.987 I print_info: f_logit_scale    = 0.0e+00
0.00.074.988 I print_info: n_ff             = 8192
0.00.074.988 I print_info: n_expert         = 0
0.00.074.988 I print_info: n_expert_used    = 0
0.00.074.988 I print_info: causal attn      = 1
0.00.074.989 I print_info: pooling type     = 0
0.00.074.989 I print_info: rope type        = 2
0.00.074.990 I print_info: rope scaling     = linear
0.00.074.993 I print_info: freq_base_train  = 10000.0
0.00.074.993 I print_info: freq_scale_train = 1
0.00.074.994 I print_info: n_ctx_orig_yarn  = 2048
0.00.074.994 I print_info: rope_finetuned   = unknown
0.00.074.994 I print_info: ssm_d_conv       = 0
0.00.074.994 I print_info: ssm_d_inner      = 0
0.00.074.994 I print_info: ssm_d_state      = 0
0.00.074.995 I print_info: ssm_dt_rank      = 0
0.00.074.996 I print_info: ssm_dt_b_c_rms   = 0
0.00.074.997 I print_info: model type       = 1.4B
0.00.074.997 I print_info: model params     = 1.41 B
0.00.074.997 I print_info: general.name     = 1.4B
0.00.074.998 I print_info: vocab type       = BPE
0.00.074.999 I print_info: n_vocab          = 50304
0.00.074.999 I print_info: n_merges         = 50009
0.00.074.999 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.074.999 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.075.000 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.075.000 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.075.000 I print_info: LF token         = 128 'Ä'
0.00.075.001 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.075.003 I print_info: max token length = 1024
0.00.077.488 I load_tensors: offloading 24 repeating layers to GPU
0.00.077.488 I load_tensors: offloading output layer to GPU
0.00.077.489 I load_tensors: offloaded 25/25 layers to GPU
0.00.077.500 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.077.501 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.077.905 I llama_init_from_model: n_seq_max     = 1
0.00.077.907 I llama_init_from_model: n_ctx         = 2048
0.00.077.907 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.077.907 I llama_init_from_model: n_batch       = 2048
0.00.077.907 I llama_init_from_model: n_ubatch      = 512
0.00.077.908 I llama_init_from_model: flash_attn    = 0
0.00.077.908 I llama_init_from_model: freq_base     = 10000.0
0.00.077.909 I llama_init_from_model: freq_scale    = 1
0.00.077.909 I ggml_metal_init: allocating
0.00.077.913 I ggml_metal_init: found device: Apple M4
0.00.077.916 I ggml_metal_init: picking default device: Apple M4
0.00.078.767 I ggml_metal_init: using embedded metal library
0.00.082.428 I ggml_metal_init: GPU name:   Apple M4
0.00.082.431 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.082.431 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.082.432 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.082.432 I ggml_metal_init: simdgroup reduction   = true
0.00.082.432 I ggml_metal_init: simdgroup matrix mul. = true
0.00.082.432 I ggml_metal_init: has bfloat            = true
0.00.082.433 I ggml_metal_init: use bfloat            = true
0.00.082.433 I ggml_metal_init: hasUnifiedMemory      = true
0.00.082.434 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.094.240 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.115.714 I init:      Metal KV buffer size =   384.00 MiB
0.00.115.720 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.115.743 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.116.754 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.116.755 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.116.756 I llama_init_from_model: graph nodes  = 967
0.00.116.756 I llama_init_from_model: graph splits = 2
0.00.116.759 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.116.888 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.116.888 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.558.246 I main: llama threadpool init, n_threads = 4
0.00.558.291 I 
0.00.558.313 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.558.314 I 
0.00.558.542 I sampler seed: 1234
0.00.558.547 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.558.597 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.558.599 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.558.599 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.236.220 I llama_perf_sampler_print:    sampling time =       1.21 ms /    71 runs   (    0.02 ms per token, 58921.16 tokens per second)
0.01.236.221 I llama_perf_context_print:        load time =     544.65 ms
0.01.236.221 I llama_perf_context_print: prompt eval time =      35.80 ms /     7 tokens (    5.11 ms per token,   195.55 tokens per second)
0.01.236.222 I llama_perf_context_print:        eval time =     638.79 ms /    63 runs   (   10.14 ms per token,    98.62 tokens per second)
0.01.236.222 I llama_perf_context_print:       total time =     677.98 ms /    70 tokens
0.01.239.108 I ggml_metal_free: deallocating

real	0m1.274s
user	0m0.133s
sys	0m0.129s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.044 I build: 4477 (28f12723) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.071 I main: llama backend init
0.00.000.073 I main: load the model and apply lora adapter, if any
0.00.008.420 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.198 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.203 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.205 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.205 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.206 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.206 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.206 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.207 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.208 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.208 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.208 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.209 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.211 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.212 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.213 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.214 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.216 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.014 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.999 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.872 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.873 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.873 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.873 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.874 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.874 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.875 I llama_model_loader: - type  f32:  194 tensors
0.00.024.875 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.875 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.875 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.876 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.876 I print_info: file format = GGUF V3 (latest)
0.00.024.877 I print_info: file type   = Q3_K - Medium
0.00.024.878 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.043.966 I load: special tokens cache size = 25
0.00.049.953 I load: token to piece cache size = 0.2984 MB
0.00.049.956 I print_info: arch             = gptneox
0.00.049.957 I print_info: vocab_only       = 0
0.00.049.957 I print_info: n_ctx_train      = 2048
0.00.049.957 I print_info: n_embd           = 2048
0.00.049.957 I print_info: n_layer          = 24
0.00.049.960 I print_info: n_head           = 16
0.00.049.961 I print_info: n_head_kv        = 16
0.00.049.962 I print_info: n_rot            = 32
0.00.049.962 I print_info: n_swa            = 0
0.00.049.963 I print_info: n_embd_head_k    = 128
0.00.049.963 I print_info: n_embd_head_v    = 128
0.00.049.963 I print_info: n_gqa            = 1
0.00.049.964 I print_info: n_embd_k_gqa     = 2048
0.00.049.965 I print_info: n_embd_v_gqa     = 2048
0.00.049.965 I print_info: f_norm_eps       = 1.0e-05
0.00.049.966 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.966 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.966 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.966 I print_info: f_logit_scale    = 0.0e+00
0.00.049.967 I print_info: n_ff             = 8192
0.00.049.967 I print_info: n_expert         = 0
0.00.049.967 I print_info: n_expert_used    = 0
0.00.049.967 I print_info: causal attn      = 1
0.00.049.967 I print_info: pooling type     = 0
0.00.049.968 I print_info: rope type        = 2
0.00.049.968 I print_info: rope scaling     = linear
0.00.049.968 I print_info: freq_base_train  = 10000.0
0.00.049.969 I print_info: freq_scale_train = 1
0.00.049.969 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.969 I print_info: rope_finetuned   = unknown
0.00.049.969 I print_info: ssm_d_conv       = 0
0.00.049.971 I print_info: ssm_d_inner      = 0
0.00.049.971 I print_info: ssm_d_state      = 0
0.00.049.971 I print_info: ssm_dt_rank      = 0
0.00.049.971 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.971 I print_info: model type       = 1.4B
0.00.049.972 I print_info: model params     = 1.41 B
0.00.049.972 I print_info: general.name     = 1.4B
0.00.049.972 I print_info: vocab type       = BPE
0.00.049.973 I print_info: n_vocab          = 50304
0.00.049.973 I print_info: n_merges         = 50009
0.00.049.973 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.973 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.974 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.974 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.978 I print_info: LF token         = 128 'Ä'
0.00.049.979 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.979 I print_info: max token length = 1024
0.00.051.587 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.587 I load_tensors: offloading output layer to GPU
0.00.051.587 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.597 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.051.598 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.051.873 I llama_init_from_model: n_seq_max     = 1
0.00.051.874 I llama_init_from_model: n_ctx         = 2048
0.00.051.874 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.051.874 I llama_init_from_model: n_batch       = 2048
0.00.051.875 I llama_init_from_model: n_ubatch      = 512
0.00.051.875 I llama_init_from_model: flash_attn    = 0
0.00.051.875 I llama_init_from_model: freq_base     = 10000.0
0.00.051.875 I llama_init_from_model: freq_scale    = 1
0.00.051.876 I ggml_metal_init: allocating
0.00.051.879 I ggml_metal_init: found device: Apple M4
0.00.051.881 I ggml_metal_init: picking default device: Apple M4
0.00.052.458 I ggml_metal_init: using embedded metal library
0.00.054.832 I ggml_metal_init: GPU name:   Apple M4
0.00.054.834 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.834 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.834 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.835 I ggml_metal_init: simdgroup reduction   = true
0.00.054.835 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.835 I ggml_metal_init: has bfloat            = true
0.00.054.835 I ggml_metal_init: use bfloat            = true
0.00.054.835 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.836 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.710 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.084.141 I init:      Metal KV buffer size =   384.00 MiB
0.00.084.146 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.165 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.085.183 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.085.184 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.085.184 I llama_init_from_model: graph nodes  = 967
0.00.085.185 I llama_init_from_model: graph splits = 2
0.00.085.187 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.085.326 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.085.327 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.536.718 I main: llama threadpool init, n_threads = 4
0.00.536.764 I 
0.00.536.788 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.536.789 I 
0.00.537.023 I sampler seed: 1234
0.00.537.029 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.537.039 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.537.040 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.537.040 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.282.731 I llama_perf_sampler_print:    sampling time =       1.19 ms /    71 runs   (    0.02 ms per token, 59463.99 tokens per second)
0.01.282.732 I llama_perf_context_print:        load time =     528.29 ms
0.01.282.732 I llama_perf_context_print: prompt eval time =      44.46 ms /     7 tokens (    6.35 ms per token,   157.45 tokens per second)
0.01.282.733 I llama_perf_context_print:        eval time =     698.23 ms /    63 runs   (   11.08 ms per token,    90.23 tokens per second)
0.01.282.733 I llama_perf_context_print:       total time =     746.02 ms /    70 tokens
0.01.285.203 I ggml_metal_free: deallocating

real	0m1.303s
user	0m0.108s
sys	0m0.128s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.047 I build: 4477 (28f12723) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.078 I main: llama backend init
0.00.000.080 I main: load the model and apply lora adapter, if any
0.00.008.833 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.591 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.015.596 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.598 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.598 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.598 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.599 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.599 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.600 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.600 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.600 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.601 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.601 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.601 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.604 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.605 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.606 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.606 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.447 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.494 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.264 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.265 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.266 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.266 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.266 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.267 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.267 I llama_model_loader: - type  f32:  194 tensors
0.00.024.268 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.268 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.268 I llama_model_loader: - type q6_K:   13 tensors
0.00.024.269 I print_info: file format = GGUF V3 (latest)
0.00.024.269 I print_info: file type   = Q4_K - Medium
0.00.024.270 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.044.108 I load: special tokens cache size = 25
0.00.050.206 I load: token to piece cache size = 0.2984 MB
0.00.050.209 I print_info: arch             = gptneox
0.00.050.209 I print_info: vocab_only       = 0
0.00.050.210 I print_info: n_ctx_train      = 2048
0.00.050.210 I print_info: n_embd           = 2048
0.00.050.210 I print_info: n_layer          = 24
0.00.050.213 I print_info: n_head           = 16
0.00.050.214 I print_info: n_head_kv        = 16
0.00.050.214 I print_info: n_rot            = 32
0.00.050.214 I print_info: n_swa            = 0
0.00.050.214 I print_info: n_embd_head_k    = 128
0.00.050.214 I print_info: n_embd_head_v    = 128
0.00.050.217 I print_info: n_gqa            = 1
0.00.050.218 I print_info: n_embd_k_gqa     = 2048
0.00.050.219 I print_info: n_embd_v_gqa     = 2048
0.00.050.219 I print_info: f_norm_eps       = 1.0e-05
0.00.050.221 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.221 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.221 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.222 I print_info: f_logit_scale    = 0.0e+00
0.00.050.222 I print_info: n_ff             = 8192
0.00.050.222 I print_info: n_expert         = 0
0.00.050.223 I print_info: n_expert_used    = 0
0.00.050.223 I print_info: causal attn      = 1
0.00.050.223 I print_info: pooling type     = 0
0.00.050.223 I print_info: rope type        = 2
0.00.050.223 I print_info: rope scaling     = linear
0.00.050.224 I print_info: freq_base_train  = 10000.0
0.00.050.224 I print_info: freq_scale_train = 1
0.00.050.224 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.224 I print_info: rope_finetuned   = unknown
0.00.050.225 I print_info: ssm_d_conv       = 0
0.00.050.226 I print_info: ssm_d_inner      = 0
0.00.050.226 I print_info: ssm_d_state      = 0
0.00.050.226 I print_info: ssm_dt_rank      = 0
0.00.050.226 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.227 I print_info: model type       = 1.4B
0.00.050.227 I print_info: model params     = 1.41 B
0.00.050.227 I print_info: general.name     = 1.4B
0.00.050.228 I print_info: vocab type       = BPE
0.00.050.228 I print_info: n_vocab          = 50304
0.00.050.228 I print_info: n_merges         = 50009
0.00.050.228 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.229 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.229 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.229 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.229 I print_info: LF token         = 128 'Ä'
0.00.050.230 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.230 I print_info: max token length = 1024
0.00.052.059 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.059 I load_tensors: offloading output layer to GPU
0.00.052.059 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.065 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.052.065 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.052.334 I llama_init_from_model: n_seq_max     = 1
0.00.052.334 I llama_init_from_model: n_ctx         = 2048
0.00.052.335 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.052.335 I llama_init_from_model: n_batch       = 2048
0.00.052.335 I llama_init_from_model: n_ubatch      = 512
0.00.052.335 I llama_init_from_model: flash_attn    = 0
0.00.052.335 I llama_init_from_model: freq_base     = 10000.0
0.00.052.336 I llama_init_from_model: freq_scale    = 1
0.00.052.336 I ggml_metal_init: allocating
0.00.052.339 I ggml_metal_init: found device: Apple M4
0.00.052.341 I ggml_metal_init: picking default device: Apple M4
0.00.052.939 I ggml_metal_init: using embedded metal library
0.00.055.361 I ggml_metal_init: GPU name:   Apple M4
0.00.055.362 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.363 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.363 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.363 I ggml_metal_init: simdgroup reduction   = true
0.00.055.364 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.364 I ggml_metal_init: has bfloat            = true
0.00.055.364 I ggml_metal_init: use bfloat            = true
0.00.055.364 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.365 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.499 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.086.962 I init:      Metal KV buffer size =   384.00 MiB
0.00.086.979 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.087.009 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.088.136 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.088.137 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.088.138 I llama_init_from_model: graph nodes  = 967
0.00.088.138 I llama_init_from_model: graph splits = 2
0.00.088.142 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.088.271 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.088.271 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.619.817 I main: llama threadpool init, n_threads = 4
0.00.619.855 I 
0.00.619.887 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.619.887 I 
0.00.620.114 I sampler seed: 1234
0.00.620.119 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.620.138 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.620.139 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.620.139 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.375.178 I llama_perf_sampler_print:    sampling time =       1.43 ms /    71 runs   (    0.02 ms per token, 49615.65 tokens per second)
0.01.375.179 I llama_perf_context_print:        load time =     610.98 ms
0.01.375.180 I llama_perf_context_print: prompt eval time =      46.98 ms /     7 tokens (    6.71 ms per token,   149.01 tokens per second)
0.01.375.180 I llama_perf_context_print:        eval time =     705.11 ms /    63 runs   (   11.19 ms per token,    89.35 tokens per second)
0.01.375.182 I llama_perf_context_print:       total time =     755.37 ms /    70 tokens
0.01.379.316 I ggml_metal_free: deallocating

real	0m1.397s
user	0m0.111s
sys	0m0.149s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4477 (28f12723) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.079 I main: llama backend init
0.00.000.081 I main: load the model and apply lora adapter, if any
0.00.009.094 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.724 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.729 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.730 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.731 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.731 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.732 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.732 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.733 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.733 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.733 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.734 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.734 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.735 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.735 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.736 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.737 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.737 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.556 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.527 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.320 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.322 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.322 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.322 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.323 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.323 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.323 I llama_model_loader: - type  f32:  194 tensors
0.00.024.324 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.324 I llama_model_loader: - type q6_K:   37 tensors
0.00.024.325 I print_info: file format = GGUF V3 (latest)
0.00.024.325 I print_info: file type   = Q5_K - Medium
0.00.024.327 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.043.512 I load: special tokens cache size = 25
0.00.049.484 I load: token to piece cache size = 0.2984 MB
0.00.049.487 I print_info: arch             = gptneox
0.00.049.487 I print_info: vocab_only       = 0
0.00.049.487 I print_info: n_ctx_train      = 2048
0.00.049.488 I print_info: n_embd           = 2048
0.00.049.488 I print_info: n_layer          = 24
0.00.049.491 I print_info: n_head           = 16
0.00.049.492 I print_info: n_head_kv        = 16
0.00.049.492 I print_info: n_rot            = 32
0.00.049.492 I print_info: n_swa            = 0
0.00.049.492 I print_info: n_embd_head_k    = 128
0.00.049.492 I print_info: n_embd_head_v    = 128
0.00.049.493 I print_info: n_gqa            = 1
0.00.049.494 I print_info: n_embd_k_gqa     = 2048
0.00.049.497 I print_info: n_embd_v_gqa     = 2048
0.00.049.497 I print_info: f_norm_eps       = 1.0e-05
0.00.049.498 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.498 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.500 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.500 I print_info: f_logit_scale    = 0.0e+00
0.00.049.501 I print_info: n_ff             = 8192
0.00.049.501 I print_info: n_expert         = 0
0.00.049.501 I print_info: n_expert_used    = 0
0.00.049.501 I print_info: causal attn      = 1
0.00.049.501 I print_info: pooling type     = 0
0.00.049.502 I print_info: rope type        = 2
0.00.049.502 I print_info: rope scaling     = linear
0.00.049.502 I print_info: freq_base_train  = 10000.0
0.00.049.503 I print_info: freq_scale_train = 1
0.00.049.503 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.503 I print_info: rope_finetuned   = unknown
0.00.049.503 I print_info: ssm_d_conv       = 0
0.00.049.503 I print_info: ssm_d_inner      = 0
0.00.049.505 I print_info: ssm_d_state      = 0
0.00.049.505 I print_info: ssm_dt_rank      = 0
0.00.049.505 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.505 I print_info: model type       = 1.4B
0.00.049.506 I print_info: model params     = 1.41 B
0.00.049.506 I print_info: general.name     = 1.4B
0.00.049.506 I print_info: vocab type       = BPE
0.00.049.506 I print_info: n_vocab          = 50304
0.00.049.507 I print_info: n_merges         = 50009
0.00.049.507 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.507 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.507 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.507 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.511 I print_info: LF token         = 128 'Ä'
0.00.049.511 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.512 I print_info: max token length = 1024
0.00.051.558 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.558 I load_tensors: offloading output layer to GPU
0.00.051.558 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.568 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.051.570 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.051.856 I llama_init_from_model: n_seq_max     = 1
0.00.051.857 I llama_init_from_model: n_ctx         = 2048
0.00.051.857 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.051.857 I llama_init_from_model: n_batch       = 2048
0.00.051.857 I llama_init_from_model: n_ubatch      = 512
0.00.051.858 I llama_init_from_model: flash_attn    = 0
0.00.051.858 I llama_init_from_model: freq_base     = 10000.0
0.00.051.858 I llama_init_from_model: freq_scale    = 1
0.00.051.859 I ggml_metal_init: allocating
0.00.051.862 I ggml_metal_init: found device: Apple M4
0.00.051.863 I ggml_metal_init: picking default device: Apple M4
0.00.052.441 I ggml_metal_init: using embedded metal library
0.00.054.757 I ggml_metal_init: GPU name:   Apple M4
0.00.054.758 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.758 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.759 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.759 I ggml_metal_init: simdgroup reduction   = true
0.00.054.759 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.759 I ggml_metal_init: has bfloat            = true
0.00.054.760 I ggml_metal_init: use bfloat            = true
0.00.054.760 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.760 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.556 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.084.220 I init:      Metal KV buffer size =   384.00 MiB
0.00.084.227 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.245 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.085.246 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.085.248 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.085.248 I llama_init_from_model: graph nodes  = 967
0.00.085.248 I llama_init_from_model: graph splits = 2
0.00.085.251 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.085.369 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.085.370 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.681.123 I main: llama threadpool init, n_threads = 4
0.00.681.162 I 
0.00.681.187 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.681.189 I 
0.00.681.428 I sampler seed: 1234
0.00.681.434 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.681.463 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.681.466 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.681.466 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.538.282 I llama_perf_sampler_print:    sampling time =       1.13 ms /    71 runs   (    0.02 ms per token, 62720.85 tokens per second)
0.01.538.282 I llama_perf_context_print:        load time =     672.02 ms
0.01.538.283 I llama_perf_context_print: prompt eval time =      58.07 ms /     7 tokens (    8.30 ms per token,   120.55 tokens per second)
0.01.538.284 I llama_perf_context_print:        eval time =     795.88 ms /    63 runs   (   12.63 ms per token,    79.16 tokens per second)
0.01.538.285 I llama_perf_context_print:       total time =     857.16 ms /    70 tokens
0.01.541.353 I ggml_metal_free: deallocating

real	0m1.561s
user	0m0.108s
sys	0m0.153s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4477 (28f12723) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.076 I main: llama backend init
0.00.000.081 I main: load the model and apply lora adapter, if any
0.00.009.178 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.023 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.027 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.029 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.030 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.032 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.032 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.033 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.034 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.034 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.034 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.035 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.035 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.035 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.036 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.038 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.038 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.038 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.870 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.895 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.782 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.783 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.784 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.784 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.785 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.787 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.788 I llama_model_loader: - type  f32:  194 tensors
0.00.024.788 I llama_model_loader: - type q6_K:   98 tensors
0.00.024.788 I print_info: file format = GGUF V3 (latest)
0.00.024.789 I print_info: file type   = Q6_K
0.00.024.790 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.045.139 I load: special tokens cache size = 25
0.00.051.287 I load: token to piece cache size = 0.2984 MB
0.00.051.291 I print_info: arch             = gptneox
0.00.051.291 I print_info: vocab_only       = 0
0.00.051.291 I print_info: n_ctx_train      = 2048
0.00.051.291 I print_info: n_embd           = 2048
0.00.051.292 I print_info: n_layer          = 24
0.00.051.297 I print_info: n_head           = 16
0.00.051.297 I print_info: n_head_kv        = 16
0.00.051.298 I print_info: n_rot            = 32
0.00.051.298 I print_info: n_swa            = 0
0.00.051.298 I print_info: n_embd_head_k    = 128
0.00.051.298 I print_info: n_embd_head_v    = 128
0.00.051.299 I print_info: n_gqa            = 1
0.00.051.300 I print_info: n_embd_k_gqa     = 2048
0.00.051.300 I print_info: n_embd_v_gqa     = 2048
0.00.051.301 I print_info: f_norm_eps       = 1.0e-05
0.00.051.302 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.302 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.303 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.303 I print_info: f_logit_scale    = 0.0e+00
0.00.051.303 I print_info: n_ff             = 8192
0.00.051.303 I print_info: n_expert         = 0
0.00.051.304 I print_info: n_expert_used    = 0
0.00.051.304 I print_info: causal attn      = 1
0.00.051.304 I print_info: pooling type     = 0
0.00.051.304 I print_info: rope type        = 2
0.00.051.304 I print_info: rope scaling     = linear
0.00.051.305 I print_info: freq_base_train  = 10000.0
0.00.051.305 I print_info: freq_scale_train = 1
0.00.051.305 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.305 I print_info: rope_finetuned   = unknown
0.00.051.306 I print_info: ssm_d_conv       = 0
0.00.051.306 I print_info: ssm_d_inner      = 0
0.00.051.306 I print_info: ssm_d_state      = 0
0.00.051.306 I print_info: ssm_dt_rank      = 0
0.00.051.306 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.306 I print_info: model type       = 1.4B
0.00.051.307 I print_info: model params     = 1.41 B
0.00.051.307 I print_info: general.name     = 1.4B
0.00.051.307 I print_info: vocab type       = BPE
0.00.051.308 I print_info: n_vocab          = 50304
0.00.051.308 I print_info: n_merges         = 50009
0.00.051.308 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.308 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.308 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.308 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.310 I print_info: LF token         = 128 'Ä'
0.00.051.310 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.311 I print_info: max token length = 1024
0.00.053.362 I load_tensors: offloading 24 repeating layers to GPU
0.00.053.362 I load_tensors: offloading output layer to GPU
0.00.053.362 I load_tensors: offloaded 25/25 layers to GPU
0.00.053.373 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.053.374 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.053.740 I llama_init_from_model: n_seq_max     = 1
0.00.053.741 I llama_init_from_model: n_ctx         = 2048
0.00.053.741 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.053.741 I llama_init_from_model: n_batch       = 2048
0.00.053.741 I llama_init_from_model: n_ubatch      = 512
0.00.053.741 I llama_init_from_model: flash_attn    = 0
0.00.053.742 I llama_init_from_model: freq_base     = 10000.0
0.00.053.742 I llama_init_from_model: freq_scale    = 1
0.00.053.743 I ggml_metal_init: allocating
0.00.053.746 I ggml_metal_init: found device: Apple M4
0.00.053.748 I ggml_metal_init: picking default device: Apple M4
0.00.054.370 I ggml_metal_init: using embedded metal library
0.00.056.830 I ggml_metal_init: GPU name:   Apple M4
0.00.056.832 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.832 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.832 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.833 I ggml_metal_init: simdgroup reduction   = true
0.00.056.833 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.833 I ggml_metal_init: has bfloat            = true
0.00.056.833 I ggml_metal_init: use bfloat            = true
0.00.056.834 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.834 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.973 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.086.313 I init:      Metal KV buffer size =   384.00 MiB
0.00.086.322 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.348 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.087.456 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.087.457 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.087.458 I llama_init_from_model: graph nodes  = 967
0.00.087.458 I llama_init_from_model: graph splits = 2
0.00.087.463 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.087.592 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.593 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.711.672 I main: llama threadpool init, n_threads = 4
0.00.711.717 I 
0.00.711.743 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.711.745 I 
0.00.711.979 I sampler seed: 1234
0.00.711.984 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.711.995 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.711.996 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.711.996 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.598.247 I llama_perf_sampler_print:    sampling time =       1.44 ms /    71 runs   (    0.02 ms per token, 49442.90 tokens per second)
0.01.598.247 I llama_perf_context_print:        load time =     702.49 ms
0.01.598.248 I llama_perf_context_print: prompt eval time =      54.19 ms /     7 tokens (    7.74 ms per token,   129.17 tokens per second)
0.01.598.249 I llama_perf_context_print:        eval time =     829.57 ms /    63 runs   (   13.17 ms per token,    75.94 tokens per second)
0.01.598.249 I llama_perf_context_print:       total time =     886.58 ms /    70 tokens
0.01.601.164 I ggml_metal_free: deallocating

real	0m1.616s
user	0m0.110s
sys	0m0.128s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.564 I build: 4477 (28f12723) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.022.955 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.035.777 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.035.791 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.035.795 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.035.796 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.035.797 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.035.797 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.035.798 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.035.800 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.035.801 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.035.801 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.035.802 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.035.803 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.035.803 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.035.804 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.035.813 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.035.814 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.035.815 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.045.070 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.047.379 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.055.346 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.055.349 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.055.350 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.055.351 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.055.351 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.055.352 I llama_model_loader: - type  f32:  194 tensors
0.00.055.352 I llama_model_loader: - type  f16:   98 tensors
0.00.055.353 I print_info: file format = GGUF V3 (latest)
0.00.055.354 I print_info: file type   = all F32 (guessed)
0.00.055.357 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.083.671 I load: special tokens cache size = 25
0.00.090.283 I load: token to piece cache size = 0.2984 MB
0.00.090.287 I print_info: arch             = gptneox
0.00.090.287 I print_info: vocab_only       = 0
0.00.090.287 I print_info: n_ctx_train      = 2048
0.00.090.287 I print_info: n_embd           = 2048
0.00.090.287 I print_info: n_layer          = 24
0.00.090.291 I print_info: n_head           = 16
0.00.090.291 I print_info: n_head_kv        = 16
0.00.090.292 I print_info: n_rot            = 32
0.00.090.292 I print_info: n_swa            = 0
0.00.090.292 I print_info: n_embd_head_k    = 128
0.00.090.292 I print_info: n_embd_head_v    = 128
0.00.090.293 I print_info: n_gqa            = 1
0.00.090.295 I print_info: n_embd_k_gqa     = 2048
0.00.090.296 I print_info: n_embd_v_gqa     = 2048
0.00.090.296 I print_info: f_norm_eps       = 1.0e-05
0.00.090.297 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.090.297 I print_info: f_clamp_kqv      = 0.0e+00
0.00.090.297 I print_info: f_max_alibi_bias = 0.0e+00
0.00.090.297 I print_info: f_logit_scale    = 0.0e+00
0.00.090.298 I print_info: n_ff             = 8192
0.00.090.298 I print_info: n_expert         = 0
0.00.090.300 I print_info: n_expert_used    = 0
0.00.090.300 I print_info: causal attn      = 1
0.00.090.300 I print_info: pooling type     = 0
0.00.090.300 I print_info: rope type        = 2
0.00.090.301 I print_info: rope scaling     = linear
0.00.090.301 I print_info: freq_base_train  = 10000.0
0.00.090.301 I print_info: freq_scale_train = 1
0.00.090.301 I print_info: n_ctx_orig_yarn  = 2048
0.00.090.302 I print_info: rope_finetuned   = unknown
0.00.090.302 I print_info: ssm_d_conv       = 0
0.00.090.302 I print_info: ssm_d_inner      = 0
0.00.090.302 I print_info: ssm_d_state      = 0
0.00.090.302 I print_info: ssm_dt_rank      = 0
0.00.090.302 I print_info: ssm_dt_b_c_rms   = 0
0.00.090.302 I print_info: model type       = 1.4B
0.00.090.303 I print_info: model params     = 1.41 B
0.00.090.303 I print_info: general.name     = 1.4B
0.00.090.304 I print_info: vocab type       = BPE
0.00.090.304 I print_info: n_vocab          = 50304
0.00.090.304 I print_info: n_merges         = 50009
0.00.090.304 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.090.304 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.090.304 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.090.305 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.090.305 I print_info: LF token         = 128 'Ä'
0.00.090.305 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.090.305 I print_info: max token length = 1024
0.00.092.170 I load_tensors: offloading 24 repeating layers to GPU
0.00.092.171 I load_tensors: offloading output layer to GPU
0.00.092.171 I load_tensors: offloaded 25/25 layers to GPU
0.00.092.181 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.092.182 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.092.488 I llama_init_from_model: n_seq_max     = 1
0.00.092.489 I llama_init_from_model: n_ctx         = 128
0.00.092.489 I llama_init_from_model: n_ctx_per_seq = 128
0.00.092.489 I llama_init_from_model: n_batch       = 128
0.00.092.490 I llama_init_from_model: n_ubatch      = 128
0.00.092.490 I llama_init_from_model: flash_attn    = 0
0.00.092.490 I llama_init_from_model: freq_base     = 10000.0
0.00.092.490 I llama_init_from_model: freq_scale    = 1
0.00.092.491 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.092.491 I ggml_metal_init: allocating
0.00.092.494 I ggml_metal_init: found device: Apple M4
0.00.092.497 I ggml_metal_init: picking default device: Apple M4
0.00.093.165 I ggml_metal_init: using embedded metal library
0.00.095.858 I ggml_metal_init: GPU name:   Apple M4
0.00.095.860 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.095.861 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.095.861 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.095.861 I ggml_metal_init: simdgroup reduction   = true
0.00.095.861 I ggml_metal_init: simdgroup matrix mul. = true
0.00.095.862 I ggml_metal_init: has bfloat            = true
0.00.095.862 I ggml_metal_init: use bfloat            = true
0.00.095.862 I ggml_metal_init: hasUnifiedMemory      = true
0.00.095.863 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.105.311 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.106.758 I init:      Metal KV buffer size =    24.00 MiB
0.00.106.763 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.106.778 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.107.644 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.107.645 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.107.646 I llama_init_from_model: graph nodes  = 967
0.00.107.646 I llama_init_from_model: graph splits = 2
0.00.107.647 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.107.648 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.099.797 I 
0.01.099.853 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.099.894 I perplexity: tokenizing the input ..
0.01.112.811 I perplexity: tokenization took 12.913 ms
0.01.112.828 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.234.840 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.236.698 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.236.750 I llama_perf_context_print:        load time =    1076.83 ms
0.01.236.752 I llama_perf_context_print: prompt eval time =     121.12 ms /   128 tokens (    0.95 ms per token,  1056.79 tokens per second)
0.01.236.753 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.236.754 I llama_perf_context_print:       total time =     136.95 ms /   129 tokens
0.01.237.915 I ggml_metal_free: deallocating

real	0m1.468s
user	0m0.125s
sys	0m0.216s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.127 I build: 4477 (28f12723) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.851 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.021.739 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.021.746 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.021.748 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.021.749 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.021.749 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.021.749 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.021.750 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.021.751 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.021.751 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.021.751 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.021.752 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.021.752 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.021.753 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.021.753 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.021.755 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.021.756 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.021.756 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.027.168 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.028.570 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.033.961 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.033.962 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.033.963 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.033.963 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.033.963 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.033.964 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.033.964 I llama_model_loader: - type  f32:  194 tensors
0.00.033.965 I llama_model_loader: - type q8_0:   98 tensors
0.00.033.966 I print_info: file format = GGUF V3 (latest)
0.00.033.966 I print_info: file type   = Q8_0
0.00.033.967 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.057.849 I load: special tokens cache size = 25
0.00.064.400 I load: token to piece cache size = 0.2984 MB
0.00.064.403 I print_info: arch             = gptneox
0.00.064.404 I print_info: vocab_only       = 0
0.00.064.404 I print_info: n_ctx_train      = 2048
0.00.064.404 I print_info: n_embd           = 2048
0.00.064.404 I print_info: n_layer          = 24
0.00.064.407 I print_info: n_head           = 16
0.00.064.408 I print_info: n_head_kv        = 16
0.00.064.410 I print_info: n_rot            = 32
0.00.064.410 I print_info: n_swa            = 0
0.00.064.410 I print_info: n_embd_head_k    = 128
0.00.064.410 I print_info: n_embd_head_v    = 128
0.00.064.411 I print_info: n_gqa            = 1
0.00.064.412 I print_info: n_embd_k_gqa     = 2048
0.00.064.412 I print_info: n_embd_v_gqa     = 2048
0.00.064.413 I print_info: f_norm_eps       = 1.0e-05
0.00.064.414 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.064.414 I print_info: f_clamp_kqv      = 0.0e+00
0.00.064.414 I print_info: f_max_alibi_bias = 0.0e+00
0.00.064.424 I print_info: f_logit_scale    = 0.0e+00
0.00.064.432 I print_info: n_ff             = 8192
0.00.064.432 I print_info: n_expert         = 0
0.00.064.432 I print_info: n_expert_used    = 0
0.00.064.432 I print_info: causal attn      = 1
0.00.064.434 I print_info: pooling type     = 0
0.00.064.434 I print_info: rope type        = 2
0.00.064.434 I print_info: rope scaling     = linear
0.00.064.435 I print_info: freq_base_train  = 10000.0
0.00.064.435 I print_info: freq_scale_train = 1
0.00.064.436 I print_info: n_ctx_orig_yarn  = 2048
0.00.064.436 I print_info: rope_finetuned   = unknown
0.00.064.436 I print_info: ssm_d_conv       = 0
0.00.064.436 I print_info: ssm_d_inner      = 0
0.00.064.437 I print_info: ssm_d_state      = 0
0.00.064.437 I print_info: ssm_dt_rank      = 0
0.00.064.437 I print_info: ssm_dt_b_c_rms   = 0
0.00.064.437 I print_info: model type       = 1.4B
0.00.064.437 I print_info: model params     = 1.41 B
0.00.064.438 I print_info: general.name     = 1.4B
0.00.064.439 I print_info: vocab type       = BPE
0.00.064.440 I print_info: n_vocab          = 50304
0.00.064.440 I print_info: n_merges         = 50009
0.00.064.440 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.064.440 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.064.440 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.064.441 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.064.441 I print_info: LF token         = 128 'Ä'
0.00.064.441 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.064.441 I print_info: max token length = 1024
0.00.066.776 I load_tensors: offloading 24 repeating layers to GPU
0.00.066.776 I load_tensors: offloading output layer to GPU
0.00.066.776 I load_tensors: offloaded 25/25 layers to GPU
0.00.066.787 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.066.788 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.00.067.094 I llama_init_from_model: n_seq_max     = 1
0.00.067.095 I llama_init_from_model: n_ctx         = 128
0.00.067.095 I llama_init_from_model: n_ctx_per_seq = 128
0.00.067.096 I llama_init_from_model: n_batch       = 128
0.00.067.096 I llama_init_from_model: n_ubatch      = 128
0.00.067.096 I llama_init_from_model: flash_attn    = 0
0.00.067.096 I llama_init_from_model: freq_base     = 10000.0
0.00.067.097 I llama_init_from_model: freq_scale    = 1
0.00.067.097 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.067.098 I ggml_metal_init: allocating
0.00.067.101 I ggml_metal_init: found device: Apple M4
0.00.067.103 I ggml_metal_init: picking default device: Apple M4
0.00.067.754 I ggml_metal_init: using embedded metal library
0.00.070.332 I ggml_metal_init: GPU name:   Apple M4
0.00.070.333 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.070.333 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.070.334 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.070.334 I ggml_metal_init: simdgroup reduction   = true
0.00.070.334 I ggml_metal_init: simdgroup matrix mul. = true
0.00.070.334 I ggml_metal_init: has bfloat            = true
0.00.070.334 I ggml_metal_init: use bfloat            = true
0.00.070.335 I ggml_metal_init: hasUnifiedMemory      = true
0.00.070.336 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.080.230 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.081.595 I init:      Metal KV buffer size =    24.00 MiB
0.00.081.598 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.081.613 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.082.586 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.082.587 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.082.587 I llama_init_from_model: graph nodes  = 967
0.00.082.587 I llama_init_from_model: graph splits = 2
0.00.082.589 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.082.589 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.844.607 I 
0.00.844.661 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.844.689 I perplexity: tokenizing the input ..
0.00.852.473 I perplexity: tokenization took 7.783 ms
0.00.852.476 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.977.052 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.00.978.290 I Final estimate: PPL = 10.1362 +/- 3.22437

0.00.978.316 I llama_perf_context_print:        load time =     832.75 ms
0.00.978.317 I llama_perf_context_print: prompt eval time =     124.35 ms /   128 tokens (    0.97 ms per token,  1029.38 tokens per second)
0.00.978.317 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.978.318 I llama_perf_context_print:       total time =     133.71 ms /   129 tokens
0.00.979.017 I ggml_metal_free: deallocating

real	0m0.996s
user	0m0.092s
sys	0m0.135s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.086 I build: 4477 (28f12723) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.024 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.228 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.017.232 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.234 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.235 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.236 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.236 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.236 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.237 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.238 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.238 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.238 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.239 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.239 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.240 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.241 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.241 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.242 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.094 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.139 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.997 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.998 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.998 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.999 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.999 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.999 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.026.000 I llama_model_loader: - type  f32:  194 tensors
0.00.026.000 I llama_model_loader: - type q4_0:   97 tensors
0.00.026.000 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.001 I print_info: file format = GGUF V3 (latest)
0.00.026.001 I print_info: file type   = Q4_0
0.00.026.002 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.045.067 I load: special tokens cache size = 25
0.00.051.032 I load: token to piece cache size = 0.2984 MB
0.00.051.035 I print_info: arch             = gptneox
0.00.051.035 I print_info: vocab_only       = 0
0.00.051.035 I print_info: n_ctx_train      = 2048
0.00.051.036 I print_info: n_embd           = 2048
0.00.051.036 I print_info: n_layer          = 24
0.00.051.039 I print_info: n_head           = 16
0.00.051.039 I print_info: n_head_kv        = 16
0.00.051.039 I print_info: n_rot            = 32
0.00.051.040 I print_info: n_swa            = 0
0.00.051.041 I print_info: n_embd_head_k    = 128
0.00.051.042 I print_info: n_embd_head_v    = 128
0.00.051.043 I print_info: n_gqa            = 1
0.00.051.044 I print_info: n_embd_k_gqa     = 2048
0.00.051.045 I print_info: n_embd_v_gqa     = 2048
0.00.051.045 I print_info: f_norm_eps       = 1.0e-05
0.00.051.046 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.046 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.046 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.046 I print_info: f_logit_scale    = 0.0e+00
0.00.051.047 I print_info: n_ff             = 8192
0.00.051.047 I print_info: n_expert         = 0
0.00.051.047 I print_info: n_expert_used    = 0
0.00.051.047 I print_info: causal attn      = 1
0.00.051.047 I print_info: pooling type     = 0
0.00.051.048 I print_info: rope type        = 2
0.00.051.048 I print_info: rope scaling     = linear
0.00.051.049 I print_info: freq_base_train  = 10000.0
0.00.051.053 I print_info: freq_scale_train = 1
0.00.051.054 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.054 I print_info: rope_finetuned   = unknown
0.00.051.055 I print_info: ssm_d_conv       = 0
0.00.051.056 I print_info: ssm_d_inner      = 0
0.00.051.056 I print_info: ssm_d_state      = 0
0.00.051.056 I print_info: ssm_dt_rank      = 0
0.00.051.056 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.056 I print_info: model type       = 1.4B
0.00.051.056 I print_info: model params     = 1.41 B
0.00.051.057 I print_info: general.name     = 1.4B
0.00.051.057 I print_info: vocab type       = BPE
0.00.051.057 I print_info: n_vocab          = 50304
0.00.051.057 I print_info: n_merges         = 50009
0.00.051.058 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.058 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.060 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.060 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.060 I print_info: LF token         = 128 'Ä'
0.00.051.061 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.061 I print_info: max token length = 1024
0.00.052.932 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.932 I load_tensors: offloading output layer to GPU
0.00.052.932 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.942 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.052.943 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.053.222 I llama_init_from_model: n_seq_max     = 1
0.00.053.223 I llama_init_from_model: n_ctx         = 128
0.00.053.223 I llama_init_from_model: n_ctx_per_seq = 128
0.00.053.223 I llama_init_from_model: n_batch       = 128
0.00.053.223 I llama_init_from_model: n_ubatch      = 128
0.00.053.224 I llama_init_from_model: flash_attn    = 0
0.00.053.224 I llama_init_from_model: freq_base     = 10000.0
0.00.053.224 I llama_init_from_model: freq_scale    = 1
0.00.053.225 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.225 I ggml_metal_init: allocating
0.00.053.228 I ggml_metal_init: found device: Apple M4
0.00.053.230 I ggml_metal_init: picking default device: Apple M4
0.00.053.797 I ggml_metal_init: using embedded metal library
0.00.056.125 I ggml_metal_init: GPU name:   Apple M4
0.00.056.126 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.126 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.127 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.127 I ggml_metal_init: simdgroup reduction   = true
0.00.056.127 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.127 I ggml_metal_init: has bfloat            = true
0.00.056.127 I ggml_metal_init: use bfloat            = true
0.00.056.128 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.130 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.065 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.067.506 I init:      Metal KV buffer size =    24.00 MiB
0.00.067.509 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.523 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.068.372 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.068.373 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.068.373 I llama_init_from_model: graph nodes  = 967
0.00.068.373 I llama_init_from_model: graph splits = 2
0.00.068.374 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.375 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.596.869 I 
0.00.596.917 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.596.931 I perplexity: tokenizing the input ..
0.00.604.854 I perplexity: tokenization took 7.921 ms
0.00.604.858 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.727.533 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.728.687 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.728.721 I llama_perf_context_print:        load time =     586.84 ms
0.00.728.722 I llama_perf_context_print: prompt eval time =     122.45 ms /   128 tokens (    0.96 ms per token,  1045.33 tokens per second)
0.00.728.723 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.728.723 I llama_perf_context_print:       total time =     131.86 ms /   129 tokens
0.00.729.389 I ggml_metal_free: deallocating

real	0m0.745s
user	0m0.078s
sys	0m0.094s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.094 I build: 4477 (28f12723) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.819 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.307 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.311 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.313 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.313 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.314 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.314 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.319 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.320 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.320 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.323 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.323 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.324 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.324 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.325 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.328 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.329 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.329 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.182 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.248 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.113 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.115 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.115 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.115 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.115 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.116 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.025.116 I llama_model_loader: - type  f32:  194 tensors
0.00.025.117 I llama_model_loader: - type q4_1:   97 tensors
0.00.025.117 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.117 I print_info: file format = GGUF V3 (latest)
0.00.025.118 I print_info: file type   = Q4_1
0.00.025.123 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.044.072 I load: special tokens cache size = 25
0.00.050.118 I load: token to piece cache size = 0.2984 MB
0.00.050.121 I print_info: arch             = gptneox
0.00.050.121 I print_info: vocab_only       = 0
0.00.050.122 I print_info: n_ctx_train      = 2048
0.00.050.122 I print_info: n_embd           = 2048
0.00.050.122 I print_info: n_layer          = 24
0.00.050.125 I print_info: n_head           = 16
0.00.050.126 I print_info: n_head_kv        = 16
0.00.050.126 I print_info: n_rot            = 32
0.00.050.126 I print_info: n_swa            = 0
0.00.050.126 I print_info: n_embd_head_k    = 128
0.00.050.126 I print_info: n_embd_head_v    = 128
0.00.050.127 I print_info: n_gqa            = 1
0.00.050.128 I print_info: n_embd_k_gqa     = 2048
0.00.050.130 I print_info: n_embd_v_gqa     = 2048
0.00.050.131 I print_info: f_norm_eps       = 1.0e-05
0.00.050.131 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.132 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.132 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.132 I print_info: f_logit_scale    = 0.0e+00
0.00.050.133 I print_info: n_ff             = 8192
0.00.050.133 I print_info: n_expert         = 0
0.00.050.133 I print_info: n_expert_used    = 0
0.00.050.133 I print_info: causal attn      = 1
0.00.050.134 I print_info: pooling type     = 0
0.00.050.134 I print_info: rope type        = 2
0.00.050.134 I print_info: rope scaling     = linear
0.00.050.136 I print_info: freq_base_train  = 10000.0
0.00.050.137 I print_info: freq_scale_train = 1
0.00.050.138 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.138 I print_info: rope_finetuned   = unknown
0.00.050.138 I print_info: ssm_d_conv       = 0
0.00.050.138 I print_info: ssm_d_inner      = 0
0.00.050.138 I print_info: ssm_d_state      = 0
0.00.050.138 I print_info: ssm_dt_rank      = 0
0.00.050.138 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.143 I print_info: model type       = 1.4B
0.00.050.143 I print_info: model params     = 1.41 B
0.00.050.143 I print_info: general.name     = 1.4B
0.00.050.144 I print_info: vocab type       = BPE
0.00.050.144 I print_info: n_vocab          = 50304
0.00.050.144 I print_info: n_merges         = 50009
0.00.050.144 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.144 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.145 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.146 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.147 I print_info: LF token         = 128 'Ä'
0.00.050.147 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.147 I print_info: max token length = 1024
0.00.052.166 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.166 I load_tensors: offloading output layer to GPU
0.00.052.166 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.177 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.052.178 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.052.456 I llama_init_from_model: n_seq_max     = 1
0.00.052.457 I llama_init_from_model: n_ctx         = 128
0.00.052.457 I llama_init_from_model: n_ctx_per_seq = 128
0.00.052.457 I llama_init_from_model: n_batch       = 128
0.00.052.458 I llama_init_from_model: n_ubatch      = 128
0.00.052.458 I llama_init_from_model: flash_attn    = 0
0.00.052.458 I llama_init_from_model: freq_base     = 10000.0
0.00.052.458 I llama_init_from_model: freq_scale    = 1
0.00.052.459 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.459 I ggml_metal_init: allocating
0.00.052.462 I ggml_metal_init: found device: Apple M4
0.00.052.464 I ggml_metal_init: picking default device: Apple M4
0.00.053.041 I ggml_metal_init: using embedded metal library
0.00.055.411 I ggml_metal_init: GPU name:   Apple M4
0.00.055.413 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.413 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.413 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.413 I ggml_metal_init: simdgroup reduction   = true
0.00.055.414 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.414 I ggml_metal_init: has bfloat            = true
0.00.055.414 I ggml_metal_init: use bfloat            = true
0.00.055.414 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.415 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.179 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.440 I init:      Metal KV buffer size =    24.00 MiB
0.00.066.444 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.468 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.067.276 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.067.277 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.067.278 I llama_init_from_model: graph nodes  = 967
0.00.067.278 I llama_init_from_model: graph splits = 2
0.00.067.279 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.279 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.635.371 I 
0.00.635.395 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.635.406 I perplexity: tokenizing the input ..
0.00.643.594 I perplexity: tokenization took 8.187 ms
0.00.643.597 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.766.626 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.767.880 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.767.907 I llama_perf_context_print:        load time =     626.55 ms
0.00.767.908 I llama_perf_context_print: prompt eval time =     122.78 ms /   128 tokens (    0.96 ms per token,  1042.49 tokens per second)
0.00.767.911 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.767.911 I llama_perf_context_print:       total time =     132.53 ms /   129 tokens
0.00.768.634 I ggml_metal_free: deallocating

real	0m0.782s
user	0m0.078s
sys	0m0.107s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.092 I build: 4477 (28f12723) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.903 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.711 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.016.716 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.717 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.718 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.718 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.719 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.719 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.720 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.720 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.721 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.721 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.722 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.722 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.722 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.725 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.725 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.725 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.514 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.508 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.330 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.331 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.332 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.332 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.332 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.333 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.025.333 I llama_model_loader: - type  f32:  194 tensors
0.00.025.333 I llama_model_loader: - type q5_0:   97 tensors
0.00.025.334 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.334 I print_info: file format = GGUF V3 (latest)
0.00.025.335 I print_info: file type   = Q5_0
0.00.025.335 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.044.484 I load: special tokens cache size = 25
0.00.050.472 I load: token to piece cache size = 0.2984 MB
0.00.050.475 I print_info: arch             = gptneox
0.00.050.475 I print_info: vocab_only       = 0
0.00.050.475 I print_info: n_ctx_train      = 2048
0.00.050.476 I print_info: n_embd           = 2048
0.00.050.476 I print_info: n_layer          = 24
0.00.050.478 I print_info: n_head           = 16
0.00.050.479 I print_info: n_head_kv        = 16
0.00.050.479 I print_info: n_rot            = 32
0.00.050.480 I print_info: n_swa            = 0
0.00.050.480 I print_info: n_embd_head_k    = 128
0.00.050.480 I print_info: n_embd_head_v    = 128
0.00.050.481 I print_info: n_gqa            = 1
0.00.050.481 I print_info: n_embd_k_gqa     = 2048
0.00.050.482 I print_info: n_embd_v_gqa     = 2048
0.00.050.483 I print_info: f_norm_eps       = 1.0e-05
0.00.050.483 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.483 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.483 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.483 I print_info: f_logit_scale    = 0.0e+00
0.00.050.484 I print_info: n_ff             = 8192
0.00.050.484 I print_info: n_expert         = 0
0.00.050.484 I print_info: n_expert_used    = 0
0.00.050.485 I print_info: causal attn      = 1
0.00.050.485 I print_info: pooling type     = 0
0.00.050.485 I print_info: rope type        = 2
0.00.050.485 I print_info: rope scaling     = linear
0.00.050.486 I print_info: freq_base_train  = 10000.0
0.00.050.487 I print_info: freq_scale_train = 1
0.00.050.487 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.487 I print_info: rope_finetuned   = unknown
0.00.050.487 I print_info: ssm_d_conv       = 0
0.00.050.487 I print_info: ssm_d_inner      = 0
0.00.050.487 I print_info: ssm_d_state      = 0
0.00.050.488 I print_info: ssm_dt_rank      = 0
0.00.050.488 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.488 I print_info: model type       = 1.4B
0.00.050.488 I print_info: model params     = 1.41 B
0.00.050.489 I print_info: general.name     = 1.4B
0.00.050.489 I print_info: vocab type       = BPE
0.00.050.489 I print_info: n_vocab          = 50304
0.00.050.489 I print_info: n_merges         = 50009
0.00.050.494 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.494 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.495 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.495 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.495 I print_info: LF token         = 128 'Ä'
0.00.050.495 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.495 I print_info: max token length = 1024
0.00.052.509 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.509 I load_tensors: offloading output layer to GPU
0.00.052.510 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.520 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.052.522 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.052.823 I llama_init_from_model: n_seq_max     = 1
0.00.052.824 I llama_init_from_model: n_ctx         = 128
0.00.052.824 I llama_init_from_model: n_ctx_per_seq = 128
0.00.052.824 I llama_init_from_model: n_batch       = 128
0.00.052.824 I llama_init_from_model: n_ubatch      = 128
0.00.052.824 I llama_init_from_model: flash_attn    = 0
0.00.052.825 I llama_init_from_model: freq_base     = 10000.0
0.00.052.825 I llama_init_from_model: freq_scale    = 1
0.00.052.825 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.826 I ggml_metal_init: allocating
0.00.052.829 I ggml_metal_init: found device: Apple M4
0.00.052.830 I ggml_metal_init: picking default device: Apple M4
0.00.053.400 I ggml_metal_init: using embedded metal library
0.00.055.766 I ggml_metal_init: GPU name:   Apple M4
0.00.055.768 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.768 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.769 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.769 I ggml_metal_init: simdgroup reduction   = true
0.00.055.769 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.769 I ggml_metal_init: has bfloat            = true
0.00.055.769 I ggml_metal_init: use bfloat            = true
0.00.055.770 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.770 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.566 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.960 I init:      Metal KV buffer size =    24.00 MiB
0.00.066.962 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.976 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.067.942 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.067.943 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.067.944 I llama_init_from_model: graph nodes  = 967
0.00.067.944 I llama_init_from_model: graph splits = 2
0.00.067.945 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.945 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.717.540 I 
0.00.717.570 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.717.582 I perplexity: tokenizing the input ..
0.00.725.929 I perplexity: tokenization took 8.346 ms
0.00.725.935 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.860.737 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.861.898 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.861.926 I llama_perf_context_print:        load time =     707.63 ms
0.00.861.927 I llama_perf_context_print: prompt eval time =     134.58 ms /   128 tokens (    1.05 ms per token,   951.14 tokens per second)
0.00.861.928 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.861.928 I llama_perf_context_print:       total time =     144.39 ms /   129 tokens
0.00.862.524 I ggml_metal_free: deallocating

real	0m0.879s
user	0m0.078s
sys	0m0.106s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.095 I build: 4477 (28f12723) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.785 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.109 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.114 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.115 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.116 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.116 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.116 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.117 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.117 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.118 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.118 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.119 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.119 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.119 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.122 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.125 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.126 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.126 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.040 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.059 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.793 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.794 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.794 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.795 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.795 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.795 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.796 I llama_model_loader: - type  f32:  194 tensors
0.00.024.796 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.796 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.797 I print_info: file format = GGUF V3 (latest)
0.00.024.798 I print_info: file type   = Q5_1
0.00.024.799 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.044.511 I load: special tokens cache size = 25
0.00.050.576 I load: token to piece cache size = 0.2984 MB
0.00.050.579 I print_info: arch             = gptneox
0.00.050.580 I print_info: vocab_only       = 0
0.00.050.580 I print_info: n_ctx_train      = 2048
0.00.050.580 I print_info: n_embd           = 2048
0.00.050.580 I print_info: n_layer          = 24
0.00.050.583 I print_info: n_head           = 16
0.00.050.584 I print_info: n_head_kv        = 16
0.00.050.584 I print_info: n_rot            = 32
0.00.050.585 I print_info: n_swa            = 0
0.00.050.585 I print_info: n_embd_head_k    = 128
0.00.050.585 I print_info: n_embd_head_v    = 128
0.00.050.586 I print_info: n_gqa            = 1
0.00.050.587 I print_info: n_embd_k_gqa     = 2048
0.00.050.587 I print_info: n_embd_v_gqa     = 2048
0.00.050.588 I print_info: f_norm_eps       = 1.0e-05
0.00.050.588 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.589 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.589 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.591 I print_info: f_logit_scale    = 0.0e+00
0.00.050.591 I print_info: n_ff             = 8192
0.00.050.591 I print_info: n_expert         = 0
0.00.050.592 I print_info: n_expert_used    = 0
0.00.050.594 I print_info: causal attn      = 1
0.00.050.594 I print_info: pooling type     = 0
0.00.050.594 I print_info: rope type        = 2
0.00.050.594 I print_info: rope scaling     = linear
0.00.050.596 I print_info: freq_base_train  = 10000.0
0.00.050.597 I print_info: freq_scale_train = 1
0.00.050.601 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.601 I print_info: rope_finetuned   = unknown
0.00.050.601 I print_info: ssm_d_conv       = 0
0.00.050.603 I print_info: ssm_d_inner      = 0
0.00.050.603 I print_info: ssm_d_state      = 0
0.00.050.603 I print_info: ssm_dt_rank      = 0
0.00.050.603 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.603 I print_info: model type       = 1.4B
0.00.050.604 I print_info: model params     = 1.41 B
0.00.050.604 I print_info: general.name     = 1.4B
0.00.050.604 I print_info: vocab type       = BPE
0.00.050.604 I print_info: n_vocab          = 50304
0.00.050.605 I print_info: n_merges         = 50009
0.00.050.605 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.605 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.606 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.606 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.610 I print_info: LF token         = 128 'Ä'
0.00.050.611 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.611 I print_info: max token length = 1024
0.00.052.630 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.630 I load_tensors: offloading output layer to GPU
0.00.052.630 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.641 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.052.642 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.052.928 I llama_init_from_model: n_seq_max     = 1
0.00.052.929 I llama_init_from_model: n_ctx         = 128
0.00.052.929 I llama_init_from_model: n_ctx_per_seq = 128
0.00.052.930 I llama_init_from_model: n_batch       = 128
0.00.052.930 I llama_init_from_model: n_ubatch      = 128
0.00.052.930 I llama_init_from_model: flash_attn    = 0
0.00.052.930 I llama_init_from_model: freq_base     = 10000.0
0.00.052.931 I llama_init_from_model: freq_scale    = 1
0.00.052.931 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.931 I ggml_metal_init: allocating
0.00.052.934 I ggml_metal_init: found device: Apple M4
0.00.052.936 I ggml_metal_init: picking default device: Apple M4
0.00.053.521 I ggml_metal_init: using embedded metal library
0.00.055.864 I ggml_metal_init: GPU name:   Apple M4
0.00.055.865 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.865 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.866 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.866 I ggml_metal_init: simdgroup reduction   = true
0.00.055.866 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.866 I ggml_metal_init: has bfloat            = true
0.00.055.866 I ggml_metal_init: use bfloat            = true
0.00.055.867 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.867 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.793 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.067.052 I init:      Metal KV buffer size =    24.00 MiB
0.00.067.059 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.075 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.067.952 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.067.953 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.067.954 I llama_init_from_model: graph nodes  = 967
0.00.067.954 I llama_init_from_model: graph splits = 2
0.00.067.955 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.955 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.754.840 I 
0.00.754.876 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.754.887 I perplexity: tokenizing the input ..
0.00.762.932 I perplexity: tokenization took 8.044 ms
0.00.762.941 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.898.135 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.899.308 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.899.328 I llama_perf_context_print:        load time =     746.05 ms
0.00.899.329 I llama_perf_context_print: prompt eval time =     134.97 ms /   128 tokens (    1.05 ms per token,   948.38 tokens per second)
0.00.899.330 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.899.330 I llama_perf_context_print:       total time =     144.49 ms /   129 tokens
0.00.899.960 I ggml_metal_free: deallocating

real	0m0.913s
user	0m0.079s
sys	0m0.124s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.086 I build: 4477 (28f12723) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.815 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.774 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.779 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.781 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.781 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.782 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.782 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.782 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.785 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.786 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.786 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.786 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.787 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.787 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.788 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.789 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.790 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.790 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.598 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.631 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.420 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.421 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.422 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.422 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.422 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.423 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.423 I llama_model_loader: - type  f32:  194 tensors
0.00.025.424 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.424 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.424 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.425 I print_info: file format = GGUF V3 (latest)
0.00.025.425 I print_info: file type   = Q2_K - Medium
0.00.025.426 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.044.659 I load: special tokens cache size = 25
0.00.050.545 I load: token to piece cache size = 0.2984 MB
0.00.050.547 I print_info: arch             = gptneox
0.00.050.548 I print_info: vocab_only       = 0
0.00.050.548 I print_info: n_ctx_train      = 2048
0.00.050.548 I print_info: n_embd           = 2048
0.00.050.548 I print_info: n_layer          = 24
0.00.050.551 I print_info: n_head           = 16
0.00.050.552 I print_info: n_head_kv        = 16
0.00.050.552 I print_info: n_rot            = 32
0.00.050.552 I print_info: n_swa            = 0
0.00.050.553 I print_info: n_embd_head_k    = 128
0.00.050.553 I print_info: n_embd_head_v    = 128
0.00.050.554 I print_info: n_gqa            = 1
0.00.050.554 I print_info: n_embd_k_gqa     = 2048
0.00.050.555 I print_info: n_embd_v_gqa     = 2048
0.00.050.556 I print_info: f_norm_eps       = 1.0e-05
0.00.050.556 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.556 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.556 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.558 I print_info: f_logit_scale    = 0.0e+00
0.00.050.559 I print_info: n_ff             = 8192
0.00.050.559 I print_info: n_expert         = 0
0.00.050.559 I print_info: n_expert_used    = 0
0.00.050.560 I print_info: causal attn      = 1
0.00.050.560 I print_info: pooling type     = 0
0.00.050.560 I print_info: rope type        = 2
0.00.050.561 I print_info: rope scaling     = linear
0.00.050.562 I print_info: freq_base_train  = 10000.0
0.00.050.562 I print_info: freq_scale_train = 1
0.00.050.562 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.562 I print_info: rope_finetuned   = unknown
0.00.050.563 I print_info: ssm_d_conv       = 0
0.00.050.563 I print_info: ssm_d_inner      = 0
0.00.050.563 I print_info: ssm_d_state      = 0
0.00.050.563 I print_info: ssm_dt_rank      = 0
0.00.050.563 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.563 I print_info: model type       = 1.4B
0.00.050.564 I print_info: model params     = 1.41 B
0.00.050.564 I print_info: general.name     = 1.4B
0.00.050.564 I print_info: vocab type       = BPE
0.00.050.565 I print_info: n_vocab          = 50304
0.00.050.565 I print_info: n_merges         = 50009
0.00.050.565 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.565 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.566 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.566 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.566 I print_info: LF token         = 128 'Ä'
0.00.050.566 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.566 I print_info: max token length = 1024
0.00.052.498 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.498 I load_tensors: offloading output layer to GPU
0.00.052.499 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.509 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.052.511 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.052.793 I llama_init_from_model: n_seq_max     = 1
0.00.052.794 I llama_init_from_model: n_ctx         = 128
0.00.052.794 I llama_init_from_model: n_ctx_per_seq = 128
0.00.052.794 I llama_init_from_model: n_batch       = 128
0.00.052.795 I llama_init_from_model: n_ubatch      = 128
0.00.052.795 I llama_init_from_model: flash_attn    = 0
0.00.052.795 I llama_init_from_model: freq_base     = 10000.0
0.00.052.795 I llama_init_from_model: freq_scale    = 1
0.00.052.796 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.796 I ggml_metal_init: allocating
0.00.052.799 I ggml_metal_init: found device: Apple M4
0.00.052.801 I ggml_metal_init: picking default device: Apple M4
0.00.053.377 I ggml_metal_init: using embedded metal library
0.00.055.734 I ggml_metal_init: GPU name:   Apple M4
0.00.055.735 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.735 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.736 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.736 I ggml_metal_init: simdgroup reduction   = true
0.00.055.736 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.736 I ggml_metal_init: has bfloat            = true
0.00.055.736 I ggml_metal_init: use bfloat            = true
0.00.055.737 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.737 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.492 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.853 I init:      Metal KV buffer size =    24.00 MiB
0.00.066.855 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.869 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.067.804 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.067.805 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.067.805 I llama_init_from_model: graph nodes  = 967
0.00.067.805 I llama_init_from_model: graph splits = 2
0.00.067.807 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.807 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.418.350 I 
0.00.418.382 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.418.399 I perplexity: tokenizing the input ..
0.00.426.279 I perplexity: tokenization took 7.877 ms
0.00.426.283 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.558.827 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.560.014 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.560.034 I llama_perf_context_print:        load time =     408.53 ms
0.00.560.034 I llama_perf_context_print: prompt eval time =     132.32 ms /   128 tokens (    1.03 ms per token,   967.34 tokens per second)
0.00.560.036 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.560.036 I llama_perf_context_print:       total time =     141.69 ms /   129 tokens
0.00.560.556 I ggml_metal_free: deallocating

real	0m0.577s
user	0m0.078s
sys	0m0.069s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.086 I build: 4477 (28f12723) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.888 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.907 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.015.912 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.914 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.914 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.915 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.915 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.917 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.918 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.919 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.922 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.922 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.923 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.929 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.932 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.935 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.936 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.936 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.690 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.706 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.458 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.459 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.460 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.460 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.460 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.461 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.461 I llama_model_loader: - type  f32:  194 tensors
0.00.024.461 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.462 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.462 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.462 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.463 I print_info: file format = GGUF V3 (latest)
0.00.024.463 I print_info: file type   = Q3_K - Medium
0.00.024.466 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.043.351 I load: special tokens cache size = 25
0.00.049.423 I load: token to piece cache size = 0.2984 MB
0.00.049.426 I print_info: arch             = gptneox
0.00.049.426 I print_info: vocab_only       = 0
0.00.049.426 I print_info: n_ctx_train      = 2048
0.00.049.427 I print_info: n_embd           = 2048
0.00.049.427 I print_info: n_layer          = 24
0.00.049.430 I print_info: n_head           = 16
0.00.049.430 I print_info: n_head_kv        = 16
0.00.049.431 I print_info: n_rot            = 32
0.00.049.431 I print_info: n_swa            = 0
0.00.049.431 I print_info: n_embd_head_k    = 128
0.00.049.431 I print_info: n_embd_head_v    = 128
0.00.049.432 I print_info: n_gqa            = 1
0.00.049.433 I print_info: n_embd_k_gqa     = 2048
0.00.049.433 I print_info: n_embd_v_gqa     = 2048
0.00.049.436 I print_info: f_norm_eps       = 1.0e-05
0.00.049.436 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.436 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.436 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.438 I print_info: f_logit_scale    = 0.0e+00
0.00.049.439 I print_info: n_ff             = 8192
0.00.049.439 I print_info: n_expert         = 0
0.00.049.439 I print_info: n_expert_used    = 0
0.00.049.439 I print_info: causal attn      = 1
0.00.049.439 I print_info: pooling type     = 0
0.00.049.441 I print_info: rope type        = 2
0.00.049.442 I print_info: rope scaling     = linear
0.00.049.442 I print_info: freq_base_train  = 10000.0
0.00.049.443 I print_info: freq_scale_train = 1
0.00.049.443 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.443 I print_info: rope_finetuned   = unknown
0.00.049.443 I print_info: ssm_d_conv       = 0
0.00.049.443 I print_info: ssm_d_inner      = 0
0.00.049.443 I print_info: ssm_d_state      = 0
0.00.049.444 I print_info: ssm_dt_rank      = 0
0.00.049.444 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.444 I print_info: model type       = 1.4B
0.00.049.444 I print_info: model params     = 1.41 B
0.00.049.444 I print_info: general.name     = 1.4B
0.00.049.445 I print_info: vocab type       = BPE
0.00.049.449 I print_info: n_vocab          = 50304
0.00.049.450 I print_info: n_merges         = 50009
0.00.049.450 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.450 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.450 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.450 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.452 I print_info: LF token         = 128 'Ä'
0.00.049.452 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.452 I print_info: max token length = 1024
0.00.051.420 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.420 I load_tensors: offloading output layer to GPU
0.00.051.421 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.431 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.051.432 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.051.733 I llama_init_from_model: n_seq_max     = 1
0.00.051.734 I llama_init_from_model: n_ctx         = 128
0.00.051.734 I llama_init_from_model: n_ctx_per_seq = 128
0.00.051.734 I llama_init_from_model: n_batch       = 128
0.00.051.734 I llama_init_from_model: n_ubatch      = 128
0.00.051.734 I llama_init_from_model: flash_attn    = 0
0.00.051.735 I llama_init_from_model: freq_base     = 10000.0
0.00.051.735 I llama_init_from_model: freq_scale    = 1
0.00.051.735 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.736 I ggml_metal_init: allocating
0.00.051.739 I ggml_metal_init: found device: Apple M4
0.00.051.741 I ggml_metal_init: picking default device: Apple M4
0.00.052.331 I ggml_metal_init: using embedded metal library
0.00.054.640 I ggml_metal_init: GPU name:   Apple M4
0.00.054.641 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.641 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.642 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.642 I ggml_metal_init: simdgroup reduction   = true
0.00.054.642 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.642 I ggml_metal_init: has bfloat            = true
0.00.054.642 I ggml_metal_init: use bfloat            = true
0.00.054.643 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.643 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.280 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.065.764 I init:      Metal KV buffer size =    24.00 MiB
0.00.065.767 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.780 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.066.665 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.066.666 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.066.667 I llama_init_from_model: graph nodes  = 967
0.00.066.667 I llama_init_from_model: graph splits = 2
0.00.066.668 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.668 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.479.892 I 
0.00.479.926 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.479.938 I perplexity: tokenizing the input ..
0.00.488.419 I perplexity: tokenization took 8.48 ms
0.00.488.422 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.620.712 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.621.876 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.621.909 I llama_perf_context_print:        load time =     471.00 ms
0.00.621.910 I llama_perf_context_print: prompt eval time =     132.05 ms /   128 tokens (    1.03 ms per token,   969.31 tokens per second)
0.00.621.911 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.621.911 I llama_perf_context_print:       total time =     142.02 ms /   129 tokens
0.00.622.732 I ggml_metal_free: deallocating

real	0m0.636s
user	0m0.078s
sys	0m0.088s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.101 I build: 4477 (28f12723) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.920 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.874 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.015.880 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.882 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.882 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.883 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.883 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.883 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.884 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.884 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.885 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.886 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.887 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.887 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.888 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.889 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.889 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.890 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.672 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.681 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.445 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.447 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.447 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.447 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.448 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.448 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.448 I llama_model_loader: - type  f32:  194 tensors
0.00.024.449 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.449 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.449 I llama_model_loader: - type q6_K:   13 tensors
0.00.024.450 I print_info: file format = GGUF V3 (latest)
0.00.024.450 I print_info: file type   = Q4_K - Medium
0.00.024.451 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.043.555 I load: special tokens cache size = 25
0.00.049.640 I load: token to piece cache size = 0.2984 MB
0.00.049.643 I print_info: arch             = gptneox
0.00.049.643 I print_info: vocab_only       = 0
0.00.049.643 I print_info: n_ctx_train      = 2048
0.00.049.643 I print_info: n_embd           = 2048
0.00.049.643 I print_info: n_layer          = 24
0.00.049.646 I print_info: n_head           = 16
0.00.049.647 I print_info: n_head_kv        = 16
0.00.049.647 I print_info: n_rot            = 32
0.00.049.647 I print_info: n_swa            = 0
0.00.049.647 I print_info: n_embd_head_k    = 128
0.00.049.648 I print_info: n_embd_head_v    = 128
0.00.049.649 I print_info: n_gqa            = 1
0.00.049.650 I print_info: n_embd_k_gqa     = 2048
0.00.049.651 I print_info: n_embd_v_gqa     = 2048
0.00.049.651 I print_info: f_norm_eps       = 1.0e-05
0.00.049.651 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.652 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.652 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.652 I print_info: f_logit_scale    = 0.0e+00
0.00.049.653 I print_info: n_ff             = 8192
0.00.049.653 I print_info: n_expert         = 0
0.00.049.653 I print_info: n_expert_used    = 0
0.00.049.653 I print_info: causal attn      = 1
0.00.049.653 I print_info: pooling type     = 0
0.00.049.654 I print_info: rope type        = 2
0.00.049.654 I print_info: rope scaling     = linear
0.00.049.654 I print_info: freq_base_train  = 10000.0
0.00.049.655 I print_info: freq_scale_train = 1
0.00.049.655 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.655 I print_info: rope_finetuned   = unknown
0.00.049.655 I print_info: ssm_d_conv       = 0
0.00.049.655 I print_info: ssm_d_inner      = 0
0.00.049.655 I print_info: ssm_d_state      = 0
0.00.049.656 I print_info: ssm_dt_rank      = 0
0.00.049.656 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.658 I print_info: model type       = 1.4B
0.00.049.658 I print_info: model params     = 1.41 B
0.00.049.658 I print_info: general.name     = 1.4B
0.00.049.659 I print_info: vocab type       = BPE
0.00.049.659 I print_info: n_vocab          = 50304
0.00.049.659 I print_info: n_merges         = 50009
0.00.049.660 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.660 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.660 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.660 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.664 I print_info: LF token         = 128 'Ä'
0.00.049.664 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.665 I print_info: max token length = 1024
0.00.051.631 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.631 I load_tensors: offloading output layer to GPU
0.00.051.632 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.642 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.051.643 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.051.927 I llama_init_from_model: n_seq_max     = 1
0.00.051.927 I llama_init_from_model: n_ctx         = 128
0.00.051.928 I llama_init_from_model: n_ctx_per_seq = 128
0.00.051.928 I llama_init_from_model: n_batch       = 128
0.00.051.928 I llama_init_from_model: n_ubatch      = 128
0.00.051.928 I llama_init_from_model: flash_attn    = 0
0.00.051.929 I llama_init_from_model: freq_base     = 10000.0
0.00.051.929 I llama_init_from_model: freq_scale    = 1
0.00.051.929 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.930 I ggml_metal_init: allocating
0.00.051.933 I ggml_metal_init: found device: Apple M4
0.00.051.935 I ggml_metal_init: picking default device: Apple M4
0.00.052.500 I ggml_metal_init: using embedded metal library
0.00.054.878 I ggml_metal_init: GPU name:   Apple M4
0.00.054.879 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.880 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.880 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.880 I ggml_metal_init: simdgroup reduction   = true
0.00.054.880 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.880 I ggml_metal_init: has bfloat            = true
0.00.054.881 I ggml_metal_init: use bfloat            = true
0.00.054.881 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.882 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.586 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.065.852 I init:      Metal KV buffer size =    24.00 MiB
0.00.065.856 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.872 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.066.820 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.066.821 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.066.821 I llama_init_from_model: graph nodes  = 967
0.00.066.822 I llama_init_from_model: graph splits = 2
0.00.066.823 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.823 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.545.600 I 
0.00.545.630 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.545.640 I perplexity: tokenizing the input ..
0.00.553.455 I perplexity: tokenization took 7.813 ms
0.00.553.458 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.688.133 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.689.317 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.689.343 I llama_perf_context_print:        load time =     536.68 ms
0.00.689.344 I llama_perf_context_print: prompt eval time =     134.45 ms /   128 tokens (    1.05 ms per token,   952.03 tokens per second)
0.00.689.345 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.689.346 I llama_perf_context_print:       total time =     143.74 ms /   129 tokens
0.00.690.104 I ggml_metal_free: deallocating

real	0m0.703s
user	0m0.078s
sys	0m0.091s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.092 I build: 4477 (28f12723) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.777 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.628 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.017.633 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.634 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.635 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.635 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.635 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.636 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.637 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.637 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.640 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.640 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.640 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.641 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.641 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.644 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.645 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.645 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.539 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.590 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.417 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.419 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.419 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.419 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.420 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.420 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.026.420 I llama_model_loader: - type  f32:  194 tensors
0.00.026.421 I llama_model_loader: - type q5_K:   61 tensors
0.00.026.421 I llama_model_loader: - type q6_K:   37 tensors
0.00.026.422 I print_info: file format = GGUF V3 (latest)
0.00.026.422 I print_info: file type   = Q5_K - Medium
0.00.026.423 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.046.195 I load: special tokens cache size = 25
0.00.052.230 I load: token to piece cache size = 0.2984 MB
0.00.052.233 I print_info: arch             = gptneox
0.00.052.233 I print_info: vocab_only       = 0
0.00.052.233 I print_info: n_ctx_train      = 2048
0.00.052.234 I print_info: n_embd           = 2048
0.00.052.234 I print_info: n_layer          = 24
0.00.052.237 I print_info: n_head           = 16
0.00.052.238 I print_info: n_head_kv        = 16
0.00.052.238 I print_info: n_rot            = 32
0.00.052.238 I print_info: n_swa            = 0
0.00.052.238 I print_info: n_embd_head_k    = 128
0.00.052.238 I print_info: n_embd_head_v    = 128
0.00.052.239 I print_info: n_gqa            = 1
0.00.052.240 I print_info: n_embd_k_gqa     = 2048
0.00.052.241 I print_info: n_embd_v_gqa     = 2048
0.00.052.241 I print_info: f_norm_eps       = 1.0e-05
0.00.052.242 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.052.243 I print_info: f_clamp_kqv      = 0.0e+00
0.00.052.243 I print_info: f_max_alibi_bias = 0.0e+00
0.00.052.243 I print_info: f_logit_scale    = 0.0e+00
0.00.052.244 I print_info: n_ff             = 8192
0.00.052.244 I print_info: n_expert         = 0
0.00.052.244 I print_info: n_expert_used    = 0
0.00.052.245 I print_info: causal attn      = 1
0.00.052.245 I print_info: pooling type     = 0
0.00.052.245 I print_info: rope type        = 2
0.00.052.245 I print_info: rope scaling     = linear
0.00.052.247 I print_info: freq_base_train  = 10000.0
0.00.052.247 I print_info: freq_scale_train = 1
0.00.052.248 I print_info: n_ctx_orig_yarn  = 2048
0.00.052.248 I print_info: rope_finetuned   = unknown
0.00.052.248 I print_info: ssm_d_conv       = 0
0.00.052.248 I print_info: ssm_d_inner      = 0
0.00.052.248 I print_info: ssm_d_state      = 0
0.00.052.248 I print_info: ssm_dt_rank      = 0
0.00.052.248 I print_info: ssm_dt_b_c_rms   = 0
0.00.052.249 I print_info: model type       = 1.4B
0.00.052.249 I print_info: model params     = 1.41 B
0.00.052.249 I print_info: general.name     = 1.4B
0.00.052.250 I print_info: vocab type       = BPE
0.00.052.254 I print_info: n_vocab          = 50304
0.00.052.254 I print_info: n_merges         = 50009
0.00.052.254 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.052.256 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.052.256 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.052.256 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.052.256 I print_info: LF token         = 128 'Ä'
0.00.052.257 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.052.257 I print_info: max token length = 1024
0.00.054.331 I load_tensors: offloading 24 repeating layers to GPU
0.00.054.331 I load_tensors: offloading output layer to GPU
0.00.054.332 I load_tensors: offloaded 25/25 layers to GPU
0.00.054.342 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.054.343 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.054.634 I llama_init_from_model: n_seq_max     = 1
0.00.054.635 I llama_init_from_model: n_ctx         = 128
0.00.054.635 I llama_init_from_model: n_ctx_per_seq = 128
0.00.054.635 I llama_init_from_model: n_batch       = 128
0.00.054.635 I llama_init_from_model: n_ubatch      = 128
0.00.054.636 I llama_init_from_model: flash_attn    = 0
0.00.054.636 I llama_init_from_model: freq_base     = 10000.0
0.00.054.636 I llama_init_from_model: freq_scale    = 1
0.00.054.637 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.637 I ggml_metal_init: allocating
0.00.054.640 I ggml_metal_init: found device: Apple M4
0.00.054.642 I ggml_metal_init: picking default device: Apple M4
0.00.055.252 I ggml_metal_init: using embedded metal library
0.00.057.627 I ggml_metal_init: GPU name:   Apple M4
0.00.057.629 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.629 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.630 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.630 I ggml_metal_init: simdgroup reduction   = true
0.00.057.630 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.630 I ggml_metal_init: has bfloat            = true
0.00.057.630 I ggml_metal_init: use bfloat            = true
0.00.057.631 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.631 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.588 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.068.867 I init:      Metal KV buffer size =    24.00 MiB
0.00.068.870 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.884 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.069.768 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.069.769 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.069.769 I llama_init_from_model: graph nodes  = 967
0.00.069.769 I llama_init_from_model: graph splits = 2
0.00.069.771 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.069.771 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.647.467 I 
0.00.647.498 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.647.510 I perplexity: tokenizing the input ..
0.00.655.293 I perplexity: tokenization took 7.781 ms
0.00.655.297 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.795.188 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.796.535 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.796.557 I llama_perf_context_print:        load time =     636.68 ms
0.00.796.558 I llama_perf_context_print: prompt eval time =     139.66 ms /   128 tokens (    1.09 ms per token,   916.52 tokens per second)
0.00.796.559 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.796.559 I llama_perf_context_print:       total time =     149.10 ms /   129 tokens
0.00.797.085 I ggml_metal_free: deallocating

real	0m0.814s
user	0m0.079s
sys	0m0.121s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.100 I build: 4477 (28f12723) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.424 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.451 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.457 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.463 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.464 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.464 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.465 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.465 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.466 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.466 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.467 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.468 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.469 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.469 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.469 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.472 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.473 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.473 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.283 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.465 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.389 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.391 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.391 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.392 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.392 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.392 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.393 I llama_model_loader: - type  f32:  194 tensors
0.00.025.393 I llama_model_loader: - type q6_K:   98 tensors
0.00.025.394 I print_info: file format = GGUF V3 (latest)
0.00.025.395 I print_info: file type   = Q6_K
0.00.025.396 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.046.017 I load: special tokens cache size = 25
0.00.052.069 I load: token to piece cache size = 0.2984 MB
0.00.052.074 I print_info: arch             = gptneox
0.00.052.075 I print_info: vocab_only       = 0
0.00.052.075 I print_info: n_ctx_train      = 2048
0.00.052.075 I print_info: n_embd           = 2048
0.00.052.075 I print_info: n_layer          = 24
0.00.052.080 I print_info: n_head           = 16
0.00.052.081 I print_info: n_head_kv        = 16
0.00.052.081 I print_info: n_rot            = 32
0.00.052.081 I print_info: n_swa            = 0
0.00.052.081 I print_info: n_embd_head_k    = 128
0.00.052.081 I print_info: n_embd_head_v    = 128
0.00.052.082 I print_info: n_gqa            = 1
0.00.052.083 I print_info: n_embd_k_gqa     = 2048
0.00.052.083 I print_info: n_embd_v_gqa     = 2048
0.00.052.084 I print_info: f_norm_eps       = 1.0e-05
0.00.052.084 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.052.084 I print_info: f_clamp_kqv      = 0.0e+00
0.00.052.085 I print_info: f_max_alibi_bias = 0.0e+00
0.00.052.085 I print_info: f_logit_scale    = 0.0e+00
0.00.052.085 I print_info: n_ff             = 8192
0.00.052.085 I print_info: n_expert         = 0
0.00.052.086 I print_info: n_expert_used    = 0
0.00.052.086 I print_info: causal attn      = 1
0.00.052.086 I print_info: pooling type     = 0
0.00.052.086 I print_info: rope type        = 2
0.00.052.086 I print_info: rope scaling     = linear
0.00.052.086 I print_info: freq_base_train  = 10000.0
0.00.052.087 I print_info: freq_scale_train = 1
0.00.052.087 I print_info: n_ctx_orig_yarn  = 2048
0.00.052.087 I print_info: rope_finetuned   = unknown
0.00.052.087 I print_info: ssm_d_conv       = 0
0.00.052.087 I print_info: ssm_d_inner      = 0
0.00.052.087 I print_info: ssm_d_state      = 0
0.00.052.087 I print_info: ssm_dt_rank      = 0
0.00.052.088 I print_info: ssm_dt_b_c_rms   = 0
0.00.052.088 I print_info: model type       = 1.4B
0.00.052.088 I print_info: model params     = 1.41 B
0.00.052.088 I print_info: general.name     = 1.4B
0.00.052.089 I print_info: vocab type       = BPE
0.00.052.089 I print_info: n_vocab          = 50304
0.00.052.089 I print_info: n_merges         = 50009
0.00.052.089 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.052.089 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.052.090 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.052.090 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.052.090 I print_info: LF token         = 128 'Ä'
0.00.052.090 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.052.091 I print_info: max token length = 1024
0.00.054.212 I load_tensors: offloading 24 repeating layers to GPU
0.00.054.213 I load_tensors: offloading output layer to GPU
0.00.054.213 I load_tensors: offloaded 25/25 layers to GPU
0.00.054.224 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.054.225 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.054.503 I llama_init_from_model: n_seq_max     = 1
0.00.054.504 I llama_init_from_model: n_ctx         = 128
0.00.054.505 I llama_init_from_model: n_ctx_per_seq = 128
0.00.054.505 I llama_init_from_model: n_batch       = 128
0.00.054.505 I llama_init_from_model: n_ubatch      = 128
0.00.054.505 I llama_init_from_model: flash_attn    = 0
0.00.054.506 I llama_init_from_model: freq_base     = 10000.0
0.00.054.506 I llama_init_from_model: freq_scale    = 1
0.00.054.506 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.507 I ggml_metal_init: allocating
0.00.054.510 I ggml_metal_init: found device: Apple M4
0.00.054.513 I ggml_metal_init: picking default device: Apple M4
0.00.055.135 I ggml_metal_init: using embedded metal library
0.00.057.539 I ggml_metal_init: GPU name:   Apple M4
0.00.057.541 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.541 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.542 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.542 I ggml_metal_init: simdgroup reduction   = true
0.00.057.542 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.542 I ggml_metal_init: has bfloat            = true
0.00.057.542 I ggml_metal_init: use bfloat            = true
0.00.057.543 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.544 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.539 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.068.939 I init:      Metal KV buffer size =    24.00 MiB
0.00.068.942 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.955 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.069.819 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.069.820 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.069.820 I llama_init_from_model: graph nodes  = 967
0.00.069.820 I llama_init_from_model: graph splits = 2
0.00.069.822 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.069.822 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.361.488 I 
0.00.361.512 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.361.522 I perplexity: tokenizing the input ..
0.00.369.085 I perplexity: tokenization took 7.561 ms
0.00.369.088 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.508.964 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.510.137 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.510.161 I llama_perf_context_print:        load time =     352.06 ms
0.00.510.162 I llama_perf_context_print: prompt eval time =     139.65 ms /   128 tokens (    1.09 ms per token,   916.57 tokens per second)
0.00.510.163 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.510.163 I llama_perf_context_print:       total time =     148.67 ms /   129 tokens
0.00.510.742 I ggml_metal_free: deallocating

real	0m0.524s
user	0m0.080s
sys	0m0.071s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.255 I build: 4477 (28f12723) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.022.253 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.037.486 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.037.490 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.037.492 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.037.493 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.037.493 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.037.500 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.037.500 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.037.502 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.037.502 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.037.503 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.037.503 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.037.503 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.037.504 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.037.504 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.037.506 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.037.507 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.037.508 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.044.557 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.046.309 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.052.635 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.052.636 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.052.637 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.052.637 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.052.638 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.052.638 I llama_model_loader: - type  f32:  194 tensors
0.00.052.639 I llama_model_loader: - type  f16:   98 tensors
0.00.052.639 I print_info: file format = GGUF V3 (latest)
0.00.052.640 I print_info: file type   = all F32 (guessed)
0.00.052.641 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.078.510 I load: special tokens cache size = 25
0.00.085.057 I load: token to piece cache size = 0.2984 MB
0.00.085.060 I print_info: arch             = gptneox
0.00.085.060 I print_info: vocab_only       = 0
0.00.085.061 I print_info: n_ctx_train      = 2048
0.00.085.061 I print_info: n_embd           = 2048
0.00.085.061 I print_info: n_layer          = 24
0.00.085.064 I print_info: n_head           = 16
0.00.085.065 I print_info: n_head_kv        = 16
0.00.085.065 I print_info: n_rot            = 32
0.00.085.065 I print_info: n_swa            = 0
0.00.085.065 I print_info: n_embd_head_k    = 128
0.00.085.065 I print_info: n_embd_head_v    = 128
0.00.085.066 I print_info: n_gqa            = 1
0.00.085.066 I print_info: n_embd_k_gqa     = 2048
0.00.085.067 I print_info: n_embd_v_gqa     = 2048
0.00.085.067 I print_info: f_norm_eps       = 1.0e-05
0.00.085.068 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.085.068 I print_info: f_clamp_kqv      = 0.0e+00
0.00.085.068 I print_info: f_max_alibi_bias = 0.0e+00
0.00.085.068 I print_info: f_logit_scale    = 0.0e+00
0.00.085.069 I print_info: n_ff             = 8192
0.00.085.069 I print_info: n_expert         = 0
0.00.085.069 I print_info: n_expert_used    = 0
0.00.085.069 I print_info: causal attn      = 1
0.00.085.069 I print_info: pooling type     = 0
0.00.085.069 I print_info: rope type        = 2
0.00.085.070 I print_info: rope scaling     = linear
0.00.085.070 I print_info: freq_base_train  = 10000.0
0.00.085.070 I print_info: freq_scale_train = 1
0.00.085.070 I print_info: n_ctx_orig_yarn  = 2048
0.00.085.072 I print_info: rope_finetuned   = unknown
0.00.085.072 I print_info: ssm_d_conv       = 0
0.00.085.072 I print_info: ssm_d_inner      = 0
0.00.085.072 I print_info: ssm_d_state      = 0
0.00.085.072 I print_info: ssm_dt_rank      = 0
0.00.085.072 I print_info: ssm_dt_b_c_rms   = 0
0.00.085.073 I print_info: model type       = 1.4B
0.00.085.073 I print_info: model params     = 1.41 B
0.00.085.073 I print_info: general.name     = 1.4B
0.00.085.074 I print_info: vocab type       = BPE
0.00.085.074 I print_info: n_vocab          = 50304
0.00.085.074 I print_info: n_merges         = 50009
0.00.085.074 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.085.075 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.085.075 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.085.076 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.085.076 I print_info: LF token         = 128 'Ä'
0.00.085.077 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.085.077 I print_info: max token length = 1024
0.00.087.628 I load_tensors: offloading 24 repeating layers to GPU
0.00.087.629 I load_tensors: offloading output layer to GPU
0.00.087.629 I load_tensors: offloaded 25/25 layers to GPU
0.00.087.639 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.087.640 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.088.045 I llama_init_from_model: n_seq_max     = 1
0.00.088.046 I llama_init_from_model: n_ctx         = 128
0.00.088.046 I llama_init_from_model: n_ctx_per_seq = 128
0.00.088.046 I llama_init_from_model: n_batch       = 128
0.00.088.046 I llama_init_from_model: n_ubatch      = 128
0.00.088.046 I llama_init_from_model: flash_attn    = 0
0.00.088.047 I llama_init_from_model: freq_base     = 10000.0
0.00.088.047 I llama_init_from_model: freq_scale    = 1
0.00.088.047 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.088.048 I ggml_metal_init: allocating
0.00.088.051 I ggml_metal_init: found device: Apple M4
0.00.088.053 I ggml_metal_init: picking default device: Apple M4
0.00.088.631 I ggml_metal_init: using embedded metal library
0.00.091.132 I ggml_metal_init: GPU name:   Apple M4
0.00.091.134 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.091.134 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.091.134 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.091.135 I ggml_metal_init: simdgroup reduction   = true
0.00.091.135 I ggml_metal_init: simdgroup matrix mul. = true
0.00.091.135 I ggml_metal_init: has bfloat            = true
0.00.091.135 I ggml_metal_init: use bfloat            = true
0.00.091.135 I ggml_metal_init: hasUnifiedMemory      = true
0.00.091.137 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.100.097 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.101.368 I init:      Metal KV buffer size =    24.00 MiB
0.00.101.372 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.101.386 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.102.270 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.102.271 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.102.271 I llama_init_from_model: graph nodes  = 967
0.00.102.272 I llama_init_from_model: graph splits = 2
0.00.102.273 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.102.273 I 
0.00.102.300 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.102.302 I compute_imatrix: tokenizing the input ..
0.00.109.364 I compute_imatrix: tokenization took 7.061 ms
0.00.109.365 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.638.333 I compute_imatrix: 1.53 seconds per pass - ETA 0.02 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.640.966 I llama_perf_context_print:        load time =    1616.08 ms
0.01.640.967 I llama_perf_context_print: prompt eval time =    1528.27 ms /   128 tokens (   11.94 ms per token,    83.75 tokens per second)
0.01.640.968 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.640.968 I llama_perf_context_print:       total time =    1618.71 ms /   129 tokens
0.01.641.804 I ggml_metal_free: deallocating

real	0m1.827s
user	0m0.165s
sys	0m0.244s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4477 (28f12723)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 'Ä'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12ee0a3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12ee0aac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12ee0b070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12ee0b620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12ee0bbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12ee0c180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12ee0c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12ee0cce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12ee0d290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12ee0d790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12ee0dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12ee0e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12ee0ecb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12ee0f460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12ee0fc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12ee10390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12ee10ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12ee111d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12ee118f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12ee120c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12ee127e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12ee12f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12ee13620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12ee13ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12ee145e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12ee148a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12ee14eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12ee15b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12ee16060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12ee16320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12ee167c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12ee16a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12ee17310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12ee17850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12ee17b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12ee17fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12ee18450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12ee188f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12ee18d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12ee19230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12ee196d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12ee19b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12ee1a010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12ee1a4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12ee1a770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12ee1ad80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12ee1b390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12ee1bcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12ee1c2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12ee1c8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12ee1cee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12ee1d4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12ee1db00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12ee1e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12ee1e900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12ee1eda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12ee1f240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12ee1f500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12ee1fb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12ee20300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12ee205c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12ee20a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12ee20f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12ee213a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12ee21840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12ee21ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12ee22180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12ee22620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12ee22ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12ee22f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12ee23400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12ee238a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12ee23d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12ee24290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12ee247e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12ee24d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12ee25280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12ee257d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12ee25d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12ee26270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12ee267c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12ee26d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12ee27260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12ee277b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12ee27d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12ee28250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12ee287a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12ee28cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12ee29240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12ee29790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12ee29ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12ee2a230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12ee2a780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12ee2acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12ee2b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12ee2b770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12ee2bcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12ee1b9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12ee2c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12ee2c8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12ee2ce30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12ee2d380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12ee2d8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12ee2de20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12ee2e370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12ee2e8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12ee2ee10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12ee2f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12ee2f8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12ee2fe00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12ee30350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12ee308a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12ee30df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12ee31290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12ee31730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12ee31bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12ee32070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12ee32510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12ee329b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12ee32e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12ee332f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12ee33790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12ee33c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12ee340d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12ee34570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12ee34a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12ee34eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12ee35350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12ee357f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12ee35c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12ee36130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12ee365d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12ee36a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12ee36f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12ee373b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12ee37850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12ee37cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12ee38190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12ee38630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12ee38ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12ee38f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12ee39410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12ee398b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12ee39d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12ee3a1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12ee3a690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12ee3ab30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12ee3afd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12ee3b470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12ee3b910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12ee3bdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12ee3c250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12ee3c6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12ee3cb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12ee3d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12ee3d4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12ee3d970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12ee3de10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12ee3e2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12ee3e750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12ee3ebf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12ee3f090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12ee3f530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12ee3f9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12ee3fe70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12ee40310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12ee407b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12ee40c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12ee410f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12ee41590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12ee41a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12ee41ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12ee42370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12ee42810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12ee42cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12ee43150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12ee435f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12ee43a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12ee43f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12ee443d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12ee44870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12ee44d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12ee451b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12ee45650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12ee45af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12ee45f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12ee46430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12ee468d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12ee46d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12ee47210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12ee476b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12ee47b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12ee47ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12ee48540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12ee48a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12ee48fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12ee49530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12ee497f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12ee49e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12ee4a410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12ee4aa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12ee4b210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12ee4b6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12ee4b970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12ee4bf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12ee4c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12ee4cd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12ee4d220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12ee4d6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12ee4db60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12ee4e310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12ee4e860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12ee4edb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12ee4f300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12ee4f850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12ee4fda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12ee502f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12ee50840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12ee50d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12ee512e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12ee51830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12ee51d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12ee522d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12ee52820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12ee52d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12ee532c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12ee53810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12ee53d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12ee542b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12ee54800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12ee54d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12ee552a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12ee557f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12ee55d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12ee56290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12ee567e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12ee56d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12ee57280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12ee577d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12ee57d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12ee58270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12ee587c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12ee58d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12ee59260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12ee597b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12ee59d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12ee5a250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12ee5a7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12ee5acf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12ee5b240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12ee5b790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12ee5bce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12ee5c230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12ee5c780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12ee5ccd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12ee5d220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12ee5d770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12ee5dcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12ee5e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12ee5e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12ee5ecb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12ee5f200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12ee5f750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12ee5fca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12ee601f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12ee60740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12ee60c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12ee61130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12ee615d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12ee61a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12ee61f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12ee623b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12ee62850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12ee62cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12ee63190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12ee63630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12ee63ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12ee63f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12ee64410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12ee648b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12ee64d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12ee651f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12ee65740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12ee65e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12ee66580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12ee66ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12ee673c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12ee67680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12ee67e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12ee68130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12ee68740 | th_max = 1024 | th_width =   32
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
init: layer   0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.158.592 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.158.595 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12ee683f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12ee4a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12ee49ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12ee4a6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12ee1d7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12ee1d1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12ee1f7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12ee4c240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12ee14b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12ee1b650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12ee1bf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12ee1c580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12ee1aa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12ee1cb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12ee13b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12ee099e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12ee1e3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12ee1fdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12ee2c3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12ee67940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12ee16d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12ee17000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12ee4c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12ee4ace0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12ee15170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12ee15430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12ee156f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12ee68ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12ee68e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12ee69120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12ee693e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12ee696a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12ee69960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12ee69c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12ee69ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12ee6a1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12ee6a460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12ee6a720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12ee6a9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12ee6aca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12ee6af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12ee6b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12ee6b4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12ee6b7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12ee6ba60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12ee6bd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12ee6bfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12ee6c2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12ee6c560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12ee6c820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12ee6cae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12ee6cda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12ee6d060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12ee6d320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12ee6d5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12ee6d8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12ee6db60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12ee6de20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12ee6e0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12ee6e3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12ee6e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12ee6e920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12ee6ebe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12ee6eea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12ee6f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12ee6f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12ee6f6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12ee6f9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12ee6fc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12ee6ff20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12ee701e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12ee704a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12ee70760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12ee70a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12ee70ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12ee70fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12ee71260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12ee71520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12ee717e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12ee71aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12ee71d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12ee72020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12ee722e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12ee725a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12ee72860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12ee72b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12ee72de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12ee730a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12ee73360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12ee73620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12ee738e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12ee73ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12ee73e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12ee74120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12ee743e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12ee746a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12ee74960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12ee74c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12ee74ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12ee751a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12ee75460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12ee75720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12ee759e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12ee75ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12ee75f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12ee76220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12ee764e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12ee767a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12ee76a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12ee76d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12ee76fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12ee772a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12ee77560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12ee77820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12ee77ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12ee77da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12ee78060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12ee78320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12ee785e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12ee788a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12ee78b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12ee78e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12ee790e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12ee793a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12ee79660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12ee79920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12ee79be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12ee79ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12ee7a160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12ee7a420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12ee7a6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12ee7a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12ee7ac60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12ee7af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12ee7b1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12ee7b4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12ee7b760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12ee7ba20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12ee7bce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12ee7bfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12ee7c260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12ee7c520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12ee7c7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12ee7caa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12ee7cd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12ee7d020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12ee7d2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12ee7d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12ee7d860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12ee7db20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12ee7dde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12ee7e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12ee7e360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12ee7e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12ee7e8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12ee7eba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12ee7ee60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12ee7f120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12ee7f3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12ee7f6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12ee7f960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12ee7fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12ee7fee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12ee801a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12ee80460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12ee80720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12ee809e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12ee80ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12ee80f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12ee81220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12ee814e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12ee817a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12ee81a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12ee81d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12ee81fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12ee822a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12ee82560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12ee82820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12ee82ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12ee82da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12ee83060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12ee83320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12ee835e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12ee838a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12ee83b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12ee83e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12ee840e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12ee843a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12ee84660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12ee84920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12ee84be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12ee84ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12ee85160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12ee85420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12ee856e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12ee859a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12ee85c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12ee85f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12ee861e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12ee864a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12ee86760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12ee86a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12ee86ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12ee86fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12ee87260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12ee87520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12ee877e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12ee87aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12ee87d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12ee88020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12ee885f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12ee888b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12ee88b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12ee88e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12ee890f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12ee893b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12ee89670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12ee89930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12ee89bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12ee89eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12ee8a170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12ee8a430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12ee8a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12ee8a9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12ee8ac70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12ee8af30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12ee8b1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12ee8b4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12ee8b770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12ee8ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12ee8bcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12ee8bfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12ee8c270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12ee8c530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12ee8c7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12ee8cab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12ee8cd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12ee8d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12ee8d2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12ee8d5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12ee8d870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12ee8ddc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12ee8e310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12ee8e860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12ee8edb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12ee8f300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12ee8f850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12ee8fda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12ee902f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12ee90840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12ee90d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12ee912e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12ee91830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12ee91d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12ee922d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12ee92820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12ee92d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12ee932c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12ee93810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12ee93d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12ee942b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12ee94800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12ee94d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12ee952a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12ee957f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12ee95d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12ee96290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12ee96550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12ee96810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12ee96d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12ee97210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12ee97710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12ee97c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12ee98110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12ee98610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12ee98b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12ee99010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12ee99510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12ee99a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12ee99f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12ee9a410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12ee9a910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12ee9ae10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12ee9b820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12ee9bf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12ee9c660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12ee9cd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12ee9d040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12ee9d830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12ee9daf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12ee9e100 | th_max = 1024 | th_width =   32
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
init: layer   0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12ef046e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12ef04b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12ef04fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12ef05430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12ef058a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12ef05d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12ef06180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12ef065f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12ef06a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12ef06ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12ef07340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12ef07a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12ef08580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12ef08d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12ef09540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12ef09c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12ef0a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12ef0aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12ef0b1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12ef0b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12ef0c010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12ef0c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12ef0ce50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12ef0d570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12ef0dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12ef0df50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12ef0e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12ef0e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12ef0eaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12ef0ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12ef0f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12ef0f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12ef0fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12ef10030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12ef104a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12ef10910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12ef10d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12ef111f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12ef11660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12ef11ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12ef11f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12ef123b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12ef12820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12ef12c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12ef13100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12ef13570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12ef139e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12ef13e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12ef142c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12ef14730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12ef14ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12ef15010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12ef15480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12ef158f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12ef15d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12ef161d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12ef16740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12ef16c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12ef170b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12ef17520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12ef17990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12ef17e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12ef18270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12ef186e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12ef18b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12ef18fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12ef19430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12ef198a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12ef19d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12ef1a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12ef1a5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12ef1aa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12ef1aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12ef1b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12ef1b7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12ef1bc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12ef1c090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12ef1c500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12ef1c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12ef1cde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12ef1d250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12ef1d6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12ef1db30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12ef1dfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12ef1e410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12ef1e880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12ef1ecf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12ef1f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12ef1f5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12ef1fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12ef1feb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12ef20320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12ef20790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12ef20c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12ef21070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12ef214e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12ef21950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12ef21dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12ef22230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12ef226a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12ef22b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12ef22f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12ef233f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12ef23c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12ef23f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12ef243b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12ef24820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12ef24c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12ef25100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12ef25570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12ef259e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12ef25e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12ef262c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12ef26730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12ef26ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12ef27010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12ef27480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12ef278f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12ef27d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12ef281d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12ef28640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12ef28ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12ef28f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12ef29390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12ef29800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12ef29c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12ef2a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12ef2a550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12ef2a9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12ef2ae30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12ef2b2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12ef2b710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12ef2bb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12ef2bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12ef2c460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12ef2c8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12ef2cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12ef2d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12ef2d620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12ef2da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12ef2df00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12ef2e370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12ef2e7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12ef2ec50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12ef2f0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12ef2f530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12ef2f9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12ef2fe10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12ef30280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12ef306f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12ef30b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12ef30fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12ef31440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12ef318b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12ef31d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12ef32190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12ef32600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12ef32a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12ef32ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12ef33350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12ef337c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12ef33c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12ef340a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12ef34510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12ef34980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12ef34df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12ef35260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12ef356d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12ef35b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12ef35fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12ef36420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12ef36890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12ef36d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12ef37170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12ef375e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12ef37a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12ef37ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12ef38330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12ef387a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12ef38c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12ef39080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12ef394f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12ef39960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12ef39dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12ef3a240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12ef3a6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12ef3ab20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12ef3af90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12ef3b400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12ef3b870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12ef3bce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12ef3c150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12ef3c5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12ef3ca30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12ef3cea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12ef3d310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12ef3d780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12ef3dbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12ef3e060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12ef3e4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12ef3e940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12ef3edb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12ef3f220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12ef3f690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12ef3fb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12ef3ff70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12ef403e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12ef40850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12ef40cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12ef41130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12ef41cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12ef41f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12ef42230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12ef426a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12ef42b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12ef42f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12ef433f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12ef43860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12ef43cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12ef44140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12ef445b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12ef44a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12ef44e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12ef45300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12ef45770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12ef45be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12ef46050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12ef464c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12ef46930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12ef46da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12ef47210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12ef47680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12ef47af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12ef47f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12ef483d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12ef48840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12ef48cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12ef49120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12ef49590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12ef49a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12ef49e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12ef4a2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12ef4a750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12ef4abc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12ef4b030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12ef4b4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12ef4b910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12ef4bd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12ef4c1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12ef4c660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12ef4cad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12ef4cf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12ef4d3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12ef4d820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12ef4dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12ef4e100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12ef4e570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12ef4e9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12ef4ee50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12ef4f2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12ef4f730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12ef4fba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12ef50010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12ef50480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12ef508f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12ef50d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12ef511d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12ef51640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12ef51ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12ef51f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12ef52390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12ef52800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12ef52c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12ef530e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12ef53550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12ef539c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12ef53e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12ef542a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12ef54710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12ef54b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12ef54ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12ef55460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12ef558d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12ef56340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12ef56a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12ef57180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12ef578a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12ef57b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12ef57fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12ef585d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12ef58be0 | th_max = 1024 | th_width =   32
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
init: layer   0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.822s
user	0m0.297s
sys	0m0.313s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4477 (28f12723)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 'Ä'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14e60d6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14e60ddd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14e60e380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14e60e930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14e60eee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14e60f490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14e60fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14e60fff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14e6105a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14e610aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14e610fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14e6114a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14e611fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14e612770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14e612f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14e6136a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14e613dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14e6144e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14e614c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14e6153d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14e615af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14e616210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14e616930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14e6171d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14e6178f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14e617bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14e6181c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14e618e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14e619370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14e619630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14e619ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14e619d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14e61a620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14e61ab60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14e61ae20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14e61b2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14e61b760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14e61bc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14e61c0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14e61c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14e61c9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14e61ce80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14e61d320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14e61d7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14e61da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14e61e090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14e61e6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14e61efc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14e61f5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14e61fbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14e6201f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14e620800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14e620e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14e621420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14e621c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14e6220b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14e622550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14e622810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14e622e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14e623610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14e6238d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14e623d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14e624210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14e6246b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14e624b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14e624ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14e625490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14e625930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14e625dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14e626270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14e626710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14e626bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14e627050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14e6275a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14e627af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14e628040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14e628590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14e628ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14e629030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14e629580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14e629ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x14e62a020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x14e62a570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x14e62aac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x14e62b010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x14e62b560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x14e62bab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x14e62c000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x14e62c550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x14e62caa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x14e62cff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x14e62d540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x14e62da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x14e62dfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x14e62e530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x14e62ea80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x14e62efd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x14e61ecb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x14e62f440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x14e62fbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x14e630140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x14e630690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x14e630be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x14e631130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x14e631680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x14e631bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x14e632120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x14e632670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x14e632bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x14e633110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x14e633660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x14e633bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x14e634100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14e6345a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14e634a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14e634ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14e635380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14e635820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14e635cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14e636160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14e636600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14e636aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14e636f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14e6373e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14e637880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14e637d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14e6381c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14e638660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14e638b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14e638fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14e639440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14e6398e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14e639d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14e63a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14e63a6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14e63ab60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14e63b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14e63b4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14e63b940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14e63bde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14e63c280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14e63c720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14e63cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14e63d060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14e63d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14e63d9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14e63de40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14e63e2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14e63e780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14e63ec20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14e63f0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14e63f560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14e63fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14e63fea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14e640340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14e6407e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14e640c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14e641120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14e6415c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14e641a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14e641f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14e6423a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14e642840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14e642ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14e643180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14e643620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14e643ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14e643f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14e644400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14e6448a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14e644d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14e6451e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14e645680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14e645b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14e645fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14e646460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14e646900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14e646da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14e647240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14e6476e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14e647b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14e648020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14e6484c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14e648960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14e648e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14e6492a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14e649740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14e649be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14e64a080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14e64a520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14e64a9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14e64ae60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14e64b300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14e64b850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14e64bda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14e64c2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14e64c840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14e64cb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14e64d110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14e64d720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14e64dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14e64e520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14e64e9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14e64ec80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14e64f290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x14e64f8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14e650090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14e650530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14e6509d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14e650e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14e651620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14e651b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14e6520c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14e652610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14e652b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14e6530b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14e653600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14e653b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14e6540a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14e6545f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14e654b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14e655090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14e6555e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14e655b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14e656080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14e6565d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14e656b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14e657070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14e6575c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14e657b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14e658060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14e6585b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14e658b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14e659050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14e6595a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14e659af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14e65a040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14e65a590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14e65aae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14e65b030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14e65b580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14e65bad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14e65c020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14e65c570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14e65cac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14e65d010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14e65d560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14e65dab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14e65e000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14e65e550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14e65eaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14e65eff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14e65f540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14e65fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14e65ffe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14e660530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14e660a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14e660fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14e661520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14e661a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14e661fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14e662510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14e662a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14e662fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14e663500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14e663a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14e663fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x14e664440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x14e6648e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14e664d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14e665220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14e6656c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14e665b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14e666000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14e6664a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14e666940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14e666de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14e667280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14e667720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14e667bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14e668060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14e668500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14e668a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14e669170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14e669890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14e669fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14e66a6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14e66a990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x14e66b180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14e66b440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14e66ba50 | th_max = 1024 | th_width =   32
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
init: layer   0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.091.665 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.091.669 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14e66b700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14e64ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14e64cdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14e64d9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14e620ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14e6204b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14e622ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14e64f550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14e617e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14e61e960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14e61f280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14e61f890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14e61dd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14e61fea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14e616e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14e6230e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14e62f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14e66ac50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14e61a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14e61a310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14e64fb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14e64dff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14e618480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14e618740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14e618a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14e66beb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14e66c170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14e66c430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14e66c6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14e66c9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14e66cc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14e66cf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14e66d1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14e66d4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14e66d770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14e66da30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14e66dcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14e66dfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14e66e270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14e66e530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14e66e7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14e66eab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14e66ed70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14e66f030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14e66f2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14e66f5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14e66f870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14e66fb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14e66fdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14e6700b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14e670370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14e670630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14e6708f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14e670bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14e670e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14e671130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14e6713f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14e6716b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14e671970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14e671c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14e671ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14e6721b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14e672470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14e672730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14e6729f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14e672cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14e672f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14e673230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14e6734f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14e6737b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14e673a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14e673d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14e673ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14e6742b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14e674570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14e674830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14e674af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14e674db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14e675070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14e675330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14e6755f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x14e6758b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x14e675b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x14e675e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x14e6760f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x14e6763b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x14e676670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x14e676930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x14e676bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x14e676eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x14e677170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x14e677430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x14e6776f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x14e6779b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x14e677c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x14e677f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x14e6781f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x14e6784b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x14e678770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x14e678a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x14e678cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x14e678fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x14e679270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x14e679530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x14e6797f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x14e679ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x14e679d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x14e67a030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x14e67a2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x14e67a5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x14e67a870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x14e67ab30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x14e67adf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14e67b0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14e67b370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14e67b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14e67b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14e67bbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14e67be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14e67c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14e67c3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14e67c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14e67c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14e67cc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14e67cef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14e67d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14e67d470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14e67d730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14e67d9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14e67dcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14e67df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14e67e230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14e67e4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14e67e7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14e67ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14e67ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14e67eff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14e67f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14e67f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14e67f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14e67faf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14e67fdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14e680070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14e680330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14e6805f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14e6808b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14e680b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14e680e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14e6810f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14e6813b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14e681670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14e681930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14e681bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14e681eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14e682170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14e682430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14e6826f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14e6829b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14e682c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14e682f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14e6831f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14e6834b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14e683770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14e683a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14e683cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14e683fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14e684270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14e684530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14e6847f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14e684ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14e684d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14e685030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14e6852f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14e6855b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14e685870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14e685b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14e685df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14e6860b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14e686370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14e686630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14e6868f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14e686bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14e686e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14e687130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14e6873f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14e6876b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14e687970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14e687c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14e687ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14e6881b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14e688470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14e688730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14e6889f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14e688cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14e688f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14e689230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14e6894f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14e6897b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14e689a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14e689d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14e689ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14e68a2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14e68a570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14e68a830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14e68aaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x14e68adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14e68b070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14e68b330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14e68b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14e68b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14e68be80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14e68c140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14e68c400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14e68c870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14e68cce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14e68d150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14e68d5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14e68da30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14e68dea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14e68e310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14e68e780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14e68ebf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14e68f060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14e68f4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14e68f940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14e68fdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14e690220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14e690690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14e690b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14e690f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14e6913e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14e691850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14e691cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14e692130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14e6925a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14e692a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14e692e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14e6932f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14e693760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14e693bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14e694040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14e6944b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14e694920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14e694d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14e695200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14e695670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14e695ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14e695f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14e6963c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14e696830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14e696ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14e697110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14e697580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14e6979f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14e697e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14e6982d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14e698740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14e698bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14e699020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14e699490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14e699900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14e699d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14e69a1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14e69a650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14e69aac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14e69af30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14e69b3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x14e69b810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x14e69bc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14e69c0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14e69c560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14e69c9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14e69ce40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14e69d2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14e69d720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14e69db90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14e69e000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14e69e470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14e69e8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14e69ed50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14e69f1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14e69f630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14e69faa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14e6a0510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14e6a0c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14e6a1350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14e6a1a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14e6a1d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x14e6a2520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14e6a27e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14e6a2df0 | th_max = 1024 | th_width =   32
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
init: layer   0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14e69fd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14e6a2aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14e6a1ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14e6a3250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14e6a3510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14e6a37d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14e6a3a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14e6a3d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14e6a4010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14e6a42d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14e6a4590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14e6a4850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14e6a4e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14e6a53f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14e6a5a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14e6a5ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14e6a5fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14e6a6260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14e6a6520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14e6a67e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14e6a6aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14e6a6d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14e6a7020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14e6a72e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14e6a75a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14e6a7860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14e6a7b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14e6a7de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14e6a80a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14e6a8360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14e6a8620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14e6a88e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14e6a8ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14e6a8e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14e6a9120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14e6a93e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14e6a96a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14e6a9960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14e6a9c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14e6a9ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14e6aa1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14e6aa460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14e6aa720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14e6aa9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14e6aaca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14e6aaf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14e6ab220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14e6ab4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14e6ab7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14e6aba60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14e6abd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14e6abfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14e6ac2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14e6ac560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14e6ac820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14e6acae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14e6acda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14e6ad060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14e6ad320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14e6ad5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14e6ad8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14e6adb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14e6ade20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14e6ae0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14e6ae3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14e6ae660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14e6ae920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14e6aebe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14e6aeea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14e6af160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14e6af420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14e6af6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14e6af9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14e6afc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14e6aff20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14e6b01e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14e6b04a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14e6b0760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14e6b0a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14e6b0ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14e6b0fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x14e6b1260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x14e6b1520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x14e6b17e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x14e6b1aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x14e6b1d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x14e6b2020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x14e6b22e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x14e6b25a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x14e6b2860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x14e6b2b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x14e6b2de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x14e6b30a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x14e6b3360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x14e6b3620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x14e6b38e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x14e6b3ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x14e6b3e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x14e6b4120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x14e6b43e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x14e6b46a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x14e6b4960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x14e6b4c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x14e6b4ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x14e6b51a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x14e6b5460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x14e6b5720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x14e6b59e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x14e6b5ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x14e6b5f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x14e6b6220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x14e6b64e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x14e6b67a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14e6b6a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14e6b6d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14e6b6fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14e6b72a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14e6b7560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14e6b7820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14e6b7ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14e6b7da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14e6b8060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14e6b8320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14e6b85e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14e6b88a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14e6b8b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14e6b8e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14e6b90e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14e6b93a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14e6b9660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14e6b9920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14e6b9be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14e6b9ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14e6ba160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14e6ba420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14e6ba6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14e6ba9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14e6bac60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14e6baf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14e6bb1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14e6bb4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14e6bb760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14e6bba20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14e6bbce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14e6bbfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14e6bc260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14e6bc520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14e6bc7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14e6bcaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14e6bcd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14e6bd020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14e6bd2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14e6bd5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14e6bd860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14e6bdb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14e6bdde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14e6be0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14e6be360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14e6be620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14e6be8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14e6beba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14e6bee60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14e6bf120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14e6bf3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14e6bf6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14e6bf960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14e6bfc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14e6bfee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14e6c01a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14e6c0460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14e6c0720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14e6c09e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14e6c0ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14e6c0f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14e6c1220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14e6c14e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14e6c17a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14e6c1a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14e6c1d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14e6c1fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14e6c22a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14e6c2560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14e6c2820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14e6c2ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14e6c2da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14e6c3060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14e6c3320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14e6c35e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14e6c38a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14e6c3b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14e6c3e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14e6c40e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14e6c43a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14e6c4660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14e6c4920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14e6c4be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14e6c4ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14e6c5160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14e6c5420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14e6c56e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14e6c59a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14e6c5c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14e6c5f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14e6c61e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14e6c64a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x14e6c6760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14e6c6a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14e6c6ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14e6c6fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14e6c7260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14e6c7830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14e6c7af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14e6c7db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14e6c8070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14e6c8330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14e6c85f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14e6c88b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14e6c8b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14e6c8e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14e6c90f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14e6c93b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14e6c9670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14e6c9930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14e6c9bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14e6c9eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14e6ca170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14e6ca430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14e6ca6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14e6ca9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14e6cac70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14e6caf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14e6cb1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14e6cb4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14e6cb770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14e6cba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14e6cbcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14e6cbfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14e6cc270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14e6cc530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14e6cc7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14e6ccab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14e6ccd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14e6cd030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14e6cd2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14e6cd5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14e6cd870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14e6cdb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14e6cddf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14e6ce0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14e6ce370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14e6ce630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14e6ce8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14e6cebb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14e6cee70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14e6cf130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14e6cf3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14e6cf6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14e6cf970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14e6cfc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14e6cfef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14e6d01b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14e6d0470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14e6d0730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14e6d09f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14e6d0cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14e6d0f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14e6d1230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x14e6d14f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x14e6d17b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14e6d1a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14e6d1d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14e6d1ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14e6d22b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14e6d2570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14e6d2830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14e6d2c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14e6d2ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14e6d33f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14e6d38f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14e6d3df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14e6d42f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14e6d47f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14e6d4cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14e6d5700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14e6d5e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14e6d6540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14e6d6c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14e6d6f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x14e6d7710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14e6d79d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14e6d7fe0 | th_max = 1024 | th_width =   32
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
init: layer   0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.920s
user	0m0.244s
sys	0m0.138s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
