Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:301 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- ARM -mcpu not found, -mcpu=native will be used
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod - Success
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sve
-- Performing Test GGML_MACHINE_SUPPORTS_sve - Failed
-- ARM feature DOTPROD enabled
-- ARM feature MATMUL_INT8 enabled
-- ARM feature FMA enabled
-- ARM feature FP16_VECTOR_ARITHMETIC enabled
-- Adding CPU backend variant ggml-cpu: -mcpu=native+dotprod+i8mm+nosve 
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (2.3s)
-- Generating done (0.2s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m2.555s
user	0m0.857s
sys	0m1.212s
++ nproc
+ make -j10
[  0%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  2%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  2%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  3%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  2%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  3%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  4%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  5%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  5%] Built target sha1
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o
[  5%] Built target sha256
[  5%] Built target build_info
[  5%] Built target xxhash
[  6%] Linking CXX shared library libggml-base.dylib
[  6%] Built target ggml-base
[  6%] Generate assembly for embedded Metal library
Embedding Metal library
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[  8%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[  8%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[  8%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 11%] Linking CXX shared library libggml-blas.dylib
[ 12%] Linking CXX shared library libggml-cpu.dylib
[ 12%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 13%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 13%] Built target ggml-blas
[ 13%] Built target ggml-cpu
[ 13%] Linking C shared library libggml-metal.dylib
[ 13%] Built target ggml-metal
[ 13%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 14%] Linking CXX shared library libggml.dylib
[ 14%] Built target ggml
[ 15%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 16%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o
[ 20%] Linking CXX executable ../../bin/llama-gguf-hash
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o
[ 21%] Linking CXX executable ../../bin/llama-gguf
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 25%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 25%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 26%] Linking CXX shared library libllama.dylib
[ 26%] Built target llama-gguf
[ 26%] Built target llama-gguf-hash
[ 26%] Built target llama
[ 26%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 27%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 29%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 29%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 30%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 30%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 30%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 30%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 31%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 31%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 32%] Linking C executable ../bin/test-c
[ 33%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 34%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 34%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 34%] Linking CXX executable ../../bin/llama-simple
[ 35%] Linking CXX executable ../../bin/llama-quantize-stats
[ 35%] Linking CXX executable ../../bin/llama-simple-chat
[ 35%] Built target llava
[ 36%] Linking CXX static library libcommon.a
[ 36%] Linking CXX shared library libllava_shared.dylib
[ 36%] Built target test-c
[ 36%] Built target llama-simple-chat
[ 37%] Linking CXX static library libllava_static.a
[ 37%] Built target llama-simple
[ 37%] Built target llama-quantize-stats
[ 37%] Built target common
[ 37%] Built target llava_static
[ 37%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 37%] Built target llava_shared
[ 39%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 42%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 42%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 43%] Linking CXX executable ../bin/test-tokenizer-0
[ 43%] Linking CXX executable ../bin/test-sampling
[ 44%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 45%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 45%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 45%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 47%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 47%] Linking CXX executable ../bin/test-grammar-parser
[ 47%] Linking CXX executable ../bin/test-llama-grammar
[ 47%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 48%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 48%] Linking CXX executable ../bin/test-grammar-integration
[ 48%] Built target test-sampling
[ 48%] Built target test-tokenizer-1-bpe
[ 48%] Built target test-tokenizer-0
[ 49%] Linking CXX executable ../bin/test-log
[ 49%] Built target test-tokenizer-1-spm
[ 49%] Linking CXX executable ../bin/test-arg-parser
[ 49%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 49%] Built target test-grammar-parser
[ 49%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 49%] Built target test-llama-grammar
[ 50%] Building CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o
[ 50%] Built target test-json-schema-to-grammar
[ 50%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 51%] Built target test-grammar-integration
[ 52%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 52%] Built target test-log
[ 54%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 54%] Linking CXX executable ../bin/test-model-load-cancel
[ 55%] Linking CXX executable ../bin/test-gguf
[ 56%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 56%] Built target test-arg-parser
[ 56%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 57%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 57%] Linking CXX executable ../bin/test-backend-ops
[ 57%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 57%] Linking CXX executable ../bin/test-chat-template
[ 58%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 58%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 59%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 59%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 60%] Linking CXX executable ../bin/test-autorelease
[ 60%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 61%] Linking CXX executable ../bin/test-barrier
[ 61%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 61%] Built target test-model-load-cancel
[ 61%] Built target test-gguf
[ 61%] Linking CXX executable ../bin/test-quantize-perf
[ 62%] Built target test-backend-ops
[ 62%] Linking CXX executable ../bin/test-quantize-fns
[ 62%] Built target test-chat-template
[ 63%] Linking CXX executable ../../bin/llama-batched-bench
[ 64%] Linking CXX executable ../bin/test-rope
[ 64%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 64%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 64%] Built target test-autorelease
[ 64%] Built target test-barrier
[ 65%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 66%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 67%] Linking CXX executable ../../bin/llama-batched
[ 67%] Built target test-quantize-perf
[ 67%] Built target test-quantize-fns
[ 67%] Linking CXX executable ../../bin/llama-embedding
[ 67%] Built target test-rope
[ 68%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 68%] Built target llama-batched-bench
[ 69%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 69%] Linking CXX executable ../../bin/llama-eval-callback
[ 69%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 70%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 71%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 71%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 71%] Linking CXX executable ../../bin/llama-gguf-split
[ 71%] Linking CXX executable ../../bin/llama-gritlm
[ 72%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 72%] Built target llama-batched
[ 72%] Linking CXX executable ../../bin/llama-imatrix
[ 72%] Built target llama-embedding
[ 72%] Linking CXX executable ../../bin/llama-infill
[ 73%] Linking CXX executable ../../bin/llama-bench
[ 73%] Built target llama-eval-callback
[ 73%] Built target llama-gbnf-validator
[ 73%] Linking CXX executable ../../bin/llama-lookahead
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 74%] Built target llama-gguf-split
[ 75%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 75%] Built target llama-gritlm
[ 76%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 77%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 77%] Built target llama-infill
[ 77%] Built target llama-imatrix
[ 77%] Linking CXX executable ../../bin/llama-lookup
[ 77%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 77%] Linking CXX executable ../../bin/llama-lookup-merge
[ 77%] Built target llama-lookahead
[ 77%] Built target llama-bench
[ 77%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 77%] Linking CXX executable ../../bin/llama-lookup-stats
[ 77%] Linking CXX executable ../../bin/llama-lookup-create
[ 77%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 77%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 78%] Linking CXX executable ../../bin/llama-cli
[ 79%] Linking CXX executable ../../bin/llama-parallel
[ 79%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 79%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 80%] Linking CXX executable ../../bin/llama-perplexity
[ 81%] Linking CXX executable ../../bin/llama-passkey
[ 81%] Built target llama-lookup-merge
[ 82%] Linking CXX executable ../../bin/llama-retrieval
[ 83%] Linking CXX executable ../../bin/llama-quantize
[ 83%] Generating loading.html.hpp
[ 83%] Built target llama-lookup-stats
[ 83%] Built target llama-lookup-create
[ 83%] Built target llama-parallel
[ 84%] Generating index.html.gz.hpp
[ 84%] Built target llama-lookup
[ 84%] Built target llama-cli
[ 85%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 85%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 86%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 86%] Built target llama-passkey
[ 86%] Built target llama-quantize
[ 87%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 87%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 87%] Built target llama-retrieval
[ 87%] Linking CXX executable ../../bin/llama-save-load-state
[ 88%] Building CXX object examples/run/CMakeFiles/llama-run.dir/linenoise.cpp/linenoise.cpp.o
[ 88%] Built target llama-perplexity
[ 88%] Linking CXX executable ../../bin/llama-speculative
[ 88%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 88%] Linking CXX executable ../../bin/llama-speculative-simple
[ 89%] Linking CXX executable ../../bin/llama-tokenize
[ 90%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 90%] Linking CXX executable ../../bin/llama-run
[ 90%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 90%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 91%] Linking CXX executable ../../bin/llama-tts
[ 91%] Linking CXX executable ../../bin/llama-gen-docs
[ 92%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 93%] Linking CXX executable ../../bin/llama-cvector-generator
[ 93%] Built target llama-speculative
[ 93%] Built target llama-save-load-state
[ 93%] Built target llama-speculative-simple
[ 93%] Built target llama-run
[ 93%] Built target llama-tokenize
[ 94%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 95%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 95%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 96%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 96%] Built target llama-tts
[ 96%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 96%] Built target llama-gen-docs
[ 96%] Built target llama-convert-llama2c-to-ggml
[ 96%] Built target llama-cvector-generator
[ 97%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 97%] Linking CXX executable ../../bin/llama-llava-cli
[ 97%] Linking CXX executable ../../bin/llama-export-lora
[ 97%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 97%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 98%] Linking CXX executable ../../bin/llama-vdot
[ 99%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Built target llama-vdot
[ 99%] Built target llama-minicpmv-cli
[ 99%] Built target llama-export-lora
[ 99%] Built target llama-llava-cli
[ 99%] Built target llama-qwen2vl-cli
[ 99%] Built target llama-q8dot
[100%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m3.094s
user	0m6.304s
sys	0m9.488s

main: quantize time =  8317.26 ms
main:    total time =  8317.26 ms

main: quantize time =  1439.87 ms
main:    total time =  1439.87 ms

main: quantize time =  1791.75 ms
main:    total time =  1791.75 ms

main: quantize time =  2761.31 ms
main:    total time =  2761.31 ms

main: quantize time =  1640.08 ms
main:    total time =  1640.08 ms

main: quantize time =  4983.65 ms
main:    total time =  4983.65 ms

main: quantize time =  5885.50 ms
main:    total time =  5885.50 ms

main: quantize time =  6910.40 ms
main:    total time =  6910.40 ms

main: quantize time =  6136.54 ms
main:    total time =  6136.54 ms

main: quantize time =  4600.06 ms
main:    total time =  4600.06 ms
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.160 I build: 4529 (12c2bdf2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.363 I main: llama backend init
0.00.000.372 I main: load the model and apply lora adapter, if any
0.00.048.900 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.061.121 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.061.133 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.061.138 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.061.139 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.061.140 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.061.141 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.061.141 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.061.144 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.061.145 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.061.145 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.061.146 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.061.147 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.061.147 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.061.148 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.061.154 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.061.155 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.061.156 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.067.974 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.070.096 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.076.885 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.076.892 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.076.892 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.076.893 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.076.894 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.076.895 I llama_model_loader: - type  f32:  194 tensors
0.00.076.896 I llama_model_loader: - type  f16:   98 tensors
0.00.076.904 I print_info: file format = GGUF V3 (latest)
0.00.076.906 I print_info: file type   = all F32 (guessed)
0.00.076.909 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.113.649 I load: special tokens cache size = 25
0.00.121.809 I load: token to piece cache size = 0.2984 MB
0.00.121.813 I print_info: arch             = gptneox
0.00.121.813 I print_info: vocab_only       = 0
0.00.121.813 I print_info: n_ctx_train      = 2048
0.00.121.813 I print_info: n_embd           = 2048
0.00.121.813 I print_info: n_layer          = 24
0.00.121.818 I print_info: n_head           = 16
0.00.121.819 I print_info: n_head_kv        = 16
0.00.121.819 I print_info: n_rot            = 32
0.00.121.819 I print_info: n_swa            = 0
0.00.121.819 I print_info: n_embd_head_k    = 128
0.00.121.819 I print_info: n_embd_head_v    = 128
0.00.121.820 I print_info: n_gqa            = 1
0.00.121.821 I print_info: n_embd_k_gqa     = 2048
0.00.121.822 I print_info: n_embd_v_gqa     = 2048
0.00.121.822 I print_info: f_norm_eps       = 1.0e-05
0.00.121.823 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.121.823 I print_info: f_clamp_kqv      = 0.0e+00
0.00.121.823 I print_info: f_max_alibi_bias = 0.0e+00
0.00.121.823 I print_info: f_logit_scale    = 0.0e+00
0.00.121.824 I print_info: n_ff             = 8192
0.00.121.824 I print_info: n_expert         = 0
0.00.121.824 I print_info: n_expert_used    = 0
0.00.121.824 I print_info: causal attn      = 1
0.00.121.825 I print_info: pooling type     = 0
0.00.121.825 I print_info: rope type        = 2
0.00.121.825 I print_info: rope scaling     = linear
0.00.121.825 I print_info: freq_base_train  = 10000.0
0.00.121.826 I print_info: freq_scale_train = 1
0.00.121.826 I print_info: n_ctx_orig_yarn  = 2048
0.00.121.826 I print_info: rope_finetuned   = unknown
0.00.121.827 I print_info: ssm_d_conv       = 0
0.00.121.827 I print_info: ssm_d_inner      = 0
0.00.121.828 I print_info: ssm_d_state      = 0
0.00.121.828 I print_info: ssm_dt_rank      = 0
0.00.121.828 I print_info: ssm_dt_b_c_rms   = 0
0.00.121.828 I print_info: model type       = 1.4B
0.00.121.829 I print_info: model params     = 1.41 B
0.00.121.829 I print_info: general.name     = 1.4B
0.00.121.830 I print_info: vocab type       = BPE
0.00.121.830 I print_info: n_vocab          = 50304
0.00.121.830 I print_info: n_merges         = 50009
0.00.121.830 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.121.831 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.121.831 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.121.831 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.121.831 I print_info: LF token         = 128 'Ä'
0.00.121.832 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.121.832 I print_info: max token length = 1024
0.00.124.600 I load_tensors: offloading 24 repeating layers to GPU
0.00.124.600 I load_tensors: offloading output layer to GPU
0.00.124.601 I load_tensors: offloaded 25/25 layers to GPU
0.00.124.620 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.124.621 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.124.981 I llama_init_from_model: n_seq_max     = 1
0.00.124.982 I llama_init_from_model: n_ctx         = 2048
0.00.124.982 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.124.982 I llama_init_from_model: n_batch       = 2048
0.00.124.982 I llama_init_from_model: n_ubatch      = 512
0.00.124.983 I llama_init_from_model: flash_attn    = 0
0.00.124.983 I llama_init_from_model: freq_base     = 10000.0
0.00.124.983 I llama_init_from_model: freq_scale    = 1
0.00.124.984 I ggml_metal_init: allocating
0.00.124.987 I ggml_metal_init: found device: Apple M4
0.00.124.989 I ggml_metal_init: picking default device: Apple M4
0.00.125.723 I ggml_metal_init: using embedded metal library
0.00.149.885 I ggml_metal_init: GPU name:   Apple M4
0.00.149.888 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.149.888 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.149.889 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.149.889 I ggml_metal_init: simdgroup reduction   = true
0.00.149.889 I ggml_metal_init: simdgroup matrix mul. = true
0.00.149.889 I ggml_metal_init: has bfloat            = true
0.00.149.889 I ggml_metal_init: use bfloat            = true
0.00.149.890 I ggml_metal_init: hasUnifiedMemory      = true
0.00.149.891 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.281.851 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.305.322 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.305.329 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.305.352 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.306.283 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.306.285 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.306.285 I llama_init_from_model: graph nodes  = 967
0.00.306.286 I llama_init_from_model: graph splits = 2
0.00.306.289 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.306.417 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.306.417 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.387.282 I main: llama threadpool init, n_threads = 4
0.00.387.317 I 
0.00.387.350 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.387.351 I 
0.00.387.424 I sampler seed: 1234
0.00.387.429 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.387.457 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.387.459 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.387.459 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.221.444 I llama_perf_sampler_print:    sampling time =       1.25 ms /    71 runs   (    0.02 ms per token, 56664.01 tokens per second)
0.02.221.445 I llama_perf_context_print:        load time =     337.14 ms
0.02.221.445 I llama_perf_context_print: prompt eval time =      43.86 ms /     7 tokens (    6.27 ms per token,   159.60 tokens per second)
0.02.221.446 I llama_perf_context_print:        eval time =    1787.15 ms /    63 runs   (   28.37 ms per token,    35.25 tokens per second)
0.02.221.446 I llama_perf_context_print:       total time =    1835.39 ms /    70 tokens
0.02.221.651 I ggml_metal_free: deallocating

real	0m2.518s
user	0m0.160s
sys	0m0.112s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.047 I build: 4529 (12c2bdf2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.088 I main: llama backend init
0.00.000.090 I main: load the model and apply lora adapter, if any
0.00.009.922 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.030.136 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.030.142 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.030.144 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.030.144 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.030.145 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.030.145 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.030.145 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.030.146 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.030.147 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.030.147 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.030.147 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.030.148 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.030.150 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.030.150 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.030.152 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.030.152 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.030.152 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.034.385 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.035.546 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.040.007 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.040.009 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.040.009 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.040.009 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.040.010 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.040.010 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.040.011 I llama_model_loader: - type  f32:  194 tensors
0.00.040.011 I llama_model_loader: - type q8_0:   98 tensors
0.00.040.012 I print_info: file format = GGUF V3 (latest)
0.00.040.013 I print_info: file type   = Q8_0
0.00.040.014 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.062.773 I load: special tokens cache size = 25
0.00.069.288 I load: token to piece cache size = 0.2984 MB
0.00.069.292 I print_info: arch             = gptneox
0.00.069.292 I print_info: vocab_only       = 0
0.00.069.293 I print_info: n_ctx_train      = 2048
0.00.069.293 I print_info: n_embd           = 2048
0.00.069.293 I print_info: n_layer          = 24
0.00.069.298 I print_info: n_head           = 16
0.00.069.299 I print_info: n_head_kv        = 16
0.00.069.299 I print_info: n_rot            = 32
0.00.069.299 I print_info: n_swa            = 0
0.00.069.299 I print_info: n_embd_head_k    = 128
0.00.069.301 I print_info: n_embd_head_v    = 128
0.00.069.301 I print_info: n_gqa            = 1
0.00.069.302 I print_info: n_embd_k_gqa     = 2048
0.00.069.303 I print_info: n_embd_v_gqa     = 2048
0.00.069.303 I print_info: f_norm_eps       = 1.0e-05
0.00.069.304 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.069.304 I print_info: f_clamp_kqv      = 0.0e+00
0.00.069.304 I print_info: f_max_alibi_bias = 0.0e+00
0.00.069.304 I print_info: f_logit_scale    = 0.0e+00
0.00.069.305 I print_info: n_ff             = 8192
0.00.069.305 I print_info: n_expert         = 0
0.00.069.305 I print_info: n_expert_used    = 0
0.00.069.305 I print_info: causal attn      = 1
0.00.069.305 I print_info: pooling type     = 0
0.00.069.308 I print_info: rope type        = 2
0.00.069.308 I print_info: rope scaling     = linear
0.00.069.308 I print_info: freq_base_train  = 10000.0
0.00.069.308 I print_info: freq_scale_train = 1
0.00.069.309 I print_info: n_ctx_orig_yarn  = 2048
0.00.069.309 I print_info: rope_finetuned   = unknown
0.00.069.309 I print_info: ssm_d_conv       = 0
0.00.069.309 I print_info: ssm_d_inner      = 0
0.00.069.309 I print_info: ssm_d_state      = 0
0.00.069.309 I print_info: ssm_dt_rank      = 0
0.00.069.310 I print_info: ssm_dt_b_c_rms   = 0
0.00.069.310 I print_info: model type       = 1.4B
0.00.069.310 I print_info: model params     = 1.41 B
0.00.069.310 I print_info: general.name     = 1.4B
0.00.069.311 I print_info: vocab type       = BPE
0.00.069.311 I print_info: n_vocab          = 50304
0.00.069.312 I print_info: n_merges         = 50009
0.00.069.312 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.069.313 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.069.313 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.069.314 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.069.314 I print_info: LF token         = 128 'Ä'
0.00.069.314 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.069.314 I print_info: max token length = 1024
0.00.071.651 I load_tensors: offloading 24 repeating layers to GPU
0.00.071.652 I load_tensors: offloading output layer to GPU
0.00.071.652 I load_tensors: offloaded 25/25 layers to GPU
0.00.071.664 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.071.665 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.00.072.042 I llama_init_from_model: n_seq_max     = 1
0.00.072.043 I llama_init_from_model: n_ctx         = 2048
0.00.072.043 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.072.043 I llama_init_from_model: n_batch       = 2048
0.00.072.043 I llama_init_from_model: n_ubatch      = 512
0.00.072.044 I llama_init_from_model: flash_attn    = 0
0.00.072.044 I llama_init_from_model: freq_base     = 10000.0
0.00.072.044 I llama_init_from_model: freq_scale    = 1
0.00.072.045 I ggml_metal_init: allocating
0.00.072.047 I ggml_metal_init: found device: Apple M4
0.00.072.050 I ggml_metal_init: picking default device: Apple M4
0.00.072.821 I ggml_metal_init: using embedded metal library
0.00.075.649 I ggml_metal_init: GPU name:   Apple M4
0.00.075.651 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.075.651 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.075.652 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.075.652 I ggml_metal_init: simdgroup reduction   = true
0.00.075.652 I ggml_metal_init: simdgroup matrix mul. = true
0.00.075.653 I ggml_metal_init: has bfloat            = true
0.00.075.653 I ggml_metal_init: use bfloat            = true
0.00.075.653 I ggml_metal_init: hasUnifiedMemory      = true
0.00.075.654 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.086.371 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.111.040 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.111.053 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.111.091 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.112.188 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.112.190 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.112.191 I llama_init_from_model: graph nodes  = 967
0.00.112.191 I llama_init_from_model: graph splits = 2
0.00.112.195 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.112.320 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.112.320 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.383.613 I main: llama threadpool init, n_threads = 4
0.01.383.673 I 
0.01.383.725 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.383.727 I 
0.01.384.191 I sampler seed: 1234
0.01.384.196 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.384.262 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.384.267 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.384.267 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.482.801 I llama_perf_sampler_print:    sampling time =       1.29 ms /    71 runs   (    0.02 ms per token, 55081.46 tokens per second)
0.02.482.801 I llama_perf_context_print:        load time =    1372.41 ms
0.02.482.802 I llama_perf_context_print: prompt eval time =      49.88 ms /     7 tokens (    7.13 ms per token,   140.33 tokens per second)
0.02.482.803 I llama_perf_context_print:        eval time =    1045.67 ms /    63 runs   (   16.60 ms per token,    60.25 tokens per second)
0.02.482.803 I llama_perf_context_print:       total time =    1100.46 ms /    70 tokens
0.02.482.995 I ggml_metal_free: deallocating

real	0m2.504s
user	0m0.127s
sys	0m0.238s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.047 I build: 4529 (12c2bdf2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.086 I main: llama backend init
0.00.000.088 I main: load the model and apply lora adapter, if any
0.00.016.604 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.025.606 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.025.612 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.025.616 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.025.617 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.025.617 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.025.617 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.025.618 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.025.619 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.025.619 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.025.620 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.025.620 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.025.620 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.025.621 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.025.621 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.025.623 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.025.623 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.025.623 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.029.655 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.030.694 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.034.750 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.034.751 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.034.751 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.034.752 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.034.752 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.034.752 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.034.753 I llama_model_loader: - type  f32:  194 tensors
0.00.034.753 I llama_model_loader: - type q4_0:   97 tensors
0.00.034.753 I llama_model_loader: - type q6_K:    1 tensors
0.00.034.754 I print_info: file format = GGUF V3 (latest)
0.00.034.755 I print_info: file type   = Q4_0
0.00.034.755 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.055.164 I load: special tokens cache size = 25
0.00.061.377 I load: token to piece cache size = 0.2984 MB
0.00.061.380 I print_info: arch             = gptneox
0.00.061.380 I print_info: vocab_only       = 0
0.00.061.381 I print_info: n_ctx_train      = 2048
0.00.061.381 I print_info: n_embd           = 2048
0.00.061.381 I print_info: n_layer          = 24
0.00.061.386 I print_info: n_head           = 16
0.00.061.387 I print_info: n_head_kv        = 16
0.00.061.387 I print_info: n_rot            = 32
0.00.061.387 I print_info: n_swa            = 0
0.00.061.387 I print_info: n_embd_head_k    = 128
0.00.061.387 I print_info: n_embd_head_v    = 128
0.00.061.391 I print_info: n_gqa            = 1
0.00.061.392 I print_info: n_embd_k_gqa     = 2048
0.00.061.392 I print_info: n_embd_v_gqa     = 2048
0.00.061.393 I print_info: f_norm_eps       = 1.0e-05
0.00.061.394 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.061.394 I print_info: f_clamp_kqv      = 0.0e+00
0.00.061.394 I print_info: f_max_alibi_bias = 0.0e+00
0.00.061.396 I print_info: f_logit_scale    = 0.0e+00
0.00.061.396 I print_info: n_ff             = 8192
0.00.061.396 I print_info: n_expert         = 0
0.00.061.396 I print_info: n_expert_used    = 0
0.00.061.397 I print_info: causal attn      = 1
0.00.061.397 I print_info: pooling type     = 0
0.00.061.397 I print_info: rope type        = 2
0.00.061.397 I print_info: rope scaling     = linear
0.00.061.397 I print_info: freq_base_train  = 10000.0
0.00.061.398 I print_info: freq_scale_train = 1
0.00.061.398 I print_info: n_ctx_orig_yarn  = 2048
0.00.061.398 I print_info: rope_finetuned   = unknown
0.00.061.398 I print_info: ssm_d_conv       = 0
0.00.061.398 I print_info: ssm_d_inner      = 0
0.00.061.398 I print_info: ssm_d_state      = 0
0.00.061.398 I print_info: ssm_dt_rank      = 0
0.00.061.399 I print_info: ssm_dt_b_c_rms   = 0
0.00.061.399 I print_info: model type       = 1.4B
0.00.061.400 I print_info: model params     = 1.41 B
0.00.061.401 I print_info: general.name     = 1.4B
0.00.061.401 I print_info: vocab type       = BPE
0.00.061.401 I print_info: n_vocab          = 50304
0.00.061.401 I print_info: n_merges         = 50009
0.00.061.402 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.061.402 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.061.402 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.061.402 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.061.402 I print_info: LF token         = 128 'Ä'
0.00.061.403 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.061.403 I print_info: max token length = 1024
0.00.063.696 I load_tensors: offloading 24 repeating layers to GPU
0.00.063.696 I load_tensors: offloading output layer to GPU
0.00.063.696 I load_tensors: offloaded 25/25 layers to GPU
0.00.063.708 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.063.709 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.064.051 I llama_init_from_model: n_seq_max     = 1
0.00.064.052 I llama_init_from_model: n_ctx         = 2048
0.00.064.052 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.064.052 I llama_init_from_model: n_batch       = 2048
0.00.064.053 I llama_init_from_model: n_ubatch      = 512
0.00.064.053 I llama_init_from_model: flash_attn    = 0
0.00.064.053 I llama_init_from_model: freq_base     = 10000.0
0.00.064.053 I llama_init_from_model: freq_scale    = 1
0.00.064.054 I ggml_metal_init: allocating
0.00.064.057 I ggml_metal_init: found device: Apple M4
0.00.064.059 I ggml_metal_init: picking default device: Apple M4
0.00.064.797 I ggml_metal_init: using embedded metal library
0.00.067.369 I ggml_metal_init: GPU name:   Apple M4
0.00.067.370 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.067.371 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.067.371 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.067.372 I ggml_metal_init: simdgroup reduction   = true
0.00.067.372 I ggml_metal_init: simdgroup matrix mul. = true
0.00.067.372 I ggml_metal_init: has bfloat            = true
0.00.067.372 I ggml_metal_init: use bfloat            = true
0.00.067.373 I ggml_metal_init: hasUnifiedMemory      = true
0.00.067.373 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.077.032 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.101.377 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.101.385 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.101.413 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.102.767 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.102.770 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.102.771 I llama_init_from_model: graph nodes  = 967
0.00.102.771 I llama_init_from_model: graph splits = 2
0.00.102.775 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.102.914 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.102.915 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.806.952 I main: llama threadpool init, n_threads = 4
0.00.806.988 I 
0.00.807.012 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.807.012 I 
0.00.807.234 I sampler seed: 1234
0.00.807.238 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.807.294 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.807.296 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.807.296 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.491.819 I llama_perf_sampler_print:    sampling time =       1.37 ms /    71 runs   (    0.02 ms per token, 51787.02 tokens per second)
0.01.491.820 I llama_perf_context_print:        load time =     789.50 ms
0.01.491.820 I llama_perf_context_print: prompt eval time =      39.79 ms /     7 tokens (    5.68 ms per token,   175.93 tokens per second)
0.01.491.821 I llama_perf_context_print:        eval time =     641.86 ms /    63 runs   (   10.19 ms per token,    98.15 tokens per second)
0.01.491.821 I llama_perf_context_print:       total time =     685.72 ms /    70 tokens
0.01.492.064 I ggml_metal_free: deallocating

real	0m1.513s
user	0m0.113s
sys	0m0.149s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4529 (12c2bdf2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.084 I main: llama backend init
0.00.000.086 I main: load the model and apply lora adapter, if any
0.00.014.434 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.034.046 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.034.050 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.034.052 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.034.052 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.034.053 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.034.053 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.034.053 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.034.054 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.034.054 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.034.054 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.034.055 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.034.055 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.034.055 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.034.056 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.034.058 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.034.060 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.034.061 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.038.182 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.039.316 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.043.793 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.043.794 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.043.795 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.043.795 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.043.795 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.043.796 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.043.796 I llama_model_loader: - type  f32:  194 tensors
0.00.043.797 I llama_model_loader: - type q4_1:   97 tensors
0.00.043.797 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.798 I print_info: file format = GGUF V3 (latest)
0.00.043.798 I print_info: file type   = Q4_1
0.00.043.799 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.068.011 I load: special tokens cache size = 25
0.00.076.590 I load: token to piece cache size = 0.2984 MB
0.00.076.593 I print_info: arch             = gptneox
0.00.076.593 I print_info: vocab_only       = 0
0.00.076.593 I print_info: n_ctx_train      = 2048
0.00.076.594 I print_info: n_embd           = 2048
0.00.076.594 I print_info: n_layer          = 24
0.00.076.597 I print_info: n_head           = 16
0.00.076.598 I print_info: n_head_kv        = 16
0.00.076.598 I print_info: n_rot            = 32
0.00.076.598 I print_info: n_swa            = 0
0.00.076.599 I print_info: n_embd_head_k    = 128
0.00.076.599 I print_info: n_embd_head_v    = 128
0.00.076.599 I print_info: n_gqa            = 1
0.00.076.603 I print_info: n_embd_k_gqa     = 2048
0.00.076.604 I print_info: n_embd_v_gqa     = 2048
0.00.076.604 I print_info: f_norm_eps       = 1.0e-05
0.00.076.605 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.076.605 I print_info: f_clamp_kqv      = 0.0e+00
0.00.076.606 I print_info: f_max_alibi_bias = 0.0e+00
0.00.076.608 I print_info: f_logit_scale    = 0.0e+00
0.00.076.609 I print_info: n_ff             = 8192
0.00.076.609 I print_info: n_expert         = 0
0.00.076.609 I print_info: n_expert_used    = 0
0.00.076.609 I print_info: causal attn      = 1
0.00.076.609 I print_info: pooling type     = 0
0.00.076.610 I print_info: rope type        = 2
0.00.076.610 I print_info: rope scaling     = linear
0.00.076.610 I print_info: freq_base_train  = 10000.0
0.00.076.611 I print_info: freq_scale_train = 1
0.00.076.611 I print_info: n_ctx_orig_yarn  = 2048
0.00.076.611 I print_info: rope_finetuned   = unknown
0.00.076.611 I print_info: ssm_d_conv       = 0
0.00.076.611 I print_info: ssm_d_inner      = 0
0.00.076.611 I print_info: ssm_d_state      = 0
0.00.076.612 I print_info: ssm_dt_rank      = 0
0.00.076.612 I print_info: ssm_dt_b_c_rms   = 0
0.00.076.612 I print_info: model type       = 1.4B
0.00.076.612 I print_info: model params     = 1.41 B
0.00.076.613 I print_info: general.name     = 1.4B
0.00.076.618 I print_info: vocab type       = BPE
0.00.076.618 I print_info: n_vocab          = 50304
0.00.076.620 I print_info: n_merges         = 50009
0.00.076.620 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.076.620 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.076.620 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.076.621 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.076.621 I print_info: LF token         = 128 'Ä'
0.00.076.621 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.076.621 I print_info: max token length = 1024
0.00.079.091 I load_tensors: offloading 24 repeating layers to GPU
0.00.079.091 I load_tensors: offloading output layer to GPU
0.00.079.091 I load_tensors: offloaded 25/25 layers to GPU
0.00.079.102 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.079.104 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.079.473 I llama_init_from_model: n_seq_max     = 1
0.00.079.474 I llama_init_from_model: n_ctx         = 2048
0.00.079.474 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.079.474 I llama_init_from_model: n_batch       = 2048
0.00.079.475 I llama_init_from_model: n_ubatch      = 512
0.00.079.475 I llama_init_from_model: flash_attn    = 0
0.00.079.475 I llama_init_from_model: freq_base     = 10000.0
0.00.079.476 I llama_init_from_model: freq_scale    = 1
0.00.079.476 I ggml_metal_init: allocating
0.00.079.480 I ggml_metal_init: found device: Apple M4
0.00.079.483 I ggml_metal_init: picking default device: Apple M4
0.00.080.309 I ggml_metal_init: using embedded metal library
0.00.084.341 I ggml_metal_init: GPU name:   Apple M4
0.00.084.344 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.084.344 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.084.345 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.084.345 I ggml_metal_init: simdgroup reduction   = true
0.00.084.345 I ggml_metal_init: simdgroup matrix mul. = true
0.00.084.346 I ggml_metal_init: has bfloat            = true
0.00.084.346 I ggml_metal_init: use bfloat            = true
0.00.084.346 I ggml_metal_init: hasUnifiedMemory      = true
0.00.084.349 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.096.805 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.119.032 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.119.044 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.119.078 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.120.208 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.120.210 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.120.211 I llama_init_from_model: graph nodes  = 967
0.00.120.211 I llama_init_from_model: graph splits = 2
0.00.120.214 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.120.342 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.120.343 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.779.627 I main: llama threadpool init, n_threads = 4
0.00.779.664 I 
0.00.779.686 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.779.686 I 
0.00.779.907 I sampler seed: 1234
0.00.779.911 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.779.964 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.779.966 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.779.966 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.519.598 I llama_perf_sampler_print:    sampling time =       1.13 ms /    71 runs   (    0.02 ms per token, 62831.86 tokens per second)
0.01.519.598 I llama_perf_context_print:        load time =     764.30 ms
0.01.519.599 I llama_perf_context_print: prompt eval time =      47.78 ms /     7 tokens (    6.83 ms per token,   146.50 tokens per second)
0.01.519.601 I llama_perf_context_print:        eval time =     688.90 ms /    63 runs   (   10.93 ms per token,    91.45 tokens per second)
0.01.519.601 I llama_perf_context_print:       total time =     740.86 ms /    70 tokens
0.01.519.854 I ggml_metal_free: deallocating

real	0m1.538s
user	0m0.127s
sys	0m0.153s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4529 (12c2bdf2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.075 I main: llama backend init
0.00.000.078 I main: load the model and apply lora adapter, if any
0.00.009.282 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.396 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.017.400 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.402 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.402 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.402 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.403 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.403 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.404 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.404 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.404 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.405 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.405 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.405 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.406 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.408 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.409 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.409 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.392 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.440 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.351 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.353 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.353 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.353 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.354 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.354 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.026.354 I llama_model_loader: - type  f32:  194 tensors
0.00.026.355 I llama_model_loader: - type q5_0:   97 tensors
0.00.026.355 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.356 I print_info: file format = GGUF V3 (latest)
0.00.026.356 I print_info: file type   = Q5_0
0.00.026.357 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.044.958 I load: special tokens cache size = 25
0.00.051.178 I load: token to piece cache size = 0.2984 MB
0.00.051.180 I print_info: arch             = gptneox
0.00.051.180 I print_info: vocab_only       = 0
0.00.051.181 I print_info: n_ctx_train      = 2048
0.00.051.181 I print_info: n_embd           = 2048
0.00.051.181 I print_info: n_layer          = 24
0.00.051.184 I print_info: n_head           = 16
0.00.051.184 I print_info: n_head_kv        = 16
0.00.051.185 I print_info: n_rot            = 32
0.00.051.185 I print_info: n_swa            = 0
0.00.051.185 I print_info: n_embd_head_k    = 128
0.00.051.186 I print_info: n_embd_head_v    = 128
0.00.051.187 I print_info: n_gqa            = 1
0.00.051.187 I print_info: n_embd_k_gqa     = 2048
0.00.051.188 I print_info: n_embd_v_gqa     = 2048
0.00.051.189 I print_info: f_norm_eps       = 1.0e-05
0.00.051.189 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.190 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.190 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.190 I print_info: f_logit_scale    = 0.0e+00
0.00.051.191 I print_info: n_ff             = 8192
0.00.051.191 I print_info: n_expert         = 0
0.00.051.191 I print_info: n_expert_used    = 0
0.00.051.191 I print_info: causal attn      = 1
0.00.051.191 I print_info: pooling type     = 0
0.00.051.193 I print_info: rope type        = 2
0.00.051.195 I print_info: rope scaling     = linear
0.00.051.195 I print_info: freq_base_train  = 10000.0
0.00.051.196 I print_info: freq_scale_train = 1
0.00.051.196 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.196 I print_info: rope_finetuned   = unknown
0.00.051.196 I print_info: ssm_d_conv       = 0
0.00.051.196 I print_info: ssm_d_inner      = 0
0.00.051.197 I print_info: ssm_d_state      = 0
0.00.051.197 I print_info: ssm_dt_rank      = 0
0.00.051.197 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.197 I print_info: model type       = 1.4B
0.00.051.197 I print_info: model params     = 1.41 B
0.00.051.198 I print_info: general.name     = 1.4B
0.00.051.198 I print_info: vocab type       = BPE
0.00.051.198 I print_info: n_vocab          = 50304
0.00.051.198 I print_info: n_merges         = 50009
0.00.051.199 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.199 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.199 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.199 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.199 I print_info: LF token         = 128 'Ä'
0.00.051.204 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.204 I print_info: max token length = 1024
0.00.053.225 I load_tensors: offloading 24 repeating layers to GPU
0.00.053.225 I load_tensors: offloading output layer to GPU
0.00.053.226 I load_tensors: offloaded 25/25 layers to GPU
0.00.053.236 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.053.238 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.053.512 I llama_init_from_model: n_seq_max     = 1
0.00.053.513 I llama_init_from_model: n_ctx         = 2048
0.00.053.513 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.053.513 I llama_init_from_model: n_batch       = 2048
0.00.053.513 I llama_init_from_model: n_ubatch      = 512
0.00.053.513 I llama_init_from_model: flash_attn    = 0
0.00.053.514 I llama_init_from_model: freq_base     = 10000.0
0.00.053.514 I llama_init_from_model: freq_scale    = 1
0.00.053.514 I ggml_metal_init: allocating
0.00.053.517 I ggml_metal_init: found device: Apple M4
0.00.053.519 I ggml_metal_init: picking default device: Apple M4
0.00.054.125 I ggml_metal_init: using embedded metal library
0.00.056.457 I ggml_metal_init: GPU name:   Apple M4
0.00.056.458 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.458 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.459 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.459 I ggml_metal_init: simdgroup reduction   = true
0.00.056.459 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.459 I ggml_metal_init: has bfloat            = true
0.00.056.459 I ggml_metal_init: use bfloat            = true
0.00.056.460 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.460 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.332 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.085.862 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.869 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.889 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.086.924 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.086.926 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.086.926 I llama_init_from_model: graph nodes  = 967
0.00.086.926 I llama_init_from_model: graph splits = 2
0.00.086.929 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.087.046 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.047 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.757.666 I main: llama threadpool init, n_threads = 4
0.00.757.701 I 
0.00.757.726 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.757.726 I 
0.00.757.989 I sampler seed: 1234
0.00.757.993 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.758.023 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.758.024 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.758.024 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.552.081 I llama_perf_sampler_print:    sampling time =       1.22 ms /    71 runs   (    0.02 ms per token, 58292.28 tokens per second)
0.01.552.082 I llama_perf_context_print:        load time =     747.52 ms
0.01.552.082 I llama_perf_context_print: prompt eval time =      43.05 ms /     7 tokens (    6.15 ms per token,   162.60 tokens per second)
0.01.552.083 I llama_perf_context_print:        eval time =     747.96 ms /    63 runs   (   11.87 ms per token,    84.23 tokens per second)
0.01.552.083 I llama_perf_context_print:       total time =     795.28 ms /    70 tokens
0.01.552.331 I ggml_metal_free: deallocating

real	0m1.571s
user	0m0.110s
sys	0m0.150s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.042 I build: 4529 (12c2bdf2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.078 I main: llama backend init
0.00.000.080 I main: load the model and apply lora adapter, if any
0.00.009.925 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.142 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.017.146 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.148 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.148 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.153 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.153 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.154 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.155 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.155 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.155 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.156 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.156 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.157 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.157 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.160 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.160 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.160 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.988 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.074 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.858 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.860 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.860 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.860 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.861 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.861 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.861 I llama_model_loader: - type  f32:  194 tensors
0.00.025.862 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.862 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.862 I print_info: file format = GGUF V3 (latest)
0.00.025.863 I print_info: file type   = Q5_1
0.00.025.864 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.044.457 I load: special tokens cache size = 25
0.00.050.500 I load: token to piece cache size = 0.2984 MB
0.00.050.503 I print_info: arch             = gptneox
0.00.050.503 I print_info: vocab_only       = 0
0.00.050.503 I print_info: n_ctx_train      = 2048
0.00.050.504 I print_info: n_embd           = 2048
0.00.050.504 I print_info: n_layer          = 24
0.00.050.507 I print_info: n_head           = 16
0.00.050.507 I print_info: n_head_kv        = 16
0.00.050.508 I print_info: n_rot            = 32
0.00.050.510 I print_info: n_swa            = 0
0.00.050.510 I print_info: n_embd_head_k    = 128
0.00.050.510 I print_info: n_embd_head_v    = 128
0.00.050.511 I print_info: n_gqa            = 1
0.00.050.512 I print_info: n_embd_k_gqa     = 2048
0.00.050.513 I print_info: n_embd_v_gqa     = 2048
0.00.050.513 I print_info: f_norm_eps       = 1.0e-05
0.00.050.514 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.514 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.514 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.514 I print_info: f_logit_scale    = 0.0e+00
0.00.050.515 I print_info: n_ff             = 8192
0.00.050.515 I print_info: n_expert         = 0
0.00.050.515 I print_info: n_expert_used    = 0
0.00.050.515 I print_info: causal attn      = 1
0.00.050.515 I print_info: pooling type     = 0
0.00.050.517 I print_info: rope type        = 2
0.00.050.518 I print_info: rope scaling     = linear
0.00.050.519 I print_info: freq_base_train  = 10000.0
0.00.050.519 I print_info: freq_scale_train = 1
0.00.050.519 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.519 I print_info: rope_finetuned   = unknown
0.00.050.519 I print_info: ssm_d_conv       = 0
0.00.050.520 I print_info: ssm_d_inner      = 0
0.00.050.520 I print_info: ssm_d_state      = 0
0.00.050.520 I print_info: ssm_dt_rank      = 0
0.00.050.520 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.521 I print_info: model type       = 1.4B
0.00.050.525 I print_info: model params     = 1.41 B
0.00.050.526 I print_info: general.name     = 1.4B
0.00.050.526 I print_info: vocab type       = BPE
0.00.050.526 I print_info: n_vocab          = 50304
0.00.050.527 I print_info: n_merges         = 50009
0.00.050.527 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.527 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.527 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.527 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.528 I print_info: LF token         = 128 'Ä'
0.00.050.528 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.528 I print_info: max token length = 1024
0.00.052.454 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.454 I load_tensors: offloading output layer to GPU
0.00.052.454 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.465 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.052.466 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.052.758 I llama_init_from_model: n_seq_max     = 1
0.00.052.758 I llama_init_from_model: n_ctx         = 2048
0.00.052.758 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.052.758 I llama_init_from_model: n_batch       = 2048
0.00.052.759 I llama_init_from_model: n_ubatch      = 512
0.00.052.759 I llama_init_from_model: flash_attn    = 0
0.00.052.759 I llama_init_from_model: freq_base     = 10000.0
0.00.052.759 I llama_init_from_model: freq_scale    = 1
0.00.052.760 I ggml_metal_init: allocating
0.00.052.763 I ggml_metal_init: found device: Apple M4
0.00.052.765 I ggml_metal_init: picking default device: Apple M4
0.00.053.350 I ggml_metal_init: using embedded metal library
0.00.055.667 I ggml_metal_init: GPU name:   Apple M4
0.00.055.669 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.669 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.670 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.670 I ggml_metal_init: simdgroup reduction   = true
0.00.055.670 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.670 I ggml_metal_init: has bfloat            = true
0.00.055.670 I ggml_metal_init: use bfloat            = true
0.00.055.671 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.671 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.404 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.085.353 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.362 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.386 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.086.518 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.086.520 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.086.520 I llama_init_from_model: graph nodes  = 967
0.00.086.521 I llama_init_from_model: graph splits = 2
0.00.086.523 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.086.663 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.086.663 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.677.428 I main: llama threadpool init, n_threads = 4
0.00.677.463 I 
0.00.677.485 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.677.485 I 
0.00.677.716 I sampler seed: 1234
0.00.677.719 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.677.730 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.677.730 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.677.731 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.521.007 I llama_perf_sampler_print:    sampling time =       1.19 ms /    71 runs   (    0.02 ms per token, 59613.77 tokens per second)
0.01.521.008 I llama_perf_context_print:        load time =     666.65 ms
0.01.521.009 I llama_perf_context_print: prompt eval time =      46.16 ms /     7 tokens (    6.59 ms per token,   151.64 tokens per second)
0.01.521.009 I llama_perf_context_print:        eval time =     794.13 ms /    63 runs   (   12.61 ms per token,    79.33 tokens per second)
0.01.521.010 I llama_perf_context_print:       total time =     844.43 ms /    70 tokens
0.01.521.258 I ggml_metal_free: deallocating

real	0m1.539s
user	0m0.109s
sys	0m0.145s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.039 I build: 4529 (12c2bdf2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.075 I main: llama backend init
0.00.000.077 I main: load the model and apply lora adapter, if any
0.00.010.007 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.585 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.590 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.592 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.593 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.593 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.593 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.594 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.594 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.595 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.595 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.596 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.596 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.596 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.597 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.598 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.598 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.599 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.483 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.489 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.356 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.357 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.357 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.357 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.358 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.358 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.359 I llama_model_loader: - type  f32:  194 tensors
0.00.025.359 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.359 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.359 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.360 I print_info: file format = GGUF V3 (latest)
0.00.025.360 I print_info: file type   = Q2_K - Medium
0.00.025.361 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.043.983 I load: special tokens cache size = 25
0.00.050.075 I load: token to piece cache size = 0.2984 MB
0.00.050.078 I print_info: arch             = gptneox
0.00.050.078 I print_info: vocab_only       = 0
0.00.050.079 I print_info: n_ctx_train      = 2048
0.00.050.079 I print_info: n_embd           = 2048
0.00.050.079 I print_info: n_layer          = 24
0.00.050.082 I print_info: n_head           = 16
0.00.050.083 I print_info: n_head_kv        = 16
0.00.050.083 I print_info: n_rot            = 32
0.00.050.083 I print_info: n_swa            = 0
0.00.050.083 I print_info: n_embd_head_k    = 128
0.00.050.084 I print_info: n_embd_head_v    = 128
0.00.050.084 I print_info: n_gqa            = 1
0.00.050.085 I print_info: n_embd_k_gqa     = 2048
0.00.050.086 I print_info: n_embd_v_gqa     = 2048
0.00.050.086 I print_info: f_norm_eps       = 1.0e-05
0.00.050.087 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.088 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.088 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.090 I print_info: f_logit_scale    = 0.0e+00
0.00.050.091 I print_info: n_ff             = 8192
0.00.050.091 I print_info: n_expert         = 0
0.00.050.091 I print_info: n_expert_used    = 0
0.00.050.091 I print_info: causal attn      = 1
0.00.050.091 I print_info: pooling type     = 0
0.00.050.093 I print_info: rope type        = 2
0.00.050.093 I print_info: rope scaling     = linear
0.00.050.093 I print_info: freq_base_train  = 10000.0
0.00.050.094 I print_info: freq_scale_train = 1
0.00.050.094 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.096 I print_info: rope_finetuned   = unknown
0.00.050.096 I print_info: ssm_d_conv       = 0
0.00.050.096 I print_info: ssm_d_inner      = 0
0.00.050.096 I print_info: ssm_d_state      = 0
0.00.050.096 I print_info: ssm_dt_rank      = 0
0.00.050.096 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.098 I print_info: model type       = 1.4B
0.00.050.098 I print_info: model params     = 1.41 B
0.00.050.098 I print_info: general.name     = 1.4B
0.00.050.099 I print_info: vocab type       = BPE
0.00.050.099 I print_info: n_vocab          = 50304
0.00.050.099 I print_info: n_merges         = 50009
0.00.050.100 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.100 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.101 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.101 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.101 I print_info: LF token         = 128 'Ä'
0.00.050.101 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.102 I print_info: max token length = 1024
0.00.051.933 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.933 I load_tensors: offloading output layer to GPU
0.00.051.933 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.944 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.051.945 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.052.216 I llama_init_from_model: n_seq_max     = 1
0.00.052.217 I llama_init_from_model: n_ctx         = 2048
0.00.052.217 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.052.217 I llama_init_from_model: n_batch       = 2048
0.00.052.217 I llama_init_from_model: n_ubatch      = 512
0.00.052.217 I llama_init_from_model: flash_attn    = 0
0.00.052.218 I llama_init_from_model: freq_base     = 10000.0
0.00.052.218 I llama_init_from_model: freq_scale    = 1
0.00.052.219 I ggml_metal_init: allocating
0.00.052.222 I ggml_metal_init: found device: Apple M4
0.00.052.223 I ggml_metal_init: picking default device: Apple M4
0.00.052.810 I ggml_metal_init: using embedded metal library
0.00.055.145 I ggml_metal_init: GPU name:   Apple M4
0.00.055.147 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.147 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.147 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.148 I ggml_metal_init: simdgroup reduction   = true
0.00.055.148 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.148 I ggml_metal_init: has bfloat            = true
0.00.055.148 I ggml_metal_init: use bfloat            = true
0.00.055.148 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.150 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.808 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.084.455 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.464 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.493 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.085.618 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.085.619 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.085.620 I llama_init_from_model: graph nodes  = 967
0.00.085.620 I llama_init_from_model: graph splits = 2
0.00.085.623 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.085.762 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.085.763 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.446.870 I main: llama threadpool init, n_threads = 4
0.00.446.908 I 
0.00.446.930 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.446.930 I 
0.00.447.210 I sampler seed: 1234
0.00.447.216 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.447.253 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.447.274 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.447.276 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.121.186 I llama_perf_sampler_print:    sampling time =       1.15 ms /    71 runs   (    0.02 ms per token, 61739.13 tokens per second)
0.01.121.186 I llama_perf_context_print:        load time =     435.98 ms
0.01.121.187 I llama_perf_context_print: prompt eval time =      35.84 ms /     7 tokens (    5.12 ms per token,   195.33 tokens per second)
0.01.121.188 I llama_perf_context_print:        eval time =     635.17 ms /    63 runs   (   10.08 ms per token,    99.19 tokens per second)
0.01.121.188 I llama_perf_context_print:       total time =     675.20 ms /    70 tokens
0.01.121.403 I ggml_metal_free: deallocating

real	0m1.138s
user	0m0.108s
sys	0m0.109s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4529 (12c2bdf2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.075 I main: llama backend init
0.00.000.077 I main: load the model and apply lora adapter, if any
0.00.009.228 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.836 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.841 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.843 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.843 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.844 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.844 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.844 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.845 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.846 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.846 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.846 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.847 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.849 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.850 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.854 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.854 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.854 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.812 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.831 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.748 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.750 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.750 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.750 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.751 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.751 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.752 I llama_model_loader: - type  f32:  194 tensors
0.00.025.752 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.752 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.752 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.753 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.753 I print_info: file format = GGUF V3 (latest)
0.00.025.754 I print_info: file type   = Q3_K - Medium
0.00.025.755 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.044.457 I load: special tokens cache size = 25
0.00.050.325 I load: token to piece cache size = 0.2984 MB
0.00.050.328 I print_info: arch             = gptneox
0.00.050.328 I print_info: vocab_only       = 0
0.00.050.328 I print_info: n_ctx_train      = 2048
0.00.050.328 I print_info: n_embd           = 2048
0.00.050.328 I print_info: n_layer          = 24
0.00.050.332 I print_info: n_head           = 16
0.00.050.332 I print_info: n_head_kv        = 16
0.00.050.332 I print_info: n_rot            = 32
0.00.050.333 I print_info: n_swa            = 0
0.00.050.333 I print_info: n_embd_head_k    = 128
0.00.050.333 I print_info: n_embd_head_v    = 128
0.00.050.334 I print_info: n_gqa            = 1
0.00.050.334 I print_info: n_embd_k_gqa     = 2048
0.00.050.335 I print_info: n_embd_v_gqa     = 2048
0.00.050.336 I print_info: f_norm_eps       = 1.0e-05
0.00.050.336 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.336 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.336 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.337 I print_info: f_logit_scale    = 0.0e+00
0.00.050.337 I print_info: n_ff             = 8192
0.00.050.338 I print_info: n_expert         = 0
0.00.050.338 I print_info: n_expert_used    = 0
0.00.050.339 I print_info: causal attn      = 1
0.00.050.341 I print_info: pooling type     = 0
0.00.050.341 I print_info: rope type        = 2
0.00.050.341 I print_info: rope scaling     = linear
0.00.050.342 I print_info: freq_base_train  = 10000.0
0.00.050.342 I print_info: freq_scale_train = 1
0.00.050.342 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.342 I print_info: rope_finetuned   = unknown
0.00.050.342 I print_info: ssm_d_conv       = 0
0.00.050.343 I print_info: ssm_d_inner      = 0
0.00.050.343 I print_info: ssm_d_state      = 0
0.00.050.343 I print_info: ssm_dt_rank      = 0
0.00.050.343 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.345 I print_info: model type       = 1.4B
0.00.050.345 I print_info: model params     = 1.41 B
0.00.050.345 I print_info: general.name     = 1.4B
0.00.050.346 I print_info: vocab type       = BPE
0.00.050.346 I print_info: n_vocab          = 50304
0.00.050.346 I print_info: n_merges         = 50009
0.00.050.346 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.346 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.347 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.347 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.348 I print_info: LF token         = 128 'Ä'
0.00.050.349 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.349 I print_info: max token length = 1024
0.00.052.271 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.272 I load_tensors: offloading output layer to GPU
0.00.052.272 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.282 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.052.284 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.052.559 I llama_init_from_model: n_seq_max     = 1
0.00.052.559 I llama_init_from_model: n_ctx         = 2048
0.00.052.559 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.052.559 I llama_init_from_model: n_batch       = 2048
0.00.052.560 I llama_init_from_model: n_ubatch      = 512
0.00.052.560 I llama_init_from_model: flash_attn    = 0
0.00.052.560 I llama_init_from_model: freq_base     = 10000.0
0.00.052.560 I llama_init_from_model: freq_scale    = 1
0.00.052.561 I ggml_metal_init: allocating
0.00.052.564 I ggml_metal_init: found device: Apple M4
0.00.052.565 I ggml_metal_init: picking default device: Apple M4
0.00.053.172 I ggml_metal_init: using embedded metal library
0.00.055.504 I ggml_metal_init: GPU name:   Apple M4
0.00.055.505 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.506 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.506 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.506 I ggml_metal_init: simdgroup reduction   = true
0.00.055.506 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.506 I ggml_metal_init: has bfloat            = true
0.00.055.507 I ggml_metal_init: use bfloat            = true
0.00.055.507 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.508 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.137 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.084.194 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.199 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.226 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.085.267 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.085.268 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.085.268 I llama_init_from_model: graph nodes  = 967
0.00.085.269 I llama_init_from_model: graph splits = 2
0.00.085.271 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.085.400 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.085.400 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.531.117 I main: llama threadpool init, n_threads = 4
0.00.531.157 I 
0.00.531.201 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.531.202 I 
0.00.531.356 I sampler seed: 1234
0.00.531.361 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.531.403 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.531.405 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.531.405 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.298.499 I llama_perf_sampler_print:    sampling time =       1.22 ms /    71 runs   (    0.02 ms per token, 58244.46 tokens per second)
0.01.298.500 I llama_perf_context_print:        load time =     521.06 ms
0.01.298.501 I llama_perf_context_print: prompt eval time =      44.20 ms /     7 tokens (    6.31 ms per token,   158.37 tokens per second)
0.01.298.502 I llama_perf_context_print:        eval time =     719.81 ms /    63 runs   (   11.43 ms per token,    87.52 tokens per second)
0.01.298.502 I llama_perf_context_print:       total time =     768.21 ms /    70 tokens
0.01.298.732 I ggml_metal_free: deallocating

real	0m1.316s
user	0m0.110s
sys	0m0.113s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4529 (12c2bdf2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.077 I main: llama backend init
0.00.000.079 I main: load the model and apply lora adapter, if any
0.00.009.765 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.574 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.017.579 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.581 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.581 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.582 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.582 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.582 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.583 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.584 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.584 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.584 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.585 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.587 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.588 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.590 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.590 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.590 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.478 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.567 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.383 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.384 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.385 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.385 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.385 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.386 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.026.386 I llama_model_loader: - type  f32:  194 tensors
0.00.026.387 I llama_model_loader: - type q4_K:   61 tensors
0.00.026.387 I llama_model_loader: - type q5_K:   24 tensors
0.00.026.387 I llama_model_loader: - type q6_K:   13 tensors
0.00.026.387 I print_info: file format = GGUF V3 (latest)
0.00.026.388 I print_info: file type   = Q4_K - Medium
0.00.026.389 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.044.970 I load: special tokens cache size = 25
0.00.051.190 I load: token to piece cache size = 0.2984 MB
0.00.051.192 I print_info: arch             = gptneox
0.00.051.192 I print_info: vocab_only       = 0
0.00.051.193 I print_info: n_ctx_train      = 2048
0.00.051.193 I print_info: n_embd           = 2048
0.00.051.193 I print_info: n_layer          = 24
0.00.051.196 I print_info: n_head           = 16
0.00.051.197 I print_info: n_head_kv        = 16
0.00.051.197 I print_info: n_rot            = 32
0.00.051.197 I print_info: n_swa            = 0
0.00.051.197 I print_info: n_embd_head_k    = 128
0.00.051.198 I print_info: n_embd_head_v    = 128
0.00.051.198 I print_info: n_gqa            = 1
0.00.051.199 I print_info: n_embd_k_gqa     = 2048
0.00.051.200 I print_info: n_embd_v_gqa     = 2048
0.00.051.200 I print_info: f_norm_eps       = 1.0e-05
0.00.051.201 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.205 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.206 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.206 I print_info: f_logit_scale    = 0.0e+00
0.00.051.206 I print_info: n_ff             = 8192
0.00.051.207 I print_info: n_expert         = 0
0.00.051.208 I print_info: n_expert_used    = 0
0.00.051.208 I print_info: causal attn      = 1
0.00.051.209 I print_info: pooling type     = 0
0.00.051.210 I print_info: rope type        = 2
0.00.051.210 I print_info: rope scaling     = linear
0.00.051.211 I print_info: freq_base_train  = 10000.0
0.00.051.211 I print_info: freq_scale_train = 1
0.00.051.211 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.213 I print_info: rope_finetuned   = unknown
0.00.051.213 I print_info: ssm_d_conv       = 0
0.00.051.213 I print_info: ssm_d_inner      = 0
0.00.051.213 I print_info: ssm_d_state      = 0
0.00.051.213 I print_info: ssm_dt_rank      = 0
0.00.051.213 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.214 I print_info: model type       = 1.4B
0.00.051.214 I print_info: model params     = 1.41 B
0.00.051.214 I print_info: general.name     = 1.4B
0.00.051.215 I print_info: vocab type       = BPE
0.00.051.215 I print_info: n_vocab          = 50304
0.00.051.215 I print_info: n_merges         = 50009
0.00.051.215 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.215 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.215 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.215 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.216 I print_info: LF token         = 128 'Ä'
0.00.051.216 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.217 I print_info: max token length = 1024
0.00.052.820 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.820 I load_tensors: offloading output layer to GPU
0.00.052.820 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.830 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.052.832 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.053.106 I llama_init_from_model: n_seq_max     = 1
0.00.053.106 I llama_init_from_model: n_ctx         = 2048
0.00.053.106 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.053.107 I llama_init_from_model: n_batch       = 2048
0.00.053.107 I llama_init_from_model: n_ubatch      = 512
0.00.053.107 I llama_init_from_model: flash_attn    = 0
0.00.053.107 I llama_init_from_model: freq_base     = 10000.0
0.00.053.108 I llama_init_from_model: freq_scale    = 1
0.00.053.108 I ggml_metal_init: allocating
0.00.053.111 I ggml_metal_init: found device: Apple M4
0.00.053.113 I ggml_metal_init: picking default device: Apple M4
0.00.053.737 I ggml_metal_init: using embedded metal library
0.00.056.059 I ggml_metal_init: GPU name:   Apple M4
0.00.056.060 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.061 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.061 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.061 I ggml_metal_init: simdgroup reduction   = true
0.00.056.061 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.061 I ggml_metal_init: has bfloat            = true
0.00.056.062 I ggml_metal_init: use bfloat            = true
0.00.056.062 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.063 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.652 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.085.348 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.356 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.376 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.086.383 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.086.385 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.086.385 I llama_init_from_model: graph nodes  = 967
0.00.086.385 I llama_init_from_model: graph splits = 2
0.00.086.388 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.086.516 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.086.517 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.622.896 I main: llama threadpool init, n_threads = 4
0.00.622.932 I 
0.00.622.966 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.622.966 I 
0.00.623.124 I sampler seed: 1234
0.00.623.129 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.623.139 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.623.140 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.623.140 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.413.793 I llama_perf_sampler_print:    sampling time =       1.28 ms /    71 runs   (    0.02 ms per token, 55555.56 tokens per second)
0.01.413.794 I llama_perf_context_print:        load time =     612.28 ms
0.01.413.794 I llama_perf_context_print: prompt eval time =      47.19 ms /     7 tokens (    6.74 ms per token,   148.34 tokens per second)
0.01.413.795 I llama_perf_context_print:        eval time =     740.35 ms /    63 runs   (   11.75 ms per token,    85.10 tokens per second)
0.01.413.795 I llama_perf_context_print:       total time =     791.75 ms /    70 tokens
0.01.414.037 I ggml_metal_free: deallocating

real	0m1.431s
user	0m0.110s
sys	0m0.140s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.043 I build: 4529 (12c2bdf2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.082 I main: llama backend init
0.00.000.084 I main: load the model and apply lora adapter, if any
0.00.009.799 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.731 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.017.736 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.742 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.742 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.743 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.743 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.745 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.746 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.746 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.747 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.747 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.747 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.748 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.748 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.750 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.750 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.750 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.759 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.797 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.809 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.810 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.810 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.811 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.811 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.811 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.026.811 I llama_model_loader: - type  f32:  194 tensors
0.00.026.812 I llama_model_loader: - type q5_K:   61 tensors
0.00.026.812 I llama_model_loader: - type q6_K:   37 tensors
0.00.026.812 I print_info: file format = GGUF V3 (latest)
0.00.026.813 I print_info: file type   = Q5_K - Medium
0.00.026.814 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.045.455 I load: special tokens cache size = 25
0.00.051.346 I load: token to piece cache size = 0.2984 MB
0.00.051.349 I print_info: arch             = gptneox
0.00.051.349 I print_info: vocab_only       = 0
0.00.051.350 I print_info: n_ctx_train      = 2048
0.00.051.350 I print_info: n_embd           = 2048
0.00.051.350 I print_info: n_layer          = 24
0.00.051.352 I print_info: n_head           = 16
0.00.051.353 I print_info: n_head_kv        = 16
0.00.051.353 I print_info: n_rot            = 32
0.00.051.353 I print_info: n_swa            = 0
0.00.051.354 I print_info: n_embd_head_k    = 128
0.00.051.354 I print_info: n_embd_head_v    = 128
0.00.051.355 I print_info: n_gqa            = 1
0.00.051.355 I print_info: n_embd_k_gqa     = 2048
0.00.051.356 I print_info: n_embd_v_gqa     = 2048
0.00.051.356 I print_info: f_norm_eps       = 1.0e-05
0.00.051.357 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.357 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.357 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.357 I print_info: f_logit_scale    = 0.0e+00
0.00.051.358 I print_info: n_ff             = 8192
0.00.051.358 I print_info: n_expert         = 0
0.00.051.358 I print_info: n_expert_used    = 0
0.00.051.358 I print_info: causal attn      = 1
0.00.051.359 I print_info: pooling type     = 0
0.00.051.359 I print_info: rope type        = 2
0.00.051.359 I print_info: rope scaling     = linear
0.00.051.359 I print_info: freq_base_train  = 10000.0
0.00.051.360 I print_info: freq_scale_train = 1
0.00.051.360 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.360 I print_info: rope_finetuned   = unknown
0.00.051.360 I print_info: ssm_d_conv       = 0
0.00.051.360 I print_info: ssm_d_inner      = 0
0.00.051.361 I print_info: ssm_d_state      = 0
0.00.051.361 I print_info: ssm_dt_rank      = 0
0.00.051.361 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.361 I print_info: model type       = 1.4B
0.00.051.361 I print_info: model params     = 1.41 B
0.00.051.362 I print_info: general.name     = 1.4B
0.00.051.363 I print_info: vocab type       = BPE
0.00.051.363 I print_info: n_vocab          = 50304
0.00.051.363 I print_info: n_merges         = 50009
0.00.051.363 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.363 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.364 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.364 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.364 I print_info: LF token         = 128 'Ä'
0.00.051.364 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.367 I print_info: max token length = 1024
0.00.052.951 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.951 I load_tensors: offloading output layer to GPU
0.00.052.951 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.962 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.052.963 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.053.302 I llama_init_from_model: n_seq_max     = 1
0.00.053.302 I llama_init_from_model: n_ctx         = 2048
0.00.053.303 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.053.303 I llama_init_from_model: n_batch       = 2048
0.00.053.303 I llama_init_from_model: n_ubatch      = 512
0.00.053.303 I llama_init_from_model: flash_attn    = 0
0.00.053.304 I llama_init_from_model: freq_base     = 10000.0
0.00.053.304 I llama_init_from_model: freq_scale    = 1
0.00.053.304 I ggml_metal_init: allocating
0.00.053.307 I ggml_metal_init: found device: Apple M4
0.00.053.309 I ggml_metal_init: picking default device: Apple M4
0.00.053.963 I ggml_metal_init: using embedded metal library
0.00.056.325 I ggml_metal_init: GPU name:   Apple M4
0.00.056.327 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.327 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.327 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.328 I ggml_metal_init: simdgroup reduction   = true
0.00.056.328 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.328 I ggml_metal_init: has bfloat            = true
0.00.056.328 I ggml_metal_init: use bfloat            = true
0.00.056.329 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.329 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.035 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.085.547 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.552 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.570 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.086.687 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.086.689 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.086.689 I llama_init_from_model: graph nodes  = 967
0.00.086.689 I llama_init_from_model: graph splits = 2
0.00.086.692 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.086.833 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.086.834 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.705.373 I main: llama threadpool init, n_threads = 4
0.00.705.417 I 
0.00.705.444 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.705.444 I 
0.00.705.585 I sampler seed: 1234
0.00.705.590 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.705.599 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.705.599 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.705.600 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.586.285 I llama_perf_sampler_print:    sampling time =       1.19 ms /    71 runs   (    0.02 ms per token, 59915.61 tokens per second)
0.01.586.287 I llama_perf_context_print:        load time =     694.73 ms
0.01.586.287 I llama_perf_context_print: prompt eval time =      51.69 ms /     7 tokens (    7.38 ms per token,   135.43 tokens per second)
0.01.586.288 I llama_perf_context_print:        eval time =     825.94 ms /    63 runs   (   13.11 ms per token,    76.28 tokens per second)
0.01.586.289 I llama_perf_context_print:       total time =     881.76 ms /    70 tokens
0.01.586.576 I ggml_metal_free: deallocating

real	0m1.606s
user	0m0.111s
sys	0m0.162s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.043 I build: 4529 (12c2bdf2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.081 I main: llama backend init
0.00.000.083 I main: load the model and apply lora adapter, if any
0.00.010.399 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.019.786 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.019.791 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.793 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.793 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.794 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.794 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.794 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.795 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.795 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.796 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.796 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.797 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.797 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.798 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.800 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.800 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.801 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.562 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.547 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.028.323 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.028.324 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.028.325 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.028.325 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.028.325 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.028.326 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.028.326 I llama_model_loader: - type  f32:  194 tensors
0.00.028.326 I llama_model_loader: - type q6_K:   98 tensors
0.00.028.327 I print_info: file format = GGUF V3 (latest)
0.00.028.328 I print_info: file type   = Q6_K
0.00.028.328 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.047.977 I load: special tokens cache size = 25
0.00.053.837 I load: token to piece cache size = 0.2984 MB
0.00.053.841 I print_info: arch             = gptneox
0.00.053.841 I print_info: vocab_only       = 0
0.00.053.841 I print_info: n_ctx_train      = 2048
0.00.053.841 I print_info: n_embd           = 2048
0.00.053.841 I print_info: n_layer          = 24
0.00.053.845 I print_info: n_head           = 16
0.00.053.846 I print_info: n_head_kv        = 16
0.00.053.846 I print_info: n_rot            = 32
0.00.053.846 I print_info: n_swa            = 0
0.00.053.846 I print_info: n_embd_head_k    = 128
0.00.053.846 I print_info: n_embd_head_v    = 128
0.00.053.847 I print_info: n_gqa            = 1
0.00.053.848 I print_info: n_embd_k_gqa     = 2048
0.00.053.849 I print_info: n_embd_v_gqa     = 2048
0.00.053.849 I print_info: f_norm_eps       = 1.0e-05
0.00.053.850 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.053.850 I print_info: f_clamp_kqv      = 0.0e+00
0.00.053.850 I print_info: f_max_alibi_bias = 0.0e+00
0.00.053.850 I print_info: f_logit_scale    = 0.0e+00
0.00.053.851 I print_info: n_ff             = 8192
0.00.053.853 I print_info: n_expert         = 0
0.00.053.853 I print_info: n_expert_used    = 0
0.00.053.853 I print_info: causal attn      = 1
0.00.053.853 I print_info: pooling type     = 0
0.00.053.853 I print_info: rope type        = 2
0.00.053.854 I print_info: rope scaling     = linear
0.00.053.856 I print_info: freq_base_train  = 10000.0
0.00.053.856 I print_info: freq_scale_train = 1
0.00.053.856 I print_info: n_ctx_orig_yarn  = 2048
0.00.053.857 I print_info: rope_finetuned   = unknown
0.00.053.857 I print_info: ssm_d_conv       = 0
0.00.053.857 I print_info: ssm_d_inner      = 0
0.00.053.857 I print_info: ssm_d_state      = 0
0.00.053.857 I print_info: ssm_dt_rank      = 0
0.00.053.857 I print_info: ssm_dt_b_c_rms   = 0
0.00.053.857 I print_info: model type       = 1.4B
0.00.053.858 I print_info: model params     = 1.41 B
0.00.053.858 I print_info: general.name     = 1.4B
0.00.053.858 I print_info: vocab type       = BPE
0.00.053.863 I print_info: n_vocab          = 50304
0.00.053.863 I print_info: n_merges         = 50009
0.00.053.863 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.053.863 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.053.863 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.053.864 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.053.866 I print_info: LF token         = 128 'Ä'
0.00.053.866 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.053.866 I print_info: max token length = 1024
0.00.055.529 I load_tensors: offloading 24 repeating layers to GPU
0.00.055.530 I load_tensors: offloading output layer to GPU
0.00.055.530 I load_tensors: offloaded 25/25 layers to GPU
0.00.055.540 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.055.541 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.055.812 I llama_init_from_model: n_seq_max     = 1
0.00.055.813 I llama_init_from_model: n_ctx         = 2048
0.00.055.813 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.055.813 I llama_init_from_model: n_batch       = 2048
0.00.055.813 I llama_init_from_model: n_ubatch      = 512
0.00.055.814 I llama_init_from_model: flash_attn    = 0
0.00.055.814 I llama_init_from_model: freq_base     = 10000.0
0.00.055.814 I llama_init_from_model: freq_scale    = 1
0.00.055.815 I ggml_metal_init: allocating
0.00.055.818 I ggml_metal_init: found device: Apple M4
0.00.055.820 I ggml_metal_init: picking default device: Apple M4
0.00.056.436 I ggml_metal_init: using embedded metal library
0.00.058.847 I ggml_metal_init: GPU name:   Apple M4
0.00.058.849 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.849 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.849 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.850 I ggml_metal_init: simdgroup reduction   = true
0.00.058.850 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.850 I ggml_metal_init: has bfloat            = true
0.00.058.850 I ggml_metal_init: use bfloat            = true
0.00.058.851 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.851 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.971 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.089.643 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.089.651 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.089.671 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.090.808 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.090.810 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.090.810 I llama_init_from_model: graph nodes  = 967
0.00.090.810 I llama_init_from_model: graph splits = 2
0.00.090.813 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.090.959 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.090.959 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.768.809 I main: llama threadpool init, n_threads = 4
0.00.768.844 I 
0.00.768.867 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.768.867 I 
0.00.769.082 I sampler seed: 1234
0.00.769.087 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.769.108 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.769.108 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.769.108 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.651.876 I llama_perf_sampler_print:    sampling time =       1.19 ms /    71 runs   (    0.02 ms per token, 59463.99 tokens per second)
0.01.651.876 I llama_perf_context_print:        load time =     757.57 ms
0.01.651.877 I llama_perf_context_print: prompt eval time =      54.40 ms /     7 tokens (    7.77 ms per token,   128.68 tokens per second)
0.01.651.878 I llama_perf_context_print:        eval time =     825.40 ms /    63 runs   (   13.10 ms per token,    76.33 tokens per second)
0.01.651.878 I llama_perf_context_print:       total time =     883.91 ms /    70 tokens
0.01.652.111 I ggml_metal_free: deallocating

real	0m1.670s
user	0m0.111s
sys	0m0.168s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.622 I build: 4529 (12c2bdf2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.024.636 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.037.618 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.037.624 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.037.625 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.037.626 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.037.626 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.037.627 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.037.632 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.037.634 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.037.634 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.037.634 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.037.637 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.037.637 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.037.638 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.037.638 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.037.642 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.037.643 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.037.644 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.044.236 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.046.866 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.053.397 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.053.399 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.053.399 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.053.400 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.053.400 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.053.401 I llama_model_loader: - type  f32:  194 tensors
0.00.053.401 I llama_model_loader: - type  f16:   98 tensors
0.00.053.402 I print_info: file format = GGUF V3 (latest)
0.00.053.403 I print_info: file type   = all F32 (guessed)
0.00.053.405 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.081.662 I load: special tokens cache size = 25
0.00.089.150 I load: token to piece cache size = 0.2984 MB
0.00.089.154 I print_info: arch             = gptneox
0.00.089.154 I print_info: vocab_only       = 0
0.00.089.154 I print_info: n_ctx_train      = 2048
0.00.089.154 I print_info: n_embd           = 2048
0.00.089.154 I print_info: n_layer          = 24
0.00.089.158 I print_info: n_head           = 16
0.00.089.158 I print_info: n_head_kv        = 16
0.00.089.158 I print_info: n_rot            = 32
0.00.089.159 I print_info: n_swa            = 0
0.00.089.159 I print_info: n_embd_head_k    = 128
0.00.089.159 I print_info: n_embd_head_v    = 128
0.00.089.160 I print_info: n_gqa            = 1
0.00.089.160 I print_info: n_embd_k_gqa     = 2048
0.00.089.161 I print_info: n_embd_v_gqa     = 2048
0.00.089.162 I print_info: f_norm_eps       = 1.0e-05
0.00.089.162 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.089.162 I print_info: f_clamp_kqv      = 0.0e+00
0.00.089.162 I print_info: f_max_alibi_bias = 0.0e+00
0.00.089.162 I print_info: f_logit_scale    = 0.0e+00
0.00.089.163 I print_info: n_ff             = 8192
0.00.089.163 I print_info: n_expert         = 0
0.00.089.163 I print_info: n_expert_used    = 0
0.00.089.163 I print_info: causal attn      = 1
0.00.089.163 I print_info: pooling type     = 0
0.00.089.164 I print_info: rope type        = 2
0.00.089.164 I print_info: rope scaling     = linear
0.00.089.164 I print_info: freq_base_train  = 10000.0
0.00.089.165 I print_info: freq_scale_train = 1
0.00.089.165 I print_info: n_ctx_orig_yarn  = 2048
0.00.089.165 I print_info: rope_finetuned   = unknown
0.00.089.166 I print_info: ssm_d_conv       = 0
0.00.089.166 I print_info: ssm_d_inner      = 0
0.00.089.166 I print_info: ssm_d_state      = 0
0.00.089.166 I print_info: ssm_dt_rank      = 0
0.00.089.167 I print_info: ssm_dt_b_c_rms   = 0
0.00.089.167 I print_info: model type       = 1.4B
0.00.089.167 I print_info: model params     = 1.41 B
0.00.089.167 I print_info: general.name     = 1.4B
0.00.089.168 I print_info: vocab type       = BPE
0.00.089.168 I print_info: n_vocab          = 50304
0.00.089.168 I print_info: n_merges         = 50009
0.00.089.169 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.089.169 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.089.169 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.089.169 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.089.169 I print_info: LF token         = 128 'Ä'
0.00.089.170 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.089.170 I print_info: max token length = 1024
0.00.091.710 I load_tensors: offloading 24 repeating layers to GPU
0.00.091.710 I load_tensors: offloading output layer to GPU
0.00.091.711 I load_tensors: offloaded 25/25 layers to GPU
0.00.091.721 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.091.723 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.091.985 I llama_init_from_model: n_seq_max     = 1
0.00.091.986 I llama_init_from_model: n_ctx         = 128
0.00.091.986 I llama_init_from_model: n_ctx_per_seq = 128
0.00.091.987 I llama_init_from_model: n_batch       = 128
0.00.091.987 I llama_init_from_model: n_ubatch      = 128
0.00.091.987 I llama_init_from_model: flash_attn    = 0
0.00.091.987 I llama_init_from_model: freq_base     = 10000.0
0.00.091.988 I llama_init_from_model: freq_scale    = 1
0.00.091.988 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.091.988 I ggml_metal_init: allocating
0.00.091.991 I ggml_metal_init: found device: Apple M4
0.00.091.993 I ggml_metal_init: picking default device: Apple M4
0.00.092.609 I ggml_metal_init: using embedded metal library
0.00.095.268 I ggml_metal_init: GPU name:   Apple M4
0.00.095.270 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.095.270 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.095.271 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.095.271 I ggml_metal_init: simdgroup reduction   = true
0.00.095.271 I ggml_metal_init: simdgroup matrix mul. = true
0.00.095.271 I ggml_metal_init: has bfloat            = true
0.00.095.271 I ggml_metal_init: use bfloat            = true
0.00.095.272 I ggml_metal_init: hasUnifiedMemory      = true
0.00.095.273 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.107.669 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.109.007 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.109.010 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.109.024 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.110.010 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.110.011 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.110.011 I llama_init_from_model: graph nodes  = 967
0.00.110.011 I llama_init_from_model: graph splits = 2
0.00.110.012 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.110.013 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.957.066 I 
0.00.957.158 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.957.195 I perplexity: tokenizing the input ..
0.00.969.047 I perplexity: tokenization took 11.85 ms
0.00.969.075 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.088.521 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.090.238 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.090.256 I llama_perf_context_print:        load time =     932.41 ms
0.01.090.257 I llama_perf_context_print: prompt eval time =     119.07 ms /   128 tokens (    0.93 ms per token,  1075.02 tokens per second)
0.01.090.259 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.090.266 I llama_perf_context_print:       total time =     133.20 ms /   129 tokens
0.01.090.678 I ggml_metal_free: deallocating

real	0m1.277s
user	0m0.120s
sys	0m0.198s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.133 I build: 4529 (12c2bdf2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.012.640 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.023.391 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.023.397 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.023.400 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.023.400 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.023.401 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.023.401 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.023.401 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.023.403 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.023.403 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.023.404 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.023.404 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.023.404 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.023.405 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.023.406 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.023.407 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.023.408 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.023.408 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.029.432 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.031.157 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.037.000 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.037.002 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.037.002 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.037.003 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.037.003 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.037.004 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.037.004 I llama_model_loader: - type  f32:  194 tensors
0.00.037.005 I llama_model_loader: - type q8_0:   98 tensors
0.00.037.005 I print_info: file format = GGUF V3 (latest)
0.00.037.008 I print_info: file type   = Q8_0
0.00.037.010 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.061.114 I load: special tokens cache size = 25
0.00.067.772 I load: token to piece cache size = 0.2984 MB
0.00.067.775 I print_info: arch             = gptneox
0.00.067.775 I print_info: vocab_only       = 0
0.00.067.776 I print_info: n_ctx_train      = 2048
0.00.067.776 I print_info: n_embd           = 2048
0.00.067.776 I print_info: n_layer          = 24
0.00.067.780 I print_info: n_head           = 16
0.00.067.781 I print_info: n_head_kv        = 16
0.00.067.781 I print_info: n_rot            = 32
0.00.067.781 I print_info: n_swa            = 0
0.00.067.781 I print_info: n_embd_head_k    = 128
0.00.067.781 I print_info: n_embd_head_v    = 128
0.00.067.782 I print_info: n_gqa            = 1
0.00.067.783 I print_info: n_embd_k_gqa     = 2048
0.00.067.783 I print_info: n_embd_v_gqa     = 2048
0.00.067.784 I print_info: f_norm_eps       = 1.0e-05
0.00.067.784 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.067.784 I print_info: f_clamp_kqv      = 0.0e+00
0.00.067.785 I print_info: f_max_alibi_bias = 0.0e+00
0.00.067.785 I print_info: f_logit_scale    = 0.0e+00
0.00.067.786 I print_info: n_ff             = 8192
0.00.067.786 I print_info: n_expert         = 0
0.00.067.786 I print_info: n_expert_used    = 0
0.00.067.786 I print_info: causal attn      = 1
0.00.067.786 I print_info: pooling type     = 0
0.00.067.786 I print_info: rope type        = 2
0.00.067.786 I print_info: rope scaling     = linear
0.00.067.789 I print_info: freq_base_train  = 10000.0
0.00.067.790 I print_info: freq_scale_train = 1
0.00.067.790 I print_info: n_ctx_orig_yarn  = 2048
0.00.067.790 I print_info: rope_finetuned   = unknown
0.00.067.790 I print_info: ssm_d_conv       = 0
0.00.067.791 I print_info: ssm_d_inner      = 0
0.00.067.791 I print_info: ssm_d_state      = 0
0.00.067.791 I print_info: ssm_dt_rank      = 0
0.00.067.791 I print_info: ssm_dt_b_c_rms   = 0
0.00.067.791 I print_info: model type       = 1.4B
0.00.067.791 I print_info: model params     = 1.41 B
0.00.067.792 I print_info: general.name     = 1.4B
0.00.067.792 I print_info: vocab type       = BPE
0.00.067.792 I print_info: n_vocab          = 50304
0.00.067.796 I print_info: n_merges         = 50009
0.00.067.796 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.067.797 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.067.797 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.067.797 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.067.797 I print_info: LF token         = 128 'Ä'
0.00.067.798 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.067.798 I print_info: max token length = 1024
0.00.070.177 I load_tensors: offloading 24 repeating layers to GPU
0.00.070.177 I load_tensors: offloading output layer to GPU
0.00.070.177 I load_tensors: offloaded 25/25 layers to GPU
0.00.070.189 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.070.190 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.00.070.504 I llama_init_from_model: n_seq_max     = 1
0.00.070.505 I llama_init_from_model: n_ctx         = 128
0.00.070.505 I llama_init_from_model: n_ctx_per_seq = 128
0.00.070.505 I llama_init_from_model: n_batch       = 128
0.00.070.505 I llama_init_from_model: n_ubatch      = 128
0.00.070.506 I llama_init_from_model: flash_attn    = 0
0.00.070.506 I llama_init_from_model: freq_base     = 10000.0
0.00.070.506 I llama_init_from_model: freq_scale    = 1
0.00.070.506 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.070.507 I ggml_metal_init: allocating
0.00.070.510 I ggml_metal_init: found device: Apple M4
0.00.070.512 I ggml_metal_init: picking default device: Apple M4
0.00.071.231 I ggml_metal_init: using embedded metal library
0.00.073.913 I ggml_metal_init: GPU name:   Apple M4
0.00.073.915 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.073.915 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.073.915 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.073.916 I ggml_metal_init: simdgroup reduction   = true
0.00.073.916 I ggml_metal_init: simdgroup matrix mul. = true
0.00.073.916 I ggml_metal_init: has bfloat            = true
0.00.073.916 I ggml_metal_init: use bfloat            = true
0.00.073.917 I ggml_metal_init: hasUnifiedMemory      = true
0.00.073.917 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.084.239 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.085.629 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.085.631 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.085.650 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.086.697 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.086.698 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.086.698 I llama_init_from_model: graph nodes  = 967
0.00.086.698 I llama_init_from_model: graph splits = 2
0.00.086.700 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.086.700 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.922.195 I 
0.00.922.227 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.922.236 I perplexity: tokenizing the input ..
0.00.930.352 I perplexity: tokenization took 8.115 ms
0.00.930.363 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.054.721 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.055.962 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.055.976 I llama_perf_context_print:        load time =     909.55 ms
0.01.055.977 I llama_perf_context_print: prompt eval time =     124.12 ms /   128 tokens (    0.97 ms per token,  1031.23 tokens per second)
0.01.055.977 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.055.978 I llama_perf_context_print:       total time =     133.78 ms /   129 tokens
0.01.056.473 I ggml_metal_free: deallocating

real	0m1.075s
user	0m0.096s
sys	0m0.140s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.089 I build: 4529 (12c2bdf2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.255 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.793 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.016.797 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.799 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.799 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.800 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.800 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.800 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.801 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.802 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.802 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.802 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.803 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.803 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.804 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.805 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.806 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.806 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.005 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.113 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.017 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.019 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.019 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.020 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.020 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.020 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.026.021 I llama_model_loader: - type  f32:  194 tensors
0.00.026.021 I llama_model_loader: - type q4_0:   97 tensors
0.00.026.021 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.022 I print_info: file format = GGUF V3 (latest)
0.00.026.023 I print_info: file type   = Q4_0
0.00.026.024 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.045.581 I load: special tokens cache size = 25
0.00.051.815 I load: token to piece cache size = 0.2984 MB
0.00.051.827 I print_info: arch             = gptneox
0.00.051.828 I print_info: vocab_only       = 0
0.00.051.828 I print_info: n_ctx_train      = 2048
0.00.051.829 I print_info: n_embd           = 2048
0.00.051.829 I print_info: n_layer          = 24
0.00.051.834 I print_info: n_head           = 16
0.00.051.835 I print_info: n_head_kv        = 16
0.00.051.835 I print_info: n_rot            = 32
0.00.051.835 I print_info: n_swa            = 0
0.00.051.835 I print_info: n_embd_head_k    = 128
0.00.051.835 I print_info: n_embd_head_v    = 128
0.00.051.836 I print_info: n_gqa            = 1
0.00.051.837 I print_info: n_embd_k_gqa     = 2048
0.00.051.837 I print_info: n_embd_v_gqa     = 2048
0.00.051.838 I print_info: f_norm_eps       = 1.0e-05
0.00.051.838 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.838 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.838 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.838 I print_info: f_logit_scale    = 0.0e+00
0.00.051.839 I print_info: n_ff             = 8192
0.00.051.839 I print_info: n_expert         = 0
0.00.051.839 I print_info: n_expert_used    = 0
0.00.051.840 I print_info: causal attn      = 1
0.00.051.840 I print_info: pooling type     = 0
0.00.051.840 I print_info: rope type        = 2
0.00.051.840 I print_info: rope scaling     = linear
0.00.051.840 I print_info: freq_base_train  = 10000.0
0.00.051.841 I print_info: freq_scale_train = 1
0.00.051.841 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.841 I print_info: rope_finetuned   = unknown
0.00.051.841 I print_info: ssm_d_conv       = 0
0.00.051.841 I print_info: ssm_d_inner      = 0
0.00.051.841 I print_info: ssm_d_state      = 0
0.00.051.842 I print_info: ssm_dt_rank      = 0
0.00.051.842 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.842 I print_info: model type       = 1.4B
0.00.051.842 I print_info: model params     = 1.41 B
0.00.051.842 I print_info: general.name     = 1.4B
0.00.051.843 I print_info: vocab type       = BPE
0.00.051.843 I print_info: n_vocab          = 50304
0.00.051.843 I print_info: n_merges         = 50009
0.00.051.843 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.844 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.844 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.844 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.844 I print_info: LF token         = 128 'Ä'
0.00.051.844 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.844 I print_info: max token length = 1024
0.00.053.823 I load_tensors: offloading 24 repeating layers to GPU
0.00.053.824 I load_tensors: offloading output layer to GPU
0.00.053.824 I load_tensors: offloaded 25/25 layers to GPU
0.00.053.835 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.053.837 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.054.164 I llama_init_from_model: n_seq_max     = 1
0.00.054.165 I llama_init_from_model: n_ctx         = 128
0.00.054.165 I llama_init_from_model: n_ctx_per_seq = 128
0.00.054.166 I llama_init_from_model: n_batch       = 128
0.00.054.166 I llama_init_from_model: n_ubatch      = 128
0.00.054.166 I llama_init_from_model: flash_attn    = 0
0.00.054.166 I llama_init_from_model: freq_base     = 10000.0
0.00.054.166 I llama_init_from_model: freq_scale    = 1
0.00.054.167 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.167 I ggml_metal_init: allocating
0.00.054.171 I ggml_metal_init: found device: Apple M4
0.00.054.173 I ggml_metal_init: picking default device: Apple M4
0.00.054.795 I ggml_metal_init: using embedded metal library
0.00.057.386 I ggml_metal_init: GPU name:   Apple M4
0.00.057.388 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.388 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.388 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.389 I ggml_metal_init: simdgroup reduction   = true
0.00.057.389 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.389 I ggml_metal_init: has bfloat            = true
0.00.057.389 I ggml_metal_init: use bfloat            = true
0.00.057.389 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.390 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.288 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.068.657 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.659 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.677 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.069.592 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.069.593 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.069.593 I llama_init_from_model: graph nodes  = 967
0.00.069.593 I llama_init_from_model: graph splits = 2
0.00.069.594 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.069.595 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.632.486 I 
0.00.632.527 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.632.541 I perplexity: tokenizing the input ..
0.00.640.565 I perplexity: tokenization took 8.022 ms
0.00.640.577 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.762.613 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.763.948 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.763.961 I llama_perf_context_print:        load time =     622.23 ms
0.00.763.965 I llama_perf_context_print: prompt eval time =     121.80 ms /   128 tokens (    0.95 ms per token,  1050.94 tokens per second)
0.00.763.966 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.763.966 I llama_perf_context_print:       total time =     131.48 ms /   129 tokens
0.00.764.320 I ggml_metal_free: deallocating

real	0m0.780s
user	0m0.080s
sys	0m0.097s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.091 I build: 4529 (12c2bdf2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.509 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.019.140 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.019.144 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.146 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.147 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.147 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.147 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.148 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.149 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.149 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.149 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.150 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.151 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.151 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.152 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.154 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.154 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.154 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.302 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.419 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.028.520 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.028.521 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.028.522 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.028.522 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.028.522 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.028.523 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.028.523 I llama_model_loader: - type  f32:  194 tensors
0.00.028.524 I llama_model_loader: - type q4_1:   97 tensors
0.00.028.524 I llama_model_loader: - type q6_K:    1 tensors
0.00.028.525 I print_info: file format = GGUF V3 (latest)
0.00.028.525 I print_info: file type   = Q4_1
0.00.028.526 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.048.952 I load: special tokens cache size = 25
0.00.055.103 I load: token to piece cache size = 0.2984 MB
0.00.055.108 I print_info: arch             = gptneox
0.00.055.109 I print_info: vocab_only       = 0
0.00.055.109 I print_info: n_ctx_train      = 2048
0.00.055.109 I print_info: n_embd           = 2048
0.00.055.110 I print_info: n_layer          = 24
0.00.055.115 I print_info: n_head           = 16
0.00.055.115 I print_info: n_head_kv        = 16
0.00.055.116 I print_info: n_rot            = 32
0.00.055.116 I print_info: n_swa            = 0
0.00.055.116 I print_info: n_embd_head_k    = 128
0.00.055.116 I print_info: n_embd_head_v    = 128
0.00.055.117 I print_info: n_gqa            = 1
0.00.055.117 I print_info: n_embd_k_gqa     = 2048
0.00.055.118 I print_info: n_embd_v_gqa     = 2048
0.00.055.118 I print_info: f_norm_eps       = 1.0e-05
0.00.055.119 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.055.119 I print_info: f_clamp_kqv      = 0.0e+00
0.00.055.119 I print_info: f_max_alibi_bias = 0.0e+00
0.00.055.119 I print_info: f_logit_scale    = 0.0e+00
0.00.055.120 I print_info: n_ff             = 8192
0.00.055.122 I print_info: n_expert         = 0
0.00.055.122 I print_info: n_expert_used    = 0
0.00.055.123 I print_info: causal attn      = 1
0.00.055.123 I print_info: pooling type     = 0
0.00.055.123 I print_info: rope type        = 2
0.00.055.123 I print_info: rope scaling     = linear
0.00.055.123 I print_info: freq_base_train  = 10000.0
0.00.055.124 I print_info: freq_scale_train = 1
0.00.055.124 I print_info: n_ctx_orig_yarn  = 2048
0.00.055.124 I print_info: rope_finetuned   = unknown
0.00.055.124 I print_info: ssm_d_conv       = 0
0.00.055.124 I print_info: ssm_d_inner      = 0
0.00.055.124 I print_info: ssm_d_state      = 0
0.00.055.125 I print_info: ssm_dt_rank      = 0
0.00.055.125 I print_info: ssm_dt_b_c_rms   = 0
0.00.055.125 I print_info: model type       = 1.4B
0.00.055.125 I print_info: model params     = 1.41 B
0.00.055.125 I print_info: general.name     = 1.4B
0.00.055.126 I print_info: vocab type       = BPE
0.00.055.126 I print_info: n_vocab          = 50304
0.00.055.126 I print_info: n_merges         = 50009
0.00.055.129 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.055.129 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.055.129 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.055.129 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.055.129 I print_info: LF token         = 128 'Ä'
0.00.055.129 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.055.130 I print_info: max token length = 1024
0.00.057.894 I load_tensors: offloading 24 repeating layers to GPU
0.00.057.894 I load_tensors: offloading output layer to GPU
0.00.057.894 I load_tensors: offloaded 25/25 layers to GPU
0.00.057.905 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.057.907 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.058.299 I llama_init_from_model: n_seq_max     = 1
0.00.058.300 I llama_init_from_model: n_ctx         = 128
0.00.058.300 I llama_init_from_model: n_ctx_per_seq = 128
0.00.058.300 I llama_init_from_model: n_batch       = 128
0.00.058.300 I llama_init_from_model: n_ubatch      = 128
0.00.058.300 I llama_init_from_model: flash_attn    = 0
0.00.058.301 I llama_init_from_model: freq_base     = 10000.0
0.00.058.301 I llama_init_from_model: freq_scale    = 1
0.00.058.301 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.058.302 I ggml_metal_init: allocating
0.00.058.305 I ggml_metal_init: found device: Apple M4
0.00.058.307 I ggml_metal_init: picking default device: Apple M4
0.00.058.907 I ggml_metal_init: using embedded metal library
0.00.061.690 I ggml_metal_init: GPU name:   Apple M4
0.00.061.692 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.061.693 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.061.693 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.061.693 I ggml_metal_init: simdgroup reduction   = true
0.00.061.693 I ggml_metal_init: simdgroup matrix mul. = true
0.00.061.694 I ggml_metal_init: has bfloat            = true
0.00.061.694 I ggml_metal_init: use bfloat            = true
0.00.061.694 I ggml_metal_init: hasUnifiedMemory      = true
0.00.061.695 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.070.977 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.072.269 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.072.275 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.072.289 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.073.140 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.073.141 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.073.142 I llama_init_from_model: graph nodes  = 967
0.00.073.142 I llama_init_from_model: graph splits = 2
0.00.073.143 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.073.143 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.693.928 I 
0.00.693.963 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.693.973 I perplexity: tokenizing the input ..
0.00.701.039 I perplexity: tokenization took 7.064 ms
0.00.701.050 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.823.307 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.824.705 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.824.720 I llama_perf_context_print:        load time =     683.41 ms
0.00.824.721 I llama_perf_context_print: prompt eval time =     122.00 ms /   128 tokens (    0.95 ms per token,  1049.19 tokens per second)
0.00.824.722 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.824.722 I llama_perf_context_print:       total time =     130.80 ms /   129 tokens
0.00.825.099 I ggml_metal_free: deallocating

real	0m0.840s
user	0m0.079s
sys	0m0.089s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.092 I build: 4529 (12c2bdf2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.608 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.935 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.018.940 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.941 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.942 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.942 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.943 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.943 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.944 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.945 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.945 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.945 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.946 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.946 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.947 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.948 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.948 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.948 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.165 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.327 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.028.525 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.028.527 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.028.527 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.028.527 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.028.528 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.028.528 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.028.528 I llama_model_loader: - type  f32:  194 tensors
0.00.028.529 I llama_model_loader: - type q5_0:   97 tensors
0.00.028.529 I llama_model_loader: - type q6_K:    1 tensors
0.00.028.530 I print_info: file format = GGUF V3 (latest)
0.00.028.530 I print_info: file type   = Q5_0
0.00.028.534 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.048.882 I load: special tokens cache size = 25
0.00.055.088 I load: token to piece cache size = 0.2984 MB
0.00.055.093 I print_info: arch             = gptneox
0.00.055.093 I print_info: vocab_only       = 0
0.00.055.093 I print_info: n_ctx_train      = 2048
0.00.055.094 I print_info: n_embd           = 2048
0.00.055.094 I print_info: n_layer          = 24
0.00.055.098 I print_info: n_head           = 16
0.00.055.099 I print_info: n_head_kv        = 16
0.00.055.099 I print_info: n_rot            = 32
0.00.055.099 I print_info: n_swa            = 0
0.00.055.099 I print_info: n_embd_head_k    = 128
0.00.055.099 I print_info: n_embd_head_v    = 128
0.00.055.100 I print_info: n_gqa            = 1
0.00.055.101 I print_info: n_embd_k_gqa     = 2048
0.00.055.102 I print_info: n_embd_v_gqa     = 2048
0.00.055.102 I print_info: f_norm_eps       = 1.0e-05
0.00.055.103 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.055.104 I print_info: f_clamp_kqv      = 0.0e+00
0.00.055.104 I print_info: f_max_alibi_bias = 0.0e+00
0.00.055.104 I print_info: f_logit_scale    = 0.0e+00
0.00.055.105 I print_info: n_ff             = 8192
0.00.055.105 I print_info: n_expert         = 0
0.00.055.105 I print_info: n_expert_used    = 0
0.00.055.106 I print_info: causal attn      = 1
0.00.055.106 I print_info: pooling type     = 0
0.00.055.106 I print_info: rope type        = 2
0.00.055.106 I print_info: rope scaling     = linear
0.00.055.108 I print_info: freq_base_train  = 10000.0
0.00.055.109 I print_info: freq_scale_train = 1
0.00.055.109 I print_info: n_ctx_orig_yarn  = 2048
0.00.055.110 I print_info: rope_finetuned   = unknown
0.00.055.110 I print_info: ssm_d_conv       = 0
0.00.055.110 I print_info: ssm_d_inner      = 0
0.00.055.110 I print_info: ssm_d_state      = 0
0.00.055.110 I print_info: ssm_dt_rank      = 0
0.00.055.110 I print_info: ssm_dt_b_c_rms   = 0
0.00.055.111 I print_info: model type       = 1.4B
0.00.055.111 I print_info: model params     = 1.41 B
0.00.055.111 I print_info: general.name     = 1.4B
0.00.055.113 I print_info: vocab type       = BPE
0.00.055.113 I print_info: n_vocab          = 50304
0.00.055.113 I print_info: n_merges         = 50009
0.00.055.113 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.055.114 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.055.114 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.055.114 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.055.114 I print_info: LF token         = 128 'Ä'
0.00.055.114 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.055.114 I print_info: max token length = 1024
0.00.057.214 I load_tensors: offloading 24 repeating layers to GPU
0.00.057.215 I load_tensors: offloading output layer to GPU
0.00.057.215 I load_tensors: offloaded 25/25 layers to GPU
0.00.057.226 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.057.227 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.057.521 I llama_init_from_model: n_seq_max     = 1
0.00.057.522 I llama_init_from_model: n_ctx         = 128
0.00.057.522 I llama_init_from_model: n_ctx_per_seq = 128
0.00.057.523 I llama_init_from_model: n_batch       = 128
0.00.057.523 I llama_init_from_model: n_ubatch      = 128
0.00.057.523 I llama_init_from_model: flash_attn    = 0
0.00.057.523 I llama_init_from_model: freq_base     = 10000.0
0.00.057.524 I llama_init_from_model: freq_scale    = 1
0.00.057.524 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.057.525 I ggml_metal_init: allocating
0.00.057.528 I ggml_metal_init: found device: Apple M4
0.00.057.530 I ggml_metal_init: picking default device: Apple M4
0.00.058.147 I ggml_metal_init: using embedded metal library
0.00.060.562 I ggml_metal_init: GPU name:   Apple M4
0.00.060.564 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.060.564 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.060.565 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.060.565 I ggml_metal_init: simdgroup reduction   = true
0.00.060.565 I ggml_metal_init: simdgroup matrix mul. = true
0.00.060.566 I ggml_metal_init: has bfloat            = true
0.00.060.566 I ggml_metal_init: use bfloat            = true
0.00.060.566 I ggml_metal_init: hasUnifiedMemory      = true
0.00.060.567 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.070.853 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.072.138 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.072.140 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.072.157 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.073.038 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.073.039 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.073.039 I llama_init_from_model: graph nodes  = 967
0.00.073.040 I llama_init_from_model: graph splits = 2
0.00.073.041 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.073.041 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.711.754 I 
0.00.711.802 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.711.815 I perplexity: tokenizing the input ..
0.00.720.103 I perplexity: tokenization took 8.287 ms
0.00.720.114 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.855.177 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.856.389 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.856.407 I llama_perf_context_print:        load time =     700.14 ms
0.00.856.408 I llama_perf_context_print: prompt eval time =     134.84 ms /   128 tokens (    1.05 ms per token,   949.30 tokens per second)
0.00.856.409 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.856.409 I llama_perf_context_print:       total time =     144.65 ms /   129 tokens
0.00.856.973 I ggml_metal_free: deallocating

real	0m0.873s
user	0m0.082s
sys	0m0.111s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.084 I build: 4529 (12c2bdf2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.884 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.507 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.015.511 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.513 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.514 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.514 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.514 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.515 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.516 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.516 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.516 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.517 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.517 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.517 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.518 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.521 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.521 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.521 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.438 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.608 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.572 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.573 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.574 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.574 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.574 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.575 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.575 I llama_model_loader: - type  f32:  194 tensors
0.00.024.576 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.576 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.576 I print_info: file format = GGUF V3 (latest)
0.00.024.577 I print_info: file type   = Q5_1
0.00.024.578 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.043.920 I load: special tokens cache size = 25
0.00.050.014 I load: token to piece cache size = 0.2984 MB
0.00.050.017 I print_info: arch             = gptneox
0.00.050.017 I print_info: vocab_only       = 0
0.00.050.017 I print_info: n_ctx_train      = 2048
0.00.050.018 I print_info: n_embd           = 2048
0.00.050.018 I print_info: n_layer          = 24
0.00.050.020 I print_info: n_head           = 16
0.00.050.021 I print_info: n_head_kv        = 16
0.00.050.021 I print_info: n_rot            = 32
0.00.050.022 I print_info: n_swa            = 0
0.00.050.023 I print_info: n_embd_head_k    = 128
0.00.050.023 I print_info: n_embd_head_v    = 128
0.00.050.024 I print_info: n_gqa            = 1
0.00.050.025 I print_info: n_embd_k_gqa     = 2048
0.00.050.026 I print_info: n_embd_v_gqa     = 2048
0.00.050.027 I print_info: f_norm_eps       = 1.0e-05
0.00.050.027 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.027 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.027 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.027 I print_info: f_logit_scale    = 0.0e+00
0.00.050.028 I print_info: n_ff             = 8192
0.00.050.028 I print_info: n_expert         = 0
0.00.050.028 I print_info: n_expert_used    = 0
0.00.050.028 I print_info: causal attn      = 1
0.00.050.029 I print_info: pooling type     = 0
0.00.050.029 I print_info: rope type        = 2
0.00.050.029 I print_info: rope scaling     = linear
0.00.050.032 I print_info: freq_base_train  = 10000.0
0.00.050.032 I print_info: freq_scale_train = 1
0.00.050.033 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.033 I print_info: rope_finetuned   = unknown
0.00.050.033 I print_info: ssm_d_conv       = 0
0.00.050.033 I print_info: ssm_d_inner      = 0
0.00.050.033 I print_info: ssm_d_state      = 0
0.00.050.033 I print_info: ssm_dt_rank      = 0
0.00.050.033 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.034 I print_info: model type       = 1.4B
0.00.050.034 I print_info: model params     = 1.41 B
0.00.050.034 I print_info: general.name     = 1.4B
0.00.050.039 I print_info: vocab type       = BPE
0.00.050.039 I print_info: n_vocab          = 50304
0.00.050.039 I print_info: n_merges         = 50009
0.00.050.039 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.040 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.040 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.040 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.040 I print_info: LF token         = 128 'Ä'
0.00.050.040 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.040 I print_info: max token length = 1024
0.00.051.624 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.624 I load_tensors: offloading output layer to GPU
0.00.051.624 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.634 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.051.635 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.051.895 I llama_init_from_model: n_seq_max     = 1
0.00.051.896 I llama_init_from_model: n_ctx         = 128
0.00.051.896 I llama_init_from_model: n_ctx_per_seq = 128
0.00.051.896 I llama_init_from_model: n_batch       = 128
0.00.051.896 I llama_init_from_model: n_ubatch      = 128
0.00.051.896 I llama_init_from_model: flash_attn    = 0
0.00.051.897 I llama_init_from_model: freq_base     = 10000.0
0.00.051.897 I llama_init_from_model: freq_scale    = 1
0.00.051.897 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.898 I ggml_metal_init: allocating
0.00.051.901 I ggml_metal_init: found device: Apple M4
0.00.051.903 I ggml_metal_init: picking default device: Apple M4
0.00.052.459 I ggml_metal_init: using embedded metal library
0.00.054.775 I ggml_metal_init: GPU name:   Apple M4
0.00.054.776 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.777 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.777 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.777 I ggml_metal_init: simdgroup reduction   = true
0.00.054.778 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.778 I ggml_metal_init: has bfloat            = true
0.00.054.778 I ggml_metal_init: use bfloat            = true
0.00.054.778 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.779 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.381 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.065.654 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.656 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.670 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.066.524 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.066.525 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.066.525 I llama_init_from_model: graph nodes  = 967
0.00.066.526 I llama_init_from_model: graph splits = 2
0.00.066.527 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.527 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.617.894 I 
0.00.617.934 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.617.949 I perplexity: tokenizing the input ..
0.00.625.549 I perplexity: tokenization took 7.599 ms
0.00.625.560 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.760.577 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.761.771 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.761.795 I llama_perf_context_print:        load time =     609.00 ms
0.00.761.796 I llama_perf_context_print: prompt eval time =     134.78 ms /   128 tokens (    1.05 ms per token,   949.66 tokens per second)
0.00.761.797 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.761.797 I llama_perf_context_print:       total time =     143.90 ms /   129 tokens
0.00.762.328 I ggml_metal_free: deallocating

real	0m0.777s
user	0m0.078s
sys	0m0.118s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.084 I build: 4529 (12c2bdf2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.326 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.922 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.927 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.928 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.929 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.929 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.929 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.930 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.931 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.931 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.932 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.932 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.932 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.933 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.933 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.934 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.935 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.935 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.842 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.887 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.790 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.791 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.792 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.792 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.792 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.793 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.793 I llama_model_loader: - type  f32:  194 tensors
0.00.024.794 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.794 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.794 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.795 I print_info: file format = GGUF V3 (latest)
0.00.024.795 I print_info: file type   = Q2_K - Medium
0.00.024.797 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.043.338 I load: special tokens cache size = 25
0.00.049.162 I load: token to piece cache size = 0.2984 MB
0.00.049.165 I print_info: arch             = gptneox
0.00.049.165 I print_info: vocab_only       = 0
0.00.049.165 I print_info: n_ctx_train      = 2048
0.00.049.165 I print_info: n_embd           = 2048
0.00.049.165 I print_info: n_layer          = 24
0.00.049.168 I print_info: n_head           = 16
0.00.049.169 I print_info: n_head_kv        = 16
0.00.049.169 I print_info: n_rot            = 32
0.00.049.169 I print_info: n_swa            = 0
0.00.049.172 I print_info: n_embd_head_k    = 128
0.00.049.172 I print_info: n_embd_head_v    = 128
0.00.049.173 I print_info: n_gqa            = 1
0.00.049.174 I print_info: n_embd_k_gqa     = 2048
0.00.049.174 I print_info: n_embd_v_gqa     = 2048
0.00.049.175 I print_info: f_norm_eps       = 1.0e-05
0.00.049.175 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.175 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.176 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.176 I print_info: f_logit_scale    = 0.0e+00
0.00.049.176 I print_info: n_ff             = 8192
0.00.049.177 I print_info: n_expert         = 0
0.00.049.177 I print_info: n_expert_used    = 0
0.00.049.177 I print_info: causal attn      = 1
0.00.049.177 I print_info: pooling type     = 0
0.00.049.177 I print_info: rope type        = 2
0.00.049.177 I print_info: rope scaling     = linear
0.00.049.181 I print_info: freq_base_train  = 10000.0
0.00.049.182 I print_info: freq_scale_train = 1
0.00.049.182 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.182 I print_info: rope_finetuned   = unknown
0.00.049.183 I print_info: ssm_d_conv       = 0
0.00.049.183 I print_info: ssm_d_inner      = 0
0.00.049.183 I print_info: ssm_d_state      = 0
0.00.049.183 I print_info: ssm_dt_rank      = 0
0.00.049.183 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.183 I print_info: model type       = 1.4B
0.00.049.184 I print_info: model params     = 1.41 B
0.00.049.185 I print_info: general.name     = 1.4B
0.00.049.186 I print_info: vocab type       = BPE
0.00.049.186 I print_info: n_vocab          = 50304
0.00.049.186 I print_info: n_merges         = 50009
0.00.049.186 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.187 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.187 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.187 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.187 I print_info: LF token         = 128 'Ä'
0.00.049.187 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.187 I print_info: max token length = 1024
0.00.051.050 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.050 I load_tensors: offloading output layer to GPU
0.00.051.050 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.061 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.051.062 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.051.337 I llama_init_from_model: n_seq_max     = 1
0.00.051.338 I llama_init_from_model: n_ctx         = 128
0.00.051.338 I llama_init_from_model: n_ctx_per_seq = 128
0.00.051.338 I llama_init_from_model: n_batch       = 128
0.00.051.338 I llama_init_from_model: n_ubatch      = 128
0.00.051.338 I llama_init_from_model: flash_attn    = 0
0.00.051.339 I llama_init_from_model: freq_base     = 10000.0
0.00.051.339 I llama_init_from_model: freq_scale    = 1
0.00.051.339 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.340 I ggml_metal_init: allocating
0.00.051.343 I ggml_metal_init: found device: Apple M4
0.00.051.344 I ggml_metal_init: picking default device: Apple M4
0.00.051.925 I ggml_metal_init: using embedded metal library
0.00.054.287 I ggml_metal_init: GPU name:   Apple M4
0.00.054.289 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.289 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.290 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.290 I ggml_metal_init: simdgroup reduction   = true
0.00.054.290 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.290 I ggml_metal_init: has bfloat            = true
0.00.054.290 I ggml_metal_init: use bfloat            = true
0.00.054.291 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.291 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.889 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.065.200 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.203 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.217 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.066.153 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.066.154 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.066.155 I llama_init_from_model: graph nodes  = 967
0.00.066.155 I llama_init_from_model: graph splits = 2
0.00.066.156 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.156 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.372.772 I 
0.00.372.817 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.372.827 I perplexity: tokenizing the input ..
0.00.380.400 I perplexity: tokenization took 7.571 ms
0.00.380.416 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.512.626 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.513.788 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.513.805 I llama_perf_context_print:        load time =     363.44 ms
0.00.513.806 I llama_perf_context_print: prompt eval time =     131.98 ms /   128 tokens (    1.03 ms per token,   969.81 tokens per second)
0.00.513.807 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.513.807 I llama_perf_context_print:       total time =     141.03 ms /   129 tokens
0.00.514.231 I ggml_metal_free: deallocating

real	0m0.530s
user	0m0.077s
sys	0m0.063s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.083 I build: 4529 (12c2bdf2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.818 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.837 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.015.842 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.844 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.844 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.844 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.845 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.845 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.846 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.846 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.847 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.847 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.848 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.848 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.848 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.851 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.851 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.852 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.820 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.864 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.780 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.781 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.782 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.782 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.782 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.782 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.783 I llama_model_loader: - type  f32:  194 tensors
0.00.024.783 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.783 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.784 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.784 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.784 I print_info: file format = GGUF V3 (latest)
0.00.024.785 I print_info: file type   = Q3_K - Medium
0.00.024.786 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.043.366 I load: special tokens cache size = 25
0.00.049.401 I load: token to piece cache size = 0.2984 MB
0.00.049.403 I print_info: arch             = gptneox
0.00.049.404 I print_info: vocab_only       = 0
0.00.049.404 I print_info: n_ctx_train      = 2048
0.00.049.404 I print_info: n_embd           = 2048
0.00.049.404 I print_info: n_layer          = 24
0.00.049.407 I print_info: n_head           = 16
0.00.049.408 I print_info: n_head_kv        = 16
0.00.049.408 I print_info: n_rot            = 32
0.00.049.408 I print_info: n_swa            = 0
0.00.049.411 I print_info: n_embd_head_k    = 128
0.00.049.411 I print_info: n_embd_head_v    = 128
0.00.049.412 I print_info: n_gqa            = 1
0.00.049.412 I print_info: n_embd_k_gqa     = 2048
0.00.049.413 I print_info: n_embd_v_gqa     = 2048
0.00.049.414 I print_info: f_norm_eps       = 1.0e-05
0.00.049.414 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.414 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.414 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.414 I print_info: f_logit_scale    = 0.0e+00
0.00.049.415 I print_info: n_ff             = 8192
0.00.049.415 I print_info: n_expert         = 0
0.00.049.415 I print_info: n_expert_used    = 0
0.00.049.415 I print_info: causal attn      = 1
0.00.049.415 I print_info: pooling type     = 0
0.00.049.416 I print_info: rope type        = 2
0.00.049.416 I print_info: rope scaling     = linear
0.00.049.420 I print_info: freq_base_train  = 10000.0
0.00.049.420 I print_info: freq_scale_train = 1
0.00.049.421 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.422 I print_info: rope_finetuned   = unknown
0.00.049.422 I print_info: ssm_d_conv       = 0
0.00.049.423 I print_info: ssm_d_inner      = 0
0.00.049.423 I print_info: ssm_d_state      = 0
0.00.049.423 I print_info: ssm_dt_rank      = 0
0.00.049.423 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.424 I print_info: model type       = 1.4B
0.00.049.424 I print_info: model params     = 1.41 B
0.00.049.424 I print_info: general.name     = 1.4B
0.00.049.425 I print_info: vocab type       = BPE
0.00.049.426 I print_info: n_vocab          = 50304
0.00.049.426 I print_info: n_merges         = 50009
0.00.049.427 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.427 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.427 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.427 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.427 I print_info: LF token         = 128 'Ä'
0.00.049.428 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.428 I print_info: max token length = 1024
0.00.051.333 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.333 I load_tensors: offloading output layer to GPU
0.00.051.333 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.344 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.051.345 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.051.617 I llama_init_from_model: n_seq_max     = 1
0.00.051.618 I llama_init_from_model: n_ctx         = 128
0.00.051.618 I llama_init_from_model: n_ctx_per_seq = 128
0.00.051.618 I llama_init_from_model: n_batch       = 128
0.00.051.618 I llama_init_from_model: n_ubatch      = 128
0.00.051.618 I llama_init_from_model: flash_attn    = 0
0.00.051.619 I llama_init_from_model: freq_base     = 10000.0
0.00.051.619 I llama_init_from_model: freq_scale    = 1
0.00.051.619 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.620 I ggml_metal_init: allocating
0.00.051.623 I ggml_metal_init: found device: Apple M4
0.00.051.625 I ggml_metal_init: picking default device: Apple M4
0.00.052.192 I ggml_metal_init: using embedded metal library
0.00.054.541 I ggml_metal_init: GPU name:   Apple M4
0.00.054.542 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.542 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.543 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.543 I ggml_metal_init: simdgroup reduction   = true
0.00.054.543 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.543 I ggml_metal_init: has bfloat            = true
0.00.054.543 I ggml_metal_init: use bfloat            = true
0.00.054.544 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.544 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.047 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.065.263 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.266 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.280 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.066.119 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.066.120 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.066.121 I llama_init_from_model: graph nodes  = 967
0.00.066.121 I llama_init_from_model: graph splits = 2
0.00.066.122 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.122 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.479.643 I 
0.00.479.693 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.479.769 I perplexity: tokenizing the input ..
0.00.487.937 I perplexity: tokenization took 8.167 ms
0.00.487.948 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.620.296 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.621.454 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.621.472 I llama_perf_context_print:        load time =     470.82 ms
0.00.621.473 I llama_perf_context_print: prompt eval time =     132.12 ms /   128 tokens (    1.03 ms per token,   968.85 tokens per second)
0.00.621.474 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.621.474 I llama_perf_context_print:       total time =     141.83 ms /   129 tokens
0.00.622.026 I ggml_metal_free: deallocating

real	0m0.636s
user	0m0.078s
sys	0m0.087s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.082 I build: 4529 (12c2bdf2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.196 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.189 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.017.194 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.200 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.201 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.201 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.201 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.202 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.202 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.203 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.203 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.204 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.204 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.204 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.205 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.206 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.207 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.207 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.221 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.331 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.338 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.339 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.339 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.340 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.340 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.340 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.026.341 I llama_model_loader: - type  f32:  194 tensors
0.00.026.341 I llama_model_loader: - type q4_K:   61 tensors
0.00.026.341 I llama_model_loader: - type q5_K:   24 tensors
0.00.026.342 I llama_model_loader: - type q6_K:   13 tensors
0.00.026.342 I print_info: file format = GGUF V3 (latest)
0.00.026.343 I print_info: file type   = Q4_K - Medium
0.00.026.344 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.045.769 I load: special tokens cache size = 25
0.00.051.867 I load: token to piece cache size = 0.2984 MB
0.00.051.870 I print_info: arch             = gptneox
0.00.051.870 I print_info: vocab_only       = 0
0.00.051.871 I print_info: n_ctx_train      = 2048
0.00.051.871 I print_info: n_embd           = 2048
0.00.051.871 I print_info: n_layer          = 24
0.00.051.874 I print_info: n_head           = 16
0.00.051.875 I print_info: n_head_kv        = 16
0.00.051.875 I print_info: n_rot            = 32
0.00.051.875 I print_info: n_swa            = 0
0.00.051.875 I print_info: n_embd_head_k    = 128
0.00.051.878 I print_info: n_embd_head_v    = 128
0.00.051.878 I print_info: n_gqa            = 1
0.00.051.879 I print_info: n_embd_k_gqa     = 2048
0.00.051.881 I print_info: n_embd_v_gqa     = 2048
0.00.051.882 I print_info: f_norm_eps       = 1.0e-05
0.00.051.882 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.882 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.882 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.883 I print_info: f_logit_scale    = 0.0e+00
0.00.051.883 I print_info: n_ff             = 8192
0.00.051.884 I print_info: n_expert         = 0
0.00.051.884 I print_info: n_expert_used    = 0
0.00.051.884 I print_info: causal attn      = 1
0.00.051.884 I print_info: pooling type     = 0
0.00.051.884 I print_info: rope type        = 2
0.00.051.884 I print_info: rope scaling     = linear
0.00.051.886 I print_info: freq_base_train  = 10000.0
0.00.051.887 I print_info: freq_scale_train = 1
0.00.051.887 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.887 I print_info: rope_finetuned   = unknown
0.00.051.887 I print_info: ssm_d_conv       = 0
0.00.051.887 I print_info: ssm_d_inner      = 0
0.00.051.887 I print_info: ssm_d_state      = 0
0.00.051.889 I print_info: ssm_dt_rank      = 0
0.00.051.889 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.889 I print_info: model type       = 1.4B
0.00.051.890 I print_info: model params     = 1.41 B
0.00.051.890 I print_info: general.name     = 1.4B
0.00.051.891 I print_info: vocab type       = BPE
0.00.051.891 I print_info: n_vocab          = 50304
0.00.051.891 I print_info: n_merges         = 50009
0.00.051.891 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.891 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.892 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.893 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.893 I print_info: LF token         = 128 'Ä'
0.00.051.893 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.893 I print_info: max token length = 1024
0.00.053.885 I load_tensors: offloading 24 repeating layers to GPU
0.00.053.886 I load_tensors: offloading output layer to GPU
0.00.053.886 I load_tensors: offloaded 25/25 layers to GPU
0.00.053.896 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.053.897 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.054.249 I llama_init_from_model: n_seq_max     = 1
0.00.054.250 I llama_init_from_model: n_ctx         = 128
0.00.054.250 I llama_init_from_model: n_ctx_per_seq = 128
0.00.054.250 I llama_init_from_model: n_batch       = 128
0.00.054.251 I llama_init_from_model: n_ubatch      = 128
0.00.054.251 I llama_init_from_model: flash_attn    = 0
0.00.054.251 I llama_init_from_model: freq_base     = 10000.0
0.00.054.251 I llama_init_from_model: freq_scale    = 1
0.00.054.252 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.252 I ggml_metal_init: allocating
0.00.054.255 I ggml_metal_init: found device: Apple M4
0.00.054.257 I ggml_metal_init: picking default device: Apple M4
0.00.054.826 I ggml_metal_init: using embedded metal library
0.00.057.195 I ggml_metal_init: GPU name:   Apple M4
0.00.057.196 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.197 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.197 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.198 I ggml_metal_init: simdgroup reduction   = true
0.00.057.198 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.198 I ggml_metal_init: has bfloat            = true
0.00.057.198 I ggml_metal_init: use bfloat            = true
0.00.057.198 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.199 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.045 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.068.384 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.386 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.402 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.069.220 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.069.221 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.069.222 I llama_init_from_model: graph nodes  = 967
0.00.069.222 I llama_init_from_model: graph splits = 2
0.00.069.223 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.069.223 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.554.285 I 
0.00.554.319 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.554.331 I perplexity: tokenizing the input ..
0.00.562.403 I perplexity: tokenization took 8.071 ms
0.00.562.413 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.696.477 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.697.633 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.697.648 I llama_perf_context_print:        load time =     544.08 ms
0.00.697.649 I llama_perf_context_print: prompt eval time =     133.84 ms /   128 tokens (    1.05 ms per token,   956.38 tokens per second)
0.00.697.652 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.697.652 I llama_perf_context_print:       total time =     143.36 ms /   129 tokens
0.00.698.096 I ggml_metal_free: deallocating

real	0m0.714s
user	0m0.079s
sys	0m0.098s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.083 I build: 4529 (12c2bdf2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.836 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.745 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.750 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.752 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.753 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.753 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.753 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.754 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.755 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.755 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.755 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.756 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.756 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.756 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.757 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.760 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.760 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.761 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.638 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.711 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.574 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.575 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.576 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.576 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.576 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.577 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.577 I llama_model_loader: - type  f32:  194 tensors
0.00.024.578 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.578 I llama_model_loader: - type q6_K:   37 tensors
0.00.024.578 I print_info: file format = GGUF V3 (latest)
0.00.024.579 I print_info: file type   = Q5_K - Medium
0.00.024.579 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.043.785 I load: special tokens cache size = 25
0.00.049.901 I load: token to piece cache size = 0.2984 MB
0.00.049.904 I print_info: arch             = gptneox
0.00.049.904 I print_info: vocab_only       = 0
0.00.049.904 I print_info: n_ctx_train      = 2048
0.00.049.904 I print_info: n_embd           = 2048
0.00.049.905 I print_info: n_layer          = 24
0.00.049.907 I print_info: n_head           = 16
0.00.049.908 I print_info: n_head_kv        = 16
0.00.049.908 I print_info: n_rot            = 32
0.00.049.908 I print_info: n_swa            = 0
0.00.049.908 I print_info: n_embd_head_k    = 128
0.00.049.909 I print_info: n_embd_head_v    = 128
0.00.049.909 I print_info: n_gqa            = 1
0.00.049.910 I print_info: n_embd_k_gqa     = 2048
0.00.049.911 I print_info: n_embd_v_gqa     = 2048
0.00.049.911 I print_info: f_norm_eps       = 1.0e-05
0.00.049.912 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.912 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.912 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.912 I print_info: f_logit_scale    = 0.0e+00
0.00.049.913 I print_info: n_ff             = 8192
0.00.049.913 I print_info: n_expert         = 0
0.00.049.913 I print_info: n_expert_used    = 0
0.00.049.913 I print_info: causal attn      = 1
0.00.049.913 I print_info: pooling type     = 0
0.00.049.913 I print_info: rope type        = 2
0.00.049.914 I print_info: rope scaling     = linear
0.00.049.914 I print_info: freq_base_train  = 10000.0
0.00.049.915 I print_info: freq_scale_train = 1
0.00.049.915 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.915 I print_info: rope_finetuned   = unknown
0.00.049.918 I print_info: ssm_d_conv       = 0
0.00.049.918 I print_info: ssm_d_inner      = 0
0.00.049.918 I print_info: ssm_d_state      = 0
0.00.049.918 I print_info: ssm_dt_rank      = 0
0.00.049.918 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.918 I print_info: model type       = 1.4B
0.00.049.919 I print_info: model params     = 1.41 B
0.00.049.919 I print_info: general.name     = 1.4B
0.00.049.919 I print_info: vocab type       = BPE
0.00.049.920 I print_info: n_vocab          = 50304
0.00.049.920 I print_info: n_merges         = 50009
0.00.049.920 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.920 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.920 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.921 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.921 I print_info: LF token         = 128 'Ä'
0.00.049.921 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.921 I print_info: max token length = 1024
0.00.051.864 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.864 I load_tensors: offloading output layer to GPU
0.00.051.864 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.875 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.051.876 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.052.148 I llama_init_from_model: n_seq_max     = 1
0.00.052.149 I llama_init_from_model: n_ctx         = 128
0.00.052.149 I llama_init_from_model: n_ctx_per_seq = 128
0.00.052.150 I llama_init_from_model: n_batch       = 128
0.00.052.150 I llama_init_from_model: n_ubatch      = 128
0.00.052.150 I llama_init_from_model: flash_attn    = 0
0.00.052.150 I llama_init_from_model: freq_base     = 10000.0
0.00.052.150 I llama_init_from_model: freq_scale    = 1
0.00.052.151 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.151 I ggml_metal_init: allocating
0.00.052.154 I ggml_metal_init: found device: Apple M4
0.00.052.156 I ggml_metal_init: picking default device: Apple M4
0.00.052.754 I ggml_metal_init: using embedded metal library
0.00.055.059 I ggml_metal_init: GPU name:   Apple M4
0.00.055.060 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.061 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.061 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.061 I ggml_metal_init: simdgroup reduction   = true
0.00.055.061 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.062 I ggml_metal_init: has bfloat            = true
0.00.055.062 I ggml_metal_init: use bfloat            = true
0.00.055.062 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.063 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.469 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.065.682 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.686 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.701 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.066.587 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.066.588 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.066.588 I llama_init_from_model: graph nodes  = 967
0.00.066.589 I llama_init_from_model: graph splits = 2
0.00.066.590 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.590 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.653.146 I 
0.00.653.186 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.653.206 I perplexity: tokenizing the input ..
0.00.661.137 I perplexity: tokenization took 7.93 ms
0.00.661.153 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.800.989 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.802.203 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.802.223 I llama_perf_context_print:        load time =     644.31 ms
0.00.802.224 I llama_perf_context_print: prompt eval time =     139.61 ms /   128 tokens (    1.09 ms per token,   916.83 tokens per second)
0.00.802.225 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.802.227 I llama_perf_context_print:       total time =     149.08 ms /   129 tokens
0.00.802.635 I ggml_metal_free: deallocating

real	0m0.817s
user	0m0.078s
sys	0m0.129s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.086 I build: 4529 (12c2bdf2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.150 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.957 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.961 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.962 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.963 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.963 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.964 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.964 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.965 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.965 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.966 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.966 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.966 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.967 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.967 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.970 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.970 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.971 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.935 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.042 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.954 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.955 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.955 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.955 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.956 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.956 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.957 I llama_model_loader: - type  f32:  194 tensors
0.00.025.957 I llama_model_loader: - type q6_K:   98 tensors
0.00.025.957 I print_info: file format = GGUF V3 (latest)
0.00.025.958 I print_info: file type   = Q6_K
0.00.025.958 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.045.295 I load: special tokens cache size = 25
0.00.051.538 I load: token to piece cache size = 0.2984 MB
0.00.051.541 I print_info: arch             = gptneox
0.00.051.541 I print_info: vocab_only       = 0
0.00.051.542 I print_info: n_ctx_train      = 2048
0.00.051.542 I print_info: n_embd           = 2048
0.00.051.542 I print_info: n_layer          = 24
0.00.051.545 I print_info: n_head           = 16
0.00.051.545 I print_info: n_head_kv        = 16
0.00.051.548 I print_info: n_rot            = 32
0.00.051.549 I print_info: n_swa            = 0
0.00.051.549 I print_info: n_embd_head_k    = 128
0.00.051.549 I print_info: n_embd_head_v    = 128
0.00.051.550 I print_info: n_gqa            = 1
0.00.051.550 I print_info: n_embd_k_gqa     = 2048
0.00.051.551 I print_info: n_embd_v_gqa     = 2048
0.00.051.551 I print_info: f_norm_eps       = 1.0e-05
0.00.051.552 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.552 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.552 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.552 I print_info: f_logit_scale    = 0.0e+00
0.00.051.553 I print_info: n_ff             = 8192
0.00.051.553 I print_info: n_expert         = 0
0.00.051.553 I print_info: n_expert_used    = 0
0.00.051.553 I print_info: causal attn      = 1
0.00.051.554 I print_info: pooling type     = 0
0.00.051.559 I print_info: rope type        = 2
0.00.051.559 I print_info: rope scaling     = linear
0.00.051.560 I print_info: freq_base_train  = 10000.0
0.00.051.560 I print_info: freq_scale_train = 1
0.00.051.560 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.560 I print_info: rope_finetuned   = unknown
0.00.051.560 I print_info: ssm_d_conv       = 0
0.00.051.561 I print_info: ssm_d_inner      = 0
0.00.051.561 I print_info: ssm_d_state      = 0
0.00.051.561 I print_info: ssm_dt_rank      = 0
0.00.051.561 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.562 I print_info: model type       = 1.4B
0.00.051.563 I print_info: model params     = 1.41 B
0.00.051.563 I print_info: general.name     = 1.4B
0.00.051.563 I print_info: vocab type       = BPE
0.00.051.564 I print_info: n_vocab          = 50304
0.00.051.564 I print_info: n_merges         = 50009
0.00.051.564 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.564 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.564 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.564 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.564 I print_info: LF token         = 128 'Ä'
0.00.051.565 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.565 I print_info: max token length = 1024
0.00.053.545 I load_tensors: offloading 24 repeating layers to GPU
0.00.053.545 I load_tensors: offloading output layer to GPU
0.00.053.545 I load_tensors: offloaded 25/25 layers to GPU
0.00.053.556 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.053.557 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.053.833 I llama_init_from_model: n_seq_max     = 1
0.00.053.834 I llama_init_from_model: n_ctx         = 128
0.00.053.834 I llama_init_from_model: n_ctx_per_seq = 128
0.00.053.834 I llama_init_from_model: n_batch       = 128
0.00.053.834 I llama_init_from_model: n_ubatch      = 128
0.00.053.834 I llama_init_from_model: flash_attn    = 0
0.00.053.835 I llama_init_from_model: freq_base     = 10000.0
0.00.053.835 I llama_init_from_model: freq_scale    = 1
0.00.053.835 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.836 I ggml_metal_init: allocating
0.00.053.839 I ggml_metal_init: found device: Apple M4
0.00.053.840 I ggml_metal_init: picking default device: Apple M4
0.00.054.427 I ggml_metal_init: using embedded metal library
0.00.056.733 I ggml_metal_init: GPU name:   Apple M4
0.00.056.735 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.735 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.735 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.736 I ggml_metal_init: simdgroup reduction   = true
0.00.056.736 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.736 I ggml_metal_init: has bfloat            = true
0.00.056.736 I ggml_metal_init: use bfloat            = true
0.00.056.736 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.737 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.170 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.067.438 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.441 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.455 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.068.381 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.068.382 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.068.383 I llama_init_from_model: graph nodes  = 967
0.00.068.383 I llama_init_from_model: graph splits = 2
0.00.068.384 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.384 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.116.888 I 
0.00.116.927 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.116.936 I perplexity: tokenizing the input ..
0.00.124.570 I perplexity: tokenization took 7.632 ms
0.00.124.584 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.262.864 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.264.017 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.264.029 I llama_perf_context_print:        load time =     106.73 ms
0.00.264.030 I llama_perf_context_print: prompt eval time =     138.05 ms /   128 tokens (    1.08 ms per token,   927.21 tokens per second)
0.00.264.031 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.264.031 I llama_perf_context_print:       total time =     147.14 ms /   129 tokens
0.00.264.383 I ggml_metal_free: deallocating

real	0m0.280s
user	0m0.078s
sys	0m0.037s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.251 I build: 4529 (12c2bdf2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.022.135 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.037.453 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.037.458 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.037.464 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.037.465 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.037.465 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.037.466 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.037.466 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.037.467 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.037.469 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.037.470 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.037.470 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.037.470 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.037.471 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.037.472 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.037.474 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.037.475 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.037.476 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.045.297 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.047.226 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.053.829 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.053.831 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.053.831 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.053.832 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.053.832 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.053.833 I llama_model_loader: - type  f32:  194 tensors
0.00.053.833 I llama_model_loader: - type  f16:   98 tensors
0.00.053.834 I print_info: file format = GGUF V3 (latest)
0.00.053.835 I print_info: file type   = all F32 (guessed)
0.00.053.840 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.079.979 I load: special tokens cache size = 25
0.00.086.576 I load: token to piece cache size = 0.2984 MB
0.00.086.579 I print_info: arch             = gptneox
0.00.086.579 I print_info: vocab_only       = 0
0.00.086.580 I print_info: n_ctx_train      = 2048
0.00.086.580 I print_info: n_embd           = 2048
0.00.086.580 I print_info: n_layer          = 24
0.00.086.583 I print_info: n_head           = 16
0.00.086.584 I print_info: n_head_kv        = 16
0.00.086.584 I print_info: n_rot            = 32
0.00.086.584 I print_info: n_swa            = 0
0.00.086.584 I print_info: n_embd_head_k    = 128
0.00.086.585 I print_info: n_embd_head_v    = 128
0.00.086.585 I print_info: n_gqa            = 1
0.00.086.586 I print_info: n_embd_k_gqa     = 2048
0.00.086.587 I print_info: n_embd_v_gqa     = 2048
0.00.086.587 I print_info: f_norm_eps       = 1.0e-05
0.00.086.588 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.086.588 I print_info: f_clamp_kqv      = 0.0e+00
0.00.086.588 I print_info: f_max_alibi_bias = 0.0e+00
0.00.086.588 I print_info: f_logit_scale    = 0.0e+00
0.00.086.589 I print_info: n_ff             = 8192
0.00.086.589 I print_info: n_expert         = 0
0.00.086.589 I print_info: n_expert_used    = 0
0.00.086.589 I print_info: causal attn      = 1
0.00.086.589 I print_info: pooling type     = 0
0.00.086.590 I print_info: rope type        = 2
0.00.086.590 I print_info: rope scaling     = linear
0.00.086.590 I print_info: freq_base_train  = 10000.0
0.00.086.590 I print_info: freq_scale_train = 1
0.00.086.592 I print_info: n_ctx_orig_yarn  = 2048
0.00.086.592 I print_info: rope_finetuned   = unknown
0.00.086.593 I print_info: ssm_d_conv       = 0
0.00.086.593 I print_info: ssm_d_inner      = 0
0.00.086.593 I print_info: ssm_d_state      = 0
0.00.086.593 I print_info: ssm_dt_rank      = 0
0.00.086.593 I print_info: ssm_dt_b_c_rms   = 0
0.00.086.593 I print_info: model type       = 1.4B
0.00.086.593 I print_info: model params     = 1.41 B
0.00.086.594 I print_info: general.name     = 1.4B
0.00.086.594 I print_info: vocab type       = BPE
0.00.086.594 I print_info: n_vocab          = 50304
0.00.086.594 I print_info: n_merges         = 50009
0.00.086.596 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.086.596 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.086.596 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.086.596 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.086.596 I print_info: LF token         = 128 'Ä'
0.00.086.597 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.086.597 I print_info: max token length = 1024
0.00.089.195 I load_tensors: offloading 24 repeating layers to GPU
0.00.089.195 I load_tensors: offloading output layer to GPU
0.00.089.195 I load_tensors: offloaded 25/25 layers to GPU
0.00.089.206 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.089.207 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.089.519 I llama_init_from_model: n_seq_max     = 1
0.00.089.520 I llama_init_from_model: n_ctx         = 128
0.00.089.520 I llama_init_from_model: n_ctx_per_seq = 128
0.00.089.520 I llama_init_from_model: n_batch       = 128
0.00.089.520 I llama_init_from_model: n_ubatch      = 128
0.00.089.521 I llama_init_from_model: flash_attn    = 0
0.00.089.521 I llama_init_from_model: freq_base     = 10000.0
0.00.089.521 I llama_init_from_model: freq_scale    = 1
0.00.089.522 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.089.522 I ggml_metal_init: allocating
0.00.089.525 I ggml_metal_init: found device: Apple M4
0.00.089.527 I ggml_metal_init: picking default device: Apple M4
0.00.090.144 I ggml_metal_init: using embedded metal library
0.00.092.664 I ggml_metal_init: GPU name:   Apple M4
0.00.092.665 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.092.666 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.092.666 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.092.667 I ggml_metal_init: simdgroup reduction   = true
0.00.092.667 I ggml_metal_init: simdgroup matrix mul. = true
0.00.092.667 I ggml_metal_init: has bfloat            = true
0.00.092.667 I ggml_metal_init: use bfloat            = true
0.00.092.668 I ggml_metal_init: hasUnifiedMemory      = true
0.00.092.668 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.102.005 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.103.227 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.103.233 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.103.247 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.104.123 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.104.124 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.104.124 I llama_init_from_model: graph nodes  = 967
0.00.104.125 I llama_init_from_model: graph splits = 2
0.00.104.126 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.104.126 I 
0.00.104.161 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.104.163 I compute_imatrix: tokenizing the input ..
0.00.111.078 I compute_imatrix: tokenization took 6.914 ms
0.00.111.079 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.575.653 I compute_imatrix: 1.46 seconds per pass - ETA 0.02 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.578.309 I llama_perf_context_print:        load time =    1553.52 ms
0.01.578.310 I llama_perf_context_print: prompt eval time =    1463.93 ms /   128 tokens (   11.44 ms per token,    87.44 tokens per second)
0.01.578.311 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.578.311 I llama_perf_context_print:       total time =    1556.17 ms /   129 tokens
0.01.578.879 I ggml_metal_free: deallocating

real	0m1.763s
user	0m0.167s
sys	0m0.244s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4529 (12c2bdf2)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 'Ä'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x157a0b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x157a0b710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x157a0bcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x157a0c270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x157a0c820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x157a0cdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x157a0d380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x157a0d930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x157a0dee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x157a0e3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x157a0e8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x157a0ede0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x157a0f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x157a100b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x157a108c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x157a10fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x157a11700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x157a11e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x157a12540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x157a12d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x157a13430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x157a13b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x157a14270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x157a14b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x157a15230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x157a154f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x157a15b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x157a16770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x157a16cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x157a16f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x157a17410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x157a176d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x157a17f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x157a184a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x157a18760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x157a18c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x157a190a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x157a19540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x157a199e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x157a19e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x157a1a320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x157a1a7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x157a1ac60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x157a1b100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x157a1b3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x157a1b9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x157a1bfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x157a1c900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x157a1cf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x157a1d520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x157a1db30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x157a1e140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x157a1e750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x157a1ed60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x157a1f550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x157a1f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x157a1fe90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x157a20150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x157a20760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x157a20f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x157a21210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x157a216b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x157a21b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x157a21ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x157a22490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x157a22930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x157a22dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x157a23270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x157a23710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x157a23bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x157a24050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x157a244f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x157a24990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x157a24ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x157a25430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x157a25980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x157a25ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x157a26420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x157a26970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x157a26ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x157a27410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x157a27960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x157a27eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x157a28400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x157a28950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x157a28ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x157a293f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x157a29940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x157a29e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x157a2a3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x157a2a930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x157a2ae80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x157a2b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x157a2b920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x157a2be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x157a2c3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x157a2c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x157a1c5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x157a2cd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x157a2d530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x157a2da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x157a2dfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x157a2e520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x157a2ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x157a2efc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x157a2f510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x157a2fa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x157a2ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x157a30500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x157a30a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x157a30fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x157a314f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x157a31a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x157a31ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x157a32380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x157a32820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x157a32cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x157a33160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x157a33600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x157a33aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x157a33f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x157a343e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x157a34880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x157a34d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x157a351c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x157a35660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x157a35b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x157a35fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x157a36440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x157a368e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x157a36d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x157a37220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x157a376c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x157a37b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x157a38000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x157a384a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x157a38940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x157a38de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x157a39280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x157a39720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x157a39bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x157a3a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x157a3a500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x157a3a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x157a3ae40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x157a3b2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x157a3b780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x157a3bc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x157a3c0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x157a3c560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x157a3ca00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x157a3cea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x157a3d340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x157a3d7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x157a3dc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x157a3e120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x157a3e5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x157a3ea60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x157a3ef00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x157a3f3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x157a3f840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x157a3fce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x157a40180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x157a40620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x157a40ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x157a40f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x157a41400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x157a418a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x157a41d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x157a421e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x157a42680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x157a42b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x157a42fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x157a43460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x157a43900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x157a43da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x157a44240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x157a446e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x157a44b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x157a45020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x157a454c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x157a45960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x157a45e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x157a462a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x157a46740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x157a46be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x157a47080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x157a47520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x157a479c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x157a47e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x157a48300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x157a487a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x157a48c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x157a49190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x157a496e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x157a49c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x157a4a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x157a4a440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x157a4aa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x157a4b060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x157a4b670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x157a4be60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x157a4c300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x157a4c5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x157a4cbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x157a4d1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x157a4d9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x157a4de70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x157a4e310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x157a4e7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x157a4ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x157a4f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x157a4fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x157a4ff50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x157a504a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x157a509f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x157a50f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x157a51490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x157a519e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x157a51f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x157a52480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x157a529d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x157a52f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x157a53470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x157a539c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x157a53f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x157a54460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x157a549b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x157a54f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x157a55450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x157a559a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x157a55ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x157a56440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x157a56990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x157a56ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x157a57430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x157a57980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x157a57ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x157a58420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x157a58970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x157a58ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x157a59410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x157a59960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x157a59eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x157a5a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x157a5a950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x157a5aea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x157a5b3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x157a5b940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x157a5be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x157a5c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x157a5c930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x157a5ce80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x157a5d3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x157a5d920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x157a5de70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x157a5e3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x157a5e910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x157a5ee60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x157a5f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x157a5f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x157a5fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x157a603a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x157a608f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x157a60e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x157a61390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x157a618e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x157a61d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x157a62220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x157a626c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x157a62b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x157a63000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x157a634a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x157a63940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x157a63de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x157a64280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x157a64720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x157a64bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x157a65060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x157a65500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x157a659a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x157a65e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x157a66390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x157a66ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x157a671d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x157a678f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x157a68010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x157a682d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x157a68ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x157a68d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x157a69390 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.138.244 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.138.248 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x157a69040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x157a4c880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x157a4a700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x157a4b320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x157a1e400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x157a1ddf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x157a20410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x157a4ce90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x157a157b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x157a1c2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x157a1cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x157a1d1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x157a1b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x157a1d7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x157a147b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x157a20a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x157a2d040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x157a68590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x157a17990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x157a17c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x157a4d4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x157a4b930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x157a15dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x157a16080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x157a16340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x157a697f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x157a69ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x157a69d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x157a6a030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x157a6a2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x157a6a5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x157a6a870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x157a6ab30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x157a6adf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x157a6b0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x157a6b370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x157a6b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x157a6b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x157a6bbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x157a6be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x157a6c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x157a6c3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x157a6c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x157a6c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x157a6cc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x157a6cef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x157a6d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x157a6d470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x157a6d730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x157a6d9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x157a6dcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x157a6df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x157a6e230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x157a6e4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x157a6e7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x157a6ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x157a6ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x157a6eff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x157a6f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x157a6f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x157a6f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x157a6faf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x157a6fdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x157a70070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x157a70330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x157a705f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x157a708b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x157a70b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x157a70e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x157a710f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x157a713b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x157a71670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x157a71930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x157a71bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x157a71eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x157a72170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x157a72430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x157a726f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x157a729b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x157a72c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x157a72f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x157a731f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x157a734b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x157a73770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x157a73a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x157a73cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x157a73fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x157a74270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x157a74530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x157a747f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x157a74ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x157a74d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x157a75030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x157a752f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x157a755b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x157a75870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x157a75b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x157a75df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x157a760b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x157a76370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x157a76630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x157a768f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x157a76bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x157a76e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x157a77130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x157a773f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x157a776b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x157a77970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x157a77c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x157a77ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x157a781b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x157a78470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x157a78730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x157a789f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x157a78cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x157a78f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x157a79230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x157a794f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x157a797b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x157a79a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x157a79d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x157a79ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x157a7a2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x157a7a570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x157a7a830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x157a7aaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x157a7adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x157a7b070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x157a7b330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x157a7b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x157a7b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x157a7bb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x157a7be30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x157a7c0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x157a7c3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x157a7c670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x157a7c930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x157a7cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x157a7ceb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x157a7d170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x157a7d430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x157a7d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x157a7d9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x157a7dc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x157a7df30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x157a7e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x157a7e4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x157a7e770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x157a7ea30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x157a7ecf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x157a7efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x157a7f270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x157a7f530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x157a7f7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x157a7fab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x157a7fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x157a80030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x157a802f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x157a805b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x157a80870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x157a80b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x157a80df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x157a810b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x157a81370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x157a81630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x157a818f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x157a81bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x157a81e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x157a82130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x157a823f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x157a826b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x157a82970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x157a82c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x157a82ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x157a831b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x157a83470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x157a83730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x157a839f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x157a83cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x157a83f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x157a84230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x157a844f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x157a847b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x157a84a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x157a84d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x157a84ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x157a852b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x157a85570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x157a85830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x157a85af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x157a85db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x157a86070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x157a86330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x157a865f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x157a868b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x157a86b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x157a86e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x157a870f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x157a873b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x157a87670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x157a87930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x157a87bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x157a87eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x157a88170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x157a88430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x157a886f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x157a889b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x157a88c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x157a88f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x157a893d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x157a89b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x157a89e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x157a8a100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x157a8a570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x157a8a9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x157a8ae50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x157a8b2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x157a8b730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x157a8bba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x157a8c010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x157a8c480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x157a8c8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x157a8cd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x157a8d1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x157a8d640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x157a8dab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x157a8df20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x157a8e390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x157a8e800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x157a8ec70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x157a8f0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x157a8f550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x157a8f9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x157a8fe30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x157a902a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x157a90710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x157a90b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x157a90ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x157a91460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x157a918d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x157a91d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x157a921b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x157a92620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x157a92a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x157a92f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x157a93370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x157a937e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x157a93c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x157a940c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x157a94530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x157a949a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x157a94e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x157a95280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x157a956f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x157a95b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x157a95fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x157a96440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x157a968b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x157a96d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x157a97190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x157a97600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x157a97a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x157a97ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x157a98350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x157a987c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x157a98c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x157a990a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x157a99510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x157a99980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x157a99df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x157a9a260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x157a9a6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x157a9ab40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x157a9afb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x157a9b420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x157a9b890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x157a9bd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x157a9c170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x157a9c5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x157a9ca50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x157a9cec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x157a9d330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x157a9d7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x157a9e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x157a9e930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x157a9f050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x157a9f770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x157a9fa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x157aa0220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x157aa04e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x157aa0af0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x157a9da60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x157aa07a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x157a9fcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x157aa0f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x157aa1210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x157aa14d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x157aa1790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x157aa1a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x157aa1d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x157aa1fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x157aa2290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x157aa2550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x157aa2b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x157aa30f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x157aa3720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x157aa39e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x157aa3ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x157aa3f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x157aa4220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x157aa44e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x157aa47a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x157aa4a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x157aa4d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x157aa4fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x157aa52a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x157aa5560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x157aa5820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x157aa5ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x157aa5da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x157aa6060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x157aa6320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x157aa65e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x157aa68a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x157aa6b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x157aa6e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x157aa70e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x157aa73a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x157aa7660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x157aa7920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x157aa7be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x157aa7ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x157aa8160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x157aa8420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x157aa86e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x157aa89a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x157aa8c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x157aa8f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x157aa91e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x157aa94a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x157aa9760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x157aa9a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x157aa9ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x157aa9fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x157aaa260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x157aaa520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x157aaa7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x157aaaaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x157aaad60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x157aab020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x157aab2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x157aab5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x157aab860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x157aabb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x157aabde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x157aac0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x157aac360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x157aac620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x157aac8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x157aacba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x157aace60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x157aad120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x157aad3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x157aad6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x157aad960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x157aadc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x157aadee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x157aae1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x157aae460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x157aae720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x157aae9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x157aaeca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x157aaef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x157aaf220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x157aaf4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x157aaf7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x157aafa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x157aafd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x157aaffe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x157ab02a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x157ab0560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x157ab0820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x157ab0ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x157ab0da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x157ab1060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x157ab1320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x157ab15e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x157ab18a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x157ab1b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x157ab1e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x157ab20e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x157ab23a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x157ab2660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x157ab2920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x157ab2be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x157ab2ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x157ab3160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x157ab3420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x157ab36e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x157ab39a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x157ab3c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x157ab3f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x157ab41e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x157ab44a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x157ab4760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x157ab4a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x157ab4ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x157ab4fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x157ab5260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x157ab5520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x157ab57e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x157ab5aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x157ab5d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x157ab6020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x157ab62e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x157ab65a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x157ab6860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x157ab6b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x157ab6de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x157ab70a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x157ab7360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x157ab7620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x157ab78e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x157ab7ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x157ab7e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x157ab8120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x157ab83e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x157ab86a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x157ab8960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x157ab8c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x157ab8ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x157ab91a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x157ab9460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x157ab9720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x157ab99e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x157ab9ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x157ab9f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x157aba220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x157aba4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x157aba7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x157abaa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x157abad20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x157abafe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x157abb2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x157abb560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x157abb820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x157abbae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x157abbda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x157abc060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x157abc320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x157abc5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x157abc8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x157abcb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x157abce20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x157abd0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x157abd3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x157abd660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x157abd920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x157abdbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x157abdea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x157abe160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x157abe420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x157abe6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x157abe9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x157abec60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x157abef20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x157abf1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x157abf4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x157abf760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x157abfa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x157abfce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x157abffa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x157ac0260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x157ac0520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x157ac07e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x157ac0aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x157ac0d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x157ac1020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x157ac12e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x157ac15a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x157ac1860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x157ac1b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x157ac1de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x157ac20a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x157ac2360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x157ac2620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x157ac28e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x157ac2ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x157ac2e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x157ac3120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x157ac33e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x157ac36a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x157ac3960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x157ac3c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x157ac3ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x157ac41a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x157ac4460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x157ac4720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x157ac49e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x157ac4ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x157ac4f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x157ac5530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x157ac57f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x157ac5ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x157ac5d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x157ac6030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x157ac62f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x157ac65b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x157ac6870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x157ac6b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x157ac6df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x157ac70b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x157ac7370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x157ac7630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x157ac78f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x157ac7bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x157ac7e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x157ac8130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x157ac83f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x157ac86b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x157ac8970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x157ac8c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x157ac8ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x157ac91b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x157ac9470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x157ac9730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x157ac99f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x157ac9cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x157ac9f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x157aca230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x157aca4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x157aca7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x157acaa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x157acad30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x157acaff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x157acb2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x157acb570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x157acb830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x157acbaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x157acbdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x157acc070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x157acc330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x157acc5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x157acc8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x157accb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x157acce30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x157acd0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x157acd3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x157acd670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x157acd930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x157acdbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x157acdeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x157ace170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x157ace430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x157ace6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x157ace9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x157acec70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x157acef30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x157acf1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x157acf4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x157acf770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x157acfa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x157acfcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x157ad00f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x157ad03b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x157ad0670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x157ad0ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x157ad0f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x157ad13c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x157ad1830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x157ad1ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x157ad2110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x157ad2580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x157ad29f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x157ad3560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x157ad3c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x157ad43a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x157ad4ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x157ad4d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x157ad5040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x157ad5570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x157ad59e0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.803s
user	0m0.292s
sys	0m0.308s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4529 (12c2bdf2)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 'Ä'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12260b520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12260bc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12260c1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12260c790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12260cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12260d2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12260d8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12260de50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12260e400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12260e900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12260ee00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12260f300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12260fe20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x1226105d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x122610de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x122611500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x122611c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x122612340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x122612a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x122613230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x122613950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x122614070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x122614790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x122615030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x122615750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x122615a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x122616020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x122616c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x1226171d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x122617490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x122617930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x122617bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x122618480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x1226189c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x122618c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x122619120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x1226195c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x122619a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x122619f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12261a3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12261a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12261ace0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12261b180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12261b620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12261b8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12261bef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12261c500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12261ce20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12261d430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12261da40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12261e050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12261e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12261ec70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12261f280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12261fa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12261ff10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x1226203b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x122620670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x122620c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x122621470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x122621730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x122621bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x122622070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x122622510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x1226229b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x122622e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x1226232f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x122623790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x122623c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x1226240d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x122624570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x122624a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x122624eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x122625400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x122625950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x122625ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x1226263f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x122626940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x122626e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x1226273e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x122627930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x122627e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x1226283d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x122628920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x122628e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x1226293c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x122629910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x122629e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12262a3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12262a900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12262ae50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12262b3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12262b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12262be40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12262c390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12262c8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12262ce30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12261cb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12262d2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12262da50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12262dfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12262e4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12262ea40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12262ef90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12262f4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12262fa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12262ff80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x1226304d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x122630a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x122630f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x1226314c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x122631a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x122631f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x122632400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x1226328a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x122632d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x1226331e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x122633680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x122633b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x122633fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x122634460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x122634900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x122634da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x122635240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x1226356e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x122635b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x122636020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x1226364c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x122636960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x122636e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x1226372a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x122637740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x122637be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x122638080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x122638520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x1226389c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x122638e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x122639300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x1226397a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x122639c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12263a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12263a580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12263aa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12263aec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12263b360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12263b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12263bca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12263c140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12263c5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12263ca80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12263cf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12263d3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12263d860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12263dd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12263e1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12263e640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12263eae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12263ef80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12263f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12263f8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12263fd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x122640200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x1226406a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x122640b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x122640fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x122641480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x122641920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x122641dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x122642260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x122642700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x122642ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x122643040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x1226434e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x122643980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x122643e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x1226442c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x122644760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x122644c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x1226450a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x122645540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x1226459e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x122645e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x122646320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x1226467c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x122646c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x122647100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x1226475a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x122647a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x122647ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x122648380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x122648820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x122648cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x122649160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x1226496b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x122649c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12264a150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12264a6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12264a960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12264af70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12264b580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12264bb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12264c380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12264c820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12264cae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12264d0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12264d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12264def0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12264e390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12264e830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12264ecd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12264f480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12264f9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12264ff20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x122650470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x1226509c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x122650f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x122651460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x1226519b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x122651f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x122652450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x1226529a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x122652ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x122653440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x122653990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x122653ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x122654430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x122654980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x122654ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x122655420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x122655970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x122655ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x122656410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x122656960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x122656eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x122657400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x122657950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x122657ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x1226583f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x122658940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x122658e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x1226593e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x122659930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x122659e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12265a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12265a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12265ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12265b3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12265b910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12265be60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12265c3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12265c900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12265ce50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12265d3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12265d8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12265de40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12265e390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12265e8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12265ee30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12265f380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12265f8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12265fe20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x122660370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x1226608c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x122660e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x122661360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x1226618b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x122661e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x1226622a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x122662740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x122662be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x122663080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x122663520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x1226639c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x122663e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x122664300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1226647a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x122664c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x1226650e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x122665580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x122665a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x122665ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x122666360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x1226668b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x122666fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x1226676f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x122667e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x122668530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x1226687f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x122668fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1226692a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x1226698b0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.085.121 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.085.135 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x126e081c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x126e08630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x126e08aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x126e08f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x126e09380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x126e097f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x126e09c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x126e0a0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x126e0a540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x126e0a9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x126e0ae20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x126e0b4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x126e0c000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x126e0c7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x126e0cfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x126e0d6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x126e0de00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x126e0e520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x126e0ec40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x126e0f410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x126e0fb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x126e10250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x126e10970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x126e11090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x126e117b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x126e11a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x126e11d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x126e121a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x126e12610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x126e12a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x126e12f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x126e13490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x126e13900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x126e13bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x126e14030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x126e144a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x126e14a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x126e14f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x126e15400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x126e15900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x126e15e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x126e16300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x126e16800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x126e16d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x126e17200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x126e17670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x126e17ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x126e17f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x126e183c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x126e18830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x126e18ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x126e19110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x126e19580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x126e199f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x126e19e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x126e1a630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x126e1aad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x126e1ad90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x126e1b3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x126e1bb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x126e1c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x126e1c4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x126e1c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x126e1ce10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x126e1d2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x126e1d750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x126e1dbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x126e1e090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x126e1e530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x126e1e9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x126e1ee70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x126e1f310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x126e1f7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x126e1fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x126e20250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x126e207a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x126e20cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x126e21240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x126e21790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x126e21ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x126e22230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x126e22780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x126e22cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x126e23220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x126e23770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x126e23cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x126e24210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x126e24760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x126e24cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x126e25200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x126e25750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x126e25ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x126e261f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x126e26740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x126e26c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x126e271e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x126e27730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x126e27c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x126e281d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x126e28720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x126e28c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x126e291c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x126e29710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x126e29c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x126e2a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x126e2a700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x126e2ac50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x126e2b1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x126e2b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x126e2bc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x126e2c190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x126e2c6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x126e2cc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x126e2d0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x126e2d570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x126e2da10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x126e2deb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x126e2e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x126e2e7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x126e2ec90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x126e2f130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x126e2f5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x126e2fa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x126e2ff10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x126e303b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x126e30850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x126e30cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x126e31190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x126e31630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x126e31ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x126e31f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x126e32410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x126e328b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x126e32d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x126e331f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x126e33690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x126e33b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x126e33fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x126e34470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x126e34910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x126e34db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x126e35250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x126e356f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x126e35b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x126e36030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x126e364d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x126e36970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x126e36e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x126e372b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x126e37750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x126e37bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x126e38090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x126e38530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x126e389d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x126e38e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x126e39310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x126e397b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x126e39c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x126e3a0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x126e3a590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x126e3aa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x126e3aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x126e3b370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x126e3b810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x126e3bcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x126e3c150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x126e3c5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x126e3ca90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x126e3cf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x126e3d3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x126e3d870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x126e3dd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x126e3e1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x126e3e650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x126e3eaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x126e3ef90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x126e3f430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x126e3f8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x126e3fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x126e40210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x126e406b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x126e40b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x126e40ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x126e41490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x126e41930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x126e41dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x126e42270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x126e42710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x126e42bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x126e43050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x126e434f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x126e43990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x126e43e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x126e44380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x126e448d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x126e44e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x126e45370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x126e45630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x126e45c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x126e46250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x126e46860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x126e47050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x126e474f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x126e477b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x126e47dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x126e483d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x126e48bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x126e49060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x126e49500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x126e499a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x126e4a150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x126e4a6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x126e4abf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x126e4b140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x126e4b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x126e4bbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x126e4c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x126e4c680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x126e4cbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x126e4d120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x126e4d670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x126e4dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x126e4e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x126e4e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x126e4ebb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x126e4f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x126e4f650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x126e4fba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x126e500f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x126e50640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x126e50b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x126e510e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x126e51630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x126e51b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x126e520d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x126e52620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x126e52b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x126e530c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x126e53610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x126e53b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x126e540b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x126e54600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x126e54b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x126e550a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x126e555f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x126e55b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x126e56090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x126e565e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x126e56b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x126e57080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x126e575d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x126e57b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x126e58070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x126e585c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x126e58b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x126e59060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x126e595b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x126e59b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x126e5a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x126e5a5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x126e5aaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x126e5b040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x126e5b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x126e5bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x126e5c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x126e5c580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x126e5cad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x126e5cf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x126e5d410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x126e5d8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x126e5dd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x126e5e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x126e5e690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x126e5eb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x126e5efd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x126e5f470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x126e5f910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x126e5fdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x126e60250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x126e606f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x126e60b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x126e61030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x126e61580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x126e61ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x126e623c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x126e62ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x126e63200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x126e634c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x126e63cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x126e63f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x126e64580 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x126e64230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x126e45f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x126e458f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x126e46510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x126e1b050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x126e48080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x126e0b7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x126e07d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x126e1b790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x126e63780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x126e1a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x126e48690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x126e0b0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x126e64cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x126e65320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x126e655e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x126e658a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x126e65b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x126e65e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x126e660e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x126e663a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x126e66660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x126e66920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x126e66be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x126e66ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x126e67160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x126e67420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x126e676e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x126e679a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x126e67c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x126e67f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x126e681e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x126e684a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x126e68760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x126e68a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x126e68ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x126e68fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x126e69260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x126e69520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x126e697e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x126e69aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x126e69d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x126e6a020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x126e6a2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x126e6a5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x126e6a860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x126e6ab20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x126e6ade0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x126e6b0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x126e6b360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x126e6b620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x126e6b8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x126e6bba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x126e6be60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x126e6c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x126e6c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x126e6c6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x126e6c960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x126e6cc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x126e6cee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x126e6d1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x126e6d460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x126e6d720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x126e6d9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x126e6dca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x126e6df60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x126e6e220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x126e6e4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x126e6e7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x126e6ea60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x126e6ed20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x126e6efe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x126e6f2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x126e6f560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x126e6f820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x126e6fae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x126e6fda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x126e70060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x126e70320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x126e705e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x126e708a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x126e70b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x126e70e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x126e710e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x126e713a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x126e71660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x126e71920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x126e71be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x126e71ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x126e72160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x126e72420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x126e726e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x126e729a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x126e72c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x126e72f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x126e731e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x126e734a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x126e73760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x126e73a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x126e73ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x126e73fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x126e74260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x126e74520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x126e747e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x126e74aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x126e74d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x126e75020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x126e752e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x126e755a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x126e75860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x126e75b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x126e75de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x126e760a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x126e76360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x126e76620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x126e768e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x126e76ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x126e76e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x126e77120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x126e773e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x126e776a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x126e77960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x126e77c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x126e77ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x126e781a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x126e78460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x126e78720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x126e789e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x126e78ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x126e78f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x126e79220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x126e794e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x126e797a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x126e79a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x126e79d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x126e79fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x126e7a2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x126e7a560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x126e7a820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x126e7aae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x126e7ada0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x126e7b060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x126e7b320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x126e7b5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x126e7b8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x126e7bb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x126e7be20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x126e7c0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x126e7c3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x126e7c660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x126e7c920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x126e7cbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x126e7cea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x126e7d160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x126e7d420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x126e7d6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x126e7d9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x126e7dc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x126e7df20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x126e7e1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x126e7e4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x126e7e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x126e7ea20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x126e7ece0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x126e7efa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x126e7f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x126e7f520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x126e7f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x126e7faa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x126e7fd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x126e80020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x126e802e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x126e805a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x126e80860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x126e80b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x126e80de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x126e810a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x126e81360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x126e81620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x126e818e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x126e81ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x126e81e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x126e82120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x126e823e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x126e826a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x126e82960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x126e82c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x126e82ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x126e831a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x126e83460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x126e83720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x126e839e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x126e83ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x126e83f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x126e84220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x126e844e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x126e847a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x126e84a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x126e84d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x126e84fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x126e852a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x126e85560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x126e85820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x126e85ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x126e85da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x126e86060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x126e86320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x126e865e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x126e868a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x126e86b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x126e87130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x126e873f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x126e876b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x126e87970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x126e87c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x126e87ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x126e881b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x126e88470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x126e88730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x126e889f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x126e88cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x126e88f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x126e89230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x126e894f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x126e897b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x126e89a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x126e89d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x126e89ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x126e8a2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x126e8a570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x126e8a830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x126e8aaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x126e8adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x126e8b070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x126e8b330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x126e8b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x126e8b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x126e8bb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x126e8be30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x126e8c0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x126e8c3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x126e8c670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x126e8c930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x126e8cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x126e8ceb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x126e8d170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x126e8d430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x126e8d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x126e8d9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x126e8dc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x126e8df30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x126e8e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x126e8e4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x126e8e770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x126e8ea30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x126e8ecf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x126e8efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x126e8f270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x126e8f530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x126e8fa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x126e8ffd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x126e90520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x126e90a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x126e90fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x126e91510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x126e91a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x126e91fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x126e92270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x126e92530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x126e92a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x126e92f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x126e93430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x126e93930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x126e93e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x126e94330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x126e94830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x126e94d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x126e95230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x126e95730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x126e95c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x126e96130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x126e96630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x126e96b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x126e97540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x126e97c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x126e98380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x126e98aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x126e98d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x126e99550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x126e99810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x126e99e20 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.917s
user	0m0.242s
sys	0m0.133s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
