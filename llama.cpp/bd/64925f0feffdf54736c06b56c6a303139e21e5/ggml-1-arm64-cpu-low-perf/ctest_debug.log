+ cmake -DCMAKE_BUILD_TYPE=Debug -DLLAMA_FATAL_WARNINGS=ON ..
-- The C compiler identification is GNU 11.4.0
-- The CXX compiler identification is GNU 11.4.0
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.34.1") 
-- Looking for pthread.h
-- Looking for pthread.h - found
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE  
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: aarch64
-- Including CPU backend
-- Found OpenMP_C: -fopenmp (found version "4.5") 
-- Found OpenMP_CXX: -fopenmp (found version "4.5") 
-- Found OpenMP: TRUE (found version "4.5")  
-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod - Success
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm - Failed
-- Performing Test GGML_MACHINE_SUPPORTS_sve
-- Performing Test GGML_MACHINE_SUPPORTS_sve - Failed
-- ARM feature DOTPROD enabled
-- ARM feature FMA enabled
-- ARM feature FP16_VECTOR_ARITHMETIC enabled
-- Adding CPU backend variant ggml-cpu: -mcpu=ares+crypto+noprofile+dotprod+noi8mm+nosve 
-- Configuring done
-- Generating done
-- Build files have been written to: /home/ggml/work/llama.cpp/build-ci-debug

real	0m2.070s
user	0m1.318s
sys	0m0.550s
++ nproc
+ make -j4
[  0%] Generating build details from Git
[  1%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  1%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
-- Found Git: /usr/bin/git (found version "2.34.1") 
[  2%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  2%] Built target sha256
[  3%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  3%] Built target xxhash
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  4%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  4%] Built target sha1
[  4%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  5%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o
[  6%] Built target build_info
[  6%] Linking CXX shared library ../../bin/libggml-base.so
[  6%] Built target ggml-base
[  6%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[  7%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[  8%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 10%] Linking CXX shared library ../../bin/libggml-cpu.so
[ 10%] Built target ggml-cpu
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 11%] Linking CXX shared library ../../bin/libggml.so
[ 11%] Built target ggml
[ 12%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 12%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 12%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 12%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o
[ 13%] Linking CXX executable ../../bin/llama-gguf
[ 14%] Linking CXX executable ../../bin/llama-gguf-hash
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o
[ 16%] Built target llama-gguf
[ 16%] Built target llama-gguf-hash
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-graph.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-io.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 23%] Linking CXX shared library ../bin/libllama.so
[ 23%] Built target llama
[ 26%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 26%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 26%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 26%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 26%] Linking C executable ../bin/test-c
[ 26%] Linking CXX executable ../../bin/llama-simple
[ 26%] Linking CXX executable ../../bin/llama-simple-chat
[ 26%] Building CXX object common/CMakeFiles/common.dir/chat.cpp.o
[ 26%] Built target test-c
[ 27%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 28%] Built target llama-simple
[ 28%] Built target llama-simple-chat
[ 28%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 29%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 29%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 29%] Building CXX object common/CMakeFiles/common.dir/llguidance.cpp.o
[ 30%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 31%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 31%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 32%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 32%] Built target llava
[ 32%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 33%] Linking CXX static library libllava_static.a
[ 34%] Linking CXX static library libcommon.a
[ 34%] Linking CXX shared library ../../bin/libllava_shared.so
[ 34%] Built target llava_static
[ 34%] Built target llava_shared
[ 34%] Built target common
[ 36%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 36%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 36%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 36%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 37%] Linking CXX executable ../bin/test-tokenizer-0
[ 37%] Linking CXX executable ../bin/test-sampling
[ 38%] Linking CXX executable ../bin/test-grammar-parser
[ 38%] Built target test-sampling
[ 38%] Built target test-grammar-parser
[ 38%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 41%] Linking CXX executable ../bin/test-llama-grammar
[ 41%] Linking CXX executable ../bin/test-grammar-integration
[ 41%] Built target test-llama-grammar
[ 42%] Building CXX object tests/CMakeFiles/test-chat.dir/test-chat.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-chat.dir/get-model.cpp.o
[ 43%] Linking CXX executable ../bin/test-chat
[ 43%] Built target test-tokenizer-0
[ 43%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 44%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 44%] Built target test-grammar-integration
[ 45%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 45%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 45%] Built target test-chat
[ 46%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 46%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 46%] Built target test-json-schema-to-grammar
[ 46%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 47%] Linking CXX executable ../bin/test-log
[ 47%] Built target test-tokenizer-1-bpe
[ 47%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 48%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 48%] Built target test-log
[ 48%] Linking CXX executable ../bin/test-arg-parser
[ 48%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 49%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 49%] Linking CXX executable ../bin/test-chat-template
[ 49%] Linking CXX executable ../../bin/llama-quantize-stats
[ 49%] Built target test-tokenizer-1-spm
[ 50%] Building CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o
[ 51%] Linking CXX executable ../bin/test-gguf
[ 51%] Built target test-gguf
[ 51%] Built target llama-quantize-stats
[ 51%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 53%] Linking CXX executable ../bin/test-model-load-cancel
[ 53%] Linking CXX executable ../bin/test-backend-ops
[ 53%] Built target test-model-load-cancel
[ 54%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 54%] Built target test-backend-ops
[ 54%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 55%] Linking CXX executable ../bin/test-autorelease
[ 56%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 57%] Linking CXX executable ../bin/test-barrier
[ 57%] Built target test-barrier
[ 57%] Built target test-autorelease
[ 58%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 58%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 58%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 59%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 60%] Linking CXX executable ../bin/test-quantize-fns
[ 60%] Linking CXX executable ../bin/test-quantize-perf
[ 60%] Built target test-quantize-fns
[ 60%] Built target test-quantize-perf
[ 61%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 62%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 62%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 62%] Linking CXX executable ../../bin/llama-batched-bench
[ 63%] Linking CXX executable ../bin/test-rope
[ 63%] Built target test-arg-parser
[ 64%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 64%] Linking CXX executable ../../bin/llama-batched
[ 64%] Built target test-rope
[ 65%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 65%] Linking CXX executable ../../bin/llama-embedding
[ 65%] Built target test-chat-template
[ 66%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 66%] Linking CXX executable ../../bin/llama-eval-callback
[ 66%] Built target llama-batched-bench
[ 66%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 67%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 67%] Built target llama-batched
[ 67%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 68%] Linking CXX executable ../../bin/llama-gguf-split
[ 68%] Built target llama-embedding
[ 68%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 69%] Linking CXX executable ../../bin/llama-gritlm
[ 69%] Built target llama-gbnf-validator
[ 69%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 70%] Linking CXX executable ../../bin/llama-imatrix
[ 70%] Built target llama-eval-callback
[ 70%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 70%] Built target llama-gguf-split
[ 71%] Linking CXX executable ../../bin/llama-infill
[ 72%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 72%] Linking CXX executable ../../bin/llama-bench
[ 72%] Built target llama-gritlm
[ 72%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 72%] Built target llama-bench
[ 72%] Linking CXX executable ../../bin/llama-lookahead
[ 73%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 73%] Linking CXX executable ../../bin/llama-lookup
[ 73%] Built target llama-imatrix
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 74%] Built target llama-infill
[ 74%] Linking CXX executable ../../bin/llama-lookup-create
[ 75%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 75%] Linking CXX executable ../../bin/llama-lookup-merge
[ 75%] Built target llama-lookup-merge
[ 76%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 76%] Linking CXX executable ../../bin/llama-lookup-stats
[ 76%] Built target llama-lookahead
[ 77%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 77%] Built target llama-lookup
[ 77%] Linking CXX executable ../../bin/llama-cli
[ 78%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 78%] Linking CXX executable ../../bin/llama-parallel
[ 78%] Built target llama-lookup-create
[ 79%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 79%] Linking CXX executable ../../bin/llama-passkey
[ 79%] Built target llama-lookup-stats
[ 80%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
/home/ggml/work/llama.cpp/examples/perplexity/perplexity.cpp: In lambda function:
/home/ggml/work/llama.cpp/examples/perplexity/perplexity.cpp:1735:41: note: parameter passing for argument of type ‘std::pair<double, double>’ when C++17 is enabled changed to match C++14 in GCC 10.1
 1735 |             return std::make_pair(0., 0.);
      |                                         ^
[ 80%] Linking CXX executable ../../bin/llama-perplexity
[ 80%] Built target llama-parallel
[ 81%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 81%] Built target llama-cli
[ 81%] Linking CXX executable ../../bin/llama-quantize
[ 81%] Built target llama-passkey
[ 81%] Generating loading.html.hpp
[ 81%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 82%] Generating index.html.gz.hpp
[ 83%] Linking CXX executable ../../bin/llama-retrieval
[ 83%] Built target llama-perplexity
[ 84%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 84%] Linking CXX executable ../../bin/llama-save-load-state
[ 84%] Built target llama-quantize
[ 84%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 85%] Building CXX object examples/run/CMakeFiles/llama-run.dir/linenoise.cpp/linenoise.cpp.o
[ 85%] Linking CXX executable ../../bin/llama-run
[ 85%] Built target llama-retrieval
[ 86%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 86%] Linking CXX executable ../../bin/llama-speculative
[ 86%] Built target llama-save-load-state
[ 87%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 87%] Linking CXX executable ../../bin/llama-speculative-simple
[ 88%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[ 88%] Linking CXX executable ../../bin/llama-server
[ 88%] Built target llama-run
[ 89%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 89%] Linking CXX executable ../../bin/llama-tokenize
[ 89%] Built target llama-speculative
[ 89%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 90%] Linking CXX executable ../../bin/llama-tts
[ 90%] Built target llama-speculative-simple
[ 90%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 91%] Linking CXX executable ../../bin/llama-gen-docs
[ 91%] Built target llama-tokenize
[ 92%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 92%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 92%] Built target llama-server
[ 93%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 93%] Linking CXX executable ../../bin/llama-cvector-generator
[ 93%] Built target llama-tts
[ 93%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 94%] Linking CXX executable ../../bin/llama-export-lora
[ 94%] Built target llama-gen-docs
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 95%] Linking CXX executable ../../bin/llama-llava-cli
[ 95%] Built target llama-convert-llama2c-to-ggml
[ 96%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 96%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 96%] Built target llama-cvector-generator
[ 96%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 97%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 97%] Built target llama-export-lora
[ 97%] Building CXX object examples/llava/CMakeFiles/llama-llava-clip-quantize-cli.dir/clip-quantize-cli.cpp.o
[ 98%] Linking CXX executable ../../bin/llama-llava-clip-quantize-cli
[ 98%] Built target llama-llava-cli
[ 98%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 99%] Linking CXX executable ../../bin/llama-vdot
[ 99%] Built target llama-llava-clip-quantize-cli
[ 99%] Built target llama-vdot
[100%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[100%] Linking CXX executable ../../bin/llama-q8dot
[100%] Built target llama-q8dot
[100%] Built target llama-minicpmv-cli
[100%] Built target llama-qwen2vl-cli

real	0m20.767s
user	0m59.512s
sys	0m10.670s
+ ctest --output-on-failure -L main -E test-opt
Test project /home/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-tokenizer-0-bert-bge
 1/29 Test  #1: test-tokenizer-0-bert-bge .........Subprocess aborted***Exception:   0.29 sec
main : reading vocab from: '/home/ggml/work/llama.cpp/tests/../models/ggml-vocab-bert-bge.gguf'
register_backend: registered backend CPU (1 devices)
register_device: registered device CPU (CPU)
llama_model_loader: loaded meta data with 20 key-value pairs and 0 tensors from /home/ggml/work/llama.cpp/tests/../models/ggml-vocab-bert-bge.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = bert
llama_model_loader: - kv   1:                               general.name str              = bert-bge
llama_model_loader: - kv   2:                           bert.block_count u32              = 12
llama_model_loader: - kv   3:                        bert.context_length u32              = 512
llama_model_loader: - kv   4:                      bert.embedding_length u32              = 384
llama_model_loader: - kv   5:                   bert.feed_forward_length u32              = 1536
llama_model_loader: - kv   6:                  bert.attention.head_count u32              = 12
llama_model_loader: - kv   7:          bert.attention.layer_norm_epsilon f32              = 0.000000
llama_model_loader: - kv   8:                          general.file_type u32              = 1
llama_model_loader: - kv   9:                      bert.attention.causal bool             = false
llama_model_loader: - kv  10:            tokenizer.ggml.token_type_count u32              = 2
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = bert
llama_model_loader: - kv  12:                         tokenizer.ggml.pre str              = bert-bge
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  15:            tokenizer.ggml.unknown_token_id u32              = 100
llama_model_loader: - kv  16:          tokenizer.ggml.seperator_token_id u32              = 102
llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  18:                tokenizer.ggml.cls_token_id u32              = 101
llama_model_loader: - kv  19:               tokenizer.ggml.mask_token_id u32              = 103
print_info: file format = GGUF V3 (latest)
print_info: file type   = F16
print_info: file size   = 0.00 MiB (nan BPW) 
init_tokenizer: initializing tokenizer for type 3
load: control token:    101 '[CLS]' is not marked as EOG
load: control token:    103 '[MASK]' is not marked as EOG
load: control token:      0 '[PAD]' is not marked as EOG
load: control token:    100 '[UNK]' is not marked as EOG
load: control token:    102 '[SEP]' is not marked as EOG
load: special tokens cache size = 5
load: token to piece cache size = 0.2032 MB
print_info: arch             = bert
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 0.00 K
print_info: general.name     = bert-bge
print_info: vocab type       = WPM
print_info: n_vocab          = 30522
print_info: n_merges         = 0
print_info: BOS token        = 101 '[CLS]'
print_info: UNK token        = 100 '[UNK]'
print_info: SEP token        = 102 '[SEP]'
print_info: PAD token        = 0 '[PAD]'
print_info: MASK token       = 103 '[MASK]'
print_info: LF token         = 0 '[PAD]'
print_info: max token length = 21
llama_model_load: vocab only - skipping tensors
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 512
llama_context: n_ctx_per_seq = 512
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: flash_attn    = 0
llama_context: freq_base     = 0.0
llama_context: freq_scale    = 1
llama_context: n_ctx_pre_seq (512) > n_ctx_train (0) -- possible training context overflow
llama_context_kv_self: n_ctx = 512
llama_context_kv_self: n_ctx = 512 (padded)
/home/ggml/work/llama.cpp/ggml/src/ggml-backend.cpp:1454: GGML_ASSERT(n_backends > 0) failed
/home/ggml/work/llama.cpp/build-ci-debug/bin/libggml-base.so(+0x532a0)[0xff72c43e32a0]
/home/ggml/work/llama.cpp/build-ci-debug/bin/libggml-base.so(+0x5346c)[0xff72c43e346c]
/home/ggml/work/llama.cpp/build-ci-debug/bin/libggml-base.so(ggml_abort+0x104)[0xff72c43e35a4]
/home/ggml/work/llama.cpp/build-ci-debug/bin/libggml-base.so(ggml_backend_sched_new+0x60)[0xff72c43fd718]
/home/ggml/work/llama.cpp/build-ci-debug/bin/libllama.so(_ZN13llama_context4initEv+0x31c)[0xff72c48e7294]
/home/ggml/work/llama.cpp/build-ci-debug/bin/libllama.so(llama_init_from_model+0x1c8)[0xff72c48b0860]
/home/ggml/work/llama.cpp/build-ci-debug/bin/test-tokenizer-0(+0x667c4)[0xab65f2d067c4]
/lib/aarch64-linux-gnu/libc.so.6(+0x273fc)[0xff72c3f073fc]
/lib/aarch64-linux-gnu/libc.so.6(__libc_start_main+0x98)[0xff72c3f074cc]
/home/ggml/work/llama.cpp/build-ci-debug/bin/test-tokenizer-0(+0x65670)[0xab65f2d05670]

      Start  2: test-tokenizer-0-command-r
 2/29 Test  #2: test-tokenizer-0-command-r ........Subprocess aborted***Exception:   3.58 sec
main : reading vocab from: '/home/ggml/work/llama.cpp/tests/../models/ggml-vocab-command-r.gguf'
register_backend: registered backend CPU (1 devices)
register_device: registered device CPU (CPU)
llama_model_loader: loaded meta data with 27 key-value pairs and 0 tensors from /home/ggml/work/llama.cpp/tests/../models/ggml-vocab-command-r.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = command-r
llama_model_loader: - kv   1:                               general.name str              = command-r
llama_model_loader: - kv   2:                      command-r.block_count u32              = 40
llama_model_loader: - kv   3:                   command-r.context_length u32              = 131072
llama_model_loader: - kv   4:                 command-r.embedding_length u32              = 8192
llama_model_loader: - kv   5:              command-r.feed_forward_length u32              = 22528
llama_model_loader: - kv   6:             command-r.attention.head_count u32              = 64
llama_model_loader: - kv   7:          command-r.attention.head_count_kv u32              = 64
llama_model_loader: - kv   8:                   command-r.rope.freq_base f32              = 8000000.000000
llama_model_loader: - kv   9:     command-r.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 1
llama_model_loader: - kv  11:                      command-r.logit_scale f32              = 0.062500
llama_model_loader: - kv  12:                command-r.rope.scaling.type str              = none
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = command-r
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,256000]  = ["<PAD>", "<UNK>", "<CLS>", "<SEP>", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,253333]  = ["Ġ Ġ", "Ġ t", "e r", "i n", "Ġ a...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 5
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 255001
llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  21:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  22:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  23:           tokenizer.chat_template.tool_use str              = {{ bos_token }}{% if messages[0]['rol...
llama_model_loader: - kv  24:                tokenizer.chat_template.rag str              = {{ bos_token }}{% if messages[0]['rol...
llama_model_loader: - kv  25:                   tokenizer.chat_templates arr[str,2]       = ["tool_use", "rag"]
llama_model_loader: - kv  26:                    tokenizer.chat_template str              = {{ bos_token }}{% if messages[0]['rol...
print_info: file format = GGUF V3 (latest)
print_info: file type   = F16
print_info: file size   = 0.00 MiB (nan BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token: 255001 '<|END_OF_TURN_TOKEN|>' is not marked as EOG
load: control token:      7 '<EOP_TOKEN>' is not marked as EOG
load: control token:      2 '<CLS>' is not marked as EOG
load: control token:      3 '<SEP>' is not marked as EOG
load: control token:      6 '<EOS_TOKEN>' is not marked as EOG
load: control token:      1 '<UNK>' is not marked as EOG
load: control token:      4 '<MASK_TOKEN>' is not marked as EOG
load: control token:      5 '<BOS_TOKEN>' is not marked as EOG
load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
load: special tokens cache size = 1008
load: token to piece cache size = 1.8528 MB
print_info: arch             = command-r
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 0.00 K
print_info: general.name     = command-r
print_info: vocab type       = BPE
print_info: n_vocab          = 256000
print_info: n_merges         = 253333
print_info: BOS token        = 5 '<BOS_TOKEN>'
print_info: EOS token        = 255001 '<|END_OF_TURN_TOKEN|>'
print_info: PAD token        = 0 '<PAD>'
print_info: LF token         = 206 'Ċ'
print_info: FIM PAD token    = 0 '<PAD>'
print_info: EOG token        = 0 '<PAD>'
print_info: EOG token        = 255001 '<|END_OF_TURN_TOKEN|>'
print_info: max token length = 1024
llama_model_load: vocab only - skipping tensors
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 512
llama_context: n_ctx_per_seq = 512
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: flash_attn    = 0
llama_context: freq_base     = 0.0
llama_context: freq_scale    = 1
llama_context: n_ctx_pre_seq (512) > n_ctx_train (0) -- possible training context overflow
llama_context_kv_self: n_ctx = 512
llama_context_kv_self: n_ctx = 512 (padded)
/home/ggml/work/llama.cpp/ggml/src/ggml-backend.cpp:1454: GGML_ASSERT(n_backends > 0) failed
/home/ggml/work/llama.cpp/build-ci-debug/bin/libggml-base.so(+0x532a0)[0xff68152732a0]
/home/ggml/work/llama.cpp/build-ci-debug/bin/libggml-base.so(+0x5346c)[0xff681527346c]
/home/ggml/work/llama.cpp/build-ci-debug/bin/libggml-base.so(ggml_abort+0x104)[0xff68152735a4]
/home/ggml/work/llama.cpp/build-ci-debug/bin/libggml-base.so(ggml_backend_sched_new+0x60)[0xff681528d718]
/home/ggml/work/llama.cpp/build-ci-debug/bin/libllama.so(_ZN13llama_context4initEv+0x31c)[0xff6815777294]
/home/ggml/work/llama.cpp/build-ci-debug/bin/libllama.so(llama_init_from_model+0x1c8)[0xff6815740860]
/home/ggml/work/llama.cpp/build-ci-debug/bin/test-tokenizer-0(+0x667c4)[0xab8b901467c4]
/lib/aarch64-linux-gnu/libc.so.6(+0x273fc)[0xff6814d973fc]
/lib/aarch64-linux-gnu/libc.so.6(__libc_start_main+0x98)[0xff6814d974cc]
/home/ggml/work/llama.cpp/build-ci-debug/bin/test-tokenizer-0(+0x65670)[0xab8b90145670]

      Start  3: test-tokenizer-0-deepseek-coder
 3/29 Test  #3: test-tokenizer-0-deepseek-coder ...Subprocess aborted***Exception:   0.48 sec
main : reading vocab from: '/home/ggml/work/llama.cpp/tests/../models/ggml-vocab-deepseek-coder.gguf'
register_backend: registered backend CPU (1 devices)
register_device: registered device CPU (CPU)
llama_model_loader: loaded meta data with 25 key-value pairs and 0 tensors from /home/ggml/work/llama.cpp/tests/../models/ggml-vocab-deepseek-coder.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = deepseek-coder
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 16384
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 100000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  10:                          general.file_type u32              = 1
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 32256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                    llama.rope.scaling.type str              = linear
llama_model_loader: - kv  14:                  llama.rope.scaling.factor f32              = 4.000000
llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = deepseek-coder
llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,32256]   = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,32256]   = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  19:                      tokenizer.ggml.merges arr[str,31757]   = ["Ġ Ġ", "Ġ t", "Ġ a", "i n", "h e...
llama_model_loader: - kv  20:                tokenizer.ggml.bos_token_id u32              = 32013
llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 32014
llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 32014
llama_model_loader: - kv  23:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  24:               tokenizer.ggml.add_eos_token bool             = false
print_info: file format = GGUF V3 (latest)
print_info: file type   = F16
print_info: file size   = 0.00 MiB (nan BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control-looking token:  32015 '<｜fim▁hole｜>' was not control-type; this is probably a bug in the model. its type will be overridden
load: control-looking token:  32017 '<｜fim▁end｜>' was not control-type; this is probably a bug in the model. its type will be overridden
load: control-looking token:  32016 '<｜fim▁begin｜>' was not control-type; this is probably a bug in the model. its type will be overridden
load: control token:  32015 '<｜fim▁hole｜>' is not marked as EOG
load: control token:  32014 '<｜end▁of▁sentence｜>' is not marked as EOG
load: control token:  32017 '<｜fim▁end｜>' is not marked as EOG
load: control token:  32016 '<｜fim▁begin｜>' is not marked as EOG
load: control token:  32013 '<｜begin▁of▁sentence｜>' is not marked as EOG
load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
load: special tokens cache size = 256
load: token to piece cache size = 0.1787 MB
print_info: arch             = llama
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 0.00 K
print_info: general.name     = deepseek-coder
print_info: vocab type       = BPE
print_info: n_vocab          = 32256
print_info: n_merges         = 31757
print_info: BOS token        = 32013 '<｜begin▁of▁sentence｜>'
print_info: EOS token        = 32014 '<｜end▁of▁sentence｜>'
print_info: EOT token        = 32014 '<｜end▁of▁sentence｜>'
print_info: PAD token        = 32014 '<｜end▁of▁sentence｜>'
print_info: LF token         = 185 'Ċ'
print_info: FIM PRE token    = 32016 '<｜fim▁begin｜>'
print_info: FIM SUF token    = 32015 '<｜fim▁hole｜>'
print_info: FIM MID token    = 32017 '<｜fim▁end｜>'
print_info: EOG token        = 32014 '<｜end▁of▁sentence｜>'
print_info: max token length = 128
llama_model_load: vocab only - skipping tensors
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 512
llama_context: n_ctx_per_seq = 512
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: flash_attn    = 0
llama_context: freq_base     = 0.0
llama_context: freq_scale    = 1
llama_context: n_ctx_pre_seq (512) > n_ctx_train (0) -- possible training context overflow
llama_context_kv_self: n_ctx = 512
llama_context_kv_self: n_ctx = 512 (padded)
/home/ggml/work/llama.cpp/ggml/src/ggml-backend.cpp:1454: GGML_ASSERT(n_backends > 0) failed
/home/ggml/work/llama.cpp/build-ci-debug/bin/libggml-base.so(+0x532a0)[0xffc6373632a0]
/home/ggml/work/llama.cpp/build-ci-debug/bin/libggml-base.so(+0x5346c)[0xffc63736346c]
/home/ggml/work/llama.cpp/build-ci-debug/bin/libggml-base.so(ggml_abort+0x104)[0xffc6373635a4]
/home/ggml/work/llama.cpp/build-ci-debug/bin/libggml-base.so(ggml_backend_sched_new+0x60)[0xffc63737d718]
/home/ggml/work/llama.cpp/build-ci-debug/bin/libllama.so(_ZN13llama_context4initEv+0x31c)[0xffc637867294]
/home/ggml/work/llama.cpp/build-ci-debug/bin/libllama.so(llama_init_from_model+0x1c8)[0xffc637830860]
/home/ggml/work/llama.cpp/build-ci-debug/bin/test-tokenizer-0(+0x667c4)[0xab67010067c4]
/lib/aarch64-linux-gnu/libc.so.6(+0x273fc)[0xffc636e873fc]
/lib/aarch64-linux-gnu/libc.so.6(__libc_start_main+0x98)[0xffc636e874cc]
/home/ggml/work/llama.cpp/build-ci-debug/bin/test-tokenizer-0(+0x65670)[0xab6701005670]

      Start  4: test-tokenizer-0-deepseek-llm
 4/29 Test  #4: test-tokenizer-0-deepseek-llm .....Subprocess aborted***Exception:   1.40 sec
main : reading vocab from: '/home/ggml/work/llama.cpp/tests/../models/ggml-vocab-deepseek-llm.gguf'
register_backend: registered backend CPU (1 devices)
register_device: registered device CPU (CPU)
llama_model_loader: loaded meta data with 23 key-value pairs and 0 tensors from /home/ggml/work/llama.cpp/tests/../models/ggml-vocab-deepseek-llm.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = deepseek-llm
llama_model_loader: - kv   2:                          llama.block_count u32              = 30
llama_model_loader: - kv   3:                       llama.context_length u32              = 4096
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 10000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  10:                          general.file_type u32              = 1
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 102400
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = deepseek-llm
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,102400]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,102400]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,99757]   = ["Ġ Ġ", "Ġ t", "Ġ a", "i n", "h e...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 100000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 100001
llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 100001
llama_model_loader: - kv  21:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  22:               tokenizer.ggml.add_eos_token bool             = false
print_info: file format = GGUF V3 (latest)
print_info: file type   = F16
print_info: file size   = 0.00 MiB (nan BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token: 100001 '<｜end▁of▁sentence｜>' is not marked as EOG
load: control token: 100000 '<｜begin▁of▁sentence｜>' is not marked as EOG
load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
load: special tokens cache size = 2400
load: token to piece cache size = 0.6658 MB
print_info: arch             = llama
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 0.00 K
print_info: general.name     = deepseek-llm
print_info: vocab type       = BPE
print_info: n_vocab          = 102400
print_info: n_merges         = 99757
print_info: BOS token        = 100000 '<｜begin▁of▁sentence｜>'
print_info: EOS token        = 100001 '<｜end▁of▁sentence｜>'
print_info: EOT token        = 100001 '<｜end▁of▁sentence｜>'
print_info: PAD token        = 100001 '<｜end▁of▁sentence｜>'
print_info: LF token         = 185 'Ċ'
print_info: EOG token        = 100001 '<｜end▁of▁sentence｜>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 512
llama_context: n_ctx_per_seq = 512
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: flash_attn    = 0
llama_context: freq_base     = 0.0
llama_context: freq_scale    = 1
llama_context: n_ctx_pre_seq (512) > n_ctx_train (0) -- possible training context overflow
llama_context_kv_self: n_ctx = 512
llama_context_kv_self: n_ctx = 512 (padded)
/home/ggml/work/llama.cpp/ggml/src/ggml-backend.cpp:1454: GGML_ASSERT(n_backends > 0) failed
/home/ggml/work/llama.cpp/build-ci-debug/bin/libggml-base.so(+0x532a0)[0xff4cc4d332a0]
/home/ggml/work/llama.cpp/build-ci-debug/bin/libggml-base.so(+0x5346c)[0xff4cc4d3346c]
/home/ggml/work/llama.cpp/build-ci-debug/bin/libggml-base.so(ggml_abort+0x104)[0xff4cc4d335a4]
/home/ggml/work/llama.cpp/build-ci-debug/bin/libggml-base.so(ggml_backend_sched_new+0x60)[0xff4cc4d4d718]
/home/ggml/work/llama.cpp/build-ci-debug/bin/libllama.so(_ZN13llama_context4initEv+0x31c)[0xff4cc5237294]
/home/ggml/work/llama.cpp/build-ci-debug/bin/libllama.so(llama_init_from_model+0x1c8)[0xff4cc5200860]
/home/ggml/work/llama.cpp/build-ci-debug/bin/test-tokenizer-0(+0x667c4)[0xab86dd9e67c4]
/lib/aarch64-linux-gnu/libc.so.6(+0x273fc)[0xff4cc48573fc]
/lib/aarch64-linux-gnu/libc.so.6(__libc_start_main+0x98)[0xff4cc48574cc]
/home/ggml/work/llama.cpp/build-ci-debug/bin/test-tokenizer-0(+0x65670)[0xab86dd9e5670]

      Start  5: test-tokenizer-0-falcon
 5/29 Test  #5: test-tokenizer-0-falcon ...........Subprocess aborted***Exception:   0.90 sec
main : reading vocab from: '/home/ggml/work/llama.cpp/tests/../models/ggml-vocab-falcon.gguf'
register_backend: registered backend CPU (1 devices)
register_device: registered device CPU (CPU)
llama_model_loader: loaded meta data with 18 key-value pairs and 0 tensors from /home/ggml/work/llama.cpp/tests/../models/ggml-vocab-falcon.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = falcon
llama_model_loader: - kv   1:                               general.name str              = Falcon
llama_model_loader: - kv   2:                      falcon.context_length u32              = 2048
llama_model_loader: - kv   3:                  falcon.tensor_data_layout str              = jploski
llama_model_loader: - kv   4:                    falcon.embedding_length u32              = 4544
llama_model_loader: - kv   5:                 falcon.feed_forward_length u32              = 18176
llama_model_loader: - kv   6:                         falcon.block_count u32              = 32
llama_model_loader: - kv   7:                falcon.attention.head_count u32              = 71
llama_model_loader: - kv   8:             falcon.attention.head_count_kv u32              = 1
llama_model_loader: - kv   9:        falcon.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 1
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  12:                         tokenizer.ggml.pre str              = falcon
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,65024]   = [">>TITLE<<", ">>ABSTRACT<<", ">>INTR...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,65024]   = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...
llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,64784]   = ["Ġ t", "Ġ a", "i n", "h e", "r e",...
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 11
llama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 11
print_info: file format = GGUF V3 (latest)
print_info: file type   = F16
print_info: file size   = 0.00 MiB (nan BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      3 '>>SUMMARY<<' is not marked as EOG
load: control token:      5 '>>ANSWER<<' is not marked as EOG
load: control token:      0 '>>TITLE<<' is not marked as EOG
load: control token:      7 '>>DOMAIN<<' is not marked as EOG
load: control token:      1 '>>ABSTRACT<<' is not marked as EOG
load: control token:      4 '>>COMMENT<<' is not marked as EOG
load: control token:      8 '>>PREFIX<<' is not marked as EOG
load: control token:      6 '>>QUESTION<<' is not marked as EOG
load: control token:     10 '>>MIDDLE<<' is not marked as EOG
load: control token:      9 '>>SUFFIX<<' is not marked as EOG
load: control token:      2 '>>INTRODUCTION<<' is not marked as EOG
load: special tokens cache size = 12
load: token to piece cache size = 0.3884 MB
print_info: arch             = falcon
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 0.00 K
print_info: general.name     = Falcon
print_info: vocab type       = BPE
print_info: n_vocab          = 65024
print_info: n_merges         = 64784
print_info: BOS token        = 11 '<|endoftext|>'
print_info: EOS token        = 11 '<|endoftext|>'
print_info: EOT token        = 11 '<|endoftext|>'
print_info: LF token         = 193 'Ċ'
print_info: EOG token        = 11 '<|endoftext|>'
print_info: max token length = 130
llama_model_load: vocab only - skipping tensors
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 512
llama_context: n_ctx_per_seq = 512
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: flash_attn    = 0
llama_context: freq_base     = 0.0
llama_context: freq_scale    = 1
llama_context: n_ctx_pre_seq (512) > n_ctx_train (0) -- possible training context overflow
llama_context_kv_self: n_ctx = 512
llama_context_kv_self: n_ctx = 512 (padded)
/home/ggml/work/llama.cpp/ggml/src/ggml-backend.cpp:1454: GGML_ASSERT(n_backends > 0) failed
/home/ggml/work/llama.cpp/build-ci-debug/bin/libggml-base.so(+0x532a0)[0xffc0754b32a0]
/home/ggml/work/llama.cpp/build-ci-debug/bin/libggml-base.so(+0x5346c)[0xffc0754b346c]
/home/ggml/work/llama.cpp/build-ci-debug/bin/libggml-base.so(ggml_abort+0x104)[0xffc0754b35a4]
/home/ggml/work/llama.cpp/build-ci-debug/bin/libggml-base.so(ggml_backend_sched_new+0x60)[0xffc0754cd718]
/home/ggml/work/llama.cpp/build-ci-debug/bin/libllama.so(_ZN13llama_context4initEv+0x31c)[0xffc0759b7294]
/home/ggml/work/llama.cpp/build-ci-debug/bin/libllama.so(llama_init_from_model+0x1c8)[0xffc075980860]
/home/ggml/work/llama.cpp/build-ci-debug/bin/test-tokenizer-0(+0x667c4)[0xab94915867c4]
/lib/aarch64-linux-gnu/libc.so.6(+0x273fc)[0xffc074fd73fc]
/lib/aarch64-linux-gnu/libc.so.6(__libc_start_main+0x98)[0xffc074fd74cc]
/home/ggml/work/llama.cpp/build-ci-debug/bin/test-tokenizer-0(+0x65670)[0xab9491585670]

      Start  6: test-tokenizer-0-gpt-2
 6/29 Test  #6: test-tokenizer-0-gpt-2 ............Subprocess aborted***Exception:   0.74 sec
main : reading vocab from: '/home/ggml/work/llama.cpp/tests/../models/ggml-vocab-gpt-2.gguf'
register_backend: registered backend CPU (1 devices)
register_device: registered device CPU (CPU)
llama_model_loader: loaded meta data with 16 key-value pairs and 0 tensors from /home/ggml/work/llama.cpp/tests/../models/ggml-vocab-gpt-2.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gpt2
llama_model_loader: - kv   1:                               general.name str              = gpt-2
llama_model_loader: - kv   2:                           gpt2.block_count u32              = 12
llama_model_loader: - kv   3:                        gpt2.context_length u32              = 1024
llama_model_loader: - kv   4:                      gpt2.embedding_length u32              = 768
llama_model_loader: - kv   5:                   gpt2.feed_forward_length u32              = 3072
llama_model_loader: - kv   6:                  gpt2.attention.head_count u32              = 12
llama_model_loader: - kv   7:          gpt2.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv   8:                          general.file_type u32              = 1
llama_model_loader: - kv   9:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  10:                         tokenizer.ggml.pre str              = gpt-2
llama_model_loader: - kv  11:                      tokenizer.ggml.tokens arr[str,50257]   = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  12:                  tokenizer.ggml.token_type arr[i32,50257]   = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  13:                      tokenizer.ggml.merges arr[str,50000]   = ["Ġ t", "Ġ a", "h e", "i n", "r e",...
llama_model_loader: - kv  14:                tokenizer.ggml.bos_token_id u32              = 50256
llama_model_loader: - kv  15:                tokenizer.ggml.eos_token_id u32              = 50256
print_info: file format = GGUF V3 (latest)
print_info: file type   = F16
print_info: file size   = 0.00 MiB (nan BPW) 
init_tokenizer: initializing tokenizer for type 2
load: special tokens cache size = 1
load: token to piece cache size = 0.3060 MB
print_info: arch             = gpt2
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 0.00 K
print_info: general.name     = gpt-2
print_info: vocab type       = BPE
print_info: n_vocab          = 50257
print_info: n_merges         = 50000
print_info: BOS token        = 50256 '<|endoftext|>'
print_info: EOS token        = 50256 '<|endoftext|>'
print_info: EOT token        = 50256 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 50256 '<|endoftext|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 512
llama_context: n_ctx_per_seq = 512
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: flash_attn    = 0
llama_context: freq_base     = 0.0
llama_context: freq_scale    = 1
llama_context: n_ctx_pre_seq (512) > n_ctx_train (0) -- possible training context overflow
llama_context_kv_self: n_ctx = 512
llama_context_kv_self: n_ctx = 512 (padded)
/home/ggml/work/llama.cpp/ggml/src/ggml-backend.cpp:1454: GGML_ASSERT(n_backends > 0) failed
/home/ggml/work/llama.cpp/build-ci-debug/bin/libggml-base.so(+0x532a0)[0xffe7b9e632a0]
/home/ggml/work/llama.cpp/build-ci-debug/bin/libggml-base.so(+0x5346c)[0xffe7b9e6346c]
/home/ggml/work/llama.cpp/build-ci-debug/bin/libggml-base.so(ggml_abort+0x104)[0xffe7b9e635a4]
/home/ggml/work/llama.cpp/build-ci-debug/bin/libggml-base.so(ggml_backend_sched_new+0x60)[0xffe7b9e7d718]
/home/ggml/work/llama.cpp/build-ci-debug/bin/libllama.so(_ZN13llama_context4initEv+0x31c)[0xffe7ba367294]
/home/ggml/work/llama.cpp/build-ci-debug/bin/libllama.so(llama_init_from_model+0x1c8)[0xffe7ba330860]
/home/ggml/work/llama.cpp/build-ci-debug/bin/test-tokenizer-0(+0x667c4)[0xab924eee67c4]
/lib/aarch64-linux-gnu/libc.so.6(+0x273fc)[0xffe7b99873fc]
/lib/aarch64-linux-gnu/libc.so.6(__libc_start_main+0x98)[0xffe7b99874cc]
/home/ggml/work/llama.cpp/build-ci-debug/bin/test-tokenizer-0(+0x65670)[0xab924eee5670]

      Start  7: test-tokenizer-0-llama-bpe
 7/29 Test  #7: test-tokenizer-0-llama-bpe ........Subprocess aborted***Exception:   2.31 sec
main : reading vocab from: '/home/ggml/work/llama.cpp/tests/../models/ggml-vocab-llama-bpe.gguf'
register_backend: registered backend CPU (1 devices)
register_device: registered device CPU (CPU)
llama_model_loader: loaded meta data with 20 key-value pairs and 0 tensors from /home/ggml/work/llama.cpp/tests/../models/ggml-vocab-llama-bpe.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = llama-bpe
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 1
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128001
print_info: file format = GGUF V3 (latest)
print_info: file type   = F16
print_info: file size   = 0.00 MiB (nan BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token: 128255 '<|reserved_special_token_250|>' is not marked as EOG
load: control token: 128254 '<|reserved_special_token_249|>' is not marked as EOG
load: control token: 128253 '<|reserved_special_token_248|>' is not marked as EOG
load: control token: 128251 '<|reserved_special_token_246|>' is not marked as EOG
load: control token: 128246 '<|reserved_special_token_241|>' is not marked as EOG
load: control token: 128243 '<|reserved_special_token_238|>' is not marked as EOG
load: control token: 128240 '<|reserved_special_token_235|>' is not marked as EOG
load: control token: 128239 '<|reserved_special_token_234|>' is not marked as EOG
load: control token: 128238 '<|reserved_special_token_233|>' is not marked as EOG
load: control token: 128237 '<|reserved_special_token_232|>' is not marked as EOG
load: control token: 128232 '<|reserved_special_token_227|>' is not marked as EOG
load: control token: 128228 '<|reserved_special_token_223|>' is not marked as EOG
load: control token: 128227 '<|reserved_special_token_222|>' is not marked as EOG
load: control token: 128225 '<|reserved_special_token_220|>' is not marked as EOG
load: control token: 128222 '<|reserved_special_token_217|>' is not marked as EOG
load: control token: 128215 '<|reserved_special_token_210|>' is not marked as EOG
load: control token: 128211 '<|reserved_special_token_206|>' is not marked as EOG
load: control token: 128210 '<|reserved_special_token_205|>' is not marked as EOG
load: control token: 128204 '<|reserved_special_token_199|>' is not marked as EOG
load: control token: 128203 '<|reserved_special_token_198|>' is not marked as EOG
load: control token: 128201 '<|reserved_special_token_196|>' is not marked as EOG
load: control token: 128197 '<|reserved_special_token_192|>' is not marked as EOG
load: control token: 128196 '<|reserved_special_token_191|>' is not marked as EOG
load: control token: 128195 '<|reserved_special_token_190|>' is not marked as EOG
load: control token: 128193 '<|reserved_special_token_188|>' is not marked as EOG
load: control token: 128191 '<|reserved_special_token_186|>' is not marked as EOG
load: control token: 128190 '<|reserved_special_token_185|>' is not marked as EOG
load: control token: 128185 '<|reserved_special_token_180|>' is not marked as EOG
load: control token: 128184 '<|reserved_special_token_179|>' is not marked as EOG
load: control token: 128182 '<|reserved_special_token_177|>' is not marked as EOG
load: control token: 128181 '<|reserved_special_token_176|>' is not marked as EOG
load: control token: 128177 '<|reserved_special_token_172|>' is not marked as EOG
load: control token: 128176 '<|reserved_special_token_171|>' is not marked as EOG
load: control token: 128175 '<|reserved_special_token_170|>' is not marked as EOG
load: control token: 128174 '<|reserved_special_token_169|>' is not marked as EOG
load: control token: 128173 '<|reserved_special_token_168|>' is not marked as EOG
load: control token: 128172 '<|reserved_special_token_167|>' is not marked as EOG
load: control token: 128168 '<|reserved_special_token_163|>' is not marked as EOG
load: control token: 128167 '<|reserved_special_token_162|>' is not marked as EOG
load: control token: 128166 '<|reserved_special_token_161|>' is not marked as EOG
load: control token: 128165 '<|reserved_special_token_160|>' is not marked as EOG
load: control token: 128162 '<|reserved_special_token_157|>' is not marked as EOG
load: control token: 128159 '<|reserved_special_token_154|>' is not marked as EOG
load: control token: 128155 '<|reserved_special_token_150|>' is not marked as EOG
load: control token: 128153 '<|reserved_special_token_148|>' is not marked as EOG
load: control token: 128152 '<|reserved_special_token_147|>' is not marked as EOG
load: control token: 128151 '<|reserved_special_token_146|>' is not marked as EOG
load: control token: 128148 '<|reserved_special_token_143|>' is not marked as EOG
load: control token: 128146 '<|reserved_special_token_141|>' is not marked as EOG
load: control token: 128144 '<|reserved_special_token_139|>' is not marked as EOG
load: control token: 128143 '<|reserved_special_token_138|>' is not marked as EOG
load: control token: 128141 '<|reserved_special_token_136|>' is not marked as EOG
load: control token: 128139 '<|reserved_special_token_134|>' is not marked as EOG
load: control token: 128138 '<|reserved_special_token_133|>' is not marked as EOG
load: control token: 128135 '<|reserved_special_token_130|>' is not marked as EOG
load: control token: 128133 '<|reserved_special_token_128|>' is not marked as EOG
load: control token: 128132 '<|reserved_special_token_127|>' is not marked as EOG
load: control token: 128131 '<|reserved_special_token_126|>' is not marked as EOG
load: control token: 128130 '<|reserved_special_token_125|>' is not marked as EOG
load: control token: 128128 '<|reserved_special_token_123|>' is not marked as EOG
load: control token: 128125 '<|reserved_special_token_120|>' is not marked as EOG
load: control token: 128121 '<|reserved_special_token_116|>' is not marked as EOG
load: control token: 128120 '<|reserved_special_token_115|>' is not marked as EOG
load: control token: 128119 '<|reserved_special_token_114|>' is not marked as EOG
load: control token: 128116 '<|reserved_special_token_111|>' is not marked as EOG
load: control token: 128112 '<|reserved_special_token_107|>' is not marked as EOG
load: control token: 128109 '<|reserved_special_token_104|>' is not marked as EOG
load: control token: 128107 '<|reserved_special_token_102|>' is not marked as EOG
load: control token: 128106 '<|reserved_special_token_101|>' is not marked as EOG
load: control token: 128105 '<|reserved_special_token_100|>' is not marked as EOG
load: control token: 128103 '<|reserved_special_token_98|>' is not marked as EOG
load: control token: 128100 '<|reserved_special_token_95|>' is not marked as EOG
load: control token: 128099 '<|reserved_special_token_94|>' is not marked as EOG
load: control token: 128098 '<|reserved_special_token_93|>' is not marked as EOG
load: control token: 128094 '<|reserved_special_token_89|>' is not marked as EOG
load: control token: 128088 '<|reserved_special_token_83|>' is not marked as EOG
load: control token: 128087 '<|reserved_special_token_82|>' is not marked as EOG
load: control token: 128086 '<|reserved_special_token_81|>' is not marked as EOG
load: control token: 128084 '<|reserved_special_token_79|>' is not marked as EOG
load: control token: 128082 '<|reserved_special_token_77|>' is not marked as EOG
load: control token: 128078 '<|reserved_special_token_73|>' is not marked as EOG
load: control token: 128075 '<|reserved_special_token_70|>' is not marked as EOG
load: control token: 128073 '<|reserved_special_token_68|>' is not marked as EOG
load: control token: 128072 '<|reserved_special_token_67|>' is not marked as EOG
load: control token: 128070 '<|reserved_special_token_65|>' is not marked as EOG
load: control token: 128065 '<|reserved_special_token_60|>' is not marked as EOG
load: control token: 128064 '<|reserved_special_token_59|>' is not marked as EOG
load: control token: 128062 '<|reserved_special_token_57|>' is not marked as EOG
load: control token: 128060 '<|reserved_special_token_55|>' is not marked as EOG
load: control token: 128059 '<|reserved_special_token_54|>' is not marked as EOG
load: control token: 128057 '<|reserved_special_token_52|>' is not marked as EOG
load: control token: 128056 '<|reserved_special_token_51|>' is not marked as EOG
load: control token: 128054 '<|reserved_special_token_49|>' is not marked as EOG
load: control token: 128051 '<|reserved_special_token_46|>' is not marked as EOG
load: control token: 128043 '<|reserved_special_token_38|>' is not marked as EOG
load: control token: 128042 '<|reserved_special_token_37|>' is not marked as EOG
load: control token: 128041 '<|reserved_special_token_36|>' is not marked as EOG
load: control token: 128040 '<|reserved_special_token_35|>' is not marked as EOG
load: control token: 128035 '<|reserved_special_token_30|>' is not marked as EOG
load: control token: 128033 '<|reserved_special_token_28|>' is not marked as EOG
load: control token: 128032 '<|reserved_special_token_27|>' is not marked as EOG
load: control token: 128029 '<|reserved_special_token_24|>' is not marked as EOG
load: control token: 128025 '<|reserved_special_token_20|>' is not marked as EOG
load: control token: 128024 '<|reserved_special_token_19|>' is not marked as EOG
load: control token: 128021 '<|reserved_special_token_16|>' is not marked as EOG
load: control token: 128020 '<|reserved_special_token_15|>' is not marked as EOG
load: control token: 128019 '<|reserved_special_token_14|>' is not marked as EOG
load: control token: 128018 '<|reserved_special_token_13|>' is not marked as EOG
load: control token: 128015 '<|reserved_special_token_10|>' is not marked as EOG
load: control token: 128013 '<|reserved_special_token_8|>' is not marked as EOG
load: control token: 128012 '<|reserved_special_token_7|>' is not marked as EOG
load: control token: 128010 '<|reserved_special_token_5|>' is not marked as EOG
load: control token: 128005 '<|reserved_special_token_3|>' is not marked as EOG
load: control token: 128004 '<|reserved_special_token_2|>' is not marked as EOG
load: control token: 128002 '<|reserved_special_token_0|>' is not marked as EOG
load: control token: 128249 '<|reserved_special_token_244|>' is not marked as EOG
load: control token: 128187 '<|reserved_special_token_182|>' is not marked as EOG
load: control token: 128180 '<|reserved_special_token_175|>' is not marked as EOG
load: control token: 128134 '<|reserved_special_token_129|>' is not marked as EOG
load: control token: 128179 '<|reserved_special_token_174|>' is not marked as EOG
load: control token: 128037 '<|reserved_special_token_32|>' is not marked as EOG
load: control token: 128045 '<|reserved_special_token_40|>' is not marked as EOG
load: control token: 128089 '<|reserved_special_token_84|>' is not marked as EOG
load: control token: 128212 '<|reserved_special_token_207|>' is not marked as EOG
load: control token: 128104 '<|reserved_special_token_99|>' is not marked as EOG
load: control token: 128205 '<|reserved_special_token_200|>' is not marked as EOG
load: control token: 128142 '<|reserved_special_token_137|>' is not marked as EOG
load: control token: 128028 '<|reserved_special_token_23|>' is not marked as EOG
load: control token: 128126 '<|reserved_special_token_121|>' is not marked as EOG
load: control token: 128198 '<|reserved_special_token_193|>' is not marked as EOG
load: control token: 128071 '<|reserved_special_token_66|>' is not marked as EOG
load: control token: 128092 '<|reserved_special_token_87|>' is not marked as EOG
load: control token: 128183 '<|reserved_special_token_178|>' is not marked as EOG
load: control token: 128140 '<|reserved_special_token_135|>' is not marked as EOG
load: control token: 128226 '<|reserved_special_token_221|>' is not marked as EOG
load: control token: 128007 '<|end_header_id|>' is not marked as EOG
load: control token: 128052 '<|reserved_special_token_47|>' is not marked as EOG
load: control token: 128053 '<|reserved_special_token_48|>' is not marked as EOG
load: control token: 128058 '<|reserved_special_token_53|>' is not marked as EOG
load: control token: 128150 '<|reserved_special_token_145|>' is not marked as EOG
load: control token: 128149 '<|reserved_special_token_144|>' is not marked as EOG
load: control token: 128209 '<|reserved_special_token_204|>' is not marked as EOG
load: control token: 128169 '<|reserved_special_token_164|>' is not marked as EOG
load: control token: 128157 '<|reserved_special_token_152|>' is not marked as EOG
load: control token: 128038 '<|reserved_special_token_33|>' is not marked as EOG
load: control token: 128178 '<|reserved_special_token_173|>' is not marked as EOG
load: control token: 128091 '<|reserved_special_token_86|>' is not marked as EOG
load: control token: 128115 '<|reserved_special_token_110|>' is not marked as EOG
load: control token: 128233 '<|reserved_special_token_228|>' is not marked as EOG
load: control token: 128145 '<|reserved_special_token_140|>' is not marked as EOG
load: control token: 128039 '<|reserved_special_token_34|>' is not marked as EOG
load: control token: 128136 '<|reserved_special_token_131|>' is not marked as EOG
load: control token: 128170 '<|reserved_special_token_165|>' is not marked as EOG
load: control token: 128236 '<|reserved_special_token_231|>' is not marked as EOG
load: control token: 128154 '<|reserved_special_token_149|>' is not marked as EOG
load: control token: 128049 '<|reserved_special_token_44|>' is not marked as EOG
load: control token: 128023 '<|reserved_special_token_18|>' is not marked as EOG
load: control token: 128003 '<|reserved_special_token_1|>' is not marked as EOG
load: control token: 128016 '<|reserved_special_token_11|>' is not marked as EOG
load: control token: 128113 '<|reserved_special_token_108|>' is not marked as EOG
load: control token: 128158 '<|reserved_special_token_153|>' is not marked as EOG
load: control token: 128223 '<|reserved_special_token_218|>' is not marked as EOG
load: control token: 128156 '<|reserved_special_token_151|>' is not marked as EOG
load: control token: 128008 '<|reserved_special_token_4|>' is not marked as EOG
load: control token: 128085 '<|reserved_special_token_80|>' is not marked as EOG
load: control token: 128160 '<|reserved_special_token_155|>' is not marked as EOG
load: control token: 128001 '<|end_of_text|>' is not marked as EOG
load: control token: 128110 '<|reserved_special_token_105|>' is not marked as EOG
load: control token: 128247 '<|reserved_special_token_242|>' is not marked as EOG
load: control token: 128122 '<|reserved_special_token_117|>' is not marked as EOG
load: control token: 128050 '<|reserved_special_token_45|>' is not marked as EOG
load: control token: 128221 '<|reserved_special_token_216|>' is not marked as EOG
load: control token: 128244 '<|reserved_special_token_239|>' is not marked as EOG
load: control token: 128248 '<|reserved_special_token_243|>' is not marked as EOG
load: control token: 128213 '<|reserved_special_token_208|>' is not marked as EOG
load: control token: 128006 '<|start_header_id|>' is not marked as EOG
load: control token: 128208 '<|reserved_special_token_203|>' is not marked as EOG
load: control token: 128074 '<|reserved_special_token_69|>' is not marked as EOG
load: control token: 128234 '<|reserved_special_token_229|>' is not marked as EOG
load: control token: 128083 '<|reserved_special_token_78|>' is not marked as EOG
load: control token: 128224 '<|reserved_special_token_219|>' is not marked as EOG
load: control token: 128055 '<|reserved_special_token_50|>' is not marked as EOG
load: control token: 128097 '<|reserved_special_token_92|>' is not marked as EOG
load: control token: 128206 '<|reserved_special_token_201|>' is not marked as EOG
load: control token: 128081 '<|reserved_special_token_76|>' is not marked as EOG
load: control token: 128068 '<|reserved_special_token_63|>' is not marked as EOG
load: control token: 128067 '<|reserved_special_token_62|>' is not marked as EOG
load: control token: 128046 '<|reserved_special_token_41|>' is not marked as EOG
load: control token: 128194 '<|reserved_special_token_189|>' is not marked as EOG
load: control token: 128069 '<|reserved_special_token_64|>' is not marked as EOG
load: control token: 128000 '<|begin_of_text|>' is not marked as EOG
load: control token: 128220 '<|reserved_special_token_215|>' is not marked as EOG
load: control token: 128214 '<|reserved_special_token_209|>' is not marked as EOG
load: control token: 128108 '<|reserved_special_token_103|>' is not marked as EOG
load: control token: 128200 '<|reserved_special_token_195|>' is not marked as EOG
load: control token: 128048 '<|reserved_special_token_43|>' is not marked as EOG
load: control token: 128027 '<|reserved_special_token_22|>' is not marked as EOG
load: control token: 128114 '<|reserved_special_token_109|>' is not marked as EOG
load: control token: 128235 '<|reserved_special_token_230|>' is not marked as EOG
load: control token: 128252 '<|reserved_special_token_247|>' is not marked as EOG
load: control token: 128199 '<|reserved_special_token_194|>' is not marked as EOG
load: control token: 128129 '<|reserved_special_token_124|>' is not marked as EOG
load: control token: 128245 '<|reserved_special_token_240|>' is not marked as EOG
load: control token: 128164 '<|reserved_special_token_159|>' is not marked as EOG
load: control token: 128124 '<|reserved_special_token_119|>' is not marked as EOG
load: control token: 128102 '<|reserved_special_token_97|>' is not marked as EOG
load: control token: 128036 '<|reserved_special_token_31|>' is not marked as EOG
load: control token: 128229 '<|reserved_special_token_224|>' is not marked as EOG
load: control token: 128163 '<|reserved_special_token_158|>' is not marked as EOG
load: control token: 128127 '<|reserved_special_token_122|>' is not marked as EOG
load: control token: 128111 '<|reserved_special_token_106|>' is not marked as EOG
load: control token: 128231 '<|reserved_special_token_226|>' is not marked as EOG
load: control token: 128188 '<|reserved_special_token_183|>' is not marked as EOG
load: control token: 128061 '<|reserved_special_token_56|>' is not marked as EOG
load: control token: 128137 '<|reserved_special_token_132|>' is not marked as EOG
load: control token: 128093 '<|reserved_special_token_88|>' is not marked as EOG
load: control token: 128095 '<|reserved_special_token_90|>' is not marked as EOG
load: control token: 128189 '<|reserved_special_token_184|>' is not marked as EOG
load: control token: 128090 '<|reserved_special_token_85|>' is not marked as EOG
load: control token: 128147 '<|reserved_special_token_142|>' is not marked as EOG
load: control token: 128219 '<|reserved_special_token_214|>' is not marked as EOG
load: control token: 128230 '<|reserved_special_token_225|>' is not marked as EOG
load: control token: 128217 '<|reserved_special_token_212|>' is not marked as EOG
load: control token: 128031 '<|reserved_special_token_26|>' is not marked as EOG
load: control token: 128030 '<|reserved_special_token_25|>' is not marked as EOG
load: control token: 128250 '<|reserved_special_token_245|>' is not marked as EOG
load: control token: 128192 '<|reserved_special_token_187|>' is not marked as EOG
load: control token: 128096 '<|reserved_special_token_91|>' is not marked as EOG
load: control token: 128186 '<|reserved_special_token_181|>' is not marked as EOG
load: control token: 128207 '<|reserved_special_token_202|>' is not marked as EOG
load: control token: 128171 '<|reserved_special_token_166|>' is not marked as EOG
load: control token: 128080 '<|reserved_special_token_75|>' is not marked as EOG
load: control token: 128077 '<|reserved_special_token_72|>' is not marked as EOG
load: control token: 128101 '<|reserved_special_token_96|>' is not marked as EOG
load: control token: 128079 '<|reserved_special_token_74|>' is not marked as EOG
load: control token: 128216 '<|reserved_special_token_211|>' is not marked as EOG
load: control token: 128014 '<|reserved_special_token_9|>' is not marked as EOG
load: control token: 128047 '<|reserved_special_token_42|>' is not marked as EOG
load: control token: 128202 '<|reserved_special_token_197|>' is not marked as EOG
load: control token: 128044 '<|reserved_special_token_39|>' is not marked as EOG
load: control token: 128161 '<|reserved_special_token_156|>' is not marked as EOG
load: control token: 128017 '<|reserved_special_token_12|>' is not marked as EOG
load: control token: 128066 '<|reserved_special_token_61|>' is not marked as EOG
load: control token: 128242 '<|reserved_special_token_237|>' is not marked as EOG
load: control token: 128118 '<|reserved_special_token_113|>' is not marked as EOG
load: control token: 128076 '<|reserved_special_token_71|>' is not marked as EOG
load: control token: 128034 '<|reserved_special_token_29|>' is not marked as EOG
load: control token: 128241 '<|reserved_special_token_236|>' is not marked as EOG
load: control token: 128026 '<|reserved_special_token_21|>' is not marked as EOG
load: control token: 128218 '<|reserved_special_token_213|>' is not marked as EOG
load: control token: 128063 '<|reserved_special_token_58|>' is not marked as EOG
load: control token: 128117 '<|reserved_special_token_112|>' is not marked as EOG
load: control token: 128011 '<|reserved_special_token_6|>' is not marked as EOG
load: control token: 128022 '<|reserved_special_token_17|>' is not marked as EOG
load: control token: 128123 '<|reserved_special_token_118|>' is not marked as EOG
load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
load: special tokens cache size = 256
load: token to piece cache size = 0.8000 MB
print_info: arch             = llama
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 0.00 K
print_info: general.name     = llama-bpe
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128001 '<|end_of_text|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128001 '<|end_of_text|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 512
llama_context: n_ctx_per_seq = 512
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: flash_attn    = 0
llama_context: freq_base     = 0.0
llama_context: freq_scale    = 1
llama_context: n_ctx_pre_seq (512) > n_ctx_train (0) -- possible training context overflow
llama_context_kv_self: n_ctx = 512
llama_context_kv_self: n_ctx = 512 (padded)
/home/ggml/work/llama.cpp/ggml/src/ggml-backend.cpp:1454: GGML_ASSERT(n_backends > 0) failed
/home/ggml/work/llama.cpp/build-ci-debug/bin/libggml-base.so(+0x532a0)[0xffdd7b7f32a0]
/home/ggml/work/llama.cpp/build-ci-debug/bin/libggml-base.so(+0x5346c)[0xffdd7b7f346c]
/home/ggml/work/llama.cpp/build-ci-debug/bin/libggml-base.so(ggml_abort+0x104)[0xffdd7b7f35a4]
/home/ggml/work/llama.cpp/build-ci-debug/bin/libggml-base.so(ggml_backend_sched_new+0x60)[0xffdd7b80d718]
/home/ggml/work/llama.cpp/build-ci-debug/bin/libllama.so(_ZN13llama_context4initEv+0x31c)[0xffdd7bcf7294]
/home/ggml/work/llama.cpp/build-ci-debug/bin/libllama.so(llama_init_from_model+0x1c8)[0xffdd7bcc0860]
/home/ggml/work/llama.cpp/build-ci-debug/bin/test-tokenizer-0(+0x667c4)[0xab19f41367c4]
/lib/aarch64-linux-gnu/libc.so.6(+0x273fc)[0xffdd7b3173fc]
/lib/aarch64-linux-gnu/libc.so.6(__libc_start_main+0x98)[0xffdd7b3174cc]
/home/ggml/work/llama.cpp/build-ci-debug/bin/test-tokenizer-0(+0x65670)[0xab19f4135670]

      Start  8: test-tokenizer-0-llama-spm
 8/29 Test  #8: test-tokenizer-0-llama-spm ........Subprocess aborted***Exception:   0.32 sec
main : reading vocab from: '/home/ggml/work/llama.cpp/tests/../models/ggml-vocab-llama-spm.gguf'
register_backend: registered backend CPU (1 devices)
register_device: registered device CPU (CPU)
llama_model_loader: loaded meta data with 22 key-value pairs and 0 tensors from /home/ggml/work/llama.cpp/tests/../models/ggml-vocab-llama-spm.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = llama-spm
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 4096
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   8:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv   9:                          general.file_type u32              = 1
llama_model_loader: - kv  10:                           llama.vocab_size u32              = 32000
llama_model_loader: - kv  11:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                         tokenizer.ggml.pre str              = default
llama_model_loader: - kv  14:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  15:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false
print_info: file format = GGUF V3 (latest)
print_info: file type   = F16
print_info: file size   = 0.00 MiB (nan BPW) 
init_tokenizer: initializing tokenizer for type 1
load: control token:      2 '</s>' is not marked as EOG
load: control token:      1 '<s>' is not marked as EOG
load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
load: special tokens cache size = 3
load: token to piece cache size = 0.1684 MB
print_info: arch             = llama
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 0.00 K
print_info: general.name     = llama-spm
print_info: vocab type       = SPM
print_info: n_vocab          = 32000
print_info: n_merges         = 0
print_info: BOS token        = 1 '<s>'
print_info: EOS token        = 2 '</s>'
print_info: UNK token        = 0 '<unk>'
print_info: LF token         = 13 '<0x0A>'
print_info: EOG token        = 2 '</s>'
print_info: max token length = 48
llama_model_load: vocab only - skipping tensors
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 512
llama_context: n_ctx_per_seq = 512
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: flash_attn    = 0
llama_context: freq_base     = 0.0
llama_context: freq_scale    = 1
llama_context: n_ctx_pre_seq (512) > n_ctx_train (0) -- possible training context overflow
llama_context_kv_self: n_ctx = 512
llama_context_kv_self: n_ctx = 512 (padded)
/home/ggml/work/llama.cpp/ggml/src/ggml-backend.cpp:1454: GGML_ASSERT(n_backends > 0) failed
/home/ggml/work/llama.cpp/build-ci-debug/bin/libggml-base.so(+0x532a0)[0xff90405d32a0]
/home/ggml/work/llama.cpp/build-ci-debug/bin/libggml-base.so(+0x5346c)[0xff90405d346c]
/home/ggml/work/llama.cpp/build-ci-debug/bin/libggml-base.so(ggml_abort+0x104)[0xff90405d35a4]
/home/ggml/work/llama.cpp/build-ci-debug/bin/libggml-base.so(ggml_backend_sched_new+0x60)[0xff90405ed718]
/home/ggml/work/llama.cpp/build-ci-debug/bin/libllama.so(_ZN13llama_context4initEv+0x31c)[0xff9040ad7294]
/home/ggml/work/llama.cpp/build-ci-debug/bin/libllama.so(llama_init_from_model+0x1c8)[0xff9040aa0860]
/home/ggml/work/llama.cpp/build-ci-debug/bin/test-tokenizer-0(+0x667c4)[0xaadbe01a67c4]
/lib/aarch64-linux-gnu/libc.so.6(+0x273fc)[0xff90400f73fc]
/lib/aarch64-linux-gnu/libc.so.6(__libc_start_main+0x98)[0xff90400f74cc]
/home/ggml/work/llama.cpp/build-ci-debug/bin/test-tokenizer-0(+0x65670)[0xaadbe01a5670]

      Start  9: test-tokenizer-0-mpt
 9/29 Test  #9: test-tokenizer-0-mpt ..............Subprocess aborted***Exception:   0.78 sec
main : reading vocab from: '/home/ggml/work/llama.cpp/tests/../models/ggml-vocab-mpt.gguf'
register_backend: registered backend CPU (1 devices)
register_device: registered device CPU (CPU)
llama_model_loader: loaded meta data with 17 key-value pairs and 0 tensors from /home/ggml/work/llama.cpp/tests/../models/ggml-vocab-mpt.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = mpt
llama_model_loader: - kv   1:                               general.name str              = mpt
llama_model_loader: - kv   2:                         mpt.context_length u32              = 2048
llama_model_loader: - kv   3:                       mpt.embedding_length u32              = 4096
llama_model_loader: - kv   4:                            mpt.block_count u32              = 32
llama_model_loader: - kv   5:                    mpt.feed_forward_length u32              = 16384
llama_model_loader: - kv   6:                   mpt.attention.head_count u32              = 32
llama_model_loader: - kv   7:           mpt.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv   8:               mpt.attention.max_alibi_bias f32              = 8.000000
llama_model_loader: - kv   9:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  10:                         tokenizer.ggml.pre str              = mpt
llama_model_loader: - kv  11:                      tokenizer.ggml.tokens arr[str,50432]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  12:                  tokenizer.ggml.token_type arr[i32,50432]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  13:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  14:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  15:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  16:            tokenizer.ggml.unknown_token_id u32              = 0
print_info: file format = GGUF V3 (latest)
print_info: file type   = all F32 (guessed)
print_info: file size   = 0.00 MiB (nan BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 180
load: token to piece cache size = 0.2999 MB
print_info: arch             = mpt
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 0.00 K
print_info: general.name     = mpt
print_info: vocab type       = BPE
print_info: n_vocab          = 50432
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 'Ċ'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
llama_model_load: vocab only - skipping tensors
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 512
llama_context: n_ctx_per_seq = 512
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: flash_attn    = 0
llama_context: freq_base     = 0.0
llama_context: freq_scale    = 1
llama_context: n_ctx_pre_seq (512) > n_ctx_train (0) -- possible training context overflow
llama_context_kv_self: n_ctx = 512
llama_context_kv_self: n_ctx = 512 (padded)
/home/ggml/work/llama.cpp/ggml/src/ggml-backend.cpp:1454: GGML_ASSERT(n_backends > 0) failed
/home/ggml/work/llama.cpp/build-ci-debug/bin/libggml-base.so(+0x532a0)[0xffd7f5ac32a0]
/home/ggml/work/llama.cpp/build-ci-debug/bin/libggml-base.so(+0x5346c)[0xffd7f5ac346c]
/home/ggml/work/llama.cpp/build-ci-debug/bin/libggml-base.so(ggml_abort+0x104)[0xffd7f5ac35a4]
/home/ggml/work/llama.cpp/build-ci-debug/bin/libggml-base.so(ggml_backend_sched_new+0x60)[0xffd7f5add718]
/home/ggml/work/llama.cpp/build-ci-debug/bin/libllama.so(_ZN13llama_context4initEv+0x31c)[0xffd7f5fc7294]
/home/ggml/work/llama.cpp/build-ci-debug/bin/libllama.so(llama_init_from_model+0x1c8)[0xffd7f5f90860]
/home/ggml/work/llama.cpp/build-ci-debug/bin/test-tokenizer-0(+0x667c4)[0xaaea83d067c4]
/lib/aarch64-linux-gnu/libc.so.6(+0x273fc)[0xffd7f55e73fc]
/lib/aarch64-linux-gnu/libc.so.6(__libc_start_main+0x98)[0xffd7f55e74cc]
/home/ggml/work/llama.cpp/build-ci-debug/bin/test-tokenizer-0(+0x65670)[0xaaea83d05670]

      Start 10: test-tokenizer-0-phi-3
10/29 Test #10: test-tokenizer-0-phi-3 ............Subprocess aborted***Exception:   0.32 sec
main : reading vocab from: '/home/ggml/work/llama.cpp/tests/../models/ggml-vocab-phi-3.gguf'
register_backend: registered backend CPU (1 devices)
register_device: registered device CPU (CPU)
llama_model_loader: loaded meta data with 26 key-value pairs and 0 tensors from /home/ggml/work/llama.cpp/tests/../models/ggml-vocab-phi-3.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = phi3
llama_model_loader: - kv   1:                               general.name str              = Phi3
llama_model_loader: - kv   2:                        phi3.context_length u32              = 4096
llama_model_loader: - kv   3:  phi3.rope.scaling.original_context_length u32              = 4096
llama_model_loader: - kv   4:                      phi3.embedding_length u32              = 3072
llama_model_loader: - kv   5:                   phi3.feed_forward_length u32              = 8192
llama_model_loader: - kv   6:                           phi3.block_count u32              = 32
llama_model_loader: - kv   7:                  phi3.attention.head_count u32              = 32
llama_model_loader: - kv   8:               phi3.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:      phi3.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                  phi3.rope.dimension_count u32              = 96
llama_model_loader: - kv  11:                        phi3.rope.freq_base f32              = 10000.000000
llama_model_loader: - kv  12:                          general.file_type u32              = 1
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = default
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,32064]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  16:                      tokenizer.ggml.scores arr[f32,32064]   = [-1000.000000, -1000.000000, -1000.00...
llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,32064]   = [3, 3, 4, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 32000
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 32000
llama_model_loader: - kv  22:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  23:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  24:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...
llama_model_loader: - kv  25:               general.quantization_version u32              = 2
print_info: file format = GGUF V3 (latest)
print_info: file type   = F16
print_info: file size   = 0.00 MiB (nan BPW) 
init_tokenizer: initializing tokenizer for type 1
load: control token:  32008 '<|placeholder5|>' is not marked as EOG
load: control token:  32006 '<|system|>' is not marked as EOG
load: control token:  32002 '<|placeholder1|>' is not marked as EOG
load: control token:  32001 '<|assistant|>' is not marked as EOG
load: control token:  32004 '<|placeholder3|>' is not marked as EOG
load: control token:  32003 '<|placeholder2|>' is not marked as EOG
load: control token:      0 '<unk>' is not marked as EOG
load: control token:  32005 '<|placeholder4|>' is not marked as EOG
load: control token:  32010 '<|user|>' is not marked as EOG
load: control token:  32009 '<|placeholder6|>' is not marked as EOG
load: control token:      1 '<s>' is not marked as EOG
load: special tokens cache size = 67
load: token to piece cache size = 0.1690 MB
print_info: arch             = phi3
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 0.00 K
print_info: general.name     = Phi3
print_info: vocab type       = SPM
print_info: n_vocab          = 32064
print_info: n_merges         = 0
print_info: BOS token        = 1 '<s>'
print_info: EOS token        = 32000 '<|endoftext|>'
print_info: EOT token        = 32007 '<|end|>'
print_info: UNK token        = 0 '<unk>'
print_info: PAD token        = 32000 '<|endoftext|>'
print_info: LF token         = 13 '<0x0A>'
print_info: EOG token        = 32000 '<|endoftext|>'
print_info: EOG token        = 32007 '<|end|>'
print_info: max token length = 48
llama_model_load: vocab only - skipping tensors
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 512
llama_context: n_ctx_per_seq = 512
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: flash_attn    = 0
llama_context: freq_base     = 0.0
llama_context: freq_scale    = 1
llama_context: n_ctx_pre_seq (512) > n_ctx_train (0) -- possible training context overflow
llama_context_kv_self: n_ctx = 512
llama_context_kv_self: n_ctx = 512 (padded)
/home/ggml/work/llama.cpp/ggml/src/ggml-backend.cpp:1454: GGML_ASSERT(n_backends > 0) failed
/home/ggml/work/llama.cpp/build-ci-debug/bin/libggml-base.so(+0x532a0)[0xff4bf9a432a0]
/home/ggml/work/llama.cpp/build-ci-debug/bin/libggml-base.so(+0x5346c)[0xff4bf9a4346c]
/home/ggml/work/llama.cpp/build-ci-debug/bin/libggml-base.so(ggml_abort+0x104)[0xff4bf9a435a4]
/home/ggml/work/llama.cpp/build-ci-debug/bin/libggml-base.so(ggml_backend_sched_new+0x60)[0xff4bf9a5d718]
/home/ggml/work/llama.cpp/build-ci-debug/bin/libllama.so(_ZN13llama_context4initEv+0x31c)[0xff4bf9f47294]
/home/ggml/work/llama.cpp/build-ci-debug/bin/libllama.so(llama_init_from_model+0x1c8)[0xff4bf9f10860]
/home/ggml/work/llama.cpp/build-ci-debug/bin/test-tokenizer-0(+0x667c4)[0xab2af3a367c4]
/lib/aarch64-linux-gnu/libc.so.6(+0x273fc)[0xff4bf95673fc]
/lib/aarch64-linux-gnu/libc.so.6(__libc_start_main+0x98)[0xff4bf95674cc]
/home/ggml/work/llama.cpp/build-ci-debug/bin/test-tokenizer-0(+0x65670)[0xab2af3a35670]

      Start 11: test-tokenizer-0-qwen2
11/29 Test #11: test-tokenizer-0-qwen2 ............Subprocess aborted***Exception:   2.02 sec
main : reading vocab from: '/home/ggml/work/llama.cpp/tests/../models/ggml-vocab-qwen2.gguf'
register_backend: registered backend CPU (1 devices)
register_device: registered device CPU (CPU)
llama_model_loader: loaded meta data with 20 key-value pairs and 0 tensors from /home/ggml/work/llama.cpp/tests/../models/ggml-vocab-qwen2.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.name str              = qwen2
llama_model_loader: - kv   2:                          qwen2.block_count u32              = 32
llama_model_loader: - kv   3:                       qwen2.context_length u32              = 32768
llama_model_loader: - kv   4:                     qwen2.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  qwen2.feed_forward_length u32              = 11008
llama_model_loader: - kv   6:                 qwen2.attention.head_count u32              = 32
llama_model_loader: - kv   7:              qwen2.attention.head_count_kv u32              = 32
llama_model_loader: - kv   8:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv   9:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  10:                          general.file_type u32              = 1
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  12:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 151643
llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  19:                    tokenizer.chat_template str              = {% for message in messages %}{% if lo...
print_info: file format = GGUF V3 (latest)
print_info: file type   = F16
print_info: file size   = 0.00 MiB (nan BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token: 151644 '<|im_start|>' is not marked as EOG
load: special tokens cache size = 293
load: token to piece cache size = 0.9338 MB
print_info: arch             = qwen2
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 0.00 K
print_info: general.name     = qwen2
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151643 '<|endoftext|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 512
llama_context: n_ctx_per_seq = 512
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: flash_attn    = 0
llama_context: freq_base     = 0.0
llama_context: freq_scale    = 1
llama_context: n_ctx_pre_seq (512) > n_ctx_train (0) -- possible training context overflow
llama_context_kv_self: n_ctx = 512
llama_context_kv_self: n_ctx = 512 (padded)
/home/ggml/work/llama.cpp/ggml/src/ggml-backend.cpp:1454: GGML_ASSERT(n_backends > 0) failed
/home/ggml/work/llama.cpp/build-ci-debug/bin/libggml-base.so(+0x532a0)[0xff83960b32a0]
/home/ggml/work/llama.cpp/build-ci-debug/bin/libggml-base.so(+0x5346c)[0xff83960b346c]
/home/ggml/work/llama.cpp/build-ci-debug/bin/libggml-base.so(ggml_abort+0x104)[0xff83960b35a4]
/home/ggml/work/llama.cpp/build-ci-debug/bin/libggml-base.so(ggml_backend_sched_new+0x60)[0xff83960cd718]
/home/ggml/work/llama.cpp/build-ci-debug/bin/libllama.so(_ZN13llama_context4initEv+0x31c)[0xff83965b7294]
/home/ggml/work/llama.cpp/build-ci-debug/bin/libllama.so(llama_init_from_model+0x1c8)[0xff8396580860]
/home/ggml/work/llama.cpp/build-ci-debug/bin/test-tokenizer-0(+0x667c4)[0xab17964767c4]
/lib/aarch64-linux-gnu/libc.so.6(+0x273fc)[0xff8395bd73fc]
/lib/aarch64-linux-gnu/libc.so.6(__libc_start_main+0x98)[0xff8395bd74cc]
/home/ggml/work/llama.cpp/build-ci-debug/bin/test-tokenizer-0(+0x65670)[0xab1796475670]

      Start 12: test-tokenizer-0-refact
12/29 Test #12: test-tokenizer-0-refact ...........Subprocess aborted***Exception:   0.79 sec
main : reading vocab from: '/home/ggml/work/llama.cpp/tests/../models/ggml-vocab-refact.gguf'
register_backend: registered backend CPU (1 devices)
register_device: registered device CPU (CPU)
llama_model_loader: loaded meta data with 18 key-value pairs and 0 tensors from /home/ggml/work/llama.cpp/tests/../models/ggml-vocab-refact.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = refact
llama_model_loader: - kv   1:                               general.name str              = Refact
llama_model_loader: - kv   2:                      refact.context_length u32              = 4096
llama_model_loader: - kv   3:                    refact.embedding_length u32              = 2048
llama_model_loader: - kv   4:                 refact.feed_forward_length u32              = 5632
llama_model_loader: - kv   5:                         refact.block_count u32              = 32
llama_model_loader: - kv   6:                refact.attention.head_count u32              = 32
llama_model_loader: - kv   7:             refact.attention.head_count_kv u32              = 1
llama_model_loader: - kv   8:    refact.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv   9:                          general.file_type u32              = 1
llama_model_loader: - kv  10:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  11:                         tokenizer.ggml.pre str              = refact
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,49216]   = ["<|endoftext|>", "<fim_prefix>", "<f...
llama_model_loader: - kv  13:                  tokenizer.ggml.token_type arr[i32,49216]   = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...
llama_model_loader: - kv  14:                      tokenizer.ggml.merges arr[str,48891]   = ["Ġ Ġ", "ĠĠ ĠĠ", "ĠĠĠĠ ĠĠ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0
print_info: file format = GGUF V3 (latest)
print_info: file type   = F16
print_info: file size   = 0.00 MiB (nan BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      2 '<fim_middle>' is not marked as EOG
load: control token:     13 '<jupyter_output>' is not marked as EOG
load: control token:      9 '<issue_closed>' is not marked as EOG
load: control token:      6 '<gh_stars>' is not marked as EOG
load: control token:     10 '<jupyter_start>' is not marked as EOG
load: control token:     14 '<empty_output>' is not marked as EOG
load: control token:     15 '<commit_before>' is not marked as EOG
load: control token:      5 '<filename>' is not marked as EOG
load: control token:     12 '<jupyter_code>' is not marked as EOG
load: control token:      4 '<fim_pad>' is not marked as EOG
load: control token:     18 '<reponame>' is not marked as EOG
load: control token:      7 '<issue_start>' is not marked as EOG
load: control token:      3 '<fim_suffix>' is not marked as EOG
load: control token:      1 '<fim_prefix>' is not marked as EOG
load: control token:      8 '<issue_comment>' is not marked as EOG
load: control token:     11 '<jupyter_text>' is not marked as EOG
load: control token:     16 '<commit_msg>' is not marked as EOG
load: control token:     17 '<commit_after>' is not marked as EOG
load: special tokens cache size = 83
load: token to piece cache size = 0.2832 MB
print_info: arch             = refact
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 0.00 K
print_info: general.name     = Refact
print_info: vocab type       = BPE
print_info: n_vocab          = 49216
print_info: n_merges         = 48891
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 203 'Ċ'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 512
llama_model_load: vocab only - skipping tensors
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 512
llama_context: n_ctx_per_seq = 512
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: flash_attn    = 0
llama_context: freq_base     = 0.0
llama_context: freq_scale    = 1
llama_context: n_ctx_pre_seq (512) > n_ctx_train (0) -- possible training context overflow
llama_context_kv_self: n_ctx = 512
llama_context_kv_self: n_ctx = 512 (padded)
/home/ggml/work/llama.cpp/ggml/src/ggml-backend.cpp:1454: GGML_ASSERT(n_backends > 0) failed
/home/ggml/work/llama.cpp/build-ci-debug/bin/libggml-base.so(+0x532a0)[0xff38abb332a0]
/home/ggml/work/llama.cpp/build-ci-debug/bin/libggml-base.so(+0x5346c)[0xff38abb3346c]
/home/ggml/work/llama.cpp/build-ci-debug/bin/libggml-base.so(ggml_abort+0x104)[0xff38abb335a4]
/home/ggml/work/llama.cpp/build-ci-debug/bin/libggml-base.so(ggml_backend_sched_new+0x60)[0xff38abb4d718]
/home/ggml/work/llama.cpp/build-ci-debug/bin/libllama.so(_ZN13llama_context4initEv+0x31c)[0xff38ac037294]
/home/ggml/work/llama.cpp/build-ci-debug/bin/libllama.so(llama_init_from_model+0x1c8)[0xff38ac000860]
/home/ggml/work/llama.cpp/build-ci-debug/bin/test-tokenizer-0(+0x667c4)[0xaaba3ed567c4]
/lib/aarch64-linux-gnu/libc.so.6(+0x273fc)[0xff38ab6573fc]
/lib/aarch64-linux-gnu/libc.so.6(__libc_start_main+0x98)[0xff38ab6574cc]
/home/ggml/work/llama.cpp/build-ci-debug/bin/test-tokenizer-0(+0x65670)[0xaaba3ed55670]

      Start 13: test-tokenizer-0-starcoder
13/29 Test #13: test-tokenizer-0-starcoder ........Subprocess aborted***Exception:   0.72 sec
main : reading vocab from: '/home/ggml/work/llama.cpp/tests/../models/ggml-vocab-starcoder.gguf'
register_backend: registered backend CPU (1 devices)
register_device: registered device CPU (CPU)
llama_model_loader: loaded meta data with 19 key-value pairs and 0 tensors from /home/ggml/work/llama.cpp/tests/../models/ggml-vocab-starcoder.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = starcoder2
llama_model_loader: - kv   1:                               general.name str              = starcoder
llama_model_loader: - kv   2:                     starcoder2.block_count u32              = 30
llama_model_loader: - kv   3:                  starcoder2.context_length u32              = 16384
llama_model_loader: - kv   4:                starcoder2.embedding_length u32              = 3072
llama_model_loader: - kv   5:             starcoder2.feed_forward_length u32              = 12288
llama_model_loader: - kv   6:            starcoder2.attention.head_count u32              = 24
llama_model_loader: - kv   7:         starcoder2.attention.head_count_kv u32              = 2
llama_model_loader: - kv   8:                  starcoder2.rope.freq_base f32              = 999999.437500
llama_model_loader: - kv   9:    starcoder2.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 1
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  12:                         tokenizer.ggml.pre str              = starcoder
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,49152]   = ["<|endoftext|>", "<fim_prefix>", "<f...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,49152]   = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...
llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,48872]   = ["Ġ Ġ", "ĠĠ ĠĠ", "ĠĠĠĠ ĠĠ...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0
print_info: file format = GGUF V3 (latest)
print_info: file type   = F16
print_info: file size   = 0.00 MiB (nan BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:     22 '<pr_file>' is not marked as EOG
load: control token:     21 '<pr_base>' is not marked as EOG
load: control token:     27 '<pr_event_id>' is not marked as EOG
load: control token:      2 '<fim_middle>' is not marked as EOG
load: control token:     18 '<pr>' is not marked as EOG
load: control token:     20 '<pr_is_merged>' is not marked as EOG
load: control token:     13 '<jupyter_output>' is not marked as EOG
load: control token:     36 '<KEY>' is not marked as EOG
load: control token:      9 '<issue_closed>' is not marked as EOG
load: control token:     10 '<jupyter_start>' is not marked as EOG
load: control token:     15 '<empty_output>' is not marked as EOG
load: control token:     30 '<pr_review_comment>' is not marked as EOG
load: control token:     34 '<NAME>' is not marked as EOG
load: control token:     32 '<pr_in_reply_to_comment_id>' is not marked as EOG
load: control token:     14 '<jupyter_script>' is not marked as EOG
load: control token:     17 '<intermediate_to_code>' is not marked as EOG
load: control token:     33 '<pr_diff_hunk_comment_line>' is not marked as EOG
load: control token:      4 '<fim_pad>' is not marked as EOG
load: control token:     35 '<EMAIL>' is not marked as EOG
load: control token:     28 '<pr_review>' is not marked as EOG
load: control token:      5 '<repo_name>' is not marked as EOG
load: control token:     12 '<jupyter_code>' is not marked as EOG
load: control token:     37 '<PASSWORD>' is not marked as EOG
load: control token:     26 '<pr_comment>' is not marked as EOG
load: control token:      7 '<issue_start>' is not marked as EOG
load: control token:     31 '<pr_in_reply_to_review_id>' is not marked as EOG
load: control token:      3 '<fim_suffix>' is not marked as EOG
load: control token:      1 '<fim_prefix>' is not marked as EOG
load: control token:     24 '<pr_diff>' is not marked as EOG
load: control token:     19 '<pr_status>' is not marked as EOG
load: control token:      8 '<issue_comment>' is not marked as EOG
load: control token:     11 '<jupyter_text>' is not marked as EOG
load: control token:     23 '<pr_base_code>' is not marked as EOG
load: control token:     29 '<pr_review_state>' is not marked as EOG
load: control token:     25 '<pr_diff_hunk>' is not marked as EOG
load: control token:     16 '<code_to_intermediate>' is not marked as EOG
load: control token:      6 '<file_sep>' is not marked as EOG
load: special tokens cache size = 38
load: token to piece cache size = 0.2828 MB
print_info: arch             = starcoder2
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 0.00 K
print_info: general.name     = starcoder
print_info: vocab type       = BPE
print_info: n_vocab          = 49152
print_info: n_merges         = 48872
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 222 'Ċ'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 512
llama_model_load: vocab only - skipping tensors
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 512
llama_context: n_ctx_per_seq = 512
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: flash_attn    = 0
llama_context: freq_base     = 0.0
llama_context: freq_scale    = 1
llama_context: n_ctx_pre_seq (512) > n_ctx_train (0) -- possible training context overflow
llama_context_kv_self: n_ctx = 512
llama_context_kv_self: n_ctx = 512 (padded)
/home/ggml/work/llama.cpp/ggml/src/ggml-backend.cpp:1454: GGML_ASSERT(n_backends > 0) failed
/home/ggml/work/llama.cpp/build-ci-debug/bin/libggml-base.so(+0x532a0)[0xffb1e18f32a0]
/home/ggml/work/llama.cpp/build-ci-debug/bin/libggml-base.so(+0x5346c)[0xffb1e18f346c]
/home/ggml/work/llama.cpp/build-ci-debug/bin/libggml-base.so(ggml_abort+0x104)[0xffb1e18f35a4]
/home/ggml/work/llama.cpp/build-ci-debug/bin/libggml-base.so(ggml_backend_sched_new+0x60)[0xffb1e190d718]
/home/ggml/work/llama.cpp/build-ci-debug/bin/libllama.so(_ZN13llama_context4initEv+0x31c)[0xffb1e1df7294]
/home/ggml/work/llama.cpp/build-ci-debug/bin/libllama.so(llama_init_from_model+0x1c8)[0xffb1e1dc0860]
/home/ggml/work/llama.cpp/build-ci-debug/bin/test-tokenizer-0(+0x667c4)[0xaacfd19267c4]
/lib/aarch64-linux-gnu/libc.so.6(+0x273fc)[0xffb1e14173fc]
/lib/aarch64-linux-gnu/libc.so.6(__libc_start_main+0x98)[0xffb1e14174cc]
/home/ggml/work/llama.cpp/build-ci-debug/bin/test-tokenizer-0(+0x65670)[0xaacfd1925670]

      Start 14: test-sampling
14/29 Test #14: test-sampling .....................   Passed    6.47 sec
      Start 15: test-grammar-parser
15/29 Test #15: test-grammar-parser ...............   Passed    0.01 sec
      Start 16: test-grammar-integration
16/29 Test #16: test-grammar-integration ..........   Passed    0.05 sec
      Start 17: test-llama-grammar
17/29 Test #17: test-llama-grammar ................   Passed    0.01 sec
      Start 18: test-chat
18/29 Test #18: test-chat .........................   Passed    7.30 sec
      Start 19: test-json-schema-to-grammar
19/29 Test #19: test-json-schema-to-grammar .......   Passed    0.07 sec
      Start 20: test-tokenizer-1-llama-spm
20/29 Test #20: test-tokenizer-1-llama-spm ........Subprocess aborted***Exception:   0.32 sec
main : reading vocab from: '/home/ggml/work/llama.cpp/tests/../models/ggml-vocab-llama-spm.gguf'
register_backend: registered backend CPU (1 devices)
register_device: registered device CPU (CPU)
llama_model_loader: loaded meta data with 22 key-value pairs and 0 tensors from /home/ggml/work/llama.cpp/tests/../models/ggml-vocab-llama-spm.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = llama-spm
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 4096
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   8:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv   9:                          general.file_type u32              = 1
llama_model_loader: - kv  10:                           llama.vocab_size u32              = 32000
llama_model_loader: - kv  11:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                         tokenizer.ggml.pre str              = default
llama_model_loader: - kv  14:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  15:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false
print_info: file format = GGUF V3 (latest)
print_info: file type   = F16
print_info: file size   = 0.00 MiB (nan BPW) 
init_tokenizer: initializing tokenizer for type 1
load: control token:      2 '</s>' is not marked as EOG
load: control token:      1 '<s>' is not marked as EOG
load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
load: special tokens cache size = 3
load: token to piece cache size = 0.1684 MB
print_info: arch             = llama
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 0.00 K
print_info: general.name     = llama-spm
print_info: vocab type       = SPM
print_info: n_vocab          = 32000
print_info: n_merges         = 0
print_info: BOS token        = 1 '<s>'
print_info: EOS token        = 2 '</s>'
print_info: UNK token        = 0 '<unk>'
print_info: LF token         = 13 '<0x0A>'
print_info: EOG token        = 2 '</s>'
print_info: max token length = 48
llama_model_load: vocab only - skipping tensors
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 512
llama_context: n_ctx_per_seq = 512
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: flash_attn    = 0
llama_context: freq_base     = 0.0
llama_context: freq_scale    = 1
llama_context: n_ctx_pre_seq (512) > n_ctx_train (0) -- possible training context overflow
llama_context_kv_self: n_ctx = 512
llama_context_kv_self: n_ctx = 512 (padded)
/home/ggml/work/llama.cpp/ggml/src/ggml-backend.cpp:1454: GGML_ASSERT(n_backends > 0) failed
/home/ggml/work/llama.cpp/build-ci-debug/bin/libggml-base.so(+0x532a0)[0xff31639032a0]
/home/ggml/work/llama.cpp/build-ci-debug/bin/libggml-base.so(+0x5346c)[0xff316390346c]
/home/ggml/work/llama.cpp/build-ci-debug/bin/libggml-base.so(ggml_abort+0x104)[0xff31639035a4]
/home/ggml/work/llama.cpp/build-ci-debug/bin/libggml-base.so(ggml_backend_sched_new+0x60)[0xff316391d718]
/home/ggml/work/llama.cpp/build-ci-debug/bin/libllama.so(_ZN13llama_context4initEv+0x31c)[0xff3163e07294]
/home/ggml/work/llama.cpp/build-ci-debug/bin/libllama.so(llama_init_from_model+0x1c8)[0xff3163dd0860]
/home/ggml/work/llama.cpp/build-ci-debug/bin/test-tokenizer-1-spm(+0x64edc)[0xab82b8aa4edc]
/lib/aarch64-linux-gnu/libc.so.6(+0x273fc)[0xff31634273fc]
/lib/aarch64-linux-gnu/libc.so.6(__libc_start_main+0x98)[0xff31634274cc]
/home/ggml/work/llama.cpp/build-ci-debug/bin/test-tokenizer-1-spm(+0x647f0)[0xab82b8aa47f0]

      Start 21: test-log
21/29 Test #21: test-log ..........................   Passed    0.02 sec
      Start 22: test-arg-parser
22/29 Test #22: test-arg-parser ...................   Passed    0.14 sec
      Start 23: test-chat-template
23/29 Test #23: test-chat-template ................   Passed    1.32 sec
      Start 24: test-gguf
24/29 Test #24: test-gguf .........................   Passed    0.56 sec
      Start 25: test-backend-ops
25/29 Test #25: test-backend-ops ..................   Passed    0.01 sec
      Start 28: test-barrier
26/29 Test #28: test-barrier ......................   Passed    1.18 sec
      Start 29: test-quantize-fns
27/29 Test #29: test-quantize-fns .................   Passed   32.90 sec
      Start 30: test-quantize-perf
28/29 Test #30: test-quantize-perf ................   Passed    0.38 sec
      Start 31: test-rope
29/29 Test #31: test-rope .........................   Passed    0.11 sec

52% tests passed, 14 tests failed out of 29

Label Time Summary:
main    =  65.49 sec*proc (29 tests)

Total Test time (real) =  65.51 sec

The following tests FAILED:
	  1 - test-tokenizer-0-bert-bge (Subprocess aborted)
	  2 - test-tokenizer-0-command-r (Subprocess aborted)
	  3 - test-tokenizer-0-deepseek-coder (Subprocess aborted)
	  4 - test-tokenizer-0-deepseek-llm (Subprocess aborted)
	  5 - test-tokenizer-0-falcon (Subprocess aborted)
	  6 - test-tokenizer-0-gpt-2 (Subprocess aborted)
	  7 - test-tokenizer-0-llama-bpe (Subprocess aborted)
	  8 - test-tokenizer-0-llama-spm (Subprocess aborted)
	  9 - test-tokenizer-0-mpt (Subprocess aborted)
	 10 - test-tokenizer-0-phi-3 (Subprocess aborted)
	 11 - test-tokenizer-0-qwen2 (Subprocess aborted)
	 12 - test-tokenizer-0-refact (Subprocess aborted)
	 13 - test-tokenizer-0-starcoder (Subprocess aborted)
	 20 - test-tokenizer-1-llama-spm (Subprocess aborted)
Errors while running CTest

real	1m5.515s
user	1m7.001s
sys	0m0.619s
