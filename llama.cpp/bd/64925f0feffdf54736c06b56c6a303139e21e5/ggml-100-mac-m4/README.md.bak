### ctest_debug

Runs ctest in debug mode
- status: 8
```
+ ctest --output-on-failure -L main -E test-opt
Test project /Users/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-tokenizer-0-bert-bge
 1/29 Test  #1: test-tokenizer-0-bert-bge .........Subprocess aborted***Exception:   1.26 sec
main : reading vocab from: '/Users/ggml/work/llama.cpp/tests/../models/ggml-vocab-bert-bge.gguf'
register_backend: registered backend Metal (1 devices)
register_device: registered device Metal (Apple M4)
register_backend: registered backend BLAS (1 devices)
register_device: registered device BLAS (Accelerate)
register_backend: registered backend CPU (1 devices)
register_device: registered device CPU (Apple M4)
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 20 key-value pairs and 0 tensors from /Users/ggml/work/llama.cpp/tests/../models/ggml-vocab-bert-bge.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = bert
llama_model_loader: - kv   1:                               general.name str              = bert-bge
llama_model_loader: - kv   2:                           bert.block_count u32              = 12
llama_model_loader: - kv   3:                        bert.context_length u32              = 512
llama_model_loader: - kv   4:                      bert.embedding_length u32              = 384
llama_model_loader: - kv   5:                   bert.feed_forward_length u32              = 1536
llama_model_loader: - kv   6:                  bert.attention.head_count u32              = 12
llama_model_loader: - kv   7:          bert.attention.layer_norm_epsilon f32              = 0.000000
llama_model_loader: - kv   8:                          general.file_type u32              = 1
llama_model_loader: - kv   9:                      bert.attention.causal bool             = false
llama_model_loader: - kv  10:            tokenizer.ggml.token_type_count u32              = 2
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = bert
llama_model_loader: - kv  12:                         tokenizer.ggml.pre str              = bert-bge
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  15:            tokenizer.ggml.unknown_token_id u32              = 100
llama_model_loader: - kv  16:          tokenizer.ggml.seperator_token_id u32              = 102
llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  18:                tokenizer.ggml.cls_token_id u32              = 101
llama_model_loader: - kv  19:               tokenizer.ggml.mask_token_id u32              = 103
print_info: file format = GGUF V3 (latest)
print_info: file type   = F16
print_info: file size   = 0.00 MiB (nan BPW) 
init_tokenizer: initializing tokenizer for type 3
load: control token:    100 '[UNK]' is not marked as EOG
load: control token:    101 '[CLS]' is not marked as EOG
load: control token:      0 '[PAD]' is not marked as EOG
load: control token:    102 '[SEP]' is not marked as EOG
load: control token:    103 '[MASK]' is not marked as EOG
load: special tokens cache size = 5
load: token to piece cache size = 0.2032 MB
print_info: arch             = bert
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 0.00 K
print_info: general.name     = bert-bge
print_info: vocab type       = WPM
print_info: n_vocab          = 30522
print_info: n_merges         = 0
print_info: BOS token        = 101 '[CLS]'
print_info: UNK token        = 100 '[UNK]'
print_info: SEP token        = 102 '[SEP]'
print_info: PAD token        = 0 '[PAD]'
print_info: MASK token       = 103 '[MASK]'
print_info: LF token         = 0 '[PAD]'
print_info: max token length = 21
llama_model_load: vocab only - skipping tensors
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 512
llama_context: n_ctx_per_seq = 512
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: flash_attn    = 0
llama_context: freq_base     = 0.0
llama_context: freq_scale    = 1
llama_context: n_ctx_pre_seq (512) > n_ctx_train (0) -- possible training context overflow
llama_context_kv_self: n_ctx = 512
llama_context_kv_self: n_ctx = 512 (padded)
/Users/ggml/work/llama.cpp/ggml/src/ggml-backend.cpp:1454: GGML_ASSERT(n_backends > 0) failed

      Start  2: test-tokenizer-0-command-r
 2/29 Test  #2: test-tokenizer-0-command-r ........Subprocess aborted***Exception:   1.06 sec
main : reading vocab from: '/Users/ggml/work/llama.cpp/tests/../models/ggml-vocab-command-r.gguf'
register_backend: registered backend Metal (1 devices)
register_device: registered device Metal (Apple M4)
register_backend: registered backend BLAS (1 devices)
register_device: registered device BLAS (Accelerate)
register_backend: registered backend CPU (1 devices)
register_device: registered device CPU (Apple M4)
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 27 key-value pairs and 0 tensors from /Users/ggml/work/llama.cpp/tests/../models/ggml-vocab-command-r.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = command-r
llama_model_loader: - kv   1:                               general.name str              = command-r
llama_model_loader: - kv   2:                      command-r.block_count u32              = 40
llama_model_loader: - kv   3:                   command-r.context_length u32              = 131072
llama_model_loader: - kv   4:                 command-r.embedding_length u32              = 8192
llama_model_loader: - kv   5:              command-r.feed_forward_length u32              = 22528
llama_model_loader: - kv   6:             command-r.attention.head_count u32              = 64
llama_model_loader: - kv   7:          command-r.attention.head_count_kv u32              = 64
llama_model_loader: - kv   8:                   command-r.rope.freq_base f32              = 8000000.000000
llama_model_loader: - kv   9:     command-r.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 1
llama_model_loader: - kv  11:                      command-r.logit_scale f32              = 0.062500
llama_model_loader: - kv  12:                command-r.rope.scaling.type str              = none
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = command-r
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,256000]  = ["<PAD>", "<UNK>", "<CLS>", "<SEP>", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,253333]  = ["Ġ Ġ", "Ġ t", "e r", "i n", "Ġ a...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 5
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 255001
llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  21:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  22:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  23:           tokenizer.chat_template.tool_use str              = {{ bos_token }}{% if messages[0]['rol...
llama_model_loader: - kv  24:                tokenizer.chat_template.rag str              = {{ bos_token }}{% if messages[0]['rol...
llama_model_loader: - kv  25:                   tokenizer.chat_templates arr[str,2]       = ["tool_use", "rag"]
llama_model_loader: - kv  26:                    tokenizer.chat_template str              = {{ bos_token }}{% if messages[0]['rol...
print_info: file format = GGUF V3 (latest)
print_info: file type   = F16
print_info: file size   = 0.00 MiB (nan BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token: 255001 '<|END_OF_TURN_TOKEN|>' is not marked as EOG
load: control token:      7 '<EOP_TOKEN>' is not marked as EOG
load: control token:      1 '<UNK>' is not marked as EOG
load: control token:      6 '<EOS_TOKEN>' is not marked as EOG
load: control token:      4 '<MASK_TOKEN>' is not marked as EOG
load: control token:      5 '<BOS_TOKEN>' is not marked as EOG
load: control token:      2 '<CLS>' is not marked as EOG
load: control token:      3 '<SEP>' is not marked as EOG
load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
load: special tokens cache size = 1008
load: token to piece cache size = 1.8528 MB
print_info: arch             = command-r
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 0.00 K
print_info: general.name     = command-r
print_info: vocab type       = BPE
print_info: n_vocab          = 256000
print_info: n_merges         = 253333
print_info: BOS token        = 5 '<BOS_TOKEN>'
print_info: EOS token        = 255001 '<|END_OF_TURN_TOKEN|>'
print_info: PAD token        = 0 '<PAD>'
print_info: LF token         = 206 'Ċ'
print_info: FIM PAD token    = 0 '<PAD>'
print_info: EOG token        = 0 '<PAD>'
print_info: EOG token        = 255001 '<|END_OF_TURN_TOKEN|>'
print_info: max token length = 1024
llama_model_load: vocab only - skipping tensors
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 512
llama_context: n_ctx_per_seq = 512
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: flash_attn    = 0
llama_context: freq_base     = 0.0
llama_context: freq_scale    = 1
llama_context: n_ctx_pre_seq (512) > n_ctx_train (0) -- possible training context overflow
llama_context_kv_self: n_ctx = 512
llama_context_kv_self: n_ctx = 512 (padded)
/Users/ggml/work/llama.cpp/ggml/src/ggml-backend.cpp:1454: GGML_ASSERT(n_backends > 0) failed

      Start  3: test-tokenizer-0-deepseek-coder
 3/29 Test  #3: test-tokenizer-0-deepseek-coder ...Subprocess aborted***Exception:   0.15 sec
main : reading vocab from: '/Users/ggml/work/llama.cpp/tests/../models/ggml-vocab-deepseek-coder.gguf'
register_backend: registered backend Metal (1 devices)
register_device: registered device Metal (Apple M4)
register_backend: registered backend BLAS (1 devices)
register_device: registered device BLAS (Accelerate)
register_backend: registered backend CPU (1 devices)
register_device: registered device CPU (Apple M4)
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 25 key-value pairs and 0 tensors from /Users/ggml/work/llama.cpp/tests/../models/ggml-vocab-deepseek-coder.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = deepseek-coder
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 16384
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 100000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  10:                          general.file_type u32              = 1
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 32256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                    llama.rope.scaling.type str              = linear
llama_model_loader: - kv  14:                  llama.rope.scaling.factor f32              = 4.000000
llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = deepseek-coder
llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,32256]   = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,32256]   = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  19:                      tokenizer.ggml.merges arr[str,31757]   = ["Ġ Ġ", "Ġ t", "Ġ a", "i n", "h e...
llama_model_loader: - kv  20:                tokenizer.ggml.bos_token_id u32              = 32013
llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 32014
llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 32014
llama_model_loader: - kv  23:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  24:               tokenizer.ggml.add_eos_token bool             = false
print_info: file format = GGUF V3 (latest)
print_info: file type   = F16
print_info: file size   = 0.00 MiB (nan BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control-looking token:  32017 '<｜fim▁end｜>' was not control-type; this is probably a bug in the model. its type will be overridden
load: control-looking token:  32016 '<｜fim▁begin｜>' was not control-type; this is probably a bug in the model. its type will be overridden
load: control-looking token:  32015 '<｜fim▁hole｜>' was not control-type; this is probably a bug in the model. its type will be overridden
load: control token:  32017 '<｜fim▁end｜>' is not marked as EOG
load: control token:  32016 '<｜fim▁begin｜>' is not marked as EOG
load: control token:  32014 '<｜end▁of▁sentence｜>' is not marked as EOG
load: control token:  32015 '<｜fim▁hole｜>' is not marked as EOG
load: control token:  32013 '<｜begin▁of▁sentence｜>' is not marked as EOG
load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
load: special tokens cache size = 256
load: token to piece cache size = 0.1787 MB
print_info: arch             = llama
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 0.00 K
print_info: general.name     = deepseek-coder
print_info: vocab type       = BPE
print_info: n_vocab          = 32256
print_info: n_merges         = 31757
print_info: BOS token        = 32013 '<｜begin▁of▁sentence｜>'
print_info: EOS token        = 32014 '<｜end▁of▁sentence｜>'
print_info: EOT token        = 32014 '<｜end▁of▁sentence｜>'
print_info: PAD token        = 32014 '<｜end▁of▁sentence｜>'
print_info: LF token         = 185 'Ċ'
print_info: FIM PRE token    = 32016 '<｜fim▁begin｜>'
print_info: FIM SUF token    = 32015 '<｜fim▁hole｜>'
print_info: FIM MID token    = 32017 '<｜fim▁end｜>'
print_info: EOG token        = 32014 '<｜end▁of▁sentence｜>'
print_info: max token length = 128
llama_model_load: vocab only - skipping tensors
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 512
llama_context: n_ctx_per_seq = 512
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: flash_attn    = 0
llama_context: freq_base     = 0.0
llama_context: freq_scale    = 1
llama_context: n_ctx_pre_seq (512) > n_ctx_train (0) -- possible training context overflow
llama_context_kv_self: n_ctx = 512
llama_context_kv_self: n_ctx = 512 (padded)
/Users/ggml/work/llama.cpp/ggml/src/ggml-backend.cpp:1454: GGML_ASSERT(n_backends > 0) failed

      Start  4: test-tokenizer-0-deepseek-llm
 4/29 Test  #4: test-tokenizer-0-deepseek-llm .....Subprocess aborted***Exception:   0.41 sec
main : reading vocab from: '/Users/ggml/work/llama.cpp/tests/../models/ggml-vocab-deepseek-llm.gguf'
register_backend: registered backend Metal (1 devices)
register_device: registered device Metal (Apple M4)
register_backend: registered backend BLAS (1 devices)
register_device: registered device BLAS (Accelerate)
register_backend: registered backend CPU (1 devices)
register_device: registered device CPU (Apple M4)
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 0 tensors from /Users/ggml/work/llama.cpp/tests/../models/ggml-vocab-deepseek-llm.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = deepseek-llm
llama_model_loader: - kv   2:                          llama.block_count u32              = 30
llama_model_loader: - kv   3:                       llama.context_length u32              = 4096
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 10000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  10:                          general.file_type u32              = 1
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 102400
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = deepseek-llm
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,102400]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,102400]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,99757]   = ["Ġ Ġ", "Ġ t", "Ġ a", "i n", "h e...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 100000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 100001
llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 100001
llama_model_loader: - kv  21:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  22:               tokenizer.ggml.add_eos_token bool             = false
print_info: file format = GGUF V3 (latest)
print_info: file type   = F16
print_info: file size   = 0.00 MiB (nan BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token: 100001 '<｜end▁of▁sentence｜>' is not marked as EOG
load: control token: 100000 '<｜begin▁of▁sentence｜>' is not marked as EOG
load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
load: special tokens cache size = 2400
load: token to piece cache size = 0.6658 MB
print_info: arch             = llama
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 0.00 K
print_info: general.name     = deepseek-llm
print_info: vocab type       = BPE
print_info: n_vocab          = 102400
print_info: n_merges         = 99757
print_info: BOS token        = 100000 '<｜begin▁of▁sentence｜>'
print_info: EOS token        = 100001 '<｜end▁of▁sentence｜>'
print_info: EOT token        = 100001 '<｜end▁of▁sentence｜>'
print_info: PAD token        = 100001 '<｜end▁of▁sentence｜>'
print_info: LF token         = 185 'Ċ'
print_info: EOG token        = 100001 '<｜end▁of▁sentence｜>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 512
llama_context: n_ctx_per_seq = 512
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: flash_attn    = 0
llama_context: freq_base     = 0.0
llama_context: freq_scale    = 1
llama_context: n_ctx_pre_seq (512) > n_ctx_train (0) -- possible training context overflow
llama_context_kv_self: n_ctx = 512
llama_context_kv_self: n_ctx = 512 (padded)
/Users/ggml/work/llama.cpp/ggml/src/ggml-backend.cpp:1454: GGML_ASSERT(n_backends > 0) failed

      Start  5: test-tokenizer-0-falcon
 5/29 Test  #5: test-tokenizer-0-falcon ...........Subprocess aborted***Exception:   0.27 sec
main : reading vocab from: '/Users/ggml/work/llama.cpp/tests/../models/ggml-vocab-falcon.gguf'
register_backend: registered backend Metal (1 devices)
register_device: registered device Metal (Apple M4)
register_backend: registered backend BLAS (1 devices)
register_device: registered device BLAS (Accelerate)
register_backend: registered backend CPU (1 devices)
register_device: registered device CPU (Apple M4)
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 18 key-value pairs and 0 tensors from /Users/ggml/work/llama.cpp/tests/../models/ggml-vocab-falcon.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = falcon
llama_model_loader: - kv   1:                               general.name str              = Falcon
llama_model_loader: - kv   2:                      falcon.context_length u32              = 2048
llama_model_loader: - kv   3:                  falcon.tensor_data_layout str              = jploski
llama_model_loader: - kv   4:                    falcon.embedding_length u32              = 4544
llama_model_loader: - kv   5:                 falcon.feed_forward_length u32              = 18176
llama_model_loader: - kv   6:                         falcon.block_count u32              = 32
llama_model_loader: - kv   7:                falcon.attention.head_count u32              = 71
llama_model_loader: - kv   8:             falcon.attention.head_count_kv u32              = 1
llama_model_loader: - kv   9:        falcon.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 1
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  12:                         tokenizer.ggml.pre str              = falcon
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,65024]   = [">>TITLE<<", ">>ABSTRACT<<", ">>INTR...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,65024]   = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...
llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,64784]   = ["Ġ t", "Ġ a", "i n", "h e", "r e",...
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 11
llama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 11
print_info: file format = GGUF V3 (latest)
print_info: file type   = F16
print_info: file size   = 0.00 MiB (nan BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      0 '>>TITLE<<' is not marked as EOG
load: control token:      9 '>>SUFFIX<<' is not marked as EOG
load: control token:     10 '>>MIDDLE<<' is not marked as EOG
load: control token:      5 '>>ANSWER<<' is not marked as EOG
load: control token:      8 '>>PREFIX<<' is not marked as EOG
load: control token:      2 '>>INTRODUCTION<<' is not marked as EOG
load: control token:      3 '>>SUMMARY<<' is not marked as EOG
load: control token:      7 '>>DOMAIN<<' is not marked as EOG
load: control token:      1 '>>ABSTRACT<<' is not marked as EOG
load: control token:      6 '>>QUESTION<<' is not marked as EOG
load: control token:      4 '>>COMMENT<<' is not marked as EOG
load: special tokens cache size = 12
load: token to piece cache size = 0.3884 MB
print_info: arch             = falcon
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 0.00 K
print_info: general.name     = Falcon
print_info: vocab type       = BPE
print_info: n_vocab          = 65024
print_info: n_merges         = 64784
print_info: BOS token        = 11 '<|endoftext|>'
print_info: EOS token        = 11 '<|endoftext|>'
print_info: EOT token        = 11 '<|endoftext|>'
print_info: LF token         = 193 'Ċ'
print_info: EOG token        = 11 '<|endoftext|>'
print_info: max token length = 130
llama_model_load: vocab only - skipping tensors
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 512
llama_context: n_ctx_per_seq = 512
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: flash_attn    = 0
llama_context: freq_base     = 0.0
llama_context: freq_scale    = 1
llama_context: n_ctx_pre_seq (512) > n_ctx_train (0) -- possible training context overflow
llama_context_kv_self: n_ctx = 512
llama_context_kv_self: n_ctx = 512 (padded)
/Users/ggml/work/llama.cpp/ggml/src/ggml-backend.cpp:1454: GGML_ASSERT(n_backends > 0) failed

      Start  6: test-tokenizer-0-gpt-2
 6/29 Test  #6: test-tokenizer-0-gpt-2 ............Subprocess aborted***Exception:   0.22 sec
main : reading vocab from: '/Users/ggml/work/llama.cpp/tests/../models/ggml-vocab-gpt-2.gguf'
register_backend: registered backend Metal (1 devices)
register_device: registered device Metal (Apple M4)
register_backend: registered backend BLAS (1 devices)
register_device: registered device BLAS (Accelerate)
register_backend: registered backend CPU (1 devices)
register_device: registered device CPU (Apple M4)
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 16 key-value pairs and 0 tensors from /Users/ggml/work/llama.cpp/tests/../models/ggml-vocab-gpt-2.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gpt2
llama_model_loader: - kv   1:                               general.name str              = gpt-2
llama_model_loader: - kv   2:                           gpt2.block_count u32              = 12
llama_model_loader: - kv   3:                        gpt2.context_length u32              = 1024
llama_model_loader: - kv   4:                      gpt2.embedding_length u32              = 768
llama_model_loader: - kv   5:                   gpt2.feed_forward_length u32              = 3072
llama_model_loader: - kv   6:                  gpt2.attention.head_count u32              = 12
llama_model_loader: - kv   7:          gpt2.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv   8:                          general.file_type u32              = 1
llama_model_loader: - kv   9:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  10:                         tokenizer.ggml.pre str              = gpt-2
llama_model_loader: - kv  11:                      tokenizer.ggml.tokens arr[str,50257]   = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  12:                  tokenizer.ggml.token_type arr[i32,50257]   = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  13:                      tokenizer.ggml.merges arr[str,50000]   = ["Ġ t", "Ġ a", "h e", "i n", "r e",...
llama_model_loader: - kv  14:                tokenizer.ggml.bos_token_id u32              = 50256
llama_model_loader: - kv  15:                tokenizer.ggml.eos_token_id u32              = 50256
print_info: file format = GGUF V3 (latest)
print_info: file type   = F16
print_info: file size   = 0.00 MiB (nan BPW) 
init_tokenizer: initializing tokenizer for type 2
load: special tokens cache size = 1
load: token to piece cache size = 0.3060 MB
print_info: arch             = gpt2
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 0.00 K
print_info: general.name     = gpt-2
print_info: vocab type       = BPE
print_info: n_vocab          = 50257
print_info: n_merges         = 50000
print_info: BOS token        = 50256 '<|endoftext|>'
print_info: EOS token        = 50256 '<|endoftext|>'
print_info: EOT token        = 50256 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 50256 '<|endoftext|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 512
llama_context: n_ctx_per_seq = 512
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: flash_attn    = 0
llama_context: freq_base     = 0.0
llama_context: freq_scale    = 1
llama_context: n_ctx_pre_seq (512) > n_ctx_train (0) -- possible training context overflow
llama_context_kv_self: n_ctx = 512
llama_context_kv_self: n_ctx = 512 (padded)
/Users/ggml/work/llama.cpp/ggml/src/ggml-backend.cpp:1454: GGML_ASSERT(n_backends > 0) failed

      Start  7: test-tokenizer-0-llama-bpe
 7/29 Test  #7: test-tokenizer-0-llama-bpe ........Subprocess aborted***Exception:   0.65 sec
main : reading vocab from: '/Users/ggml/work/llama.cpp/tests/../models/ggml-vocab-llama-bpe.gguf'
register_backend: registered backend Metal (1 devices)
register_device: registered device Metal (Apple M4)
register_backend: registered backend BLAS (1 devices)
register_device: registered device BLAS (Accelerate)
register_backend: registered backend CPU (1 devices)
register_device: registered device CPU (Apple M4)
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 20 key-value pairs and 0 tensors from /Users/ggml/work/llama.cpp/tests/../models/ggml-vocab-llama-bpe.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = llama-bpe
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 1
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128001
print_info: file format = GGUF V3 (latest)
print_info: file type   = F16
print_info: file size   = 0.00 MiB (nan BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token: 128255 '<|reserved_special_token_250|>' is not marked as EOG
load: control token: 128253 '<|reserved_special_token_248|>' is not marked as EOG
load: control token: 128251 '<|reserved_special_token_246|>' is not marked as EOG
load: control token: 128249 '<|reserved_special_token_244|>' is not marked as EOG
load: control token: 128248 '<|reserved_special_token_243|>' is not marked as EOG
load: control token: 128247 '<|reserved_special_token_242|>' is not marked as EOG
load: control token: 128245 '<|reserved_special_token_240|>' is not marked as EOG
load: control token: 128244 '<|reserved_special_token_239|>' is not marked as EOG
load: control token: 128242 '<|reserved_special_token_237|>' is not marked as EOG
load: control token: 128241 '<|reserved_special_token_236|>' is not marked as EOG
load: control token: 128240 '<|reserved_special_token_235|>' is not marked as EOG
load: control token: 128237 '<|reserved_special_token_232|>' is not marked as EOG
load: control token: 128235 '<|reserved_special_token_230|>' is not marked as EOG
load: control token: 128232 '<|reserved_special_token_227|>' is not marked as EOG
load: control token: 128231 '<|reserved_special_token_226|>' is not marked as EOG
load: control token: 128226 '<|reserved_special_token_221|>' is not marked as EOG
load: control token: 128224 '<|reserved_special_token_219|>' is not marked as EOG
load: control token: 128223 '<|reserved_special_token_218|>' is not marked as EOG
load: control token: 128221 '<|reserved_special_token_216|>' is not marked as EOG
load: control token: 128220 '<|reserved_special_token_215|>' is not marked as EOG
load: control token: 128218 '<|reserved_special_token_213|>' is not marked as EOG
load: control token: 128216 '<|reserved_special_token_211|>' is not marked as EOG
load: control token: 128215 '<|reserved_special_token_210|>' is not marked as EOG
load: control token: 128214 '<|reserved_special_token_209|>' is not marked as EOG
load: control token: 128213 '<|reserved_special_token_208|>' is not marked as EOG
load: control token: 128212 '<|reserved_special_token_207|>' is not marked as EOG
load: control token: 128210 '<|reserved_special_token_205|>' is not marked as EOG
load: control token: 128208 '<|reserved_special_token_203|>' is not marked as EOG
load: control token: 128207 '<|reserved_special_token_202|>' is not marked as EOG
load: control token: 128206 '<|reserved_special_token_201|>' is not marked as EOG
load: control token: 128205 '<|reserved_special_token_200|>' is not marked as EOG
load: control token: 128204 '<|reserved_special_token_199|>' is not marked as EOG
load: control token: 128201 '<|reserved_special_token_196|>' is not marked as EOG
load: control token: 128199 '<|reserved_special_token_194|>' is not marked as EOG
load: control token: 128194 '<|reserved_special_token_189|>' is not marked as EOG
load: control token: 128192 '<|reserved_special_token_187|>' is not marked as EOG
load: control token: 128191 '<|reserved_special_token_186|>' is not marked as EOG
load: control token: 128188 '<|reserved_special_token_183|>' is not marked as EOG
load: control token: 128187 '<|reserved_special_token_182|>' is not marked as EOG
load: control token: 128185 '<|reserved_special_token_180|>' is not marked as EOG
load: control token: 128184 '<|reserved_special_token_179|>' is not marked as EOG
load: control token: 128182 '<|reserved_special_token_177|>' is not marked as EOG
load: control token: 128181 '<|reserved_special_token_176|>' is not marked as EOG
load: control token: 128180 '<|reserved_special_token_175|>' is not marked as EOG
load: control token: 128175 '<|reserved_special_token_170|>' is not marked as EOG
load: control token: 128174 '<|reserved_special_token_169|>' is not marked as EOG
load: control token: 128173 '<|reserved_special_token_168|>' is not marked as EOG
load: control token: 128172 '<|reserved_special_token_167|>' is not marked as EOG
load: control token: 128171 '<|reserved_special_token_166|>' is not marked as EOG
load: control token: 128170 '<|reserved_special_token_165|>' is not marked as EOG
load: control token: 128169 '<|reserved_special_token_164|>' is not marked as EOG
load: control token: 128166 '<|reserved_special_token_161|>' is not marked as EOG
load: control token: 128164 '<|reserved_special_token_159|>' is not marked as EOG
load: control token: 128163 '<|reserved_special_token_158|>' is not marked as EOG
load: control token: 128157 '<|reserved_special_token_152|>' is not marked as EOG
load: control token: 128156 '<|reserved_special_token_151|>' is not marked as EOG
load: control token: 128154 '<|reserved_special_token_149|>' is not marked as EOG
load: control token: 128153 '<|reserved_special_token_148|>' is not marked as EOG
load: control token: 128151 '<|reserved_special_token_146|>' is not marked as EOG
load: control token: 128149 '<|reserved_special_token_144|>' is not marked as EOG
load: control token: 128148 '<|reserved_special_token_143|>' is not marked as EOG
load: control token: 128147 '<|reserved_special_token_142|>' is not marked as EOG
load: control token: 128144 '<|reserved_special_token_139|>' is not marked as EOG
load: control token: 128141 '<|reserved_special_token_136|>' is not marked as EOG
load: control token: 128139 '<|reserved_special_token_134|>' is not marked as EOG
load: control token: 128138 '<|reserved_special_token_133|>' is not marked as EOG
load: control token: 128137 '<|reserved_special_token_132|>' is not marked as EOG
load: control token: 128130 '<|reserved_special_token_125|>' is not marked as EOG
load: control token: 128127 '<|reserved_special_token_122|>' is not marked as EOG
load: control token: 128125 '<|reserved_special_token_120|>' is not marked as EOG
load: control token: 128124 '<|reserved_special_token_119|>' is not marked as EOG
load: control token: 128123 '<|reserved_special_token_118|>' is not marked as EOG
load: control token: 128122 '<|reserved_special_token_117|>' is not marked as EOG
load: control token: 128121 '<|reserved_special_token_116|>' is not marked as EOG
load: control token: 128120 '<|reserved_special_token_115|>' is not marked as EOG
load: control token: 128119 '<|reserved_special_token_114|>' is not marked as EOG
load: control token: 128118 '<|reserved_special_token_113|>' is not marked as EOG
load: control token: 128117 '<|reserved_special_token_112|>' is not marked as EOG
load: control token: 128116 '<|reserved_special_token_111|>' is not marked as EOG
load: control token: 128113 '<|reserved_special_token_108|>' is not marked as EOG
load: control token: 128112 '<|reserved_special_token_107|>' is not marked as EOG
load: control token: 128111 '<|reserved_special_token_106|>' is not marked as EOG
load: control token: 128110 '<|reserved_special_token_105|>' is not marked as EOG
load: control token: 128108 '<|reserved_special_token_103|>' is not marked as EOG
load: control token: 128107 '<|reserved_special_token_102|>' is not marked as EOG
load: control token: 128104 '<|reserved_special_token_99|>' is not marked as EOG
load: control token: 128103 '<|reserved_special_token_98|>' is not marked as EOG
load: control token: 128102 '<|reserved_special_token_97|>' is not marked as EOG
load: control token: 128101 '<|reserved_special_token_96|>' is not marked as EOG
load: control token: 128100 '<|reserved_special_token_95|>' is not marked as EOG
load: control token: 128097 '<|reserved_special_token_92|>' is not marked as EOG
load: control token: 128094 '<|reserved_special_token_89|>' is not marked as EOG
load: control token: 128093 '<|reserved_special_token_88|>' is not marked as EOG
load: control token: 128091 '<|reserved_special_token_86|>' is not marked as EOG
load: control token: 128090 '<|reserved_special_token_85|>' is not marked as EOG
load: control token: 128087 '<|reserved_special_token_82|>' is not marked as EOG
load: control token: 128086 '<|reserved_special_token_81|>' is not marked as EOG
load: control token: 128084 '<|reserved_special_token_79|>' is not marked as EOG
load: control token: 128082 '<|reserved_special_token_77|>' is not marked as EOG
load: control token: 128077 '<|reserved_special_token_72|>' is not marked as EOG
load: control token: 128074 '<|reserved_special_token_69|>' is not marked as EOG
load: control token: 128073 '<|reserved_special_token_68|>' is not marked as EOG
load: control token: 128070 '<|reserved_special_token_65|>' is not marked as EOG
load: control token: 128067 '<|reserved_special_token_62|>' is not marked as EOG
load: control token: 128066 '<|reserved_special_token_61|>' is not marked as EOG
load: control token: 128064 '<|reserved_special_token_59|>' is not marked as EOG
load: control token: 128061 '<|reserved_special_token_56|>' is not marked as EOG
load: control token: 128059 '<|reserved_special_token_54|>' is not marked as EOG
load: control token: 128058 '<|reserved_special_token_53|>' is not marked as EOG
load: control token: 128057 '<|reserved_special_token_52|>' is not marked as EOG
load: control token: 128051 '<|reserved_special_token_46|>' is not marked as EOG
load: control token: 128042 '<|reserved_special_token_37|>' is not marked as EOG
load: control token: 128041 '<|reserved_special_token_36|>' is not marked as EOG
load: control token: 128040 '<|reserved_special_token_35|>' is not marked as EOG
load: control token: 128039 '<|reserved_special_token_34|>' is not marked as EOG
load: control token: 128035 '<|reserved_special_token_30|>' is not marked as EOG
load: control token: 128034 '<|reserved_special_token_29|>' is not marked as EOG
load: control token: 128032 '<|reserved_special_token_27|>' is not marked as EOG
load: control token: 128031 '<|reserved_special_token_26|>' is not marked as EOG
load: control token: 128030 '<|reserved_special_token_25|>' is not marked as EOG
load: control token: 128029 '<|reserved_special_token_24|>' is not marked as EOG
load: control token: 128027 '<|reserved_special_token_22|>' is not marked as EOG
load: control token: 128026 '<|reserved_special_token_21|>' is not marked as EOG
load: control token: 128025 '<|reserved_special_token_20|>' is not marked as EOG
load: control token: 128023 '<|reserved_special_token_18|>' is not marked as EOG
load: control token: 128022 '<|reserved_special_token_17|>' is not marked as EOG
load: control token: 128021 '<|reserved_special_token_16|>' is not marked as EOG
load: control token: 128019 '<|reserved_special_token_14|>' is not marked as EOG
load: control token: 128017 '<|reserved_special_token_12|>' is not marked as EOG
load: control token: 128014 '<|reserved_special_token_9|>' is not marked as EOG
load: control token: 128013 '<|reserved_special_token_8|>' is not marked as EOG
load: control token: 128012 '<|reserved_special_token_7|>' is not marked as EOG
load: control token: 128011 '<|reserved_special_token_6|>' is not marked as EOG
load: control token: 128010 '<|reserved_special_token_5|>' is not marked as EOG
load: control token: 128006 '<|start_header_id|>' is not marked as EOG
load: control token: 128005 '<|reserved_special_token_3|>' is not marked as EOG
load: control token: 128003 '<|reserved_special_token_1|>' is not marked as EOG
load: control token: 128002 '<|reserved_special_token_0|>' is not marked as EOG
load: control token: 128000 '<|begin_of_text|>' is not marked as EOG
load: control token: 128038 '<|reserved_special_token_33|>' is not marked as EOG
load: control token: 128060 '<|reserved_special_token_55|>' is not marked as EOG
load: control token: 128043 '<|reserved_special_token_38|>' is not marked as EOG
load: control token: 128007 '<|end_header_id|>' is not marked as EOG
load: control token: 128062 '<|reserved_special_token_57|>' is not marked as EOG
load: control token: 128168 '<|reserved_special_token_163|>' is not marked as EOG
load: control token: 128159 '<|reserved_special_token_154|>' is not marked as EOG
load: control token: 128162 '<|reserved_special_token_157|>' is not marked as EOG
load: control token: 128054 '<|reserved_special_token_49|>' is not marked as EOG
load: control token: 128047 '<|reserved_special_token_42|>' is not marked as EOG
load: control token: 128053 '<|reserved_special_token_48|>' is not marked as EOG
load: control token: 128227 '<|reserved_special_token_222|>' is not marked as EOG
load: control token: 128095 '<|reserved_special_token_90|>' is not marked as EOG
load: control token: 128150 '<|reserved_special_token_145|>' is not marked as EOG
load: control token: 128081 '<|reserved_special_token_76|>' is not marked as EOG
load: control token: 128079 '<|reserved_special_token_74|>' is not marked as EOG
load: control token: 128099 '<|reserved_special_token_94|>' is not marked as EOG
load: control token: 128250 '<|reserved_special_token_245|>' is not marked as EOG
load: control token: 128176 '<|reserved_special_token_171|>' is not marked as EOG
load: control token: 128068 '<|reserved_special_token_63|>' is not marked as EOG
load: control token: 128132 '<|reserved_special_token_127|>' is not marked as EOG
load: control token: 128158 '<|reserved_special_token_153|>' is not marked as EOG
load: control token: 128161 '<|reserved_special_token_156|>' is not marked as EOG
load: control token: 128131 '<|reserved_special_token_126|>' is not marked as EOG
load: control token: 128246 '<|reserved_special_token_241|>' is not marked as EOG
load: control token: 128254 '<|reserved_special_token_249|>' is not marked as EOG
load: control token: 128033 '<|reserved_special_token_28|>' is not marked as EOG
load: control token: 128145 '<|reserved_special_token_140|>' is not marked as EOG
load: control token: 128178 '<|reserved_special_token_173|>' is not marked as EOG
load: control token: 128219 '<|reserved_special_token_214|>' is not marked as EOG
load: control token: 128072 '<|reserved_special_token_67|>' is not marked as EOG
load: control token: 128238 '<|reserved_special_token_233|>' is not marked as EOG
load: control token: 128048 '<|reserved_special_token_43|>' is not marked as EOG
load: control token: 128065 '<|reserved_special_token_60|>' is not marked as EOG
load: control token: 128146 '<|reserved_special_token_141|>' is not marked as EOG
load: control token: 128198 '<|reserved_special_token_193|>' is not marked as EOG
load: control token: 128055 '<|reserved_special_token_50|>' is not marked as EOG
load: control token: 128143 '<|reserved_special_token_138|>' is not marked as EOG
load: control token: 128140 '<|reserved_special_token_135|>' is not marked as EOG
load: control token: 128020 '<|reserved_special_token_15|>' is not marked as EOG
load: control token: 128036 '<|reserved_special_token_31|>' is not marked as EOG
load: control token: 128129 '<|reserved_special_token_124|>' is not marked as EOG
load: control token: 128098 '<|reserved_special_token_93|>' is not marked as EOG
load: control token: 128209 '<|reserved_special_token_204|>' is not marked as EOG
load: control token: 128186 '<|reserved_special_token_181|>' is not marked as EOG
load: control token: 128222 '<|reserved_special_token_217|>' is not marked as EOG
load: control token: 128126 '<|reserved_special_token_121|>' is not marked as EOG
load: control token: 128004 '<|reserved_special_token_2|>' is not marked as EOG
load: control token: 128075 '<|reserved_special_token_70|>' is not marked as EOG
load: control token: 128160 '<|reserved_special_token_155|>' is not marked as EOG
load: control token: 128069 '<|reserved_special_token_64|>' is not marked as EOG
load: control token: 128109 '<|reserved_special_token_104|>' is not marked as EOG
load: control token: 128183 '<|reserved_special_token_178|>' is not marked as EOG
load: control token: 128092 '<|reserved_special_token_87|>' is not marked as EOG
load: control token: 128106 '<|reserved_special_token_101|>' is not marked as EOG
load: control token: 128096 '<|reserved_special_token_91|>' is not marked as EOG
load: control token: 128135 '<|reserved_special_token_130|>' is not marked as EOG
load: control token: 128190 '<|reserved_special_token_185|>' is not marked as EOG
load: control token: 128196 '<|reserved_special_token_191|>' is not marked as EOG
load: control token: 128045 '<|reserved_special_token_40|>' is not marked as EOG
load: control token: 128085 '<|reserved_special_token_80|>' is not marked as EOG
load: control token: 128189 '<|reserved_special_token_184|>' is not marked as EOG
load: control token: 128133 '<|reserved_special_token_128|>' is not marked as EOG
load: control token: 128089 '<|reserved_special_token_84|>' is not marked as EOG
load: control token: 128155 '<|reserved_special_token_150|>' is not marked as EOG
load: control token: 128001 '<|end_of_text|>' is not marked as EOG
load: control token: 128046 '<|reserved_special_token_41|>' is not marked as EOG
load: control token: 128028 '<|reserved_special_token_23|>' is not marked as EOG
load: control token: 128252 '<|reserved_special_token_247|>' is not marked as EOG
load: control token: 128179 '<|reserved_special_token_174|>' is not marked as EOG
load: control token: 128063 '<|reserved_special_token_58|>' is not marked as EOG
load: control token: 128177 '<|reserved_special_token_172|>' is not marked as EOG
load: control token: 128230 '<|reserved_special_token_225|>' is not marked as EOG
load: control token: 128076 '<|reserved_special_token_71|>' is not marked as EOG
load: control token: 128078 '<|reserved_special_token_73|>' is not marked as EOG
load: control token: 128228 '<|reserved_special_token_223|>' is not marked as EOG
load: control token: 128193 '<|reserved_special_token_188|>' is not marked as EOG
load: control token: 128044 '<|reserved_special_token_39|>' is not marked as EOG
load: control token: 128080 '<|reserved_special_token_75|>' is not marked as EOG
load: control token: 128136 '<|reserved_special_token_131|>' is not marked as EOG
load: control token: 128128 '<|reserved_special_token_123|>' is not marked as EOG
load: control token: 128115 '<|reserved_special_token_110|>' is not marked as EOG
load: control token: 128050 '<|reserved_special_token_45|>' is not marked as EOG
load: control token: 128217 '<|reserved_special_token_212|>' is not marked as EOG
load: control token: 128105 '<|reserved_special_token_100|>' is not marked as EOG
load: control token: 128088 '<|reserved_special_token_83|>' is not marked as EOG
load: control token: 128200 '<|reserved_special_token_195|>' is not marked as EOG
load: control token: 128056 '<|reserved_special_token_51|>' is not marked as EOG
load: control token: 128016 '<|reserved_special_token_11|>' is not marked as EOG
load: control token: 128167 '<|reserved_special_token_162|>' is not marked as EOG
load: control token: 128202 '<|reserved_special_token_197|>' is not marked as EOG
load: control token: 128037 '<|reserved_special_token_32|>' is not marked as EOG
load: control token: 128197 '<|reserved_special_token_192|>' is not marked as EOG
load: control token: 128233 '<|reserved_special_token_228|>' is not marked as EOG
load: control token: 128142 '<|reserved_special_token_137|>' is not marked as EOG
load: control token: 128165 '<|reserved_special_token_160|>' is not marked as EOG
load: control token: 128211 '<|reserved_special_token_206|>' is not marked as EOG
load: control token: 128134 '<|reserved_special_token_129|>' is not marked as EOG
load: control token: 128229 '<|reserved_special_token_224|>' is not marked as EOG
load: control token: 128236 '<|reserved_special_token_231|>' is not marked as EOG
load: control token: 128052 '<|reserved_special_token_47|>' is not marked as EOG
load: control token: 128225 '<|reserved_special_token_220|>' is not marked as EOG
load: control token: 128203 '<|reserved_special_token_198|>' is not marked as EOG
load: control token: 128015 '<|reserved_special_token_10|>' is not marked as EOG
load: control token: 128008 '<|reserved_special_token_4|>' is not marked as EOG
load: control token: 128195 '<|reserved_special_token_190|>' is not marked as EOG
load: control token: 128018 '<|reserved_special_token_13|>' is not marked as EOG
load: control token: 128083 '<|reserved_special_token_78|>' is not marked as EOG
load: control token: 128071 '<|reserved_special_token_66|>' is not marked as EOG
load: control token: 128024 '<|reserved_special_token_19|>' is not marked as EOG
load: control token: 128239 '<|reserved_special_token_234|>' is not marked as EOG
load: control token: 128152 '<|reserved_special_token_147|>' is not marked as EOG
load: control token: 128049 '<|reserved_special_token_44|>' is not marked as EOG
load: control token: 128243 '<|reserved_special_token_238|>' is not marked as EOG
load: control token: 128114 '<|reserved_special_token_109|>' is not marked as EOG
load: control token: 128234 '<|reserved_special_token_229|>' is not marked as EOG
load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
load: special tokens cache size = 256
load: token to piece cache size = 0.8000 MB
print_info: arch             = llama
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 0.00 K
print_info: general.name     = llama-bpe
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128001 '<|end_of_text|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128001 '<|end_of_text|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 512
llama_context: n_ctx_per_seq = 512
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: flash_attn    = 0
llama_context: freq_base     = 0.0
llama_context: freq_scale    = 1
llama_context: n_ctx_pre_seq (512) > n_ctx_train (0) -- possible training context overflow
llama_context_kv_self: n_ctx = 512
llama_context_kv_self: n_ctx = 512 (padded)
/Users/ggml/work/llama.cpp/ggml/src/ggml-backend.cpp:1454: GGML_ASSERT(n_backends > 0) failed

      Start  8: test-tokenizer-0-llama-spm
 8/29 Test  #8: test-tokenizer-0-llama-spm ........Subprocess aborted***Exception:   0.09 sec
main : reading vocab from: '/Users/ggml/work/llama.cpp/tests/../models/ggml-vocab-llama-spm.gguf'
register_backend: registered backend Metal (1 devices)
register_device: registered device Metal (Apple M4)
register_backend: registered backend BLAS (1 devices)
register_device: registered device BLAS (Accelerate)
register_backend: registered backend CPU (1 devices)
register_device: registered device CPU (Apple M4)
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 22 key-value pairs and 0 tensors from /Users/ggml/work/llama.cpp/tests/../models/ggml-vocab-llama-spm.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = llama-spm
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 4096
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   8:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv   9:                          general.file_type u32              = 1
llama_model_loader: - kv  10:                           llama.vocab_size u32              = 32000
llama_model_loader: - kv  11:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                         tokenizer.ggml.pre str              = default
llama_model_loader: - kv  14:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  15:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false
print_info: file format = GGUF V3 (latest)
print_info: file type   = F16
print_info: file size   = 0.00 MiB (nan BPW) 
init_tokenizer: initializing tokenizer for type 1
load: control token:      1 '<s>' is not marked as EOG
load: control token:      2 '</s>' is not marked as EOG
load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
load: special tokens cache size = 3
load: token to piece cache size = 0.1684 MB
print_info: arch             = llama
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 0.00 K
print_info: general.name     = llama-spm
print_info: vocab type       = SPM
print_info: n_vocab          = 32000
print_info: n_merges         = 0
print_info: BOS token        = 1 '<s>'
print_info: EOS token        = 2 '</s>'
print_info: UNK token        = 0 '<unk>'
print_info: LF token         = 13 '<0x0A>'
print_info: EOG token        = 2 '</s>'
print_info: max token length = 48
llama_model_load: vocab only - skipping tensors
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 512
llama_context: n_ctx_per_seq = 512
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: flash_attn    = 0
llama_context: freq_base     = 0.0
llama_context: freq_scale    = 1
llama_context: n_ctx_pre_seq (512) > n_ctx_train (0) -- possible training context overflow
llama_context_kv_self: n_ctx = 512
llama_context_kv_self: n_ctx = 512 (padded)
/Users/ggml/work/llama.cpp/ggml/src/ggml-backend.cpp:1454: GGML_ASSERT(n_backends > 0) failed

      Start  9: test-tokenizer-0-mpt
 9/29 Test  #9: test-tokenizer-0-mpt ..............Subprocess aborted***Exception:   0.23 sec
main : reading vocab from: '/Users/ggml/work/llama.cpp/tests/../models/ggml-vocab-mpt.gguf'
register_backend: registered backend Metal (1 devices)
register_device: registered device Metal (Apple M4)
register_backend: registered backend BLAS (1 devices)
register_device: registered device BLAS (Accelerate)
register_backend: registered backend CPU (1 devices)
register_device: registered device CPU (Apple M4)
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 17 key-value pairs and 0 tensors from /Users/ggml/work/llama.cpp/tests/../models/ggml-vocab-mpt.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = mpt
llama_model_loader: - kv   1:                               general.name str              = mpt
llama_model_loader: - kv   2:                         mpt.context_length u32              = 2048
llama_model_loader: - kv   3:                       mpt.embedding_length u32              = 4096
llama_model_loader: - kv   4:                            mpt.block_count u32              = 32
llama_model_loader: - kv   5:                    mpt.feed_forward_length u32              = 16384
llama_model_loader: - kv   6:                   mpt.attention.head_count u32              = 32
llama_model_loader: - kv   7:           mpt.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv   8:               mpt.attention.max_alibi_bias f32              = 8.000000
llama_model_loader: - kv   9:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  10:                         tokenizer.ggml.pre str              = mpt
llama_model_loader: - kv  11:                      tokenizer.ggml.tokens arr[str,50432]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  12:                  tokenizer.ggml.token_type arr[i32,50432]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  13:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  14:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  15:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  16:            tokenizer.ggml.unknown_token_id u32              = 0
print_info: file format = GGUF V3 (latest)
print_info: file type   = all F32 (guessed)
print_info: file size   = 0.00 MiB (nan BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 180
load: token to piece cache size = 0.2999 MB
print_info: arch             = mpt
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 0.00 K
print_info: general.name     = mpt
print_info: vocab type       = BPE
print_info: n_vocab          = 50432
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 'Ċ'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
llama_model_load: vocab only - skipping tensors
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 512
llama_context: n_ctx_per_seq = 512
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: flash_attn    = 0
llama_context: freq_base     = 0.0
llama_context: freq_scale    = 1
llama_context: n_ctx_pre_seq (512) > n_ctx_train (0) -- possible training context overflow
llama_context_kv_self: n_ctx = 512
llama_context_kv_self: n_ctx = 512 (padded)
/Users/ggml/work/llama.cpp/ggml/src/ggml-backend.cpp:1454: GGML_ASSERT(n_backends > 0) failed

      Start 10: test-tokenizer-0-phi-3
10/29 Test #10: test-tokenizer-0-phi-3 ............Subprocess aborted***Exception:   0.09 sec
main : reading vocab from: '/Users/ggml/work/llama.cpp/tests/../models/ggml-vocab-phi-3.gguf'
register_backend: registered backend Metal (1 devices)
register_device: registered device Metal (Apple M4)
register_backend: registered backend BLAS (1 devices)
register_device: registered device BLAS (Accelerate)
register_backend: registered backend CPU (1 devices)
register_device: registered device CPU (Apple M4)
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 26 key-value pairs and 0 tensors from /Users/ggml/work/llama.cpp/tests/../models/ggml-vocab-phi-3.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = phi3
llama_model_loader: - kv   1:                               general.name str              = Phi3
llama_model_loader: - kv   2:                        phi3.context_length u32              = 4096
llama_model_loader: - kv   3:  phi3.rope.scaling.original_context_length u32              = 4096
llama_model_loader: - kv   4:                      phi3.embedding_length u32              = 3072
llama_model_loader: - kv   5:                   phi3.feed_forward_length u32              = 8192
llama_model_loader: - kv   6:                           phi3.block_count u32              = 32
llama_model_loader: - kv   7:                  phi3.attention.head_count u32              = 32
llama_model_loader: - kv   8:               phi3.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:      phi3.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                  phi3.rope.dimension_count u32              = 96
llama_model_loader: - kv  11:                        phi3.rope.freq_base f32              = 10000.000000
llama_model_loader: - kv  12:                          general.file_type u32              = 1
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = default
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,32064]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  16:                      tokenizer.ggml.scores arr[f32,32064]   = [-1000.000000, -1000.000000, -1000.00...
llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,32064]   = [3, 3, 4, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 32000
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 32000
llama_model_loader: - kv  22:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  23:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  24:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...
llama_model_loader: - kv  25:               general.quantization_version u32              = 2
print_info: file format = GGUF V3 (latest)
print_info: file type   = F16
print_info: file size   = 0.00 MiB (nan BPW) 
init_tokenizer: initializing tokenizer for type 1
load: control token:  32010 '<|user|>' is not marked as EOG
load: control token:  32009 '<|placeholder6|>' is not marked as EOG
load: control token:  32008 '<|placeholder5|>' is not marked as EOG
load: control token:  32006 '<|system|>' is not marked as EOG
load: control token:  32005 '<|placeholder4|>' is not marked as EOG
load: control token:  32004 '<|placeholder3|>' is not marked as EOG
load: control token:  32001 '<|assistant|>' is not marked as EOG
load: control token:  32002 '<|placeholder1|>' is not marked as EOG
load: control token:  32003 '<|placeholder2|>' is not marked as EOG
load: control token:      1 '<s>' is not marked as EOG
load: control token:      0 '<unk>' is not marked as EOG
load: special tokens cache size = 67
load: token to piece cache size = 0.1690 MB
print_info: arch             = phi3
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 0.00 K
print_info: general.name     = Phi3
print_info: vocab type       = SPM
print_info: n_vocab          = 32064
print_info: n_merges         = 0
print_info: BOS token        = 1 '<s>'
print_info: EOS token        = 32000 '<|endoftext|>'
print_info: EOT token        = 32007 '<|end|>'
print_info: UNK token        = 0 '<unk>'
print_info: PAD token        = 32000 '<|endoftext|>'
print_info: LF token         = 13 '<0x0A>'
print_info: EOG token        = 32000 '<|endoftext|>'
print_info: EOG token        = 32007 '<|end|>'
print_info: max token length = 48
llama_model_load: vocab only - skipping tensors
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 512
llama_context: n_ctx_per_seq = 512
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: flash_attn    = 0
llama_context: freq_base     = 0.0
llama_context: freq_scale    = 1
llama_context: n_ctx_pre_seq (512) > n_ctx_train (0) -- possible training context overflow
llama_context_kv_self: n_ctx = 512
llama_context_kv_self: n_ctx = 512 (padded)
/Users/ggml/work/llama.cpp/ggml/src/ggml-backend.cpp:1454: GGML_ASSERT(n_backends > 0) failed

      Start 11: test-tokenizer-0-qwen2
11/29 Test #11: test-tokenizer-0-qwen2 ............Subprocess aborted***Exception:   0.58 sec
main : reading vocab from: '/Users/ggml/work/llama.cpp/tests/../models/ggml-vocab-qwen2.gguf'
register_backend: registered backend Metal (1 devices)
register_device: registered device Metal (Apple M4)
register_backend: registered backend BLAS (1 devices)
register_device: registered device BLAS (Accelerate)
register_backend: registered backend CPU (1 devices)
register_device: registered device CPU (Apple M4)
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 20 key-value pairs and 0 tensors from /Users/ggml/work/llama.cpp/tests/../models/ggml-vocab-qwen2.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.name str              = qwen2
llama_model_loader: - kv   2:                          qwen2.block_count u32              = 32
llama_model_loader: - kv   3:                       qwen2.context_length u32              = 32768
llama_model_loader: - kv   4:                     qwen2.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  qwen2.feed_forward_length u32              = 11008
llama_model_loader: - kv   6:                 qwen2.attention.head_count u32              = 32
llama_model_loader: - kv   7:              qwen2.attention.head_count_kv u32              = 32
llama_model_loader: - kv   8:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv   9:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  10:                          general.file_type u32              = 1
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  12:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 151643
llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  19:                    tokenizer.chat_template str              = {% for message in messages %}{% if lo...
print_info: file format = GGUF V3 (latest)
print_info: file type   = F16
print_info: file size   = 0.00 MiB (nan BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token: 151644 '<|im_start|>' is not marked as EOG
load: special tokens cache size = 293
load: token to piece cache size = 0.9338 MB
print_info: arch             = qwen2
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 0.00 K
print_info: general.name     = qwen2
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151643 '<|endoftext|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 512
llama_context: n_ctx_per_seq = 512
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: flash_attn    = 0
llama_context: freq_base     = 0.0
llama_context: freq_scale    = 1
llama_context: n_ctx_pre_seq (512) > n_ctx_train (0) -- possible training context overflow
llama_context_kv_self: n_ctx = 512
llama_context_kv_self: n_ctx = 512 (padded)
/Users/ggml/work/llama.cpp/ggml/src/ggml-backend.cpp:1454: GGML_ASSERT(n_backends > 0) failed

      Start 12: test-tokenizer-0-refact
12/29 Test #12: test-tokenizer-0-refact ...........Subprocess aborted***Exception:   0.22 sec
main : reading vocab from: '/Users/ggml/work/llama.cpp/tests/../models/ggml-vocab-refact.gguf'
register_backend: registered backend Metal (1 devices)
register_device: registered device Metal (Apple M4)
register_backend: registered backend BLAS (1 devices)
register_device: registered device BLAS (Accelerate)
register_backend: registered backend CPU (1 devices)
register_device: registered device CPU (Apple M4)
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 18 key-value pairs and 0 tensors from /Users/ggml/work/llama.cpp/tests/../models/ggml-vocab-refact.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = refact
llama_model_loader: - kv   1:                               general.name str              = Refact
llama_model_loader: - kv   2:                      refact.context_length u32              = 4096
llama_model_loader: - kv   3:                    refact.embedding_length u32              = 2048
llama_model_loader: - kv   4:                 refact.feed_forward_length u32              = 5632
llama_model_loader: - kv   5:                         refact.block_count u32              = 32
llama_model_loader: - kv   6:                refact.attention.head_count u32              = 32
llama_model_loader: - kv   7:             refact.attention.head_count_kv u32              = 1
llama_model_loader: - kv   8:    refact.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv   9:                          general.file_type u32              = 1
llama_model_loader: - kv  10:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  11:                         tokenizer.ggml.pre str              = refact
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,49216]   = ["<|endoftext|>", "<fim_prefix>", "<f...
llama_model_loader: - kv  13:                  tokenizer.ggml.token_type arr[i32,49216]   = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...
llama_model_loader: - kv  14:                      tokenizer.ggml.merges arr[str,48891]   = ["Ġ Ġ", "ĠĠ ĠĠ", "ĠĠĠĠ ĠĠ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0
print_info: file format = GGUF V3 (latest)
print_info: file type   = F16
print_info: file size   = 0.00 MiB (nan BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      6 '<gh_stars>' is not marked as EOG
load: control token:      3 '<fim_suffix>' is not marked as EOG
load: control token:      1 '<fim_prefix>' is not marked as EOG
load: control token:      9 '<issue_closed>' is not marked as EOG
load: control token:     13 '<jupyter_output>' is not marked as EOG
load: control token:      7 '<issue_start>' is not marked as EOG
load: control token:      5 '<filename>' is not marked as EOG
load: control token:     18 '<reponame>' is not marked as EOG
load: control token:      2 '<fim_middle>' is not marked as EOG
load: control token:     15 '<commit_before>' is not marked as EOG
load: control token:     17 '<commit_after>' is not marked as EOG
load: control token:     14 '<empty_output>' is not marked as EOG
load: control token:     11 '<jupyter_text>' is not marked as EOG
load: control token:      4 '<fim_pad>' is not marked as EOG
load: control token:      8 '<issue_comment>' is not marked as EOG
load: control token:     12 '<jupyter_code>' is not marked as EOG
load: control token:     10 '<jupyter_start>' is not marked as EOG
load: control token:     16 '<commit_msg>' is not marked as EOG
load: special tokens cache size = 83
load: token to piece cache size = 0.2832 MB
print_info: arch             = refact
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 0.00 K
print_info: general.name     = Refact
print_info: vocab type       = BPE
print_info: n_vocab          = 49216
print_info: n_merges         = 48891
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 203 'Ċ'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 512
llama_model_load: vocab only - skipping tensors
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 512
llama_context: n_ctx_per_seq = 512
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: flash_attn    = 0
llama_context: freq_base     = 0.0
llama_context: freq_scale    = 1
llama_context: n_ctx_pre_seq (512) > n_ctx_train (0) -- possible training context overflow
llama_context_kv_self: n_ctx = 512
llama_context_kv_self: n_ctx = 512 (padded)
/Users/ggml/work/llama.cpp/ggml/src/ggml-backend.cpp:1454: GGML_ASSERT(n_backends > 0) failed

      Start 13: test-tokenizer-0-starcoder
13/29 Test #13: test-tokenizer-0-starcoder ........Subprocess aborted***Exception:   0.22 sec
main : reading vocab from: '/Users/ggml/work/llama.cpp/tests/../models/ggml-vocab-starcoder.gguf'
register_backend: registered backend Metal (1 devices)
register_device: registered device Metal (Apple M4)
register_backend: registered backend BLAS (1 devices)
register_device: registered device BLAS (Accelerate)
register_backend: registered backend CPU (1 devices)
register_device: registered device CPU (Apple M4)
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 19 key-value pairs and 0 tensors from /Users/ggml/work/llama.cpp/tests/../models/ggml-vocab-starcoder.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = starcoder2
llama_model_loader: - kv   1:                               general.name str              = starcoder
llama_model_loader: - kv   2:                     starcoder2.block_count u32              = 30
llama_model_loader: - kv   3:                  starcoder2.context_length u32              = 16384
llama_model_loader: - kv   4:                starcoder2.embedding_length u32              = 3072
llama_model_loader: - kv   5:             starcoder2.feed_forward_length u32              = 12288
llama_model_loader: - kv   6:            starcoder2.attention.head_count u32              = 24
llama_model_loader: - kv   7:         starcoder2.attention.head_count_kv u32              = 2
llama_model_loader: - kv   8:                  starcoder2.rope.freq_base f32              = 999999.437500
llama_model_loader: - kv   9:    starcoder2.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 1
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  12:                         tokenizer.ggml.pre str              = starcoder
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,49152]   = ["<|endoftext|>", "<fim_prefix>", "<f...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,49152]   = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...
llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,48872]   = ["Ġ Ġ", "ĠĠ ĠĠ", "ĠĠĠĠ ĠĠ...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0
print_info: file format = GGUF V3 (latest)
print_info: file type   = F16
print_info: file size   = 0.00 MiB (nan BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:     18 '<pr>' is not marked as EOG
load: control token:     33 '<pr_diff_hunk_comment_line>' is not marked as EOG
load: control token:     36 '<KEY>' is not marked as EOG
load: control token:     30 '<pr_review_comment>' is not marked as EOG
load: control token:     23 '<pr_base_code>' is not marked as EOG
load: control token:      3 '<fim_suffix>' is not marked as EOG
load: control token:     16 '<code_to_intermediate>' is not marked as EOG
load: control token:     28 '<pr_review>' is not marked as EOG
load: control token:      1 '<fim_prefix>' is not marked as EOG
load: control token:      9 '<issue_closed>' is not marked as EOG
load: control token:      7 '<issue_start>' is not marked as EOG
load: control token:     13 '<jupyter_output>' is not marked as EOG
load: control token:     35 '<EMAIL>' is not marked as EOG
load: control token:      2 '<fim_middle>' is not marked as EOG
load: control token:     14 '<jupyter_script>' is not marked as EOG
load: control token:     31 '<pr_in_reply_to_review_id>' is not marked as EOG
load: control token:     27 '<pr_event_id>' is not marked as EOG
load: control token:     15 '<empty_output>' is not marked as EOG
load: control token:     11 '<jupyter_text>' is not marked as EOG
load: control token:     20 '<pr_is_merged>' is not marked as EOG
load: control token:      5 '<repo_name>' is not marked as EOG
load: control token:      4 '<fim_pad>' is not marked as EOG
load: control token:     37 '<PASSWORD>' is not marked as EOG
load: control token:      6 '<file_sep>' is not marked as EOG
load: control token:     17 '<intermediate_to_code>' is not marked as EOG
load: control token:      8 '<issue_comment>' is not marked as EOG
load: control token:     25 '<pr_diff_hunk>' is not marked as EOG
load: control token:     22 '<pr_file>' is not marked as EOG
load: control token:     12 '<jupyter_code>' is not marked as EOG
load: control token:     29 '<pr_review_state>' is not marked as EOG
load: control token:     21 '<pr_base>' is not marked as EOG
load: control token:     24 '<pr_diff>' is not marked as EOG
load: control token:     19 '<pr_status>' is not marked as EOG
load: control token:     10 '<jupyter_start>' is not marked as EOG
load: control token:     32 '<pr_in_reply_to_comment_id>' is not marked as EOG
load: control token:     26 '<pr_comment>' is not marked as EOG
load: control token:     34 '<NAME>' is not marked as EOG
load: special tokens cache size = 38
load: token to piece cache size = 0.2828 MB
print_info: arch             = starcoder2
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 0.00 K
print_info: general.name     = starcoder
print_info: vocab type       = BPE
print_info: n_vocab          = 49152
print_info: n_merges         = 48872
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 222 'Ċ'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 512
llama_model_load: vocab only - skipping tensors
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 512
llama_context: n_ctx_per_seq = 512
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: flash_attn    = 0
llama_context: freq_base     = 0.0
llama_context: freq_scale    = 1
llama_context: n_ctx_pre_seq (512) > n_ctx_train (0) -- possible training context overflow
llama_context_kv_self: n_ctx = 512
llama_context_kv_self: n_ctx = 512 (padded)
/Users/ggml/work/llama.cpp/ggml/src/ggml-backend.cpp:1454: GGML_ASSERT(n_backends > 0) failed

      Start 14: test-sampling
14/29 Test #14: test-sampling .....................   Passed    2.13 sec
      Start 15: test-grammar-parser
15/29 Test #15: test-grammar-parser ...............   Passed    0.18 sec
      Start 16: test-grammar-integration
16/29 Test #16: test-grammar-integration ..........   Passed    0.27 sec
      Start 17: test-llama-grammar
17/29 Test #17: test-llama-grammar ................   Passed    0.20 sec
      Start 18: test-chat
18/29 Test #18: test-chat .........................   Passed   17.17 sec
      Start 19: test-json-schema-to-grammar
19/29 Test #19: test-json-schema-to-grammar .......   Passed    2.29 sec
      Start 20: test-tokenizer-1-llama-spm
20/29 Test #20: test-tokenizer-1-llama-spm ........Subprocess aborted***Exception:   0.31 sec
main : reading vocab from: '/Users/ggml/work/llama.cpp/tests/../models/ggml-vocab-llama-spm.gguf'
register_backend: registered backend Metal (1 devices)
register_device: registered device Metal (Apple M4)
register_backend: registered backend BLAS (1 devices)
register_device: registered device BLAS (Accelerate)
register_backend: registered backend CPU (1 devices)
register_device: registered device CPU (Apple M4)
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 22 key-value pairs and 0 tensors from /Users/ggml/work/llama.cpp/tests/../models/ggml-vocab-llama-spm.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = llama-spm
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 4096
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   8:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv   9:                          general.file_type u32              = 1
llama_model_loader: - kv  10:                           llama.vocab_size u32              = 32000
llama_model_loader: - kv  11:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                         tokenizer.ggml.pre str              = default
llama_model_loader: - kv  14:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  15:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false
print_info: file format = GGUF V3 (latest)
print_info: file type   = F16
print_info: file size   = 0.00 MiB (nan BPW) 
init_tokenizer: initializing tokenizer for type 1
load: control token:      1 '<s>' is not marked as EOG
load: control token:      2 '</s>' is not marked as EOG
load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
load: special tokens cache size = 3
load: token to piece cache size = 0.1684 MB
print_info: arch             = llama
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 0.00 K
print_info: general.name     = llama-spm
print_info: vocab type       = SPM
print_info: n_vocab          = 32000
print_info: n_merges         = 0
print_info: BOS token        = 1 '<s>'
print_info: EOS token        = 2 '</s>'
print_info: UNK token        = 0 '<unk>'
print_info: LF token         = 13 '<0x0A>'
print_info: EOG token        = 2 '</s>'
print_info: max token length = 48
llama_model_load: vocab only - skipping tensors
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 512
llama_context: n_ctx_per_seq = 512
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: flash_attn    = 0
llama_context: freq_base     = 0.0
llama_context: freq_scale    = 1
llama_context: n_ctx_pre_seq (512) > n_ctx_train (0) -- possible training context overflow
llama_context_kv_self: n_ctx = 512
llama_context_kv_self: n_ctx = 512 (padded)
/Users/ggml/work/llama.cpp/ggml/src/ggml-backend.cpp:1454: GGML_ASSERT(n_backends > 0) failed

      Start 21: test-log
21/29 Test #21: test-log ..........................   Passed    0.19 sec
      Start 22: test-arg-parser
22/29 Test #22: test-arg-parser ...................   Passed    0.25 sec
      Start 23: test-chat-template
23/29 Test #23: test-chat-template ................   Passed    2.93 sec
      Start 24: test-gguf
24/29 Test #24: test-gguf .........................   Passed    0.84 sec
      Start 25: test-backend-ops
25/29 Test #25: test-backend-ops ..................   Passed  191.51 sec
      Start 28: test-barrier
26/29 Test #28: test-barrier ......................   Passed    0.89 sec
      Start 29: test-quantize-fns
27/29 Test #29: test-quantize-fns .................   Passed   26.07 sec
      Start 30: test-quantize-perf
28/29 Test #30: test-quantize-perf ................   Passed    0.33 sec
      Start 31: test-rope
29/29 Test #31: test-rope .........................   Passed    0.22 sec

52% tests passed, 14 tests failed out of 29

Label Time Summary:
main    = 251.25 sec*proc (29 tests)

Total Test time (real) = 251.26 sec

The following tests FAILED:
	  1 - test-tokenizer-0-bert-bge (Subprocess aborted)    main
	  2 - test-tokenizer-0-command-r (Subprocess aborted)   main
	  3 - test-tokenizer-0-deepseek-coder (Subprocess aborted) main
	  4 - test-tokenizer-0-deepseek-llm (Subprocess aborted) main
	  5 - test-tokenizer-0-falcon (Subprocess aborted)      main
	  6 - test-tokenizer-0-gpt-2 (Subprocess aborted)       main
	  7 - test-tokenizer-0-llama-bpe (Subprocess aborted)   main
	  8 - test-tokenizer-0-llama-spm (Subprocess aborted)   main
	  9 - test-tokenizer-0-mpt (Subprocess aborted)         main
	 10 - test-tokenizer-0-phi-3 (Subprocess aborted)       main
	 11 - test-tokenizer-0-qwen2 (Subprocess aborted)       main
	 12 - test-tokenizer-0-refact (Subprocess aborted)      main
	 13 - test-tokenizer-0-starcoder (Subprocess aborted)   main
	 20 - test-tokenizer-1-llama-spm (Subprocess aborted)   main
Errors while running CTest
```

