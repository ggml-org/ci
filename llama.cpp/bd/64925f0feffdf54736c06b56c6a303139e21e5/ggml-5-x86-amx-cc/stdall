Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cpu, https://download.pytorch.org/whl/cpu, https://download.pytorch.org/whl/cpu, https://download.pytorch.org/whl/cpu
Requirement already satisfied: numpy~=1.26.4 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from -r /home/ggml/work/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 1)) (1.26.4)
Requirement already satisfied: sentencepiece~=0.2.0 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from -r /home/ggml/work/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 2)) (0.2.0)
Requirement already satisfied: transformers<5.0.0,>=4.45.1 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from -r /home/ggml/work/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (4.48.2)
Requirement already satisfied: gguf>=0.1.0 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from -r /home/ggml/work/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 4)) (0.15.0)
Requirement already satisfied: protobuf<5.0.0,>=4.21.0 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from -r /home/ggml/work/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 5)) (4.25.6)
Requirement already satisfied: torch~=2.2.1 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from -r /home/ggml/work/llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 3)) (2.2.2+cpu)
Requirement already satisfied: filelock in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from transformers<5.0.0,>=4.45.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (3.17.0)
Requirement already satisfied: pyyaml>=5.1 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from transformers<5.0.0,>=4.45.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (6.0.2)
Requirement already satisfied: tokenizers<0.22,>=0.21 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from transformers<5.0.0,>=4.45.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (0.21.0)
Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from transformers<5.0.0,>=4.45.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (0.28.1)
Requirement already satisfied: tqdm>=4.27 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from transformers<5.0.0,>=4.45.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (4.67.1)
Requirement already satisfied: regex!=2019.12.17 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from transformers<5.0.0,>=4.45.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (2024.11.6)
Requirement already satisfied: packaging>=20.0 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from transformers<5.0.0,>=4.45.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (24.2)
Requirement already satisfied: requests in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from transformers<5.0.0,>=4.45.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (2.32.3)
Requirement already satisfied: safetensors>=0.4.1 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from transformers<5.0.0,>=4.45.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (0.5.2)
Requirement already satisfied: jinja2 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from torch~=2.2.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 3)) (3.1.5)
Requirement already satisfied: sympy in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from torch~=2.2.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 3)) (1.13.3)
Requirement already satisfied: fsspec in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from torch~=2.2.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 3)) (2025.2.0)
Requirement already satisfied: typing-extensions>=4.8.0 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from torch~=2.2.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 3)) (4.12.2)
Requirement already satisfied: networkx in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from torch~=2.2.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 3)) (3.4.2)
Requirement already satisfied: MarkupSafe>=2.0 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from jinja2->torch~=2.2.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 3)) (3.0.2)
Requirement already satisfied: urllib3<3,>=1.21.1 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from requests->transformers<5.0.0,>=4.45.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (2.3.0)
Requirement already satisfied: charset-normalizer<4,>=2 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from requests->transformers<5.0.0,>=4.45.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (3.4.1)
Requirement already satisfied: certifi>=2017.4.17 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from requests->transformers<5.0.0,>=4.45.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (2025.1.31)
Requirement already satisfied: idna<4,>=2.5 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from requests->transformers<5.0.0,>=4.45.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (3.10)
Requirement already satisfied: mpmath<1.4,>=1.1.0 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from sympy->torch~=2.2.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 3)) (1.3.0)
Obtaining file:///home/ggml/work/llama.cpp/gguf-py
  Installing build dependencies: started
  Installing build dependencies: finished with status 'done'
  Checking if build backend supports build_editable: started
  Checking if build backend supports build_editable: finished with status 'done'
  Getting requirements to build editable: started
  Getting requirements to build editable: finished with status 'done'
  Preparing editable metadata (pyproject.toml): started
  Preparing editable metadata (pyproject.toml): finished with status 'done'
Requirement already satisfied: pyyaml>=5.1 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from gguf==0.15.0) (6.0.2)
Requirement already satisfied: tqdm>=4.27 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from gguf==0.15.0) (4.67.1)
Requirement already satisfied: numpy>=1.17 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from gguf==0.15.0) (1.26.4)
Requirement already satisfied: sentencepiece<=0.2.0,>=0.1.98 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from gguf==0.15.0) (0.2.0)
Building wheels for collected packages: gguf
  Building editable for gguf (pyproject.toml): started
  Building editable for gguf (pyproject.toml): finished with status 'done'
  Created wheel for gguf: filename=gguf-0.15.0-py3-none-any.whl size=3463 sha256=10c32970a6e7e43d1565f30e5877134e711a09a1c8a63176f923d7cc41855163
  Stored in directory: /tmp/pip-ephem-wheel-cache-sheic0di/wheels/a3/4c/52/c5934ad001d1a70ca5434f11ddc622cad9c0a484e9bf6feda3
Successfully built gguf
Installing collected packages: gguf
  Attempting uninstall: gguf
    Found existing installation: gguf 0.15.0
    Uninstalling gguf-0.15.0:
      Successfully uninstalled gguf-0.15.0
Successfully installed gguf-0.15.0
+ gg_run_ctest_debug
+ cd /home/ggml/work/llama.cpp
+ rm -rf build-ci-debug
+ tee /home/ggml/results/llama.cpp/bd/64925f0feffdf54736c06b56c6a303139e21e5/ggml-5-x86-amx-cc/ctest_debug.log
+ mkdir build-ci-debug
+ cd build-ci-debug
+ set -e
+ gg_check_build_requirements
+ command -v cmake
+ command -v make
+ command -v ctest
+ tee -a /home/ggml/results/llama.cpp/bd/64925f0feffdf54736c06b56c6a303139e21e5/ggml-5-x86-amx-cc/ctest_debug-cmake.log
+ cmake -DCMAKE_BUILD_TYPE=Debug -DLLAMA_FATAL_WARNINGS=ON ..
-- The C compiler identification is GNU 11.4.0
-- The CXX compiler identification is GNU 11.4.0
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.34.1") 
-- Looking for pthread.h
-- Looking for pthread.h - found
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE  
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: x86_64
-- Including CPU backend
-- Found OpenMP_C: -fopenmp (found version "4.5") 
-- Found OpenMP_CXX: -fopenmp (found version "4.5") 
-- Found OpenMP: TRUE (found version "4.5")  
-- x86 detected
-- Adding CPU backend variant ggml-cpu: -march=native 
-- Configuring done
-- Generating done
-- Build files have been written to: /home/ggml/work/llama.cpp/build-ci-debug

real	0m0.751s
user	0m0.482s
sys	0m0.272s
+ tee -a /home/ggml/results/llama.cpp/bd/64925f0feffdf54736c06b56c6a303139e21e5/ggml-5-x86-amx-cc/ctest_debug-make.log
++ nproc
+ make -j8
[  0%] Generating build details from Git
[  0%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  1%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  1%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  1%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  3%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  3%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  4%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
-- Found Git: /usr/bin/git (found version "2.34.1") 
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  4%] Built target sha1
[  5%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  5%] Built target sha256
[  5%] Built target xxhash
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o
[  6%] Linking CXX shared library ../../bin/libggml-base.so
[  6%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  6%] Built target build_info
[  6%] Built target ggml-base
[  6%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[  6%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[  8%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 10%] Linking CXX shared library ../../bin/libggml-cpu.so
[ 10%] Built target ggml-cpu
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 11%] Linking CXX shared library ../../bin/libggml.so
[ 11%] Built target ggml
[ 12%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 12%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 12%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o
[ 13%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o
[ 13%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 13%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o
[ 13%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o
[ 14%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o
[ 15%] Linking CXX executable ../../bin/llama-gguf
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 17%] Linking CXX executable ../../bin/llama-gguf-hash
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-graph.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-io.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 23%] Built target llama-gguf
[ 23%] Built target llama-gguf-hash
[ 23%] Linking CXX shared library ../bin/libllama.so
[ 23%] Built target llama
[ 24%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 24%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 25%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 25%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 27%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 28%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/chat.cpp.o
[ 28%] Linking C executable ../bin/test-c
[ 28%] Linking CXX executable ../../bin/llama-simple-chat
[ 28%] Linking CXX executable ../../bin/llama-simple
[ 29%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 29%] Built target llava
[ 29%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 30%] Linking CXX static library libllava_static.a
[ 31%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 31%] Linking CXX shared library ../../bin/libllava_shared.so
[ 31%] Built target test-c
[ 31%] Building CXX object common/CMakeFiles/common.dir/llguidance.cpp.o
[ 32%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 32%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 33%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 33%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 34%] Linking CXX static library libcommon.a
[ 34%] Built target llava_static
[ 34%] Built target llama-simple
[ 34%] Built target llama-simple-chat
[ 34%] Built target llava_shared
[ 34%] Built target common
[ 34%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 35%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 36%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-chat.dir/test-chat.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 39%] Linking CXX executable ../bin/test-tokenizer-0
[ 40%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 40%] Linking CXX executable ../bin/test-sampling
[ 41%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 42%] Linking CXX executable ../bin/test-grammar-parser
[ 43%] Linking CXX executable ../bin/test-llama-grammar
[ 43%] Linking CXX executable ../bin/test-grammar-integration
[ 43%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 43%] Building CXX object tests/CMakeFiles/test-chat.dir/get-model.cpp.o
[ 44%] Linking CXX executable ../bin/test-chat
[ 44%] Built target test-sampling
[ 44%] Built target test-grammar-parser
[ 44%] Built target test-llama-grammar
[ 45%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 46%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 46%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 46%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 47%] Linking CXX executable ../bin/test-log
[ 47%] Built target test-log
[ 47%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 48%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 48%] Linking CXX executable ../bin/test-arg-parser
[ 48%] Built target test-tokenizer-0
[ 48%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 48%] Built target test-grammar-integration
[ 49%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 49%] Linking CXX executable ../bin/test-chat-template
[ 50%] Building CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o
[ 51%] Linking CXX executable ../bin/test-gguf
[ 51%] Built target test-json-schema-to-grammar
[ 51%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 52%] Linking CXX executable ../bin/test-backend-ops
[ 52%] Built target test-gguf
[ 52%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 52%] Built target test-tokenizer-1-bpe
[ 52%] Built target test-tokenizer-1-spm
[ 53%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 54%] Linking CXX executable ../bin/test-model-load-cancel
[ 55%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 56%] Linking CXX executable ../bin/test-autorelease
[ 57%] Linking CXX executable ../bin/test-barrier
[ 57%] Built target test-backend-ops
[ 57%] Built target test-chat
[ 58%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 58%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 58%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 59%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 60%] Linking CXX executable ../bin/test-quantize-fns
[ 60%] Linking CXX executable ../bin/test-quantize-perf
[ 60%] Built target test-barrier
[ 60%] Built target test-model-load-cancel
[ 61%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 62%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 62%] Built target test-autorelease
[ 62%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 62%] Linking CXX executable ../../bin/llama-batched-bench
[ 63%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 64%] Linking CXX executable ../bin/test-rope
[ 64%] Linking CXX executable ../../bin/llama-batched
[ 64%] Built target test-quantize-fns
[ 64%] Built target test-quantize-perf
[ 65%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 66%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 66%] Linking CXX executable ../../bin/llama-embedding
[ 66%] Linking CXX executable ../../bin/llama-eval-callback
[ 66%] Built target test-rope
[ 66%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 67%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 67%] Built target test-arg-parser
[ 67%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 68%] Linking CXX executable ../../bin/llama-gguf-split
[ 68%] Built target llama-gguf-split
[ 68%] Built target llama-gbnf-validator
[ 68%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 68%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 69%] Linking CXX executable ../../bin/llama-gritlm
[ 70%] Linking CXX executable ../../bin/llama-imatrix
[ 70%] Built target test-chat-template
[ 70%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 71%] Linking CXX executable ../../bin/llama-infill
[ 71%] Linking CXX executable ../../bin/llama-quantize-stats
[ 71%] Built target llama-batched-bench
[ 71%] Built target llama-batched
[ 72%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 72%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 72%] Linking CXX executable ../../bin/llama-bench
[ 72%] Linking CXX executable ../../bin/llama-lookahead
[ 72%] Built target llama-embedding
[ 72%] Built target llama-eval-callback
[ 73%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 74%] Built target llama-quantize-stats
[ 74%] Linking CXX executable ../../bin/llama-lookup
[ 74%] Linking CXX executable ../../bin/llama-lookup-create
[ 75%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 75%] Linking CXX executable ../../bin/llama-lookup-merge
[ 75%] Built target llama-lookup-merge
[ 76%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 76%] Linking CXX executable ../../bin/llama-lookup-stats
[ 76%] Built target llama-gritlm
[ 76%] Built target llama-imatrix
[ 77%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 78%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 78%] Linking CXX executable ../../bin/llama-cli
[ 78%] Linking CXX executable ../../bin/llama-parallel
[ 78%] Built target llama-infill
[ 79%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 79%] Linking CXX executable ../../bin/llama-passkey
[ 79%] Built target llama-bench
[ 80%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 80%] Linking CXX executable ../../bin/llama-perplexity
[ 80%] Built target llama-lookahead
[ 81%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 81%] Linking CXX executable ../../bin/llama-quantize
[ 81%] Built target llama-lookup
[ 81%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 81%] Built target llama-lookup-create
[ 82%] Linking CXX executable ../../bin/llama-retrieval
[ 82%] Generating loading.html.hpp
[ 83%] Generating index.html.gz.hpp
[ 83%] Built target llama-lookup-stats
[ 84%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 84%] Linking CXX executable ../../bin/llama-save-load-state
[ 84%] Built target llama-parallel
[ 84%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 85%] Building CXX object examples/run/CMakeFiles/llama-run.dir/linenoise.cpp/linenoise.cpp.o
[ 85%] Linking CXX executable ../../bin/llama-run
[ 85%] Built target llama-cli
[ 86%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 86%] Linking CXX executable ../../bin/llama-speculative
[ 86%] Built target llama-passkey
[ 87%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 87%] Linking CXX executable ../../bin/llama-speculative-simple
[ 87%] Built target llama-quantize
[ 88%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 88%] Linking CXX executable ../../bin/llama-tokenize
[ 88%] Built target llama-perplexity
[ 88%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 89%] Linking CXX executable ../../bin/llama-tts
[ 89%] Built target llama-retrieval
[ 89%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 90%] Linking CXX executable ../../bin/llama-gen-docs
[ 90%] Built target llama-save-load-state
[ 91%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 91%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 92%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[ 92%] Built target llama-run
[ 93%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 93%] Linking CXX executable ../../bin/llama-cvector-generator
[ 93%] Built target llama-speculative
[ 93%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 94%] Linking CXX executable ../../bin/llama-export-lora
[ 94%] Built target llama-tokenize
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 95%] Linking CXX executable ../../bin/llama-llava-cli
[ 95%] Built target llama-speculative-simple
[ 96%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 96%] Linking CXX executable ../../bin/llama-server
[ 96%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 96%] Built target llama-tts
[ 96%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 97%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 97%] Built target llama-gen-docs
[ 97%] Built target llama-convert-llama2c-to-ggml
[ 97%] Building CXX object examples/llava/CMakeFiles/llama-llava-clip-quantize-cli.dir/clip-quantize-cli.cpp.o
[ 97%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 98%] Linking CXX executable ../../bin/llama-llava-clip-quantize-cli
[ 99%] Linking CXX executable ../../bin/llama-vdot
[ 99%] Built target llama-vdot
[100%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[100%] Linking CXX executable ../../bin/llama-q8dot
[100%] Built target llama-llava-clip-quantize-cli
[100%] Built target llama-q8dot
[100%] Built target llama-cvector-generator
[100%] Built target llama-export-lora
[100%] Built target llama-llava-cli
[100%] Built target llama-minicpmv-cli
[100%] Built target llama-qwen2vl-cli
[100%] Built target llama-server

real	0m10.237s
user	0m51.490s
sys	0m9.521s
+ tee -a /home/ggml/results/llama.cpp/bd/64925f0feffdf54736c06b56c6a303139e21e5/ggml-5-x86-amx-cc/ctest_debug-ctest.log
+ ctest --output-on-failure -L main -E test-opt
Test project /home/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-tokenizer-0-bert-bge
 1/29 Test  #1: test-tokenizer-0-bert-bge .........Subprocess aborted***Exception:   0.17 sec
main : reading vocab from: '/home/ggml/work/llama.cpp/tests/../models/ggml-vocab-bert-bge.gguf'
register_backend: registered backend CPU (1 devices)
register_device: registered device CPU (Intel(R) Xeon(R) Platinum 8473C)
llama_model_loader: loaded meta data with 20 key-value pairs and 0 tensors from /home/ggml/work/llama.cpp/tests/../models/ggml-vocab-bert-bge.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = bert
llama_model_loader: - kv   1:                               general.name str              = bert-bge
llama_model_loader: - kv   2:                           bert.block_count u32              = 12
llama_model_loader: - kv   3:                        bert.context_length u32              = 512
llama_model_loader: - kv   4:                      bert.embedding_length u32              = 384
llama_model_loader: - kv   5:                   bert.feed_forward_length u32              = 1536
llama_model_loader: - kv   6:                  bert.attention.head_count u32              = 12
llama_model_loader: - kv   7:          bert.attention.layer_norm_epsilon f32              = 0.000000
llama_model_loader: - kv   8:                          general.file_type u32              = 1
llama_model_loader: - kv   9:                      bert.attention.causal bool             = false
llama_model_loader: - kv  10:            tokenizer.ggml.token_type_count u32              = 2
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = bert
llama_model_loader: - kv  12:                         tokenizer.ggml.pre str              = bert-bge
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  15:            tokenizer.ggml.unknown_token_id u32              = 100
llama_model_loader: - kv  16:          tokenizer.ggml.seperator_token_id u32              = 102
llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  18:                tokenizer.ggml.cls_token_id u32              = 101
llama_model_loader: - kv  19:               tokenizer.ggml.mask_token_id u32              = 103
print_info: file format = GGUF V3 (latest)
print_info: file type   = F16
print_info: file size   = 0.00 MiB (-nan BPW) 
init_tokenizer: initializing tokenizer for type 3
load: control token:    101 '[CLS]' is not marked as EOG
load: control token:    103 '[MASK]' is not marked as EOG
load: control token:      0 '[PAD]' is not marked as EOG
load: control token:    100 '[UNK]' is not marked as EOG
load: control token:    102 '[SEP]' is not marked as EOG
load: special tokens cache size = 5
load: token to piece cache size = 0.2032 MB
print_info: arch             = bert
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 0.00 K
print_info: general.name     = bert-bge
print_info: vocab type       = WPM
print_info: n_vocab          = 30522
print_info: n_merges         = 0
print_info: BOS token        = 101 '[CLS]'
print_info: UNK token        = 100 '[UNK]'
print_info: SEP token        = 102 '[SEP]'
print_info: PAD token        = 0 '[PAD]'
print_info: MASK token       = 103 '[MASK]'
print_info: LF token         = 0 '[PAD]'
print_info: max token length = 21
llama_model_load: vocab only - skipping tensors
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 512
llama_context: n_ctx_per_seq = 512
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: flash_attn    = 0
llama_context: freq_base     = 0.0
llama_context: freq_scale    = 1
llama_context: n_ctx_pre_seq (512) > n_ctx_train (0) -- possible training context overflow
llama_context_kv_self: n_ctx = 512
llama_context_kv_self: n_ctx = 512 (padded)
/home/ggml/work/llama.cpp/ggml/src/ggml-backend.cpp:1454: GGML_ASSERT(n_backends > 0) failed
/home/ggml/work/llama.cpp/build-ci-debug/bin/libggml-base.so(+0x5c5c1)[0x7c021475f5c1]
/home/ggml/work/llama.cpp/build-ci-debug/bin/libggml-base.so(+0x5c77e)[0x7c021475f77e]
/home/ggml/work/llama.cpp/build-ci-debug/bin/libggml-base.so(ggml_abort+0x117)[0x7c021475f8ae]
/home/ggml/work/llama.cpp/build-ci-debug/bin/libggml-base.so(ggml_backend_sched_new+0x67)[0x7c021477ad7a]
/home/ggml/work/llama.cpp/build-ci-debug/bin/libllama.so(_ZN13llama_context4initEv+0x434)[0x7c0214cfe9d6]
/home/ggml/work/llama.cpp/build-ci-debug/bin/libllama.so(llama_init_from_model+0x1d7)[0x7c0214cc6bc2]
/home/ggml/work/llama.cpp/build-ci-debug/bin/test-tokenizer-0(+0x65a08)[0x6361e96b6a08]
/lib/x86_64-linux-gnu/libc.so.6(+0x29d90)[0x7c0214029d90]
/lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0x80)[0x7c0214029e40]
/home/ggml/work/llama.cpp/build-ci-debug/bin/test-tokenizer-0(+0x644a5)[0x6361e96b54a5]

      Start  2: test-tokenizer-0-command-r
 2/29 Test  #2: test-tokenizer-0-command-r ........Subprocess aborted***Exception:   1.72 sec
main : reading vocab from: '/home/ggml/work/llama.cpp/tests/../models/ggml-vocab-command-r.gguf'
register_backend: registered backend CPU (1 devices)
register_device: registered device CPU (Intel(R) Xeon(R) Platinum 8473C)
llama_model_loader: loaded meta data with 27 key-value pairs and 0 tensors from /home/ggml/work/llama.cpp/tests/../models/ggml-vocab-command-r.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = command-r
llama_model_loader: - kv   1:                               general.name str              = command-r
llama_model_loader: - kv   2:                      command-r.block_count u32              = 40
llama_model_loader: - kv   3:                   command-r.context_length u32              = 131072
llama_model_loader: - kv   4:                 command-r.embedding_length u32              = 8192
llama_model_loader: - kv   5:              command-r.feed_forward_length u32              = 22528
llama_model_loader: - kv   6:             command-r.attention.head_count u32              = 64
llama_model_loader: - kv   7:          command-r.attention.head_count_kv u32              = 64
llama_model_loader: - kv   8:                   command-r.rope.freq_base f32              = 8000000.000000
llama_model_loader: - kv   9:     command-r.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 1
llama_model_loader: - kv  11:                      command-r.logit_scale f32              = 0.062500
llama_model_loader: - kv  12:                command-r.rope.scaling.type str              = none
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = command-r
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,256000]  = ["<PAD>", "<UNK>", "<CLS>", "<SEP>", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,253333]  = ["Ġ Ġ", "Ġ t", "e r", "i n", "Ġ a...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 5
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 255001
llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  21:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  22:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  23:           tokenizer.chat_template.tool_use str              = {{ bos_token }}{% if messages[0]['rol...
llama_model_loader: - kv  24:                tokenizer.chat_template.rag str              = {{ bos_token }}{% if messages[0]['rol...
llama_model_loader: - kv  25:                   tokenizer.chat_templates arr[str,2]       = ["tool_use", "rag"]
llama_model_loader: - kv  26:                    tokenizer.chat_template str              = {{ bos_token }}{% if messages[0]['rol...
print_info: file format = GGUF V3 (latest)
print_info: file type   = F16
print_info: file size   = 0.00 MiB (-nan BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token: 255001 '<|END_OF_TURN_TOKEN|>' is not marked as EOG
load: control token:      7 '<EOP_TOKEN>' is not marked as EOG
load: control token:      2 '<CLS>' is not marked as EOG
load: control token:      3 '<SEP>' is not marked as EOG
load: control token:      6 '<EOS_TOKEN>' is not marked as EOG
load: control token:      1 '<UNK>' is not marked as EOG
load: control token:      4 '<MASK_TOKEN>' is not marked as EOG
load: control token:      5 '<BOS_TOKEN>' is not marked as EOG
load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
load: special tokens cache size = 1008
load: token to piece cache size = 1.8528 MB
print_info: arch             = command-r
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 0.00 K
print_info: general.name     = command-r
print_info: vocab type       = BPE
print_info: n_vocab          = 256000
print_info: n_merges         = 253333
print_info: BOS token        = 5 '<BOS_TOKEN>'
print_info: EOS token        = 255001 '<|END_OF_TURN_TOKEN|>'
print_info: PAD token        = 0 '<PAD>'
print_info: LF token         = 206 'Ċ'
print_info: FIM PAD token    = 0 '<PAD>'
print_info: EOG token        = 0 '<PAD>'
print_info: EOG token        = 255001 '<|END_OF_TURN_TOKEN|>'
print_info: max token length = 1024
llama_model_load: vocab only - skipping tensors
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 512
llama_context: n_ctx_per_seq = 512
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: flash_attn    = 0
llama_context: freq_base     = 0.0
llama_context: freq_scale    = 1
llama_context: n_ctx_pre_seq (512) > n_ctx_train (0) -- possible training context overflow
llama_context_kv_self: n_ctx = 512
llama_context_kv_self: n_ctx = 512 (padded)
/home/ggml/work/llama.cpp/ggml/src/ggml-backend.cpp:1454: GGML_ASSERT(n_backends > 0) failed
/home/ggml/work/llama.cpp/build-ci-debug/bin/libggml-base.so(+0x5c5c1)[0x78066215f5c1]
/home/ggml/work/llama.cpp/build-ci-debug/bin/libggml-base.so(+0x5c77e)[0x78066215f77e]
/home/ggml/work/llama.cpp/build-ci-debug/bin/libggml-base.so(ggml_abort+0x117)[0x78066215f8ae]
/home/ggml/work/llama.cpp/build-ci-debug/bin/libggml-base.so(ggml_backend_sched_new+0x67)[0x78066217ad7a]
/home/ggml/work/llama.cpp/build-ci-debug/bin/libllama.so(_ZN13llama_context4initEv+0x434)[0x7806626fe9d6]
/home/ggml/work/llama.cpp/build-ci-debug/bin/libllama.so(llama_init_from_model+0x1d7)[0x7806626c6bc2]
/home/ggml/work/llama.cpp/build-ci-debug/bin/test-tokenizer-0(+0x65a08)[0x5c4048f49a08]
/lib/x86_64-linux-gnu/libc.so.6(+0x29d90)[0x780661a29d90]
/lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0x80)[0x780661a29e40]
/home/ggml/work/llama.cpp/build-ci-debug/bin/test-tokenizer-0(+0x644a5)[0x5c4048f484a5]

      Start  3: test-tokenizer-0-deepseek-coder
 3/29 Test  #3: test-tokenizer-0-deepseek-coder ...Subprocess aborted***Exception:   0.26 sec
main : reading vocab from: '/home/ggml/work/llama.cpp/tests/../models/ggml-vocab-deepseek-coder.gguf'
register_backend: registered backend CPU (1 devices)
register_device: registered device CPU (Intel(R) Xeon(R) Platinum 8473C)
llama_model_loader: loaded meta data with 25 key-value pairs and 0 tensors from /home/ggml/work/llama.cpp/tests/../models/ggml-vocab-deepseek-coder.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = deepseek-coder
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 16384
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 100000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  10:                          general.file_type u32              = 1
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 32256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                    llama.rope.scaling.type str              = linear
llama_model_loader: - kv  14:                  llama.rope.scaling.factor f32              = 4.000000
llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = deepseek-coder
llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,32256]   = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,32256]   = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  19:                      tokenizer.ggml.merges arr[str,31757]   = ["Ġ Ġ", "Ġ t", "Ġ a", "i n", "h e...
llama_model_loader: - kv  20:                tokenizer.ggml.bos_token_id u32              = 32013
llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 32014
llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 32014
llama_model_loader: - kv  23:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  24:               tokenizer.ggml.add_eos_token bool             = false
print_info: file format = GGUF V3 (latest)
print_info: file type   = F16
print_info: file size   = 0.00 MiB (-nan BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control-looking token:  32015 '<｜fim▁hole｜>' was not control-type; this is probably a bug in the model. its type will be overridden
load: control-looking token:  32017 '<｜fim▁end｜>' was not control-type; this is probably a bug in the model. its type will be overridden
load: control-looking token:  32016 '<｜fim▁begin｜>' was not control-type; this is probably a bug in the model. its type will be overridden
load: control token:  32015 '<｜fim▁hole｜>' is not marked as EOG
load: control token:  32014 '<｜end▁of▁sentence｜>' is not marked as EOG
load: control token:  32017 '<｜fim▁end｜>' is not marked as EOG
load: control token:  32016 '<｜fim▁begin｜>' is not marked as EOG
load: control token:  32013 '<｜begin▁of▁sentence｜>' is not marked as EOG
load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
load: special tokens cache size = 256
load: token to piece cache size = 0.1787 MB
print_info: arch             = llama
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 0.00 K
print_info: general.name     = deepseek-coder
print_info: vocab type       = BPE
print_info: n_vocab          = 32256
print_info: n_merges         = 31757
print_info: BOS token        = 32013 '<｜begin▁of▁sentence｜>'
print_info: EOS token        = 32014 '<｜end▁of▁sentence｜>'
print_info: EOT token        = 32014 '<｜end▁of▁sentence｜>'
print_info: PAD token        = 32014 '<｜end▁of▁sentence｜>'
print_info: LF token         = 185 'Ċ'
print_info: FIM PRE token    = 32016 '<｜fim▁begin｜>'
print_info: FIM SUF token    = 32015 '<｜fim▁hole｜>'
print_info: FIM MID token    = 32017 '<｜fim▁end｜>'
print_info: EOG token        = 32014 '<｜end▁of▁sentence｜>'
print_info: max token length = 128
llama_model_load: vocab only - skipping tensors
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 512
llama_context: n_ctx_per_seq = 512
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: flash_attn    = 0
llama_context: freq_base     = 0.0
llama_context: freq_scale    = 1
llama_context: n_ctx_pre_seq (512) > n_ctx_train (0) -- possible training context overflow
llama_context_kv_self: n_ctx = 512
llama_context_kv_self: n_ctx = 512 (padded)
/home/ggml/work/llama.cpp/ggml/src/ggml-backend.cpp:1454: GGML_ASSERT(n_backends > 0) failed
/home/ggml/work/llama.cpp/build-ci-debug/bin/libggml-base.so(+0x5c5c1)[0x7a9b525145c1]
/home/ggml/work/llama.cpp/build-ci-debug/bin/libggml-base.so(+0x5c77e)[0x7a9b5251477e]
/home/ggml/work/llama.cpp/build-ci-debug/bin/libggml-base.so(ggml_abort+0x117)[0x7a9b525148ae]
/home/ggml/work/llama.cpp/build-ci-debug/bin/libggml-base.so(ggml_backend_sched_new+0x67)[0x7a9b5252fd7a]
/home/ggml/work/llama.cpp/build-ci-debug/bin/libllama.so(_ZN13llama_context4initEv+0x434)[0x7a9b528fe9d6]
/home/ggml/work/llama.cpp/build-ci-debug/bin/libllama.so(llama_init_from_model+0x1d7)[0x7a9b528c6bc2]
/home/ggml/work/llama.cpp/build-ci-debug/bin/test-tokenizer-0(+0x65a08)[0x61a48ceaca08]
/lib/x86_64-linux-gnu/libc.so.6(+0x29d90)[0x7a9b51e29d90]
/lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0x80)[0x7a9b51e29e40]
/home/ggml/work/llama.cpp/build-ci-debug/bin/test-tokenizer-0(+0x644a5)[0x61a48ceab4a5]

      Start  4: test-tokenizer-0-deepseek-llm
 4/29 Test  #4: test-tokenizer-0-deepseek-llm .....Subprocess aborted***Exception:   0.69 sec
main : reading vocab from: '/home/ggml/work/llama.cpp/tests/../models/ggml-vocab-deepseek-llm.gguf'
register_backend: registered backend CPU (1 devices)
register_device: registered device CPU (Intel(R) Xeon(R) Platinum 8473C)
llama_model_loader: loaded meta data with 23 key-value pairs and 0 tensors from /home/ggml/work/llama.cpp/tests/../models/ggml-vocab-deepseek-llm.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = deepseek-llm
llama_model_loader: - kv   2:                          llama.block_count u32              = 30
llama_model_loader: - kv   3:                       llama.context_length u32              = 4096
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 10000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  10:                          general.file_type u32              = 1
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 102400
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = deepseek-llm
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,102400]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,102400]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,99757]   = ["Ġ Ġ", "Ġ t", "Ġ a", "i n", "h e...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 100000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 100001
llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 100001
llama_model_loader: - kv  21:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  22:               tokenizer.ggml.add_eos_token bool             = false
print_info: file format = GGUF V3 (latest)
print_info: file type   = F16
print_info: file size   = 0.00 MiB (-nan BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token: 100001 '<｜end▁of▁sentence｜>' is not marked as EOG
load: control token: 100000 '<｜begin▁of▁sentence｜>' is not marked as EOG
load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
load: special tokens cache size = 2400
load: token to piece cache size = 0.6658 MB
print_info: arch             = llama
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 0.00 K
print_info: general.name     = deepseek-llm
print_info: vocab type       = BPE
print_info: n_vocab          = 102400
print_info: n_merges         = 99757
print_info: BOS token        = 100000 '<｜begin▁of▁sentence｜>'
print_info: EOS token        = 100001 '<｜end▁of▁sentence｜>'
print_info: EOT token        = 100001 '<｜end▁of▁sentence｜>'
print_info: PAD token        = 100001 '<｜end▁of▁sentence｜>'
print_info: LF token         = 185 'Ċ'
print_info: EOG token        = 100001 '<｜end▁of▁sentence｜>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 512
llama_context: n_ctx_per_seq = 512
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: flash_attn    = 0
llama_context: freq_base     = 0.0
llama_context: freq_scale    = 1
llama_context: n_ctx_pre_seq (512) > n_ctx_train (0) -- possible training context overflow
llama_context_kv_self: n_ctx = 512
llama_context_kv_self: n_ctx = 512 (padded)
/home/ggml/work/llama.cpp/ggml/src/ggml-backend.cpp:1454: GGML_ASSERT(n_backends > 0) failed
/home/ggml/work/llama.cpp/build-ci-debug/bin/libggml-base.so(+0x5c5c1)[0x7218cb31f5c1]
/home/ggml/work/llama.cpp/build-ci-debug/bin/libggml-base.so(+0x5c77e)[0x7218cb31f77e]
/home/ggml/work/llama.cpp/build-ci-debug/bin/libggml-base.so(ggml_abort+0x117)[0x7218cb31f8ae]
/home/ggml/work/llama.cpp/build-ci-debug/bin/libggml-base.so(ggml_backend_sched_new+0x67)[0x7218cb33ad7a]
/home/ggml/work/llama.cpp/build-ci-debug/bin/libllama.so(_ZN13llama_context4initEv+0x434)[0x7218cb8fe9d6]
/home/ggml/work/llama.cpp/build-ci-debug/bin/libllama.so(llama_init_from_model+0x1d7)[0x7218cb8c6bc2]
/home/ggml/work/llama.cpp/build-ci-debug/bin/test-tokenizer-0(+0x65a08)[0x5aacca13ca08]
/lib/x86_64-linux-gnu/libc.so.6(+0x29d90)[0x7218cac29d90]
/lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0x80)[0x7218cac29e40]
/home/ggml/work/llama.cpp/build-ci-debug/bin/test-tokenizer-0(+0x644a5)[0x5aacca13b4a5]

      Start  5: test-tokenizer-0-falcon
 5/29 Test  #5: test-tokenizer-0-falcon ...........Subprocess aborted***Exception:   0.46 sec
main : reading vocab from: '/home/ggml/work/llama.cpp/tests/../models/ggml-vocab-falcon.gguf'
register_backend: registered backend CPU (1 devices)
register_device: registered device CPU (Intel(R) Xeon(R) Platinum 8473C)
llama_model_loader: loaded meta data with 18 key-value pairs and 0 tensors from /home/ggml/work/llama.cpp/tests/../models/ggml-vocab-falcon.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = falcon
llama_model_loader: - kv   1:                               general.name str              = Falcon
llama_model_loader: - kv   2:                      falcon.context_length u32              = 2048
llama_model_loader: - kv   3:                  falcon.tensor_data_layout str              = jploski
llama_model_loader: - kv   4:                    falcon.embedding_length u32              = 4544
llama_model_loader: - kv   5:                 falcon.feed_forward_length u32              = 18176
llama_model_loader: - kv   6:                         falcon.block_count u32              = 32
llama_model_loader: - kv   7:                falcon.attention.head_count u32              = 71
llama_model_loader: - kv   8:             falcon.attention.head_count_kv u32              = 1
llama_model_loader: - kv   9:        falcon.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 1
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  12:                         tokenizer.ggml.pre str              = falcon
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,65024]   = [">>TITLE<<", ">>ABSTRACT<<", ">>INTR...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,65024]   = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...
llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,64784]   = ["Ġ t", "Ġ a", "i n", "h e", "r e",...
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 11
llama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 11
print_info: file format = GGUF V3 (latest)
print_info: file type   = F16
print_info: file size   = 0.00 MiB (-nan BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      3 '>>SUMMARY<<' is not marked as EOG
load: control token:      5 '>>ANSWER<<' is not marked as EOG
load: control token:      0 '>>TITLE<<' is not marked as EOG
load: control token:      7 '>>DOMAIN<<' is not marked as EOG
load: control token:      1 '>>ABSTRACT<<' is not marked as EOG
load: control token:      4 '>>COMMENT<<' is not marked as EOG
load: control token:      8 '>>PREFIX<<' is not marked as EOG
load: control token:      6 '>>QUESTION<<' is not marked as EOG
load: control token:     10 '>>MIDDLE<<' is not marked as EOG
load: control token:      9 '>>SUFFIX<<' is not marked as EOG
load: control token:      2 '>>INTRODUCTION<<' is not marked as EOG
load: special tokens cache size = 12
load: token to piece cache size = 0.3884 MB
print_info: arch             = falcon
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 0.00 K
print_info: general.name     = Falcon
print_info: vocab type       = BPE
print_info: n_vocab          = 65024
print_info: n_merges         = 64784
print_info: BOS token        = 11 '<|endoftext|>'
print_info: EOS token        = 11 '<|endoftext|>'
print_info: EOT token        = 11 '<|endoftext|>'
print_info: LF token         = 193 'Ċ'
print_info: EOG token        = 11 '<|endoftext|>'
print_info: max token length = 130
llama_model_load: vocab only - skipping tensors
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 512
llama_context: n_ctx_per_seq = 512
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: flash_attn    = 0
llama_context: freq_base     = 0.0
llama_context: freq_scale    = 1
llama_context: n_ctx_pre_seq (512) > n_ctx_train (0) -- possible training context overflow
llama_context_kv_self: n_ctx = 512
llama_context_kv_self: n_ctx = 512 (padded)
/home/ggml/work/llama.cpp/ggml/src/ggml-backend.cpp:1454: GGML_ASSERT(n_backends > 0) failed
/home/ggml/work/llama.cpp/build-ci-debug/bin/libggml-base.so(+0x5c5c1)[0x7dd8f73b95c1]
/home/ggml/work/llama.cpp/build-ci-debug/bin/libggml-base.so(+0x5c77e)[0x7dd8f73b977e]
/home/ggml/work/llama.cpp/build-ci-debug/bin/libggml-base.so(ggml_abort+0x117)[0x7dd8f73b98ae]
/home/ggml/work/llama.cpp/build-ci-debug/bin/libggml-base.so(ggml_backend_sched_new+0x67)[0x7dd8f73d4d7a]
/home/ggml/work/llama.cpp/build-ci-debug/bin/libllama.so(_ZN13llama_context4initEv+0x434)[0x7dd8f70fe9d6]
/home/ggml/work/llama.cpp/build-ci-debug/bin/libllama.so(llama_init_from_model+0x1d7)[0x7dd8f70c6bc2]
/home/ggml/work/llama.cpp/build-ci-debug/bin/test-tokenizer-0(+0x65a08)[0x61051a8fda08]
/lib/x86_64-linux-gnu/libc.so.6(+0x29d90)[0x7dd8f6629d90]
/lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0x80)[0x7dd8f6629e40]
/home/ggml/work/llama.cpp/build-ci-debug/bin/test-tokenizer-0(+0x644a5)[0x61051a8fc4a5]

      Start  6: test-tokenizer-0-gpt-2
 6/29 Test  #6: test-tokenizer-0-gpt-2 ............Subprocess aborted***Exception:   0.37 sec
main : reading vocab from: '/home/ggml/work/llama.cpp/tests/../models/ggml-vocab-gpt-2.gguf'
register_backend: registered backend CPU (1 devices)
register_device: registered device CPU (Intel(R) Xeon(R) Platinum 8473C)
llama_model_loader: loaded meta data with 16 key-value pairs and 0 tensors from /home/ggml/work/llama.cpp/tests/../models/ggml-vocab-gpt-2.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gpt2
llama_model_loader: - kv   1:                               general.name str              = gpt-2
llama_model_loader: - kv   2:                           gpt2.block_count u32              = 12
llama_model_loader: - kv   3:                        gpt2.context_length u32              = 1024
llama_model_loader: - kv   4:                      gpt2.embedding_length u32              = 768
llama_model_loader: - kv   5:                   gpt2.feed_forward_length u32              = 3072
llama_model_loader: - kv   6:                  gpt2.attention.head_count u32              = 12
llama_model_loader: - kv   7:          gpt2.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv   8:                          general.file_type u32              = 1
llama_model_loader: - kv   9:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  10:                         tokenizer.ggml.pre str              = gpt-2
llama_model_loader: - kv  11:                      tokenizer.ggml.tokens arr[str,50257]   = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  12:                  tokenizer.ggml.token_type arr[i32,50257]   = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  13:                      tokenizer.ggml.merges arr[str,50000]   = ["Ġ t", "Ġ a", "h e", "i n", "r e",...
llama_model_loader: - kv  14:                tokenizer.ggml.bos_token_id u32              = 50256
llama_model_loader: - kv  15:                tokenizer.ggml.eos_token_id u32              = 50256
print_info: file format = GGUF V3 (latest)
print_info: file type   = F16
print_info: file size   = 0.00 MiB (-nan BPW) 
init_tokenizer: initializing tokenizer for type 2
load: special tokens cache size = 1
load: token to piece cache size = 0.3060 MB
print_info: arch             = gpt2
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 0.00 K
print_info: general.name     = gpt-2
print_info: vocab type       = BPE
print_info: n_vocab          = 50257
print_info: n_merges         = 50000
print_info: BOS token        = 50256 '<|endoftext|>'
print_info: EOS token        = 50256 '<|endoftext|>'
print_info: EOT token        = 50256 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 50256 '<|endoftext|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 512
llama_context: n_ctx_per_seq = 512
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: flash_attn    = 0
llama_context: freq_base     = 0.0
llama_context: freq_scale    = 1
llama_context: n_ctx_pre_seq (512) > n_ctx_train (0) -- possible training context overflow
llama_context_kv_self: n_ctx = 512
llama_context_kv_self: n_ctx = 512 (padded)
/home/ggml/work/llama.cpp/ggml/src/ggml-backend.cpp:1454: GGML_ASSERT(n_backends > 0) failed
/home/ggml/work/llama.cpp/build-ci-debug/bin/libggml-base.so(+0x5c5c1)[0x7223bf15f5c1]
/home/ggml/work/llama.cpp/build-ci-debug/bin/libggml-base.so(+0x5c77e)[0x7223bf15f77e]
/home/ggml/work/llama.cpp/build-ci-debug/bin/libggml-base.so(ggml_abort+0x117)[0x7223bf15f8ae]
/home/ggml/work/llama.cpp/build-ci-debug/bin/libggml-base.so(ggml_backend_sched_new+0x67)[0x7223bf17ad7a]
/home/ggml/work/llama.cpp/build-ci-debug/bin/libllama.so(_ZN13llama_context4initEv+0x434)[0x7223bf6fe9d6]
/home/ggml/work/llama.cpp/build-ci-debug/bin/libllama.so(llama_init_from_model+0x1d7)[0x7223bf6c6bc2]
/home/ggml/work/llama.cpp/build-ci-debug/bin/test-tokenizer-0(+0x65a08)[0x5bcab7b21a08]
/lib/x86_64-linux-gnu/libc.so.6(+0x29d90)[0x7223bea29d90]
/lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0x80)[0x7223bea29e40]
/home/ggml/work/llama.cpp/build-ci-debug/bin/test-tokenizer-0(+0x644a5)[0x5bcab7b204a5]

      Start  7: test-tokenizer-0-llama-bpe
 7/29 Test  #7: test-tokenizer-0-llama-bpe ........Subprocess aborted***Exception:   1.13 sec
main : reading vocab from: '/home/ggml/work/llama.cpp/tests/../models/ggml-vocab-llama-bpe.gguf'
register_backend: registered backend CPU (1 devices)
register_device: registered device CPU (Intel(R) Xeon(R) Platinum 8473C)
llama_model_loader: loaded meta data with 20 key-value pairs and 0 tensors from /home/ggml/work/llama.cpp/tests/../models/ggml-vocab-llama-bpe.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = llama-bpe
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 1
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128001
print_info: file format = GGUF V3 (latest)
print_info: file type   = F16
print_info: file size   = 0.00 MiB (-nan BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token: 128255 '<|reserved_special_token_250|>' is not marked as EOG
load: control token: 128254 '<|reserved_special_token_249|>' is not marked as EOG
load: control token: 128253 '<|reserved_special_token_248|>' is not marked as EOG
load: control token: 128251 '<|reserved_special_token_246|>' is not marked as EOG
load: control token: 128246 '<|reserved_special_token_241|>' is not marked as EOG
load: control token: 128243 '<|reserved_special_token_238|>' is not marked as EOG
load: control token: 128240 '<|reserved_special_token_235|>' is not marked as EOG
load: control token: 128239 '<|reserved_special_token_234|>' is not marked as EOG
load: control token: 128238 '<|reserved_special_token_233|>' is not marked as EOG
load: control token: 128237 '<|reserved_special_token_232|>' is not marked as EOG
load: control token: 128232 '<|reserved_special_token_227|>' is not marked as EOG
load: control token: 128228 '<|reserved_special_token_223|>' is not marked as EOG
load: control token: 128227 '<|reserved_special_token_222|>' is not marked as EOG
load: control token: 128225 '<|reserved_special_token_220|>' is not marked as EOG
load: control token: 128222 '<|reserved_special_token_217|>' is not marked as EOG
load: control token: 128215 '<|reserved_special_token_210|>' is not marked as EOG
load: control token: 128211 '<|reserved_special_token_206|>' is not marked as EOG
load: control token: 128210 '<|reserved_special_token_205|>' is not marked as EOG
load: control token: 128204 '<|reserved_special_token_199|>' is not marked as EOG
load: control token: 128203 '<|reserved_special_token_198|>' is not marked as EOG
load: control token: 128201 '<|reserved_special_token_196|>' is not marked as EOG
load: control token: 128197 '<|reserved_special_token_192|>' is not marked as EOG
load: control token: 128196 '<|reserved_special_token_191|>' is not marked as EOG
load: control token: 128195 '<|reserved_special_token_190|>' is not marked as EOG
load: control token: 128193 '<|reserved_special_token_188|>' is not marked as EOG
load: control token: 128191 '<|reserved_special_token_186|>' is not marked as EOG
load: control token: 128190 '<|reserved_special_token_185|>' is not marked as EOG
load: control token: 128185 '<|reserved_special_token_180|>' is not marked as EOG
load: control token: 128184 '<|reserved_special_token_179|>' is not marked as EOG
load: control token: 128182 '<|reserved_special_token_177|>' is not marked as EOG
load: control token: 128181 '<|reserved_special_token_176|>' is not marked as EOG
load: control token: 128177 '<|reserved_special_token_172|>' is not marked as EOG
load: control token: 128176 '<|reserved_special_token_171|>' is not marked as EOG
load: control token: 128175 '<|reserved_special_token_170|>' is not marked as EOG
load: control token: 128174 '<|reserved_special_token_169|>' is not marked as EOG
load: control token: 128173 '<|reserved_special_token_168|>' is not marked as EOG
load: control token: 128172 '<|reserved_special_token_167|>' is not marked as EOG
load: control token: 128168 '<|reserved_special_token_163|>' is not marked as EOG
load: control token: 128167 '<|reserved_special_token_162|>' is not marked as EOG
load: control token: 128166 '<|reserved_special_token_161|>' is not marked as EOG
load: control token: 128165 '<|reserved_special_token_160|>' is not marked as EOG
load: control token: 128162 '<|reserved_special_token_157|>' is not marked as EOG
load: control token: 128159 '<|reserved_special_token_154|>' is not marked as EOG
load: control token: 128155 '<|reserved_special_token_150|>' is not marked as EOG
load: control token: 128153 '<|reserved_special_token_148|>' is not marked as EOG
load: control token: 128152 '<|reserved_special_token_147|>' is not marked as EOG
load: control token: 128151 '<|reserved_special_token_146|>' is not marked as EOG
load: control token: 128148 '<|reserved_special_token_143|>' is not marked as EOG
load: control token: 128146 '<|reserved_special_token_141|>' is not marked as EOG
load: control token: 128144 '<|reserved_special_token_139|>' is not marked as EOG
load: control token: 128143 '<|reserved_special_token_138|>' is not marked as EOG
load: control token: 128141 '<|reserved_special_token_136|>' is not marked as EOG
load: control token: 128139 '<|reserved_special_token_134|>' is not marked as EOG
load: control token: 128138 '<|reserved_special_token_133|>' is not marked as EOG
load: control token: 128135 '<|reserved_special_token_130|>' is not marked as EOG
load: control token: 128133 '<|reserved_special_token_128|>' is not marked as EOG
load: control token: 128132 '<|reserved_special_token_127|>' is not marked as EOG
load: control token: 128131 '<|reserved_special_token_126|>' is not marked as EOG
load: control token: 128130 '<|reserved_special_token_125|>' is not marked as EOG
load: control token: 128128 '<|reserved_special_token_123|>' is not marked as EOG
load: control token: 128125 '<|reserved_special_token_120|>' is not marked as EOG
load: control token: 128121 '<|reserved_special_token_116|>' is not marked as EOG
load: control token: 128120 '<|reserved_special_token_115|>' is not marked as EOG
load: control token: 128119 '<|reserved_special_token_114|>' is not marked as EOG
load: control token: 128116 '<|reserved_special_token_111|>' is not marked as EOG
load: control token: 128112 '<|reserved_special_token_107|>' is not marked as EOG
load: control token: 128109 '<|reserved_special_token_104|>' is not marked as EOG
load: control token: 128107 '<|reserved_special_token_102|>' is not marked as EOG
load: control token: 128106 '<|reserved_special_token_101|>' is not marked as EOG
load: control token: 128105 '<|reserved_special_token_100|>' is not marked as EOG
load: control token: 128103 '<|reserved_special_token_98|>' is not marked as EOG
load: control token: 128100 '<|reserved_special_token_95|>' is not marked as EOG
load: control token: 128099 '<|reserved_special_token_94|>' is not marked as EOG
load: control token: 128098 '<|reserved_special_token_93|>' is not marked as EOG
load: control token: 128094 '<|reserved_special_token_89|>' is not marked as EOG
load: control token: 128088 '<|reserved_special_token_83|>' is not marked as EOG
load: control token: 128087 '<|reserved_special_token_82|>' is not marked as EOG
load: control token: 128086 '<|reserved_special_token_81|>' is not marked as EOG
load: control token: 128084 '<|reserved_special_token_79|>' is not marked as EOG
load: control token: 128082 '<|reserved_special_token_77|>' is not marked as EOG
load: control token: 128078 '<|reserved_special_token_73|>' is not marked as EOG
load: control token: 128075 '<|reserved_special_token_70|>' is not marked as EOG
load: control token: 128073 '<|reserved_special_token_68|>' is not marked as EOG
load: control token: 128072 '<|reserved_special_token_67|>' is not marked as EOG
load: control token: 128070 '<|reserved_special_token_65|>' is not marked as EOG
load: control token: 128065 '<|reserved_special_token_60|>' is not marked as EOG
load: control token: 128064 '<|reserved_special_token_59|>' is not marked as EOG
load: control token: 128062 '<|reserved_special_token_57|>' is not marked as EOG
load: control token: 128060 '<|reserved_special_token_55|>' is not marked as EOG
load: control token: 128059 '<|reserved_special_token_54|>' is not marked as EOG
load: control token: 128057 '<|reserved_special_token_52|>' is not marked as EOG
load: control token: 128056 '<|reserved_special_token_51|>' is not marked as EOG
load: control token: 128054 '<|reserved_special_token_49|>' is not marked as EOG
load: control token: 128051 '<|reserved_special_token_46|>' is not marked as EOG
load: control token: 128043 '<|reserved_special_token_38|>' is not marked as EOG
load: control token: 128042 '<|reserved_special_token_37|>' is not marked as EOG
load: control token: 128041 '<|reserved_special_token_36|>' is not marked as EOG
load: control token: 128040 '<|reserved_special_token_35|>' is not marked as EOG
load: control token: 128035 '<|reserved_special_token_30|>' is not marked as EOG
load: control token: 128033 '<|reserved_special_token_28|>' is not marked as EOG
load: control token: 128032 '<|reserved_special_token_27|>' is not marked as EOG
load: control token: 128029 '<|reserved_special_token_24|>' is not marked as EOG
load: control token: 128025 '<|reserved_special_token_20|>' is not marked as EOG
load: control token: 128024 '<|reserved_special_token_19|>' is not marked as EOG
load: control token: 128021 '<|reserved_special_token_16|>' is not marked as EOG
load: control token: 128020 '<|reserved_special_token_15|>' is not marked as EOG
load: control token: 128019 '<|reserved_special_token_14|>' is not marked as EOG
load: control token: 128018 '<|reserved_special_token_13|>' is not marked as EOG
load: control token: 128015 '<|reserved_special_token_10|>' is not marked as EOG
load: control token: 128013 '<|reserved_special_token_8|>' is not marked as EOG
load: control token: 128012 '<|reserved_special_token_7|>' is not marked as EOG
load: control token: 128010 '<|reserved_special_token_5|>' is not marked as EOG
load: control token: 128005 '<|reserved_special_token_3|>' is not marked as EOG
load: control token: 128004 '<|reserved_special_token_2|>' is not marked as EOG
load: control token: 128002 '<|reserved_special_token_0|>' is not marked as EOG
load: control token: 128249 '<|reserved_special_token_244|>' is not marked as EOG
load: control token: 128187 '<|reserved_special_token_182|>' is not marked as EOG
load: control token: 128180 '<|reserved_special_token_175|>' is not marked as EOG
load: control token: 128134 '<|reserved_special_token_129|>' is not marked as EOG
load: control token: 128179 '<|reserved_special_token_174|>' is not marked as EOG
load: control token: 128037 '<|reserved_special_token_32|>' is not marked as EOG
load: control token: 128045 '<|reserved_special_token_40|>' is not marked as EOG
load: control token: 128089 '<|reserved_special_token_84|>' is not marked as EOG
load: control token: 128212 '<|reserved_special_token_207|>' is not marked as EOG
load: control token: 128104 '<|reserved_special_token_99|>' is not marked as EOG
load: control token: 128205 '<|reserved_special_token_200|>' is not marked as EOG
load: control token: 128142 '<|reserved_special_token_137|>' is not marked as EOG
load: control token: 128028 '<|reserved_special_token_23|>' is not marked as EOG
load: control token: 128126 '<|reserved_special_token_121|>' is not marked as EOG
load: control token: 128198 '<|reserved_special_token_193|>' is not marked as EOG
load: control token: 128071 '<|reserved_special_token_66|>' is not marked as EOG
load: control token: 128092 '<|reserved_special_token_87|>' is not marked as EOG
load: control token: 128183 '<|reserved_special_token_178|>' is not marked as EOG
load: control token: 128140 '<|reserved_special_token_135|>' is not marked as EOG
load: control token: 128226 '<|reserved_special_token_221|>' is not marked as EOG
load: control token: 128007 '<|end_header_id|>' is not marked as EOG
load: control token: 128052 '<|reserved_special_token_47|>' is not marked as EOG
load: control token: 128053 '<|reserved_special_token_48|>' is not marked as EOG
load: control token: 128058 '<|reserved_special_token_53|>' is not marked as EOG
load: control token: 128150 '<|reserved_special_token_145|>' is not marked as EOG
load: control token: 128149 '<|reserved_special_token_144|>' is not marked as EOG
load: control token: 128209 '<|reserved_special_token_204|>' is not marked as EOG
load: control token: 128169 '<|reserved_special_token_164|>' is not marked as EOG
load: control token: 128157 '<|reserved_special_token_152|>' is not marked as EOG
load: control token: 128038 '<|reserved_special_token_33|>' is not marked as EOG
load: control token: 128178 '<|reserved_special_token_173|>' is not marked as EOG
load: control token: 128091 '<|reserved_special_token_86|>' is not marked as EOG
load: control token: 128115 '<|reserved_special_token_110|>' is not marked as EOG
load: control token: 128233 '<|reserved_special_token_228|>' is not marked as EOG
load: control token: 128145 '<|reserved_special_token_140|>' is not marked as EOG
load: control token: 128039 '<|reserved_special_token_34|>' is not marked as EOG
load: control token: 128136 '<|reserved_special_token_131|>' is not marked as EOG
load: control token: 128170 '<|reserved_special_token_165|>' is not marked as EOG
load: control token: 128236 '<|reserved_special_token_231|>' is not marked as EOG
load: control token: 128154 '<|reserved_special_token_149|>' is not marked as EOG
load: control token: 128049 '<|reserved_special_token_44|>' is not marked as EOG
load: control token: 128023 '<|reserved_special_token_18|>' is not marked as EOG
load: control token: 128003 '<|reserved_special_token_1|>' is not marked as EOG
load: control token: 128016 '<|reserved_special_token_11|>' is not marked as EOG
load: control token: 128113 '<|reserved_special_token_108|>' is not marked as EOG
load: control token: 128158 '<|reserved_special_token_153|>' is not marked as EOG
load: control token: 128223 '<|reserved_special_token_218|>' is not marked as EOG
load: control token: 128156 '<|reserved_special_token_151|>' is not marked as EOG
load: control token: 128008 '<|reserved_special_token_4|>' is not marked as EOG
load: control token: 128085 '<|reserved_special_token_80|>' is not marked as EOG
load: control token: 128160 '<|reserved_special_token_155|>' is not marked as EOG
load: control token: 128001 '<|end_of_text|>' is not marked as EOG
load: control token: 128110 '<|reserved_special_token_105|>' is not marked as EOG
load: control token: 128247 '<|reserved_special_token_242|>' is not marked as EOG
load: control token: 128122 '<|reserved_special_token_117|>' is not marked as EOG
load: control token: 128050 '<|reserved_special_token_45|>' is not marked as EOG
load: control token: 128221 '<|reserved_special_token_216|>' is not marked as EOG
load: control token: 128244 '<|reserved_special_token_239|>' is not marked as EOG
load: control token: 128248 '<|reserved_special_token_243|>' is not marked as EOG
load: control token: 128213 '<|reserved_special_token_208|>' is not marked as EOG
load: control token: 128006 '<|start_header_id|>' is not marked as EOG
load: control token: 128208 '<|reserved_special_token_203|>' is not marked as EOG
load: control token: 128074 '<|reserved_special_token_69|>' is not marked as EOG
load: control token: 128234 '<|reserved_special_token_229|>' is not marked as EOG
load: control token: 128083 '<|reserved_special_token_78|>' is not marked as EOG
load: control token: 128224 '<|reserved_special_token_219|>' is not marked as EOG
load: control token: 128055 '<|reserved_special_token_50|>' is not marked as EOG
load: control token: 128097 '<|reserved_special_token_92|>' is not marked as EOG
load: control token: 128206 '<|reserved_special_token_201|>' is not marked as EOG
load: control token: 128081 '<|reserved_special_token_76|>' is not marked as EOG
load: control token: 128068 '<|reserved_special_token_63|>' is not marked as EOG
load: control token: 128067 '<|reserved_special_token_62|>' is not marked as EOG
load: control token: 128046 '<|reserved_special_token_41|>' is not marked as EOG
load: control token: 128194 '<|reserved_special_token_189|>' is not marked as EOG
load: control token: 128069 '<|reserved_special_token_64|>' is not marked as EOG
load: control token: 128000 '<|begin_of_text|>' is not marked as EOG
load: control token: 128220 '<|reserved_special_token_215|>' is not marked as EOG
load: control token: 128214 '<|reserved_special_token_209|>' is not marked as EOG
load: control token: 128108 '<|reserved_special_token_103|>' is not marked as EOG
load: control token: 128200 '<|reserved_special_token_195|>' is not marked as EOG
load: control token: 128048 '<|reserved_special_token_43|>' is not marked as EOG
load: control token: 128027 '<|reserved_special_token_22|>' is not marked as EOG
load: control token: 128114 '<|reserved_special_token_109|>' is not marked as EOG
load: control token: 128235 '<|reserved_special_token_230|>' is not marked as EOG
load: control token: 128252 '<|reserved_special_token_247|>' is not marked as EOG
load: control token: 128199 '<|reserved_special_token_194|>' is not marked as EOG
load: control token: 128129 '<|reserved_special_token_124|>' is not marked as EOG
load: control token: 128245 '<|reserved_special_token_240|>' is not marked as EOG
load: control token: 128164 '<|reserved_special_token_159|>' is not marked as EOG
load: control token: 128124 '<|reserved_special_token_119|>' is not marked as EOG
load: control token: 128102 '<|reserved_special_token_97|>' is not marked as EOG
load: control token: 128036 '<|reserved_special_token_31|>' is not marked as EOG
load: control token: 128229 '<|reserved_special_token_224|>' is not marked as EOG
load: control token: 128163 '<|reserved_special_token_158|>' is not marked as EOG
load: control token: 128127 '<|reserved_special_token_122|>' is not marked as EOG
load: control token: 128111 '<|reserved_special_token_106|>' is not marked as EOG
load: control token: 128231 '<|reserved_special_token_226|>' is not marked as EOG
load: control token: 128188 '<|reserved_special_token_183|>' is not marked as EOG
load: control token: 128061 '<|reserved_special_token_56|>' is not marked as EOG
load: control token: 128137 '<|reserved_special_token_132|>' is not marked as EOG
load: control token: 128093 '<|reserved_special_token_88|>' is not marked as EOG
load: control token: 128095 '<|reserved_special_token_90|>' is not marked as EOG
load: control token: 128189 '<|reserved_special_token_184|>' is not marked as EOG
load: control token: 128090 '<|reserved_special_token_85|>' is not marked as EOG
load: control token: 128147 '<|reserved_special_token_142|>' is not marked as EOG
load: control token: 128219 '<|reserved_special_token_214|>' is not marked as EOG
load: control token: 128230 '<|reserved_special_token_225|>' is not marked as EOG
load: control token: 128217 '<|reserved_special_token_212|>' is not marked as EOG
load: control token: 128031 '<|reserved_special_token_26|>' is not marked as EOG
load: control token: 128030 '<|reserved_special_token_25|>' is not marked as EOG
load: control token: 128250 '<|reserved_special_token_245|>' is not marked as EOG
load: control token: 128192 '<|reserved_special_token_187|>' is not marked as EOG
load: control token: 128096 '<|reserved_special_token_91|>' is not marked as EOG
load: control token: 128186 '<|reserved_special_token_181|>' is not marked as EOG
load: control token: 128207 '<|reserved_special_token_202|>' is not marked as EOG
load: control token: 128171 '<|reserved_special_token_166|>' is not marked as EOG
load: control token: 128080 '<|reserved_special_token_75|>' is not marked as EOG
load: control token: 128077 '<|reserved_special_token_72|>' is not marked as EOG
load: control token: 128101 '<|reserved_special_token_96|>' is not marked as EOG
load: control token: 128079 '<|reserved_special_token_74|>' is not marked as EOG
load: control token: 128216 '<|reserved_special_token_211|>' is not marked as EOG
load: control token: 128014 '<|reserved_special_token_9|>' is not marked as EOG
load: control token: 128047 '<|reserved_special_token_42|>' is not marked as EOG
load: control token: 128202 '<|reserved_special_token_197|>' is not marked as EOG
load: control token: 128044 '<|reserved_special_token_39|>' is not marked as EOG
load: control token: 128161 '<|reserved_special_token_156|>' is not marked as EOG
load: control token: 128017 '<|reserved_special_token_12|>' is not marked as EOG
load: control token: 128066 '<|reserved_special_token_61|>' is not marked as EOG
load: control token: 128242 '<|reserved_special_token_237|>' is not marked as EOG
load: control token: 128118 '<|reserved_special_token_113|>' is not marked as EOG
load: control token: 128076 '<|reserved_special_token_71|>' is not marked as EOG
load: control token: 128034 '<|reserved_special_token_29|>' is not marked as EOG
load: control token: 128241 '<|reserved_special_token_236|>' is not marked as EOG
load: control token: 128026 '<|reserved_special_token_21|>' is not marked as EOG
load: control token: 128218 '<|reserved_special_token_213|>' is not marked as EOG
load: control token: 128063 '<|reserved_special_token_58|>' is not marked as EOG
load: control token: 128117 '<|reserved_special_token_112|>' is not marked as EOG
load: control token: 128011 '<|reserved_special_token_6|>' is not marked as EOG
load: control token: 128022 '<|reserved_special_token_17|>' is not marked as EOG
load: control token: 128123 '<|reserved_special_token_118|>' is not marked as EOG
load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
load: special tokens cache size = 256
load: token to piece cache size = 0.8000 MB
print_info: arch             = llama
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 0.00 K
print_info: general.name     = llama-bpe
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128001 '<|end_of_text|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128001 '<|end_of_text|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 512
llama_context: n_ctx_per_seq = 512
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: flash_attn    = 0
llama_context: freq_base     = 0.0
llama_context: freq_scale    = 1
llama_context: n_ctx_pre_seq (512) > n_ctx_train (0) -- possible training context overflow
llama_context_kv_self: n_ctx = 512
llama_context_kv_self: n_ctx = 512 (padded)
/home/ggml/work/llama.cpp/ggml/src/ggml-backend.cpp:1454: GGML_ASSERT(n_backends > 0) failed
/home/ggml/work/llama.cpp/build-ci-debug/bin/libggml-base.so(+0x5c5c1)[0x734c20d5f5c1]
/home/ggml/work/llama.cpp/build-ci-debug/bin/libggml-base.so(+0x5c77e)[0x734c20d5f77e]
/home/ggml/work/llama.cpp/build-ci-debug/bin/libggml-base.so(ggml_abort+0x117)[0x734c20d5f8ae]
/home/ggml/work/llama.cpp/build-ci-debug/bin/libggml-base.so(ggml_backend_sched_new+0x67)[0x734c20d7ad7a]
/home/ggml/work/llama.cpp/build-ci-debug/bin/libllama.so(_ZN13llama_context4initEv+0x434)[0x734c212fe9d6]
/home/ggml/work/llama.cpp/build-ci-debug/bin/libllama.so(llama_init_from_model+0x1d7)[0x734c212c6bc2]
/home/ggml/work/llama.cpp/build-ci-debug/bin/test-tokenizer-0(+0x65a08)[0x5e89e20e3a08]
/lib/x86_64-linux-gnu/libc.so.6(+0x29d90)[0x734c20629d90]
/lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0x80)[0x734c20629e40]
/home/ggml/work/llama.cpp/build-ci-debug/bin/test-tokenizer-0(+0x644a5)[0x5e89e20e24a5]

      Start  8: test-tokenizer-0-llama-spm
 8/29 Test  #8: test-tokenizer-0-llama-spm ........Subprocess aborted***Exception:   0.18 sec
main : reading vocab from: '/home/ggml/work/llama.cpp/tests/../models/ggml-vocab-llama-spm.gguf'
register_backend: registered backend CPU (1 devices)
register_device: registered device CPU (Intel(R) Xeon(R) Platinum 8473C)
llama_model_loader: loaded meta data with 22 key-value pairs and 0 tensors from /home/ggml/work/llama.cpp/tests/../models/ggml-vocab-llama-spm.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = llama-spm
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 4096
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   8:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv   9:                          general.file_type u32              = 1
llama_model_loader: - kv  10:                           llama.vocab_size u32              = 32000
llama_model_loader: - kv  11:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                         tokenizer.ggml.pre str              = default
llama_model_loader: - kv  14:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  15:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false
print_info: file format = GGUF V3 (latest)
print_info: file type   = F16
print_info: file size   = 0.00 MiB (-nan BPW) 
init_tokenizer: initializing tokenizer for type 1
load: control token:      2 '</s>' is not marked as EOG
load: control token:      1 '<s>' is not marked as EOG
load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
load: special tokens cache size = 3
load: token to piece cache size = 0.1684 MB
print_info: arch             = llama
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 0.00 K
print_info: general.name     = llama-spm
print_info: vocab type       = SPM
print_info: n_vocab          = 32000
print_info: n_merges         = 0
print_info: BOS token        = 1 '<s>'
print_info: EOS token        = 2 '</s>'
print_info: UNK token        = 0 '<unk>'
print_info: LF token         = 13 '<0x0A>'
print_info: EOG token        = 2 '</s>'
print_info: max token length = 48
llama_model_load: vocab only - skipping tensors
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 512
llama_context: n_ctx_per_seq = 512
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: flash_attn    = 0
llama_context: freq_base     = 0.0
llama_context: freq_scale    = 1
llama_context: n_ctx_pre_seq (512) > n_ctx_train (0) -- possible training context overflow
llama_context_kv_self: n_ctx = 512
llama_context_kv_self: n_ctx = 512 (padded)
/home/ggml/work/llama.cpp/ggml/src/ggml-backend.cpp:1454: GGML_ASSERT(n_backends > 0) failed
/home/ggml/work/llama.cpp/build-ci-debug/bin/libggml-base.so(+0x5c5c1)[0x793f4915f5c1]
/home/ggml/work/llama.cpp/build-ci-debug/bin/libggml-base.so(+0x5c77e)[0x793f4915f77e]
/home/ggml/work/llama.cpp/build-ci-debug/bin/libggml-base.so(ggml_abort+0x117)[0x793f4915f8ae]
/home/ggml/work/llama.cpp/build-ci-debug/bin/libggml-base.so(ggml_backend_sched_new+0x67)[0x793f4917ad7a]
/home/ggml/work/llama.cpp/build-ci-debug/bin/libllama.so(_ZN13llama_context4initEv+0x434)[0x793f496fe9d6]
/home/ggml/work/llama.cpp/build-ci-debug/bin/libllama.so(llama_init_from_model+0x1d7)[0x793f496c6bc2]
/home/ggml/work/llama.cpp/build-ci-debug/bin/test-tokenizer-0(+0x65a08)[0x590bcdb37a08]
/lib/x86_64-linux-gnu/libc.so.6(+0x29d90)[0x793f48a29d90]
/lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0x80)[0x793f48a29e40]
/home/ggml/work/llama.cpp/build-ci-debug/bin/test-tokenizer-0(+0x644a5)[0x590bcdb364a5]

      Start  9: test-tokenizer-0-mpt
 9/29 Test  #9: test-tokenizer-0-mpt ..............Subprocess aborted***Exception:   0.38 sec
main : reading vocab from: '/home/ggml/work/llama.cpp/tests/../models/ggml-vocab-mpt.gguf'
register_backend: registered backend CPU (1 devices)
register_device: registered device CPU (Intel(R) Xeon(R) Platinum 8473C)
llama_model_loader: loaded meta data with 17 key-value pairs and 0 tensors from /home/ggml/work/llama.cpp/tests/../models/ggml-vocab-mpt.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = mpt
llama_model_loader: - kv   1:                               general.name str              = mpt
llama_model_loader: - kv   2:                         mpt.context_length u32              = 2048
llama_model_loader: - kv   3:                       mpt.embedding_length u32              = 4096
llama_model_loader: - kv   4:                            mpt.block_count u32              = 32
llama_model_loader: - kv   5:                    mpt.feed_forward_length u32              = 16384
llama_model_loader: - kv   6:                   mpt.attention.head_count u32              = 32
llama_model_loader: - kv   7:           mpt.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv   8:               mpt.attention.max_alibi_bias f32              = 8.000000
llama_model_loader: - kv   9:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  10:                         tokenizer.ggml.pre str              = mpt
llama_model_loader: - kv  11:                      tokenizer.ggml.tokens arr[str,50432]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  12:                  tokenizer.ggml.token_type arr[i32,50432]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  13:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  14:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  15:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  16:            tokenizer.ggml.unknown_token_id u32              = 0
print_info: file format = GGUF V3 (latest)
print_info: file type   = all F32 (guessed)
print_info: file size   = 0.00 MiB (-nan BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 180
load: token to piece cache size = 0.2999 MB
print_info: arch             = mpt
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 0.00 K
print_info: general.name     = mpt
print_info: vocab type       = BPE
print_info: n_vocab          = 50432
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 'Ċ'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
llama_model_load: vocab only - skipping tensors
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 512
llama_context: n_ctx_per_seq = 512
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: flash_attn    = 0
llama_context: freq_base     = 0.0
llama_context: freq_scale    = 1
llama_context: n_ctx_pre_seq (512) > n_ctx_train (0) -- possible training context overflow
llama_context_kv_self: n_ctx = 512
llama_context_kv_self: n_ctx = 512 (padded)
/home/ggml/work/llama.cpp/ggml/src/ggml-backend.cpp:1454: GGML_ASSERT(n_backends > 0) failed
/home/ggml/work/llama.cpp/build-ci-debug/bin/libggml-base.so(+0x5c5c1)[0x7f40d315f5c1]
/home/ggml/work/llama.cpp/build-ci-debug/bin/libggml-base.so(+0x5c77e)[0x7f40d315f77e]
/home/ggml/work/llama.cpp/build-ci-debug/bin/libggml-base.so(ggml_abort+0x117)[0x7f40d315f8ae]
/home/ggml/work/llama.cpp/build-ci-debug/bin/libggml-base.so(ggml_backend_sched_new+0x67)[0x7f40d317ad7a]
/home/ggml/work/llama.cpp/build-ci-debug/bin/libllama.so(_ZN13llama_context4initEv+0x434)[0x7f40d36fe9d6]
/home/ggml/work/llama.cpp/build-ci-debug/bin/libllama.so(llama_init_from_model+0x1d7)[0x7f40d36c6bc2]
/home/ggml/work/llama.cpp/build-ci-debug/bin/test-tokenizer-0(+0x65a08)[0x5cc7176a2a08]
/lib/x86_64-linux-gnu/libc.so.6(+0x29d90)[0x7f40d2a29d90]
/lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0x80)[0x7f40d2a29e40]
/home/ggml/work/llama.cpp/build-ci-debug/bin/test-tokenizer-0(+0x644a5)[0x5cc7176a14a5]

      Start 10: test-tokenizer-0-phi-3
10/29 Test #10: test-tokenizer-0-phi-3 ............Subprocess aborted***Exception:   0.18 sec
main : reading vocab from: '/home/ggml/work/llama.cpp/tests/../models/ggml-vocab-phi-3.gguf'
register_backend: registered backend CPU (1 devices)
register_device: registered device CPU (Intel(R) Xeon(R) Platinum 8473C)
llama_model_loader: loaded meta data with 26 key-value pairs and 0 tensors from /home/ggml/work/llama.cpp/tests/../models/ggml-vocab-phi-3.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = phi3
llama_model_loader: - kv   1:                               general.name str              = Phi3
llama_model_loader: - kv   2:                        phi3.context_length u32              = 4096
llama_model_loader: - kv   3:  phi3.rope.scaling.original_context_length u32              = 4096
llama_model_loader: - kv   4:                      phi3.embedding_length u32              = 3072
llama_model_loader: - kv   5:                   phi3.feed_forward_length u32              = 8192
llama_model_loader: - kv   6:                           phi3.block_count u32              = 32
llama_model_loader: - kv   7:                  phi3.attention.head_count u32              = 32
llama_model_loader: - kv   8:               phi3.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:      phi3.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                  phi3.rope.dimension_count u32              = 96
llama_model_loader: - kv  11:                        phi3.rope.freq_base f32              = 10000.000000
llama_model_loader: - kv  12:                          general.file_type u32              = 1
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = default
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,32064]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  16:                      tokenizer.ggml.scores arr[f32,32064]   = [-1000.000000, -1000.000000, -1000.00...
llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,32064]   = [3, 3, 4, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 32000
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 32000
llama_model_loader: - kv  22:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  23:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  24:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...
llama_model_loader: - kv  25:               general.quantization_version u32              = 2
print_info: file format = GGUF V3 (latest)
print_info: file type   = F16
print_info: file size   = 0.00 MiB (-nan BPW) 
init_tokenizer: initializing tokenizer for type 1
load: control token:  32008 '<|placeholder5|>' is not marked as EOG
load: control token:  32006 '<|system|>' is not marked as EOG
load: control token:  32002 '<|placeholder1|>' is not marked as EOG
load: control token:  32001 '<|assistant|>' is not marked as EOG
load: control token:  32004 '<|placeholder3|>' is not marked as EOG
load: control token:  32003 '<|placeholder2|>' is not marked as EOG
load: control token:      0 '<unk>' is not marked as EOG
load: control token:  32005 '<|placeholder4|>' is not marked as EOG
load: control token:  32010 '<|user|>' is not marked as EOG
load: control token:  32009 '<|placeholder6|>' is not marked as EOG
load: control token:      1 '<s>' is not marked as EOG
load: special tokens cache size = 67
load: token to piece cache size = 0.1690 MB
print_info: arch             = phi3
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 0.00 K
print_info: general.name     = Phi3
print_info: vocab type       = SPM
print_info: n_vocab          = 32064
print_info: n_merges         = 0
print_info: BOS token        = 1 '<s>'
print_info: EOS token        = 32000 '<|endoftext|>'
print_info: EOT token        = 32007 '<|end|>'
print_info: UNK token        = 0 '<unk>'
print_info: PAD token        = 32000 '<|endoftext|>'
print_info: LF token         = 13 '<0x0A>'
print_info: EOG token        = 32000 '<|endoftext|>'
print_info: EOG token        = 32007 '<|end|>'
print_info: max token length = 48
llama_model_load: vocab only - skipping tensors
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 512
llama_context: n_ctx_per_seq = 512
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: flash_attn    = 0
llama_context: freq_base     = 0.0
llama_context: freq_scale    = 1
llama_context: n_ctx_pre_seq (512) > n_ctx_train (0) -- possible training context overflow
llama_context_kv_self: n_ctx = 512
llama_context_kv_self: n_ctx = 512 (padded)
/home/ggml/work/llama.cpp/ggml/src/ggml-backend.cpp:1454: GGML_ASSERT(n_backends > 0) failed
/home/ggml/work/llama.cpp/build-ci-debug/bin/libggml-base.so(+0x5c5c1)[0x7ee4b09f45c1]
/home/ggml/work/llama.cpp/build-ci-debug/bin/libggml-base.so(+0x5c77e)[0x7ee4b09f477e]
/home/ggml/work/llama.cpp/build-ci-debug/bin/libggml-base.so(ggml_abort+0x117)[0x7ee4b09f48ae]
/home/ggml/work/llama.cpp/build-ci-debug/bin/libggml-base.so(ggml_backend_sched_new+0x67)[0x7ee4b0a0fd7a]
/home/ggml/work/llama.cpp/build-ci-debug/bin/libllama.so(_ZN13llama_context4initEv+0x434)[0x7ee4b06fe9d6]
/home/ggml/work/llama.cpp/build-ci-debug/bin/libllama.so(llama_init_from_model+0x1d7)[0x7ee4b06c6bc2]
/home/ggml/work/llama.cpp/build-ci-debug/bin/test-tokenizer-0(+0x65a08)[0x5dbca7f24a08]
/lib/x86_64-linux-gnu/libc.so.6(+0x29d90)[0x7ee4afc29d90]
/lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0x80)[0x7ee4afc29e40]
/home/ggml/work/llama.cpp/build-ci-debug/bin/test-tokenizer-0(+0x644a5)[0x5dbca7f234a5]

      Start 11: test-tokenizer-0-qwen2
11/29 Test #11: test-tokenizer-0-qwen2 ............Subprocess aborted***Exception:   0.97 sec
main : reading vocab from: '/home/ggml/work/llama.cpp/tests/../models/ggml-vocab-qwen2.gguf'
register_backend: registered backend CPU (1 devices)
register_device: registered device CPU (Intel(R) Xeon(R) Platinum 8473C)
llama_model_loader: loaded meta data with 20 key-value pairs and 0 tensors from /home/ggml/work/llama.cpp/tests/../models/ggml-vocab-qwen2.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.name str              = qwen2
llama_model_loader: - kv   2:                          qwen2.block_count u32              = 32
llama_model_loader: - kv   3:                       qwen2.context_length u32              = 32768
llama_model_loader: - kv   4:                     qwen2.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  qwen2.feed_forward_length u32              = 11008
llama_model_loader: - kv   6:                 qwen2.attention.head_count u32              = 32
llama_model_loader: - kv   7:              qwen2.attention.head_count_kv u32              = 32
llama_model_loader: - kv   8:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv   9:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  10:                          general.file_type u32              = 1
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  12:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 151643
llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  19:                    tokenizer.chat_template str              = {% for message in messages %}{% if lo...
print_info: file format = GGUF V3 (latest)
print_info: file type   = F16
print_info: file size   = 0.00 MiB (-nan BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token: 151644 '<|im_start|>' is not marked as EOG
load: special tokens cache size = 293
load: token to piece cache size = 0.9338 MB
print_info: arch             = qwen2
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 0.00 K
print_info: general.name     = qwen2
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151643 '<|endoftext|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 512
llama_context: n_ctx_per_seq = 512
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: flash_attn    = 0
llama_context: freq_base     = 0.0
llama_context: freq_scale    = 1
llama_context: n_ctx_pre_seq (512) > n_ctx_train (0) -- possible training context overflow
llama_context_kv_self: n_ctx = 512
llama_context_kv_self: n_ctx = 512 (padded)
/home/ggml/work/llama.cpp/ggml/src/ggml-backend.cpp:1454: GGML_ASSERT(n_backends > 0) failed
/home/ggml/work/llama.cpp/build-ci-debug/bin/libggml-base.so(+0x5c5c1)[0x704c97f5f5c1]
/home/ggml/work/llama.cpp/build-ci-debug/bin/libggml-base.so(+0x5c77e)[0x704c97f5f77e]
/home/ggml/work/llama.cpp/build-ci-debug/bin/libggml-base.so(ggml_abort+0x117)[0x704c97f5f8ae]
/home/ggml/work/llama.cpp/build-ci-debug/bin/libggml-base.so(ggml_backend_sched_new+0x67)[0x704c97f7ad7a]
/home/ggml/work/llama.cpp/build-ci-debug/bin/libllama.so(_ZN13llama_context4initEv+0x434)[0x704c984fe9d6]
/home/ggml/work/llama.cpp/build-ci-debug/bin/libllama.so(llama_init_from_model+0x1d7)[0x704c984c6bc2]
/home/ggml/work/llama.cpp/build-ci-debug/bin/test-tokenizer-0(+0x65a08)[0x646d5be69a08]
/lib/x86_64-linux-gnu/libc.so.6(+0x29d90)[0x704c97829d90]
/lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0x80)[0x704c97829e40]
/home/ggml/work/llama.cpp/build-ci-debug/bin/test-tokenizer-0(+0x644a5)[0x646d5be684a5]

      Start 12: test-tokenizer-0-refact
12/29 Test #12: test-tokenizer-0-refact ...........Subprocess aborted***Exception:   0.37 sec
main : reading vocab from: '/home/ggml/work/llama.cpp/tests/../models/ggml-vocab-refact.gguf'
register_backend: registered backend CPU (1 devices)
register_device: registered device CPU (Intel(R) Xeon(R) Platinum 8473C)
llama_model_loader: loaded meta data with 18 key-value pairs and 0 tensors from /home/ggml/work/llama.cpp/tests/../models/ggml-vocab-refact.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = refact
llama_model_loader: - kv   1:                               general.name str              = Refact
llama_model_loader: - kv   2:                      refact.context_length u32              = 4096
llama_model_loader: - kv   3:                    refact.embedding_length u32              = 2048
llama_model_loader: - kv   4:                 refact.feed_forward_length u32              = 5632
llama_model_loader: - kv   5:                         refact.block_count u32              = 32
llama_model_loader: - kv   6:                refact.attention.head_count u32              = 32
llama_model_loader: - kv   7:             refact.attention.head_count_kv u32              = 1
llama_model_loader: - kv   8:    refact.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv   9:                          general.file_type u32              = 1
llama_model_loader: - kv  10:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  11:                         tokenizer.ggml.pre str              = refact
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,49216]   = ["<|endoftext|>", "<fim_prefix>", "<f...
llama_model_loader: - kv  13:                  tokenizer.ggml.token_type arr[i32,49216]   = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...
llama_model_loader: - kv  14:                      tokenizer.ggml.merges arr[str,48891]   = ["Ġ Ġ", "ĠĠ ĠĠ", "ĠĠĠĠ ĠĠ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0
print_info: file format = GGUF V3 (latest)
print_info: file type   = F16
print_info: file size   = 0.00 MiB (-nan BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      2 '<fim_middle>' is not marked as EOG
load: control token:     13 '<jupyter_output>' is not marked as EOG
load: control token:      9 '<issue_closed>' is not marked as EOG
load: control token:      6 '<gh_stars>' is not marked as EOG
load: control token:     10 '<jupyter_start>' is not marked as EOG
load: control token:     14 '<empty_output>' is not marked as EOG
load: control token:     15 '<commit_before>' is not marked as EOG
load: control token:      5 '<filename>' is not marked as EOG
load: control token:     12 '<jupyter_code>' is not marked as EOG
load: control token:      4 '<fim_pad>' is not marked as EOG
load: control token:     18 '<reponame>' is not marked as EOG
load: control token:      7 '<issue_start>' is not marked as EOG
load: control token:      3 '<fim_suffix>' is not marked as EOG
load: control token:      1 '<fim_prefix>' is not marked as EOG
load: control token:      8 '<issue_comment>' is not marked as EOG
load: control token:     11 '<jupyter_text>' is not marked as EOG
load: control token:     16 '<commit_msg>' is not marked as EOG
load: control token:     17 '<commit_after>' is not marked as EOG
load: special tokens cache size = 83
load: token to piece cache size = 0.2832 MB
print_info: arch             = refact
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 0.00 K
print_info: general.name     = Refact
print_info: vocab type       = BPE
print_info: n_vocab          = 49216
print_info: n_merges         = 48891
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 203 'Ċ'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 512
llama_model_load: vocab only - skipping tensors
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 512
llama_context: n_ctx_per_seq = 512
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: flash_attn    = 0
llama_context: freq_base     = 0.0
llama_context: freq_scale    = 1
llama_context: n_ctx_pre_seq (512) > n_ctx_train (0) -- possible training context overflow
llama_context_kv_self: n_ctx = 512
llama_context_kv_self: n_ctx = 512 (padded)
/home/ggml/work/llama.cpp/ggml/src/ggml-backend.cpp:1454: GGML_ASSERT(n_backends > 0) failed
/home/ggml/work/llama.cpp/build-ci-debug/bin/libggml-base.so(+0x5c5c1)[0x703b4915f5c1]
/home/ggml/work/llama.cpp/build-ci-debug/bin/libggml-base.so(+0x5c77e)[0x703b4915f77e]
/home/ggml/work/llama.cpp/build-ci-debug/bin/libggml-base.so(ggml_abort+0x117)[0x703b4915f8ae]
/home/ggml/work/llama.cpp/build-ci-debug/bin/libggml-base.so(ggml_backend_sched_new+0x67)[0x703b4917ad7a]
/home/ggml/work/llama.cpp/build-ci-debug/bin/libllama.so(_ZN13llama_context4initEv+0x434)[0x703b496fe9d6]
/home/ggml/work/llama.cpp/build-ci-debug/bin/libllama.so(llama_init_from_model+0x1d7)[0x703b496c6bc2]
/home/ggml/work/llama.cpp/build-ci-debug/bin/test-tokenizer-0(+0x65a08)[0x64ca19393a08]
/lib/x86_64-linux-gnu/libc.so.6(+0x29d90)[0x703b48a29d90]
/lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0x80)[0x703b48a29e40]
/home/ggml/work/llama.cpp/build-ci-debug/bin/test-tokenizer-0(+0x644a5)[0x64ca193924a5]

      Start 13: test-tokenizer-0-starcoder
13/29 Test #13: test-tokenizer-0-starcoder ........Subprocess aborted***Exception:   0.37 sec
main : reading vocab from: '/home/ggml/work/llama.cpp/tests/../models/ggml-vocab-starcoder.gguf'
register_backend: registered backend CPU (1 devices)
register_device: registered device CPU (Intel(R) Xeon(R) Platinum 8473C)
llama_model_loader: loaded meta data with 19 key-value pairs and 0 tensors from /home/ggml/work/llama.cpp/tests/../models/ggml-vocab-starcoder.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = starcoder2
llama_model_loader: - kv   1:                               general.name str              = starcoder
llama_model_loader: - kv   2:                     starcoder2.block_count u32              = 30
llama_model_loader: - kv   3:                  starcoder2.context_length u32              = 16384
llama_model_loader: - kv   4:                starcoder2.embedding_length u32              = 3072
llama_model_loader: - kv   5:             starcoder2.feed_forward_length u32              = 12288
llama_model_loader: - kv   6:            starcoder2.attention.head_count u32              = 24
llama_model_loader: - kv   7:         starcoder2.attention.head_count_kv u32              = 2
llama_model_loader: - kv   8:                  starcoder2.rope.freq_base f32              = 999999.437500
llama_model_loader: - kv   9:    starcoder2.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 1
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  12:                         tokenizer.ggml.pre str              = starcoder
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,49152]   = ["<|endoftext|>", "<fim_prefix>", "<f...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,49152]   = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...
llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,48872]   = ["Ġ Ġ", "ĠĠ ĠĠ", "ĠĠĠĠ ĠĠ...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0
print_info: file format = GGUF V3 (latest)
print_info: file type   = F16
print_info: file size   = 0.00 MiB (-nan BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:     22 '<pr_file>' is not marked as EOG
load: control token:     21 '<pr_base>' is not marked as EOG
load: control token:     27 '<pr_event_id>' is not marked as EOG
load: control token:      2 '<fim_middle>' is not marked as EOG
load: control token:     18 '<pr>' is not marked as EOG
load: control token:     20 '<pr_is_merged>' is not marked as EOG
load: control token:     13 '<jupyter_output>' is not marked as EOG
load: control token:     36 '<KEY>' is not marked as EOG
load: control token:      9 '<issue_closed>' is not marked as EOG
load: control token:     10 '<jupyter_start>' is not marked as EOG
load: control token:     15 '<empty_output>' is not marked as EOG
load: control token:     30 '<pr_review_comment>' is not marked as EOG
load: control token:     34 '<NAME>' is not marked as EOG
load: control token:     32 '<pr_in_reply_to_comment_id>' is not marked as EOG
load: control token:     14 '<jupyter_script>' is not marked as EOG
load: control token:     17 '<intermediate_to_code>' is not marked as EOG
load: control token:     33 '<pr_diff_hunk_comment_line>' is not marked as EOG
load: control token:      4 '<fim_pad>' is not marked as EOG
load: control token:     35 '<EMAIL>' is not marked as EOG
load: control token:     28 '<pr_review>' is not marked as EOG
load: control token:      5 '<repo_name>' is not marked as EOG
load: control token:     12 '<jupyter_code>' is not marked as EOG
load: control token:     37 '<PASSWORD>' is not marked as EOG
load: control token:     26 '<pr_comment>' is not marked as EOG
load: control token:      7 '<issue_start>' is not marked as EOG
load: control token:     31 '<pr_in_reply_to_review_id>' is not marked as EOG
load: control token:      3 '<fim_suffix>' is not marked as EOG
load: control token:      1 '<fim_prefix>' is not marked as EOG
load: control token:     24 '<pr_diff>' is not marked as EOG
load: control token:     19 '<pr_status>' is not marked as EOG
load: control token:      8 '<issue_comment>' is not marked as EOG
load: control token:     11 '<jupyter_text>' is not marked as EOG
load: control token:     23 '<pr_base_code>' is not marked as EOG
load: control token:     29 '<pr_review_state>' is not marked as EOG
load: control token:     25 '<pr_diff_hunk>' is not marked as EOG
load: control token:     16 '<code_to_intermediate>' is not marked as EOG
load: control token:      6 '<file_sep>' is not marked as EOG
load: special tokens cache size = 38
load: token to piece cache size = 0.2828 MB
print_info: arch             = starcoder2
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 0.00 K
print_info: general.name     = starcoder
print_info: vocab type       = BPE
print_info: n_vocab          = 49152
print_info: n_merges         = 48872
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 222 'Ċ'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 512
llama_model_load: vocab only - skipping tensors
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 512
llama_context: n_ctx_per_seq = 512
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: flash_attn    = 0
llama_context: freq_base     = 0.0
llama_context: freq_scale    = 1
llama_context: n_ctx_pre_seq (512) > n_ctx_train (0) -- possible training context overflow
llama_context_kv_self: n_ctx = 512
llama_context_kv_self: n_ctx = 512 (padded)
/home/ggml/work/llama.cpp/ggml/src/ggml-backend.cpp:1454: GGML_ASSERT(n_backends > 0) failed
/home/ggml/work/llama.cpp/build-ci-debug/bin/libggml-base.so(+0x5c5c1)[0x73b0b975f5c1]
/home/ggml/work/llama.cpp/build-ci-debug/bin/libggml-base.so(+0x5c77e)[0x73b0b975f77e]
/home/ggml/work/llama.cpp/build-ci-debug/bin/libggml-base.so(ggml_abort+0x117)[0x73b0b975f8ae]
/home/ggml/work/llama.cpp/build-ci-debug/bin/libggml-base.so(ggml_backend_sched_new+0x67)[0x73b0b977ad7a]
/home/ggml/work/llama.cpp/build-ci-debug/bin/libllama.so(_ZN13llama_context4initEv+0x434)[0x73b0b9cfe9d6]
/home/ggml/work/llama.cpp/build-ci-debug/bin/libllama.so(llama_init_from_model+0x1d7)[0x73b0b9cc6bc2]
/home/ggml/work/llama.cpp/build-ci-debug/bin/test-tokenizer-0(+0x65a08)[0x5d70b4f28a08]
/lib/x86_64-linux-gnu/libc.so.6(+0x29d90)[0x73b0b9029d90]
/lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0x80)[0x73b0b9029e40]
/home/ggml/work/llama.cpp/build-ci-debug/bin/test-tokenizer-0(+0x644a5)[0x5d70b4f274a5]

      Start 14: test-sampling
14/29 Test #14: test-sampling .....................   Passed    4.67 sec
      Start 15: test-grammar-parser
15/29 Test #15: test-grammar-parser ...............   Passed    0.01 sec
      Start 16: test-grammar-integration
16/29 Test #16: test-grammar-integration ..........   Passed    0.03 sec
      Start 17: test-llama-grammar
17/29 Test #17: test-llama-grammar ................   Passed    0.00 sec
      Start 18: test-chat
18/29 Test #18: test-chat .........................   Passed    4.22 sec
      Start 19: test-json-schema-to-grammar
19/29 Test #19: test-json-schema-to-grammar .......   Passed    1.82 sec
      Start 20: test-tokenizer-1-llama-spm
20/29 Test #20: test-tokenizer-1-llama-spm ........Subprocess aborted***Exception:   0.18 sec
main : reading vocab from: '/home/ggml/work/llama.cpp/tests/../models/ggml-vocab-llama-spm.gguf'
register_backend: registered backend CPU (1 devices)
register_device: registered device CPU (Intel(R) Xeon(R) Platinum 8473C)
llama_model_loader: loaded meta data with 22 key-value pairs and 0 tensors from /home/ggml/work/llama.cpp/tests/../models/ggml-vocab-llama-spm.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = llama-spm
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 4096
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   8:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv   9:                          general.file_type u32              = 1
llama_model_loader: - kv  10:                           llama.vocab_size u32              = 32000
llama_model_loader: - kv  11:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                         tokenizer.ggml.pre str              = default
llama_model_loader: - kv  14:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  15:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false
print_info: file format = GGUF V3 (latest)
print_info: file type   = F16
print_info: file size   = 0.00 MiB (-nan BPW) 
init_tokenizer: initializing tokenizer for type 1
load: control token:      2 '</s>' is not marked as EOG
load: control token:      1 '<s>' is not marked as EOG
load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
load: special tokens cache size = 3
load: token to piece cache size = 0.1684 MB
print_info: arch             = llama
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 0.00 K
print_info: general.name     = llama-spm
print_info: vocab type       = SPM
print_info: n_vocab          = 32000
print_info: n_merges         = 0
print_info: BOS token        = 1 '<s>'
print_info: EOS token        = 2 '</s>'
print_info: UNK token        = 0 '<unk>'
print_info: LF token         = 13 '<0x0A>'
print_info: EOG token        = 2 '</s>'
print_info: max token length = 48
llama_model_load: vocab only - skipping tensors
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 512
llama_context: n_ctx_per_seq = 512
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: flash_attn    = 0
llama_context: freq_base     = 0.0
llama_context: freq_scale    = 1
llama_context: n_ctx_pre_seq (512) > n_ctx_train (0) -- possible training context overflow
llama_context_kv_self: n_ctx = 512
llama_context_kv_self: n_ctx = 512 (padded)
/home/ggml/work/llama.cpp/ggml/src/ggml-backend.cpp:1454: GGML_ASSERT(n_backends > 0) failed
/home/ggml/work/llama.cpp/build-ci-debug/bin/libggml-base.so(+0x5c5c1)[0x74dd3cbe05c1]
/home/ggml/work/llama.cpp/build-ci-debug/bin/libggml-base.so(+0x5c77e)[0x74dd3cbe077e]
/home/ggml/work/llama.cpp/build-ci-debug/bin/libggml-base.so(ggml_abort+0x117)[0x74dd3cbe08ae]
/home/ggml/work/llama.cpp/build-ci-debug/bin/libggml-base.so(ggml_backend_sched_new+0x67)[0x74dd3cbfbd7a]
/home/ggml/work/llama.cpp/build-ci-debug/bin/libllama.so(_ZN13llama_context4initEv+0x434)[0x74dd3c8fe9d6]
/home/ggml/work/llama.cpp/build-ci-debug/bin/libllama.so(llama_init_from_model+0x1d7)[0x74dd3c8c6bc2]
/home/ggml/work/llama.cpp/build-ci-debug/bin/test-tokenizer-1-spm(+0x63b45)[0x58ef5a84ab45]
/lib/x86_64-linux-gnu/libc.so.6(+0x29d90)[0x74dd3be29d90]
/lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0x80)[0x74dd3be29e40]
/home/ggml/work/llama.cpp/build-ci-debug/bin/test-tokenizer-1-spm(+0x633c5)[0x58ef5a84a3c5]

      Start 21: test-log
21/29 Test #21: test-log ..........................   Passed    0.02 sec
      Start 22: test-arg-parser
22/29 Test #22: test-arg-parser ...................   Passed    0.08 sec
      Start 23: test-chat-template
23/29 Test #23: test-chat-template ................   Passed    0.77 sec
      Start 24: test-gguf
24/29 Test #24: test-gguf .........................   Passed    0.26 sec
      Start 25: test-backend-ops
25/29 Test #25: test-backend-ops ..................   Passed    0.01 sec
      Start 28: test-barrier
26/29 Test #28: test-barrier ......................   Passed    1.21 sec
      Start 29: test-quantize-fns
27/29 Test #29: test-quantize-fns .................   Passed   22.38 sec
      Start 30: test-quantize-perf
28/29 Test #30: test-quantize-perf ................   Passed    0.16 sec
      Start 31: test-rope
29/29 Test #31: test-rope .........................   Passed    0.08 sec

52% tests passed, 14 tests failed out of 29

Label Time Summary:
main    =  43.16 sec*proc (29 tests)

Total Test time (real) =  43.18 sec

The following tests FAILED:
	  1 - test-tokenizer-0-bert-bge (Subprocess aborted)
	  2 - test-tokenizer-0-command-r (Subprocess aborted)
	  3 - test-tokenizer-0-deepseek-coder (Subprocess aborted)
	  4 - test-tokenizer-0-deepseek-llm (Subprocess aborted)
	  5 - test-tokenizer-0-falcon (Subprocess aborted)
	  6 - test-tokenizer-0-gpt-2 (Subprocess aborted)
	  7 - test-tokenizer-0-llama-bpe (Subprocess aborted)
	  8 - test-tokenizer-0-llama-spm (Subprocess aborted)
	  9 - test-tokenizer-0-mpt (Subprocess aborted)
	 10 - test-tokenizer-0-phi-3 (Subprocess aborted)
	 11 - test-tokenizer-0-qwen2 (Subprocess aborted)
	 12 - test-tokenizer-0-refact (Subprocess aborted)
	 13 - test-tokenizer-0-starcoder (Subprocess aborted)
	 20 - test-tokenizer-1-llama-spm (Subprocess aborted)
Errors while running CTest

real	0m43.182s
user	0m45.202s
sys	0m0.707s
+ cur=8
+ echo 8
+ set +x
