### ctest_debug

Runs ctest in debug mode
- status: 0
```
+ ctest --output-on-failure -L main -E test-opt
Test project /Users/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-tokenizer-0-bert-bge
 1/28 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.47 sec
      Start  2: test-tokenizer-0-command-r
 2/28 Test  #2: test-tokenizer-0-command-r ........   Passed    1.09 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/28 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.16 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/28 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.44 sec
      Start  5: test-tokenizer-0-falcon
 5/28 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.28 sec
      Start  6: test-tokenizer-0-gpt-2
 6/28 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.22 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/28 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.67 sec
      Start  8: test-tokenizer-0-llama-spm
 8/28 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.08 sec
      Start  9: test-tokenizer-0-mpt
 9/28 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.22 sec
      Start 10: test-tokenizer-0-phi-3
10/28 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.08 sec
      Start 11: test-tokenizer-0-qwen2
11/28 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.60 sec
      Start 12: test-tokenizer-0-refact
12/28 Test #12: test-tokenizer-0-refact ...........   Passed    0.22 sec
      Start 13: test-tokenizer-0-starcoder
13/28 Test #13: test-tokenizer-0-starcoder ........   Passed    0.22 sec
      Start 14: test-sampling
14/28 Test #14: test-sampling .....................   Passed    2.17 sec
      Start 15: test-grammar-parser
15/28 Test #15: test-grammar-parser ...............   Passed    0.20 sec
      Start 16: test-grammar-integration
16/28 Test #16: test-grammar-integration ..........   Passed    0.30 sec
      Start 17: test-llama-grammar
17/28 Test #17: test-llama-grammar ................   Passed    0.20 sec
      Start 18: test-json-schema-to-grammar
18/28 Test #18: test-json-schema-to-grammar .......   Passed    2.28 sec
      Start 19: test-tokenizer-1-llama-spm
19/28 Test #19: test-tokenizer-1-llama-spm ........   Passed    1.13 sec
      Start 20: test-log
20/28 Test #20: test-log ..........................   Passed    0.23 sec
      Start 21: test-arg-parser
21/28 Test #21: test-arg-parser ...................   Passed    0.40 sec
      Start 22: test-chat-template
22/28 Test #22: test-chat-template ................   Passed    2.90 sec
      Start 23: test-gguf
23/28 Test #23: test-gguf .........................   Passed    1.01 sec
      Start 24: test-backend-ops
24/28 Test #24: test-backend-ops ..................   Passed  190.50 sec
      Start 27: test-barrier
25/28 Test #27: test-barrier ......................   Passed    1.16 sec
      Start 28: test-quantize-fns
26/28 Test #28: test-quantize-fns .................   Passed   26.16 sec
      Start 29: test-quantize-perf
27/28 Test #29: test-quantize-perf ................   Passed    0.34 sec
      Start 30: test-rope
28/28 Test #30: test-rope .........................   Passed    0.22 sec

100% tests passed, 0 tests failed out of 28

Label Time Summary:
main    = 234.94 sec*proc (28 tests)

Total Test time (real) = 234.95 sec

real	3m54.984s
user	8m11.016s
sys	0m7.287s
```

### ctest_release

Runs ctest in release mode
- status: 0
```
+ ctest --output-on-failure -L main
Test project /Users/ggml/work/llama.cpp/build-ci-release
      Start  1: test-tokenizer-0-bert-bge
 1/28 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.36 sec
      Start  2: test-tokenizer-0-command-r
 2/28 Test  #2: test-tokenizer-0-command-r ........   Passed    0.23 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/28 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.04 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/28 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.08 sec
      Start  5: test-tokenizer-0-falcon
 5/28 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.06 sec
      Start  6: test-tokenizer-0-gpt-2
 6/28 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.05 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/28 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.14 sec
      Start  8: test-tokenizer-0-llama-spm
 8/28 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.03 sec
      Start  9: test-tokenizer-0-mpt
 9/28 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.05 sec
      Start 10: test-tokenizer-0-phi-3
10/28 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.03 sec
      Start 11: test-tokenizer-0-qwen2
11/28 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.11 sec
      Start 12: test-tokenizer-0-refact
12/28 Test #12: test-tokenizer-0-refact ...........   Passed    0.05 sec
      Start 13: test-tokenizer-0-starcoder
13/28 Test #13: test-tokenizer-0-starcoder ........   Passed    0.05 sec
      Start 14: test-sampling
14/28 Test #14: test-sampling .....................   Passed    0.91 sec
      Start 15: test-grammar-parser
15/28 Test #15: test-grammar-parser ...............   Passed    0.17 sec
      Start 16: test-grammar-integration
16/28 Test #16: test-grammar-integration ..........   Passed    0.21 sec
      Start 17: test-llama-grammar
17/28 Test #17: test-llama-grammar ................   Passed    0.18 sec
      Start 18: test-json-schema-to-grammar
18/28 Test #18: test-json-schema-to-grammar .......   Passed    2.21 sec
      Start 19: test-tokenizer-1-llama-spm
19/28 Test #19: test-tokenizer-1-llama-spm ........   Passed    0.35 sec
      Start 20: test-log
20/28 Test #20: test-log ..........................   Passed    0.19 sec
      Start 21: test-arg-parser
21/28 Test #21: test-arg-parser ...................   Passed    0.24 sec
      Start 22: test-chat-template
22/28 Test #22: test-chat-template ................   Passed    0.47 sec
      Start 23: test-gguf
23/28 Test #23: test-gguf .........................   Passed    0.40 sec
      Start 24: test-backend-ops
24/28 Test #24: test-backend-ops ..................   Passed   30.79 sec
      Start 27: test-barrier
25/28 Test #27: test-barrier ......................   Passed    0.39 sec
      Start 28: test-quantize-fns
26/28 Test #28: test-quantize-fns .................   Passed   14.08 sec
      Start 29: test-quantize-perf
27/28 Test #29: test-quantize-perf ................   Passed    0.23 sec
      Start 30: test-rope
28/28 Test #30: test-rope .........................   Passed    0.22 sec

100% tests passed, 0 tests failed out of 28

Label Time Summary:
main    =  53.29 sec*proc (28 tests)

Total Test time (real) =  53.30 sec

real	0m53.313s
user	1m15.650s
sys	0m6.289s
```
### embd_bge_small

BGE Small (BERT):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.113 I build: 4586 (2711d021) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.023.017 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.028.573 I llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.028.581 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.028.584 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.028.585 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.028.585 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.028.586 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.028.587 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.028.588 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.028.589 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.028.590 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.028.591 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.028.591 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.028.594 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.028.595 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.028.596 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.028.596 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.028.597 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.028.597 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.028.598 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.033.708 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.035.039 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.035.041 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.035.042 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.035.043 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.035.043 I llama_model_loader: - kv  22:               tokenizer.ggml.mask_token_id u32              = 103
0.00.035.043 I llama_model_loader: - kv  23:               general.quantization_version u32              = 2
0.00.035.044 I llama_model_loader: - type  f32:  124 tensors
0.00.035.045 I llama_model_loader: - type  f16:   73 tensors
0.00.035.045 I print_info: file format = GGUF V3 (latest)
0.00.035.046 I print_info: file type   = F16
0.00.035.048 I print_info: file size   = 63.84 MiB (16.12 BPW) 
0.00.039.664 I load: special tokens cache size = 5
0.00.042.067 I load: token to piece cache size = 0.2032 MB
0.00.042.071 I print_info: arch             = bert
0.00.042.071 I print_info: vocab_only       = 0
0.00.042.071 I print_info: n_ctx_train      = 512
0.00.042.072 I print_info: n_embd           = 384
0.00.042.072 I print_info: n_layer          = 12
0.00.042.075 I print_info: n_head           = 12
0.00.042.076 I print_info: n_head_kv        = 12
0.00.042.077 I print_info: n_rot            = 32
0.00.042.077 I print_info: n_swa            = 0
0.00.042.077 I print_info: n_embd_head_k    = 32
0.00.042.077 I print_info: n_embd_head_v    = 32
0.00.042.081 I print_info: n_gqa            = 1
0.00.042.082 I print_info: n_embd_k_gqa     = 384
0.00.042.083 I print_info: n_embd_v_gqa     = 384
0.00.042.084 I print_info: f_norm_eps       = 1.0e-12
0.00.042.091 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.042.091 I print_info: f_clamp_kqv      = 0.0e+00
0.00.042.092 I print_info: f_max_alibi_bias = 0.0e+00
0.00.042.092 I print_info: f_logit_scale    = 0.0e+00
0.00.042.093 I print_info: n_ff             = 1536
0.00.042.093 I print_info: n_expert         = 0
0.00.042.093 I print_info: n_expert_used    = 0
0.00.042.094 I print_info: causal attn      = 0
0.00.042.094 I print_info: pooling type     = 2
0.00.042.094 I print_info: rope type        = 2
0.00.042.095 I print_info: rope scaling     = linear
0.00.042.095 I print_info: freq_base_train  = 10000.0
0.00.042.096 I print_info: freq_scale_train = 1
0.00.042.096 I print_info: n_ctx_orig_yarn  = 512
0.00.042.097 I print_info: rope_finetuned   = unknown
0.00.042.097 I print_info: ssm_d_conv       = 0
0.00.042.097 I print_info: ssm_d_inner      = 0
0.00.042.097 I print_info: ssm_d_state      = 0
0.00.042.097 I print_info: ssm_dt_rank      = 0
0.00.042.098 I print_info: ssm_dt_b_c_rms   = 0
0.00.042.098 I print_info: model type       = 33M
0.00.042.099 I print_info: model params     = 33.21 M
0.00.042.099 I print_info: general.name     = Bge Small
0.00.042.100 I print_info: vocab type       = WPM
0.00.042.100 I print_info: n_vocab          = 30522
0.00.042.100 I print_info: n_merges         = 0
0.00.042.101 I print_info: BOS token        = 101 '[CLS]'
0.00.042.101 I print_info: UNK token        = 100 '[UNK]'
0.00.042.101 I print_info: SEP token        = 102 '[SEP]'
0.00.042.102 I print_info: PAD token        = 0 '[PAD]'
0.00.042.102 I print_info: MASK token       = 103 '[MASK]'
0.00.042.102 I print_info: LF token         = 0 '[PAD]'
0.00.042.103 I print_info: max token length = 21
0.00.045.301 I load_tensors: offloading 12 repeating layers to GPU
0.00.045.303 I load_tensors: offloading output layer to GPU
0.00.045.303 I load_tensors: offloaded 13/13 layers to GPU
0.00.045.327 I load_tensors: Metal_Mapped model buffer size =    40.73 MiB
0.00.045.329 I load_tensors:   CPU_Mapped model buffer size =    23.11 MiB
0.00.045.583 I llama_init_from_model: n_seq_max     = 1
0.00.045.585 I llama_init_from_model: n_ctx         = 512
0.00.045.585 I llama_init_from_model: n_ctx_per_seq = 512
0.00.045.585 I llama_init_from_model: n_batch       = 2048
0.00.045.586 I llama_init_from_model: n_ubatch      = 2048
0.00.045.586 I llama_init_from_model: flash_attn    = 0
0.00.045.586 I llama_init_from_model: freq_base     = 10000.0
0.00.045.587 I llama_init_from_model: freq_scale    = 1
0.00.045.588 I ggml_metal_init: allocating
0.00.045.593 I ggml_metal_init: found device: Apple M4
0.00.045.598 I ggml_metal_init: picking default device: Apple M4
0.00.046.311 I ggml_metal_init: using embedded metal library
0.00.050.485 I ggml_metal_init: GPU name:   Apple M4
0.00.050.487 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.050.488 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.050.488 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.050.489 I ggml_metal_init: simdgroup reduction   = true
0.00.050.489 I ggml_metal_init: simdgroup matrix mul. = true
0.00.050.489 I ggml_metal_init: has residency sets    = true
0.00.050.489 I ggml_metal_init: has bfloat            = true
0.00.050.489 I ggml_metal_init: use bfloat            = true
0.00.050.490 I ggml_metal_init: hasUnifiedMemory      = true
0.00.050.491 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.096 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.063.779 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.063.781 I llama_init_from_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.063.783 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.064.973 I llama_init_from_model:      Metal compute buffer size =    16.00 MiB
0.00.064.975 I llama_init_from_model:        CPU compute buffer size =     2.51 MiB
0.00.064.975 I llama_init_from_model: graph nodes  = 429
0.00.064.975 I llama_init_from_model: graph splits = 2
0.00.064.977 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.064.977 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.070.739 I 
0.00.070.769 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.071.449 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.043984 -0.019895  0.007670 -0.000838  0.001372 -0.037036  0.109436  0.042589  0.092046 -0.015918  0.006773 -0.035692 -0.017889  0.015040  0.018126  0.015860 -0.011308  0.010431 -0.085223 -0.008454  0.091368 -0.017067 -0.060328 -0.024484  0.027524  0.076059  0.027984 -0.014564  0.017645 -0.033268 -0.037874 -0.019012  0.068670 -0.009845 -0.025030  0.072346 -0.046570  0.011026 -0.050252  0.047693  0.032387 -0.011761  0.022055  0.049645  0.010465  0.005795 -0.028860  0.008933 -0.018507 -0.051466 -0.046036  0.030497 -0.035418  0.054204 -0.069650  0.044238  0.029786  0.046305  0.073410 -0.042580  0.076109  0.038851 -0.181182  0.082503  0.042250 -0.064549 -0.060097 -0.017853  0.006472  0.005873  0.017196 -0.026615  0.064578  0.112612  0.035159 -0.067416  0.027073 -0.067295 -0.033493 -0.033219  0.033252  0.013523 -0.003346 -0.037473 -0.052055  0.055155 -0.001982 -0.038276  0.064447  0.028824 -0.043334 -0.029236 -0.039462  0.036315  0.008379 -0.015481 -0.036573  0.018130  0.028600  0.342831 -0.044484  0.056114  0.017624 -0.020867 -0.066808  0.000146 -0.037885 -0.030063 -0.008541 -0.021595  0.000548 -0.003219  0.004020  0.018908 -0.008548  0.025835  0.049433  0.000091  0.050920 -0.042479 -0.031884  0.023606  0.030691 -0.023148 -0.046271 -0.079278  0.115198  0.046757  0.027829 -0.040710  0.067777 -0.022955  0.010332 -0.032937 -0.018305  0.043844  0.024268  0.052413  0.007469  0.008905  0.011244 -0.074655 -0.065561 -0.026750 -0.041192 -0.023884  0.026710  0.006912  0.027741  0.052873 -0.036673  0.057707 -0.000170  0.031764 -0.019760 -0.022083  0.041053 -0.058903  0.019619  0.043144  0.043582  0.041593 -0.022527  0.027037 -0.021833  0.005430 -0.041317 -0.001239  0.024452  0.002088  0.044333 -0.022745  0.043662  0.064763  0.055430  0.037060 -0.000919  0.046111  0.045790 -0.008487  0.063057 -0.073235 -0.011935  0.032107  0.023957  0.014725 -0.033688  0.001083 -0.015827 -0.019002  0.047872  0.110842  0.028428  0.031366 -0.013288 -0.057499  0.006644  0.005140 -0.012262 -0.051443 -0.000965 -0.017642 -0.019440 -0.040937  0.009209 -0.057957  0.050961  0.052340 -0.009617 -0.040259 -0.014075 -0.024877 -0.017251  0.006302  0.006588 -0.026919  0.015600  0.030760  0.002578  0.023206 -0.022199 -0.098557 -0.051105 -0.278031 -0.014994 -0.061553 -0.027218  0.017666 -0.010939 -0.017077  0.035055  0.046994 -0.015432  0.015232 -0.025477  0.047844 -0.005949 -0.000749 -0.061008 -0.068946 -0.060377 -0.035942  0.043319 -0.055047  0.015075  0.000537 -0.058175 -0.010439  0.012643  0.151495  0.127111 -0.013622  0.041993 -0.025672  0.014026 -0.001037 -0.150469  0.044857  0.005320 -0.036274 -0.029801 -0.020192 -0.034891  0.010235  0.033548 -0.048178 -0.051795 -0.017463 -0.023492  0.047349  0.052076 -0.016783 -0.055452  0.025836 -0.005707  0.010710  0.038705  0.008191 -0.009764 -0.105779 -0.027430 -0.096101  0.025068 -0.011260  0.092355  0.056101  0.003758  0.027791  0.002092 -0.051085 -0.039890 -0.013542 -0.044956 -0.015335  0.002910 -0.043498 -0.077951  0.065217 -0.006840 -0.001630 -0.014642  0.071560  0.023714 -0.037180  0.009175  0.001562 -0.032268  0.015468  0.037880  0.000337 -0.053207  0.021321 -0.039829  0.000031  0.013400  0.019798 -0.057879  0.006473 -0.049531 -0.267858  0.039156 -0.067972  0.038248 -0.012333  0.041485 -0.016117  0.052383 -0.071363  0.011369  0.024719 -0.007232  0.082087  0.028535 -0.021504  0.040494 -0.004570 -0.074594 -0.014749  0.020037  0.002304  0.023148  0.197212 -0.043222 -0.025983 -0.004959 -0.019278  0.074258  0.001716 -0.031979 -0.036599 -0.045082  0.000547 -0.011560  0.018126 -0.029461 -0.008463  0.006419  0.050799 -0.014959  0.006176  0.026096 -0.030809  0.048055  0.114093 -0.040826 -0.011457  0.005419 -0.003599  0.025154 -0.059142  0.013753 -0.010399  0.038709  0.051449  0.035413  0.035031 -0.017027  0.026360 -0.014490 -0.050032  0.003219  0.054121  0.039723 -0.039127 

0.00.076.718 I llama_perf_context_print:        load time =      47.72 ms
0.00.076.719 I llama_perf_context_print: prompt eval time =       5.10 ms /     9 tokens (    0.57 ms per token,  1766.09 tokens per second)
0.00.076.720 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.076.722 I llama_perf_context_print:       total time =       5.98 ms /    10 tokens
0.00.076.887 I ggml_metal_free: deallocating

real	0m0.288s
user	0m0.053s
sys	0m0.037s
```
- q8_0:
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.041 I build: 4586 (2711d021) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.581 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.012.325 I llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.012.329 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.012.330 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.012.331 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.012.331 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.012.331 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.012.332 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.012.333 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.012.333 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.012.333 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.012.334 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.012.334 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.012.337 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.012.337 I llama_model_loader: - kv  11:                      bert.attention.causal bool             = false
0.00.012.337 I llama_model_loader: - kv  12:                          bert.pooling_type u32              = 2
0.00.012.338 I llama_model_loader: - kv  13:            tokenizer.ggml.token_type_count u32              = 2
0.00.012.338 I llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = bert
0.00.012.338 I llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.014.942 I llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.015.590 I llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.015.592 I llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.015.592 I llama_model_loader: - kv  19:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.015.592 I llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 0
0.00.015.592 I llama_model_loader: - kv  21:               tokenizer.ggml.mask_token_id u32              = 103
0.00.015.593 I llama_model_loader: - kv  22:               general.quantization_version u32              = 2
0.00.015.593 I llama_model_loader: - kv  23:                          general.file_type u32              = 7
0.00.015.593 I llama_model_loader: - type  f32:  124 tensors
0.00.015.594 I llama_model_loader: - type q8_0:   73 tensors
0.00.015.594 I print_info: file format = GGUF V3 (latest)
0.00.015.595 I print_info: file type   = Q8_0
0.00.015.595 I print_info: file size   = 34.38 MiB (8.68 BPW) 
0.00.017.973 I load: special tokens cache size = 5
0.00.019.268 I load: token to piece cache size = 0.2032 MB
0.00.019.271 I print_info: arch             = bert
0.00.019.271 I print_info: vocab_only       = 0
0.00.019.271 I print_info: n_ctx_train      = 512
0.00.019.271 I print_info: n_embd           = 384
0.00.019.271 I print_info: n_layer          = 12
0.00.019.274 I print_info: n_head           = 12
0.00.019.275 I print_info: n_head_kv        = 12
0.00.019.275 I print_info: n_rot            = 32
0.00.019.275 I print_info: n_swa            = 0
0.00.019.275 I print_info: n_embd_head_k    = 32
0.00.019.277 I print_info: n_embd_head_v    = 32
0.00.019.278 I print_info: n_gqa            = 1
0.00.019.278 I print_info: n_embd_k_gqa     = 384
0.00.019.279 I print_info: n_embd_v_gqa     = 384
0.00.019.279 I print_info: f_norm_eps       = 1.0e-12
0.00.019.280 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.019.280 I print_info: f_clamp_kqv      = 0.0e+00
0.00.019.280 I print_info: f_max_alibi_bias = 0.0e+00
0.00.019.280 I print_info: f_logit_scale    = 0.0e+00
0.00.019.280 I print_info: n_ff             = 1536
0.00.019.281 I print_info: n_expert         = 0
0.00.019.281 I print_info: n_expert_used    = 0
0.00.019.281 I print_info: causal attn      = 0
0.00.019.281 I print_info: pooling type     = 2
0.00.019.281 I print_info: rope type        = 2
0.00.019.282 I print_info: rope scaling     = linear
0.00.019.282 I print_info: freq_base_train  = 10000.0
0.00.019.282 I print_info: freq_scale_train = 1
0.00.019.282 I print_info: n_ctx_orig_yarn  = 512
0.00.019.283 I print_info: rope_finetuned   = unknown
0.00.019.283 I print_info: ssm_d_conv       = 0
0.00.019.283 I print_info: ssm_d_inner      = 0
0.00.019.285 I print_info: ssm_d_state      = 0
0.00.019.285 I print_info: ssm_dt_rank      = 0
0.00.019.285 I print_info: ssm_dt_b_c_rms   = 0
0.00.019.285 I print_info: model type       = 33M
0.00.019.286 I print_info: model params     = 33.21 M
0.00.019.286 I print_info: general.name     = Bge Small
0.00.019.286 I print_info: vocab type       = WPM
0.00.019.287 I print_info: n_vocab          = 30522
0.00.019.287 I print_info: n_merges         = 0
0.00.019.287 I print_info: BOS token        = 101 '[CLS]'
0.00.019.287 I print_info: UNK token        = 100 '[UNK]'
0.00.019.289 I print_info: SEP token        = 102 '[SEP]'
0.00.019.289 I print_info: PAD token        = 0 '[PAD]'
0.00.019.289 I print_info: MASK token       = 103 '[MASK]'
0.00.019.289 I print_info: LF token         = 0 '[PAD]'
0.00.019.289 I print_info: max token length = 21
0.00.021.178 I load_tensors: offloading 12 repeating layers to GPU
0.00.021.179 I load_tensors: offloading output layer to GPU
0.00.021.179 I load_tensors: offloaded 13/13 layers to GPU
0.00.021.186 I load_tensors: Metal_Mapped model buffer size =    21.75 MiB
0.00.021.186 I load_tensors:   CPU_Mapped model buffer size =    12.63 MiB
0.00.021.368 I llama_init_from_model: n_seq_max     = 1
0.00.021.369 I llama_init_from_model: n_ctx         = 512
0.00.021.369 I llama_init_from_model: n_ctx_per_seq = 512
0.00.021.370 I llama_init_from_model: n_batch       = 2048
0.00.021.370 I llama_init_from_model: n_ubatch      = 2048
0.00.021.370 I llama_init_from_model: flash_attn    = 0
0.00.021.370 I llama_init_from_model: freq_base     = 10000.0
0.00.021.371 I llama_init_from_model: freq_scale    = 1
0.00.021.371 I ggml_metal_init: allocating
0.00.021.382 I ggml_metal_init: found device: Apple M4
0.00.021.386 I ggml_metal_init: picking default device: Apple M4
0.00.021.921 I ggml_metal_init: using embedded metal library
0.00.024.497 I ggml_metal_init: GPU name:   Apple M4
0.00.024.498 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.024.499 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.024.499 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.024.499 I ggml_metal_init: simdgroup reduction   = true
0.00.024.500 I ggml_metal_init: simdgroup matrix mul. = true
0.00.024.500 I ggml_metal_init: has residency sets    = true
0.00.024.500 I ggml_metal_init: has bfloat            = true
0.00.024.500 I ggml_metal_init: use bfloat            = true
0.00.024.501 I ggml_metal_init: hasUnifiedMemory      = true
0.00.024.502 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.034.830 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.035.463 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.035.468 I llama_init_from_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.035.473 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.036.496 I llama_init_from_model:      Metal compute buffer size =    16.00 MiB
0.00.036.498 I llama_init_from_model:        CPU compute buffer size =     2.51 MiB
0.00.036.498 I llama_init_from_model: graph nodes  = 429
0.00.036.498 I llama_init_from_model: graph splits = 2
0.00.036.500 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.036.500 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.040.731 I 
0.00.040.759 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.041.319 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.044994 -0.019858  0.007986 -0.001410  0.001531 -0.036276  0.109404  0.043235  0.092836 -0.015201  0.005971 -0.036330 -0.018159  0.015015  0.017667  0.014510 -0.011792  0.011812 -0.084764 -0.008081  0.091650 -0.017638 -0.061084 -0.025066  0.026879  0.076180  0.028256 -0.014273  0.017420 -0.033553 -0.037520 -0.018280  0.068843 -0.010129 -0.024896  0.071968 -0.046406  0.011471 -0.050333  0.048931  0.032317 -0.012107  0.021812  0.049796  0.010366  0.005264 -0.028665  0.008581 -0.018508 -0.052358 -0.046410  0.029426 -0.034989  0.053289 -0.069849  0.043767  0.029291  0.046759  0.073664 -0.043090  0.075213  0.038641 -0.180496  0.081648  0.043389 -0.065615 -0.059878 -0.017387  0.006913  0.005155  0.017165 -0.026679  0.064347  0.112564  0.034871 -0.067783  0.027124 -0.066989 -0.034296 -0.033344  0.033189  0.014325 -0.004253 -0.036945 -0.052008  0.054151 -0.001715 -0.038112  0.063016  0.028668 -0.041665 -0.028879 -0.039263  0.036879  0.007718 -0.015657 -0.036270  0.018699  0.029792  0.344948 -0.043880  0.056487  0.018022 -0.021448 -0.065281  0.000253 -0.037910 -0.029663 -0.008828 -0.020643  0.001162 -0.003510  0.003091  0.018576 -0.009637  0.025316  0.049298 -0.000937  0.051568 -0.042363 -0.031550  0.023315  0.030044 -0.023267 -0.045483 -0.079644  0.114489  0.047503  0.027208 -0.041477  0.067717 -0.022845  0.010232 -0.033486 -0.017199  0.044319  0.022956  0.052194  0.008122  0.008112  0.009951 -0.074643 -0.065035 -0.025938 -0.040806 -0.024533  0.027409  0.006078  0.027693  0.052243 -0.036587  0.058682  0.001467  0.032191 -0.019964 -0.021840  0.041805 -0.058801  0.019348  0.042626  0.043999  0.040911 -0.021909  0.028025 -0.022801  0.005520 -0.041266 -0.000085  0.024325  0.001532  0.044802 -0.022888  0.043056  0.065030  0.056292  0.037666 -0.001004  0.047521  0.045624 -0.008614  0.062270 -0.073313 -0.011718  0.032708  0.023348  0.014650 -0.033924  0.001158 -0.015872 -0.018974  0.047982  0.110740  0.029177  0.030881 -0.012325 -0.057836  0.006939  0.004378 -0.011864 -0.051754 -0.003018 -0.017766 -0.020051 -0.041018  0.009118 -0.058568  0.050632  0.051795 -0.008968 -0.040668 -0.014765 -0.024320 -0.015980  0.005823  0.007147 -0.027335  0.016275  0.030734  0.002342  0.023167 -0.022169 -0.098059 -0.050901 -0.277240 -0.015100 -0.061566 -0.026219  0.017141 -0.010285 -0.017522  0.034725  0.047897 -0.016594  0.015489 -0.024132  0.048613 -0.004993 -0.000001 -0.060860 -0.068346 -0.059622 -0.036157  0.043605 -0.056353  0.014713  0.000292 -0.058627 -0.010995  0.012274  0.151597  0.127025 -0.012582  0.042765 -0.025227  0.013629 -0.000381 -0.150686  0.043974  0.004714 -0.036955 -0.029586 -0.019853 -0.034063  0.009522  0.034004 -0.048909 -0.051765 -0.017243 -0.025080  0.047588  0.050981 -0.017553 -0.056746  0.024054 -0.006285  0.011003  0.038719  0.008083 -0.008595 -0.106361 -0.027365 -0.096966  0.025294 -0.011096  0.091579  0.056180  0.004494  0.028066  0.001727 -0.051388 -0.039497 -0.013817 -0.045907 -0.015238  0.002920 -0.043938 -0.077716  0.066102 -0.006195 -0.000885 -0.013768  0.071562  0.023826 -0.036363  0.007848  0.001487 -0.032916  0.016336  0.037378  0.000625 -0.052389  0.021549 -0.039695 -0.000026  0.013496  0.020436 -0.057513  0.005900 -0.049906 -0.267546  0.038686 -0.067509  0.037450 -0.011471  0.041547 -0.015904  0.051347 -0.072313  0.012571  0.024808 -0.007420  0.082069  0.028175 -0.021956  0.040736 -0.003945 -0.074192 -0.014923  0.020417  0.002644  0.023207  0.197144 -0.043068 -0.025876 -0.004886 -0.018582  0.073936  0.001341 -0.031663 -0.037140 -0.044262  0.000049 -0.010741  0.018225 -0.028045 -0.008828  0.006047  0.050183 -0.015017  0.007037  0.025894 -0.030628  0.048075  0.112795 -0.039982 -0.011695  0.005039 -0.002800  0.025072 -0.059720  0.014182 -0.010019  0.038100  0.051088  0.034726  0.035947 -0.017035  0.027243 -0.015391 -0.051195  0.003744  0.053928  0.040014 -0.039140 

0.00.045.786 I llama_perf_context_print:        load time =      31.15 ms
0.00.045.790 I llama_perf_context_print: prompt eval time =       4.34 ms /     9 tokens (    0.48 ms per token,  2071.82 tokens per second)
0.00.045.791 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.045.791 I llama_perf_context_print:       total time =       5.06 ms /    10 tokens
0.00.045.983 I ggml_metal_free: deallocating

real	0m0.059s
user	0m0.031s
sys	0m0.017s
```
### rerank_tiny

Rerank Tiny (Jina):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/rerank-tiny/ggml-model-f16.gguf -p 'what is panda?</s></s>hi\nwhat is panda?</s></s>it'\''s a bear\nwhat is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.' -ngl 99 -c 0 --pooling rank --embd-normalize -1 --verbose-prompt
0.00.000.195 I build: 4586 (2711d021) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.023.725 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.036.373 I llama_model_loader: loaded meta data with 28 key-value pairs and 70 tensors from ../models-mnt/rerank-tiny/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.036.379 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.036.381 I llama_model_loader: - kv   0:                       general.architecture str              = jina-bert-v2
0.00.036.389 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.036.390 I llama_model_loader: - kv   2:                               general.name str              = Jina Bert Implementation
0.00.036.391 I llama_model_loader: - kv   3:                       general.organization str              = Jinaai
0.00.036.391 I llama_model_loader: - kv   4:                         general.size_label str              = 33M
0.00.036.393 I llama_model_loader: - kv   5:                   jina-bert-v2.block_count u32              = 4
0.00.036.393 I llama_model_loader: - kv   6:                jina-bert-v2.context_length u32              = 8192
0.00.036.394 I llama_model_loader: - kv   7:              jina-bert-v2.embedding_length u32              = 384
0.00.036.395 I llama_model_loader: - kv   8:           jina-bert-v2.feed_forward_length u32              = 1536
0.00.036.395 I llama_model_loader: - kv   9:          jina-bert-v2.attention.head_count u32              = 12
0.00.036.398 I llama_model_loader: - kv  10:  jina-bert-v2.attention.layer_norm_epsilon f32              = 0.000000
0.00.036.399 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.036.400 I llama_model_loader: - kv  12:              jina-bert-v2.attention.causal bool             = false
0.00.036.400 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.036.401 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = jina-v1-en
0.00.042.931 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,61056]   = ["<s>", "<pad>", "</s>", "<unk>", "<m...
0.00.045.099 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,61056]   = [3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, ...
0.00.049.728 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,39382]   = ["t h", "i n", "a n", "e r", "th e", ...
0.00.049.730 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.049.731 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
0.00.049.731 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 3
0.00.049.732 I llama_model_loader: - kv  21:          tokenizer.ggml.seperator_token_id u32              = 2
0.00.049.732 I llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 1
0.00.049.732 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 4
0.00.049.733 I llama_model_loader: - kv  24:            tokenizer.ggml.token_type_count u32              = 2
0.00.049.733 I llama_model_loader: - kv  25:               tokenizer.ggml.add_bos_token bool             = true
0.00.049.733 I llama_model_loader: - kv  26:               tokenizer.ggml.add_eos_token bool             = true
0.00.049.734 I llama_model_loader: - kv  27:               general.quantization_version u32              = 2
0.00.049.734 I llama_model_loader: - type  f32:   40 tensors
0.00.049.734 I llama_model_loader: - type  f16:   30 tensors
0.00.049.735 I print_info: file format = GGUF V3 (latest)
0.00.049.736 I print_info: file type   = F16
0.00.049.737 I print_info: file size   = 62.78 MiB (16.01 BPW) 
0.00.054.161 W load: empty token at index 5
0.00.059.821 W load: model vocab missing newline token, using special_pad_id instead
0.00.061.472 W load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
0.00.061.510 I load: special tokens cache size = 5
0.00.334.835 I load: token to piece cache size = 1.5060 MB
0.00.334.844 I print_info: arch             = jina-bert-v2
0.00.334.845 I print_info: vocab_only       = 0
0.00.334.853 I print_info: n_ctx_train      = 8192
0.00.334.853 I print_info: n_embd           = 384
0.00.334.853 I print_info: n_layer          = 4
0.00.334.857 I print_info: n_head           = 12
0.00.334.858 I print_info: n_head_kv        = 12
0.00.334.858 I print_info: n_rot            = 32
0.00.334.858 I print_info: n_swa            = 0
0.00.334.858 I print_info: n_embd_head_k    = 32
0.00.334.858 I print_info: n_embd_head_v    = 32
0.00.334.859 I print_info: n_gqa            = 1
0.00.334.859 I print_info: n_embd_k_gqa     = 384
0.00.334.859 I print_info: n_embd_v_gqa     = 384
0.00.334.860 I print_info: f_norm_eps       = 1.0e-12
0.00.334.861 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.334.861 I print_info: f_clamp_kqv      = 0.0e+00
0.00.334.861 I print_info: f_max_alibi_bias = 8.0e+00
0.00.334.861 I print_info: f_logit_scale    = 0.0e+00
0.00.334.861 I print_info: n_ff             = 1536
0.00.334.862 I print_info: n_expert         = 0
0.00.334.862 I print_info: n_expert_used    = 0
0.00.334.862 I print_info: causal attn      = 0
0.00.334.862 I print_info: pooling type     = -1
0.00.334.862 I print_info: rope type        = -1
0.00.334.864 I print_info: rope scaling     = linear
0.00.334.864 I print_info: freq_base_train  = 10000.0
0.00.334.864 I print_info: freq_scale_train = 1
0.00.334.864 I print_info: n_ctx_orig_yarn  = 8192
0.00.334.867 I print_info: rope_finetuned   = unknown
0.00.334.867 I print_info: ssm_d_conv       = 0
0.00.334.867 I print_info: ssm_d_inner      = 0
0.00.334.867 I print_info: ssm_d_state      = 0
0.00.334.867 I print_info: ssm_dt_rank      = 0
0.00.334.868 I print_info: ssm_dt_b_c_rms   = 0
0.00.334.869 I print_info: model type       = 33M
0.00.334.869 I print_info: model params     = 32.90 M
0.00.334.869 I print_info: general.name     = Jina Bert Implementation
0.00.334.870 I print_info: vocab type       = BPE
0.00.334.870 I print_info: n_vocab          = 61056
0.00.334.870 I print_info: n_merges         = 39382
0.00.334.871 I print_info: BOS token        = 0 '<s>'
0.00.334.871 I print_info: EOS token        = 2 '</s>'
0.00.334.871 I print_info: UNK token        = 3 '<unk>'
0.00.334.871 I print_info: SEP token        = 2 '</s>'
0.00.334.871 I print_info: PAD token        = 1 '<pad>'
0.00.334.872 I print_info: MASK token       = 4 '<mask>'
0.00.334.872 I print_info: EOG token        = 2 '</s>'
0.00.334.872 I print_info: max token length = 45
0.00.335.915 I load_tensors: offloading 4 repeating layers to GPU
0.00.335.916 I load_tensors: offloading output layer to GPU
0.00.335.916 I load_tensors: offloaded 5/5 layers to GPU
0.00.335.934 I load_tensors: Metal_Mapped model buffer size =    18.06 MiB
0.00.335.935 I load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
0.00.336.097 I llama_init_from_model: n_seq_max     = 1
0.00.336.098 I llama_init_from_model: n_ctx         = 8192
0.00.336.098 I llama_init_from_model: n_ctx_per_seq = 8192
0.00.336.098 I llama_init_from_model: n_batch       = 2048
0.00.336.099 I llama_init_from_model: n_ubatch      = 2048
0.00.336.099 I llama_init_from_model: flash_attn    = 0
0.00.336.099 I llama_init_from_model: freq_base     = 10000.0
0.00.336.099 I llama_init_from_model: freq_scale    = 1
0.00.336.100 I ggml_metal_init: allocating
0.00.336.103 I ggml_metal_init: found device: Apple M4
0.00.336.106 I ggml_metal_init: picking default device: Apple M4
0.00.336.615 I ggml_metal_init: using embedded metal library
0.00.339.191 I ggml_metal_init: GPU name:   Apple M4
0.00.339.193 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.339.193 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.339.194 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.339.194 I ggml_metal_init: simdgroup reduction   = true
0.00.339.194 I ggml_metal_init: simdgroup matrix mul. = true
0.00.339.194 I ggml_metal_init: has residency sets    = true
0.00.339.194 I ggml_metal_init: has bfloat            = true
0.00.339.195 I ggml_metal_init: use bfloat            = true
0.00.339.195 I ggml_metal_init: hasUnifiedMemory      = true
0.00.339.196 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.348.147 I llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 4, can_shift = 1
0.00.351.111 I llama_kv_cache_init:      Metal KV buffer size =    48.00 MiB
0.00.351.113 I llama_init_from_model: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB
0.00.351.115 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.357.299 I llama_init_from_model:      Metal compute buffer size =   220.01 MiB
0.00.357.301 I llama_init_from_model:        CPU compute buffer size =    22.02 MiB
0.00.357.301 I llama_init_from_model: graph nodes  = 154
0.00.357.301 I llama_init_from_model: graph splits = 2
0.00.357.303 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 8192
0.00.357.303 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.363.332 I 
0.00.363.367 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.363.467 I main: prompt 0: 'what is panda?</s></s>hi'
0.00.363.468 I main: number of tokens in prompt = 9
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 23233 -> 'hi'
     2 -> '</s>'


0.00.363.471 I main: prompt 1: 'what is panda?</s></s>it's a bear'
0.00.363.472 I main: number of tokens in prompt = 13
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21153 -> 'it'
    37 -> '''
    87 -> 's'
    69 -> 'a'
 25706 -> 'bear'
     2 -> '</s>'


0.00.363.475 I main: prompt 2: 'what is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'
0.00.363.476 I main: number of tokens in prompt = 40
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21215 -> 'he'
 28390 -> 'giant'
 49990 -> 'panda'
    38 -> '('
 21163 -> 'il'
 26237 -> 'uro'
 21223 -> 'po'
 23179 -> 'da'
 36906 -> 'melan'
 26791 -> 'ole'
    89 -> 'u'
 21402 -> 'ca'
 21686 -> '),'
 23314 -> 'sometimes'
 22517 -> 'called'
    69 -> 'a'
 49990 -> 'panda'
 25706 -> 'bear'
 21142 -> 'or'
 22810 -> 'simply'
 49990 -> 'panda'
    42 -> ','
 21152 -> 'is'
    69 -> 'a'
 25706 -> 'bear'
 25677 -> 'species'
 28930 -> 'ende'
 22024 -> 'mic'
 21148 -> 'to'
    76 -> 'h'
 22344 -> 'ina'
    44 -> '.'
     2 -> '</s>'


0.00.363.975 I batch_decode: n_tokens = 62, n_seq = 3

rerank score 0:    0.023
rerank score 1:    0.024
rerank score 2:    0.199

0.00.367.440 I llama_perf_context_print:        load time =     339.60 ms
0.00.367.441 I llama_perf_context_print: prompt eval time =       3.44 ms /    62 tokens (    0.06 ms per token, 18023.26 tokens per second)
0.00.367.442 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.367.443 I llama_perf_context_print:       total time =       4.11 ms /    63 tokens
0.00.367.693 I ggml_metal_free: deallocating

real	0m1.202s
user	0m0.340s
sys	0m0.045s
  - rerank score 0 @ 0.023 OK
  - rerank score 1 @ 0.024 OK
  - rerank score 2 @ 0.199 OK
```
### pythia_1_4b

Pythia 1.4B:
- status: 0
- perplexity:
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
- imatrix:
```
Final estimate: PPL = 10.1498 +/- 3.22650
```
- f16: 
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.147 I build: 4586 (2711d021) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.319 I main: llama backend init
0.00.000.325 I main: load the model and apply lora adapter, if any
0.00.046.163 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.059.132 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.059.151 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.059.155 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.059.155 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.059.156 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.059.157 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.059.157 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.059.159 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.059.159 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.059.160 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.059.161 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.059.161 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.059.162 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.059.163 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.059.168 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.059.173 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.059.173 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.066.429 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.068.745 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.076.847 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.076.854 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.076.855 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.076.855 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.076.856 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.076.857 I llama_model_loader: - type  f32:  194 tensors
0.00.076.858 I llama_model_loader: - type  f16:   98 tensors
0.00.076.859 I print_info: file format = GGUF V3 (latest)
0.00.076.864 I print_info: file type   = all F32 (guessed)
0.00.076.865 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.093.478 I load: special tokens cache size = 25
0.00.103.033 I load: token to piece cache size = 0.2984 MB
0.00.103.037 I print_info: arch             = gptneox
0.00.103.037 I print_info: vocab_only       = 0
0.00.103.038 I print_info: n_ctx_train      = 2048
0.00.103.038 I print_info: n_embd           = 2048
0.00.103.038 I print_info: n_layer          = 24
0.00.103.042 I print_info: n_head           = 16
0.00.103.043 I print_info: n_head_kv        = 16
0.00.103.044 I print_info: n_rot            = 32
0.00.103.044 I print_info: n_swa            = 0
0.00.103.044 I print_info: n_embd_head_k    = 128
0.00.103.044 I print_info: n_embd_head_v    = 128
0.00.103.047 I print_info: n_gqa            = 1
0.00.103.048 I print_info: n_embd_k_gqa     = 2048
0.00.103.049 I print_info: n_embd_v_gqa     = 2048
0.00.103.050 I print_info: f_norm_eps       = 1.0e-05
0.00.103.051 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.103.051 I print_info: f_clamp_kqv      = 0.0e+00
0.00.103.051 I print_info: f_max_alibi_bias = 0.0e+00
0.00.103.051 I print_info: f_logit_scale    = 0.0e+00
0.00.103.052 I print_info: n_ff             = 8192
0.00.103.052 I print_info: n_expert         = 0
0.00.103.053 I print_info: n_expert_used    = 0
0.00.103.053 I print_info: causal attn      = 1
0.00.103.053 I print_info: pooling type     = 0
0.00.103.053 I print_info: rope type        = 2
0.00.103.060 I print_info: rope scaling     = linear
0.00.103.063 I print_info: freq_base_train  = 10000.0
0.00.103.065 I print_info: freq_scale_train = 1
0.00.103.066 I print_info: n_ctx_orig_yarn  = 2048
0.00.103.066 I print_info: rope_finetuned   = unknown
0.00.103.066 I print_info: ssm_d_conv       = 0
0.00.103.066 I print_info: ssm_d_inner      = 0
0.00.103.067 I print_info: ssm_d_state      = 0
0.00.103.067 I print_info: ssm_dt_rank      = 0
0.00.103.067 I print_info: ssm_dt_b_c_rms   = 0
0.00.103.067 I print_info: model type       = 1.4B
0.00.103.068 I print_info: model params     = 1.41 B
0.00.103.069 I print_info: general.name     = 1.4B
0.00.103.069 I print_info: vocab type       = BPE
0.00.103.070 I print_info: n_vocab          = 50304
0.00.103.070 I print_info: n_merges         = 50009
0.00.103.070 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.103.070 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.103.071 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.103.071 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.103.071 I print_info: LF token         = 128 'Ä'
0.00.103.072 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.103.072 I print_info: max token length = 1024
0.00.146.661 I load_tensors: offloading 24 repeating layers to GPU
0.00.146.665 I load_tensors: offloading output layer to GPU
0.00.146.665 I load_tensors: offloaded 25/25 layers to GPU
0.00.146.692 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.146.694 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.147.062 I llama_init_from_model: n_seq_max     = 1
0.00.147.064 I llama_init_from_model: n_ctx         = 2048
0.00.147.064 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.147.065 I llama_init_from_model: n_batch       = 2048
0.00.147.065 I llama_init_from_model: n_ubatch      = 512
0.00.147.065 I llama_init_from_model: flash_attn    = 0
0.00.147.066 I llama_init_from_model: freq_base     = 10000.0
0.00.147.066 I llama_init_from_model: freq_scale    = 1
0.00.147.068 I ggml_metal_init: allocating
0.00.147.107 I ggml_metal_init: found device: Apple M4
0.00.147.111 I ggml_metal_init: picking default device: Apple M4
0.00.147.732 I ggml_metal_init: using embedded metal library
0.00.169.024 I ggml_metal_init: GPU name:   Apple M4
0.00.169.026 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.169.026 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.169.027 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.169.027 I ggml_metal_init: simdgroup reduction   = true
0.00.169.027 I ggml_metal_init: simdgroup matrix mul. = true
0.00.169.027 I ggml_metal_init: has residency sets    = true
0.00.169.027 I ggml_metal_init: has bfloat            = true
0.00.169.027 I ggml_metal_init: use bfloat            = true
0.00.169.028 I ggml_metal_init: hasUnifiedMemory      = true
0.00.169.029 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.196.279 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.224.150 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.224.155 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.224.176 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.227.615 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.227.617 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.227.618 I llama_init_from_model: graph nodes  = 967
0.00.227.618 I llama_init_from_model: graph splits = 2
0.00.227.623 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.227.752 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.227.752 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.293.610 I main: llama threadpool init, n_threads = 4
0.00.293.653 I 
0.00.293.685 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.293.686 I 
0.00.293.861 I sampler seed: 1234
0.00.293.866 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.293.890 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.293.892 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.293.892 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.123.865 I llama_perf_sampler_print:    sampling time =       1.16 ms /    71 runs   (    0.02 ms per token, 61154.18 tokens per second)
0.02.123.866 I llama_perf_context_print:        load time =     246.42 ms
0.02.123.866 I llama_perf_context_print: prompt eval time =      43.65 ms /     7 tokens (    6.24 ms per token,   160.35 tokens per second)
0.02.123.868 I llama_perf_context_print:        eval time =    1783.54 ms /    63 runs   (   28.31 ms per token,    35.32 tokens per second)
0.02.123.868 I llama_perf_context_print:       total time =    1831.27 ms /    70 tokens
0.02.124.159 I ggml_metal_free: deallocating

real	0m2.482s
user	0m0.135s
sys	0m0.141s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.728 I build: 4586 (2711d021) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.027.132 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.041.659 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.041.667 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.041.677 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.041.678 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.041.679 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.041.680 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.041.680 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.041.682 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.041.683 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.041.684 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.041.687 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.041.687 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.041.688 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.041.689 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.041.692 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.041.694 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.041.695 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.050.310 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.052.440 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.059.198 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.059.201 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.059.201 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.059.202 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.059.202 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.059.203 I llama_model_loader: - type  f32:  194 tensors
0.00.059.203 I llama_model_loader: - type  f16:   98 tensors
0.00.059.204 I print_info: file format = GGUF V3 (latest)
0.00.059.205 I print_info: file type   = all F32 (guessed)
0.00.059.208 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.072.239 I load: special tokens cache size = 25
0.00.080.310 I load: token to piece cache size = 0.2984 MB
0.00.080.313 I print_info: arch             = gptneox
0.00.080.313 I print_info: vocab_only       = 0
0.00.080.313 I print_info: n_ctx_train      = 2048
0.00.080.313 I print_info: n_embd           = 2048
0.00.080.314 I print_info: n_layer          = 24
0.00.080.318 I print_info: n_head           = 16
0.00.080.319 I print_info: n_head_kv        = 16
0.00.080.319 I print_info: n_rot            = 32
0.00.080.319 I print_info: n_swa            = 0
0.00.080.320 I print_info: n_embd_head_k    = 128
0.00.080.320 I print_info: n_embd_head_v    = 128
0.00.080.321 I print_info: n_gqa            = 1
0.00.080.322 I print_info: n_embd_k_gqa     = 2048
0.00.080.322 I print_info: n_embd_v_gqa     = 2048
0.00.080.323 I print_info: f_norm_eps       = 1.0e-05
0.00.080.323 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.080.323 I print_info: f_clamp_kqv      = 0.0e+00
0.00.080.324 I print_info: f_max_alibi_bias = 0.0e+00
0.00.080.324 I print_info: f_logit_scale    = 0.0e+00
0.00.080.325 I print_info: n_ff             = 8192
0.00.080.325 I print_info: n_expert         = 0
0.00.080.325 I print_info: n_expert_used    = 0
0.00.080.325 I print_info: causal attn      = 1
0.00.080.325 I print_info: pooling type     = 0
0.00.080.325 I print_info: rope type        = 2
0.00.080.328 I print_info: rope scaling     = linear
0.00.080.328 I print_info: freq_base_train  = 10000.0
0.00.080.329 I print_info: freq_scale_train = 1
0.00.080.329 I print_info: n_ctx_orig_yarn  = 2048
0.00.080.329 I print_info: rope_finetuned   = unknown
0.00.080.329 I print_info: ssm_d_conv       = 0
0.00.080.329 I print_info: ssm_d_inner      = 0
0.00.080.330 I print_info: ssm_d_state      = 0
0.00.080.330 I print_info: ssm_dt_rank      = 0
0.00.080.330 I print_info: ssm_dt_b_c_rms   = 0
0.00.080.330 I print_info: model type       = 1.4B
0.00.080.331 I print_info: model params     = 1.41 B
0.00.080.331 I print_info: general.name     = 1.4B
0.00.080.335 I print_info: vocab type       = BPE
0.00.080.335 I print_info: n_vocab          = 50304
0.00.080.335 I print_info: n_merges         = 50009
0.00.080.336 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.080.336 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.080.336 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.080.336 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.080.337 I print_info: LF token         = 128 'Ä'
0.00.080.337 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.080.337 I print_info: max token length = 1024
0.01.486.495 I load_tensors: offloading 24 repeating layers to GPU
0.01.486.499 I load_tensors: offloading output layer to GPU
0.01.486.499 I load_tensors: offloaded 25/25 layers to GPU
0.01.486.523 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.486.525 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.01.487.356 I llama_init_from_model: n_seq_max     = 1
0.01.487.357 I llama_init_from_model: n_ctx         = 128
0.01.487.357 I llama_init_from_model: n_ctx_per_seq = 128
0.01.487.358 I llama_init_from_model: n_batch       = 128
0.01.487.358 I llama_init_from_model: n_ubatch      = 128
0.01.487.359 I llama_init_from_model: flash_attn    = 0
0.01.487.360 I llama_init_from_model: freq_base     = 10000.0
0.01.487.360 I llama_init_from_model: freq_scale    = 1
0.01.487.361 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.487.362 I ggml_metal_init: allocating
0.01.487.419 I ggml_metal_init: found device: Apple M4
0.01.487.427 I ggml_metal_init: picking default device: Apple M4
0.01.488.503 I ggml_metal_init: using embedded metal library
0.01.492.455 I ggml_metal_init: GPU name:   Apple M4
0.01.492.457 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.492.458 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.492.458 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.492.459 I ggml_metal_init: simdgroup reduction   = true
0.01.492.459 I ggml_metal_init: simdgroup matrix mul. = true
0.01.492.459 I ggml_metal_init: has residency sets    = true
0.01.492.459 I ggml_metal_init: has bfloat            = true
0.01.492.460 I ggml_metal_init: use bfloat            = true
0.01.492.460 I ggml_metal_init: hasUnifiedMemory      = true
0.01.492.461 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.503.680 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.505.539 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.505.541 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.505.555 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.507.257 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.507.258 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.507.258 I llama_init_from_model: graph nodes  = 967
0.01.507.259 I llama_init_from_model: graph splits = 2
0.01.507.260 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.507.260 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.543.136 I 
0.01.543.179 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.543.200 I perplexity: tokenizing the input ..
0.01.548.408 I perplexity: tokenization took 5.206 ms
0.01.548.430 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.667.835 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.669.183 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.669.198 I llama_perf_context_print:        load time =    1515.99 ms
0.01.669.200 I llama_perf_context_print: prompt eval time =     119.05 ms /   128 tokens (    0.93 ms per token,  1075.14 tokens per second)
0.01.669.200 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.669.201 I llama_perf_context_print:       total time =     126.06 ms /   129 tokens
0.01.669.629 I ggml_metal_free: deallocating

real	0m1.896s
user	0m0.101s
sys	0m0.311s
```
- q8_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.049 I build: 4586 (2711d021) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.091 I main: llama backend init
0.00.000.094 I main: load the model and apply lora adapter, if any
0.00.010.277 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.024.845 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.024.851 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.024.852 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.024.853 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.024.853 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.024.854 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.024.854 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.024.855 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.024.855 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.024.856 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.024.856 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.024.857 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.024.857 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.024.858 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.024.861 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.024.861 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.024.861 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.028.779 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.029.808 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.033.646 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.033.647 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.033.648 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.033.648 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.033.648 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.033.649 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.033.650 I llama_model_loader: - type  f32:  194 tensors
0.00.033.651 I llama_model_loader: - type q8_0:   98 tensors
0.00.033.651 I print_info: file format = GGUF V3 (latest)
0.00.033.652 I print_info: file type   = Q8_0
0.00.033.653 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.041.848 I load: special tokens cache size = 25
0.00.047.857 I load: token to piece cache size = 0.2984 MB
0.00.047.863 I print_info: arch             = gptneox
0.00.047.863 I print_info: vocab_only       = 0
0.00.047.864 I print_info: n_ctx_train      = 2048
0.00.047.864 I print_info: n_embd           = 2048
0.00.047.864 I print_info: n_layer          = 24
0.00.047.870 I print_info: n_head           = 16
0.00.047.871 I print_info: n_head_kv        = 16
0.00.047.871 I print_info: n_rot            = 32
0.00.047.871 I print_info: n_swa            = 0
0.00.047.872 I print_info: n_embd_head_k    = 128
0.00.047.872 I print_info: n_embd_head_v    = 128
0.00.047.872 I print_info: n_gqa            = 1
0.00.047.873 I print_info: n_embd_k_gqa     = 2048
0.00.047.874 I print_info: n_embd_v_gqa     = 2048
0.00.047.874 I print_info: f_norm_eps       = 1.0e-05
0.00.047.874 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.047.875 I print_info: f_clamp_kqv      = 0.0e+00
0.00.047.875 I print_info: f_max_alibi_bias = 0.0e+00
0.00.047.875 I print_info: f_logit_scale    = 0.0e+00
0.00.047.876 I print_info: n_ff             = 8192
0.00.047.876 I print_info: n_expert         = 0
0.00.047.876 I print_info: n_expert_used    = 0
0.00.047.876 I print_info: causal attn      = 1
0.00.047.876 I print_info: pooling type     = 0
0.00.047.876 I print_info: rope type        = 2
0.00.047.877 I print_info: rope scaling     = linear
0.00.047.877 I print_info: freq_base_train  = 10000.0
0.00.047.877 I print_info: freq_scale_train = 1
0.00.047.877 I print_info: n_ctx_orig_yarn  = 2048
0.00.047.878 I print_info: rope_finetuned   = unknown
0.00.047.878 I print_info: ssm_d_conv       = 0
0.00.047.878 I print_info: ssm_d_inner      = 0
0.00.047.878 I print_info: ssm_d_state      = 0
0.00.047.878 I print_info: ssm_dt_rank      = 0
0.00.047.878 I print_info: ssm_dt_b_c_rms   = 0
0.00.047.882 I print_info: model type       = 1.4B
0.00.047.882 I print_info: model params     = 1.41 B
0.00.047.882 I print_info: general.name     = 1.4B
0.00.047.883 I print_info: vocab type       = BPE
0.00.047.884 I print_info: n_vocab          = 50304
0.00.047.884 I print_info: n_merges         = 50009
0.00.047.884 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.047.884 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.047.884 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.047.884 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.047.885 I print_info: LF token         = 128 'Ä'
0.00.047.885 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.047.885 I print_info: max token length = 1024
0.01.186.816 I load_tensors: offloading 24 repeating layers to GPU
0.01.186.820 I load_tensors: offloading output layer to GPU
0.01.186.820 I load_tensors: offloaded 25/25 layers to GPU
0.01.186.840 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.01.186.843 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.01.187.562 I llama_init_from_model: n_seq_max     = 1
0.01.187.564 I llama_init_from_model: n_ctx         = 2048
0.01.187.565 I llama_init_from_model: n_ctx_per_seq = 2048
0.01.187.565 I llama_init_from_model: n_batch       = 2048
0.01.187.565 I llama_init_from_model: n_ubatch      = 512
0.01.187.566 I llama_init_from_model: flash_attn    = 0
0.01.187.566 I llama_init_from_model: freq_base     = 10000.0
0.01.187.567 I llama_init_from_model: freq_scale    = 1
0.01.187.568 I ggml_metal_init: allocating
0.01.187.585 I ggml_metal_init: found device: Apple M4
0.01.187.594 I ggml_metal_init: picking default device: Apple M4
0.01.188.871 I ggml_metal_init: using embedded metal library
0.01.194.313 I ggml_metal_init: GPU name:   Apple M4
0.01.194.316 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.194.317 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.194.318 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.194.318 I ggml_metal_init: simdgroup reduction   = true
0.01.194.318 I ggml_metal_init: simdgroup matrix mul. = true
0.01.194.319 I ggml_metal_init: has residency sets    = true
0.01.194.319 I ggml_metal_init: has bfloat            = true
0.01.194.319 I ggml_metal_init: use bfloat            = true
0.01.194.320 I ggml_metal_init: hasUnifiedMemory      = true
0.01.194.321 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.214.817 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.276.179 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.01.276.185 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.01.276.210 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.280.654 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.01.280.656 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.01.280.657 I llama_init_from_model: graph nodes  = 967
0.01.280.657 I llama_init_from_model: graph splits = 2
0.01.280.667 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.01.280.803 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.280.804 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.335.910 I main: llama threadpool init, n_threads = 4
0.01.335.952 I 
0.01.335.977 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.335.980 I 
0.01.336.150 I sampler seed: 1234
0.01.336.155 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.336.173 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.336.174 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.336.174 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.430.700 I llama_perf_sampler_print:    sampling time =       1.25 ms /    71 runs   (    0.02 ms per token, 57028.11 tokens per second)
0.02.430.700 I llama_perf_context_print:        load time =    1324.61 ms
0.02.430.701 I llama_perf_context_print: prompt eval time =      48.77 ms /     7 tokens (    6.97 ms per token,   143.53 tokens per second)
0.02.430.702 I llama_perf_context_print:        eval time =    1042.87 ms /    63 runs   (   16.55 ms per token,    60.41 tokens per second)
0.02.430.702 I llama_perf_context_print:       total time =    1095.81 ms /    70 tokens
0.02.430.977 I ggml_metal_free: deallocating

real	0m2.451s
user	0m0.112s
sys	0m0.268s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.286 I build: 4586 (2711d021) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.297 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.612 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.017.618 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.625 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.625 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.626 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.626 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.626 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.627 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.629 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.629 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.630 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.630 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.630 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.631 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.633 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.633 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.633 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.397 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.443 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.172 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.173 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.174 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.174 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.175 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.175 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.026.176 I llama_model_loader: - type  f32:  194 tensors
0.00.026.176 I llama_model_loader: - type q8_0:   98 tensors
0.00.026.177 I print_info: file format = GGUF V3 (latest)
0.00.026.177 I print_info: file type   = Q8_0
0.00.026.179 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.034.240 I load: special tokens cache size = 25
0.00.041.325 I load: token to piece cache size = 0.2984 MB
0.00.041.329 I print_info: arch             = gptneox
0.00.041.329 I print_info: vocab_only       = 0
0.00.041.329 I print_info: n_ctx_train      = 2048
0.00.041.329 I print_info: n_embd           = 2048
0.00.041.330 I print_info: n_layer          = 24
0.00.041.333 I print_info: n_head           = 16
0.00.041.337 I print_info: n_head_kv        = 16
0.00.041.337 I print_info: n_rot            = 32
0.00.041.337 I print_info: n_swa            = 0
0.00.041.337 I print_info: n_embd_head_k    = 128
0.00.041.338 I print_info: n_embd_head_v    = 128
0.00.041.338 I print_info: n_gqa            = 1
0.00.041.343 I print_info: n_embd_k_gqa     = 2048
0.00.041.344 I print_info: n_embd_v_gqa     = 2048
0.00.041.345 I print_info: f_norm_eps       = 1.0e-05
0.00.041.345 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.345 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.345 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.346 I print_info: f_logit_scale    = 0.0e+00
0.00.041.346 I print_info: n_ff             = 8192
0.00.041.347 I print_info: n_expert         = 0
0.00.041.347 I print_info: n_expert_used    = 0
0.00.041.347 I print_info: causal attn      = 1
0.00.041.347 I print_info: pooling type     = 0
0.00.041.347 I print_info: rope type        = 2
0.00.041.348 I print_info: rope scaling     = linear
0.00.041.348 I print_info: freq_base_train  = 10000.0
0.00.041.348 I print_info: freq_scale_train = 1
0.00.041.348 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.349 I print_info: rope_finetuned   = unknown
0.00.041.350 I print_info: ssm_d_conv       = 0
0.00.041.350 I print_info: ssm_d_inner      = 0
0.00.041.350 I print_info: ssm_d_state      = 0
0.00.041.351 I print_info: ssm_dt_rank      = 0
0.00.041.351 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.351 I print_info: model type       = 1.4B
0.00.041.351 I print_info: model params     = 1.41 B
0.00.041.352 I print_info: general.name     = 1.4B
0.00.041.352 I print_info: vocab type       = BPE
0.00.041.352 I print_info: n_vocab          = 50304
0.00.041.352 I print_info: n_merges         = 50009
0.00.041.353 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.353 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.353 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.353 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.354 I print_info: LF token         = 128 'Ä'
0.00.041.354 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.354 I print_info: max token length = 1024
0.00.905.409 I load_tensors: offloading 24 repeating layers to GPU
0.00.905.413 I load_tensors: offloading output layer to GPU
0.00.905.414 I load_tensors: offloaded 25/25 layers to GPU
0.00.905.440 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.905.442 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.00.906.385 I llama_init_from_model: n_seq_max     = 1
0.00.906.387 I llama_init_from_model: n_ctx         = 128
0.00.906.388 I llama_init_from_model: n_ctx_per_seq = 128
0.00.906.388 I llama_init_from_model: n_batch       = 128
0.00.906.390 I llama_init_from_model: n_ubatch      = 128
0.00.906.390 I llama_init_from_model: flash_attn    = 0
0.00.906.391 I llama_init_from_model: freq_base     = 10000.0
0.00.906.391 I llama_init_from_model: freq_scale    = 1
0.00.906.392 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.906.393 I ggml_metal_init: allocating
0.00.906.432 I ggml_metal_init: found device: Apple M4
0.00.906.441 I ggml_metal_init: picking default device: Apple M4
0.00.907.718 I ggml_metal_init: using embedded metal library
0.00.913.525 I ggml_metal_init: GPU name:   Apple M4
0.00.913.529 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.913.530 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.913.531 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.913.531 I ggml_metal_init: simdgroup reduction   = true
0.00.913.531 I ggml_metal_init: simdgroup matrix mul. = true
0.00.913.532 I ggml_metal_init: has residency sets    = true
0.00.913.532 I ggml_metal_init: has bfloat            = true
0.00.913.532 I ggml_metal_init: use bfloat            = true
0.00.913.533 I ggml_metal_init: hasUnifiedMemory      = true
0.00.913.536 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.930.048 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.933.646 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.933.649 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.933.676 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.936.790 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.936.792 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.936.793 I llama_init_from_model: graph nodes  = 967
0.00.936.793 I llama_init_from_model: graph splits = 2
0.00.936.796 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.936.796 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.963.117 I 
0.00.963.209 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.963.246 I perplexity: tokenizing the input ..
0.00.970.068 I perplexity: tokenization took 6.818 ms
0.00.970.086 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.108.813 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.110.117 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.110.137 I llama_perf_context_print:        load time =     952.81 ms
0.01.110.138 I llama_perf_context_print: prompt eval time =     137.74 ms /   128 tokens (    1.08 ms per token,   929.31 tokens per second)
0.01.110.139 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.110.139 I llama_perf_context_print:       total time =     147.03 ms /   129 tokens
0.01.110.548 I ggml_metal_free: deallocating

real	0m1.127s
user	0m0.080s
sys	0m0.176s
```
- q4_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.046 I build: 4586 (2711d021) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.086 I main: llama backend init
0.00.000.089 I main: load the model and apply lora adapter, if any
0.00.014.141 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.023.499 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.023.504 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.023.506 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.023.506 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.023.507 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.023.507 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.023.507 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.023.509 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.023.509 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.023.509 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.023.510 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.023.510 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.023.510 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.023.511 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.023.513 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.023.513 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.023.513 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.027.658 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.028.700 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.032.703 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.032.704 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.032.704 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.032.705 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.032.705 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.032.706 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.032.706 I llama_model_loader: - type  f32:  194 tensors
0.00.032.706 I llama_model_loader: - type q4_0:   97 tensors
0.00.032.707 I llama_model_loader: - type q6_K:    1 tensors
0.00.032.707 I print_info: file format = GGUF V3 (latest)
0.00.032.708 I print_info: file type   = Q4_0
0.00.032.709 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.040.990 I load: special tokens cache size = 25
0.00.046.911 I load: token to piece cache size = 0.2984 MB
0.00.046.914 I print_info: arch             = gptneox
0.00.046.914 I print_info: vocab_only       = 0
0.00.046.915 I print_info: n_ctx_train      = 2048
0.00.046.915 I print_info: n_embd           = 2048
0.00.046.915 I print_info: n_layer          = 24
0.00.046.919 I print_info: n_head           = 16
0.00.046.920 I print_info: n_head_kv        = 16
0.00.046.924 I print_info: n_rot            = 32
0.00.046.924 I print_info: n_swa            = 0
0.00.046.924 I print_info: n_embd_head_k    = 128
0.00.046.925 I print_info: n_embd_head_v    = 128
0.00.046.925 I print_info: n_gqa            = 1
0.00.046.926 I print_info: n_embd_k_gqa     = 2048
0.00.046.927 I print_info: n_embd_v_gqa     = 2048
0.00.046.928 I print_info: f_norm_eps       = 1.0e-05
0.00.046.928 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.046.930 I print_info: f_clamp_kqv      = 0.0e+00
0.00.046.930 I print_info: f_max_alibi_bias = 0.0e+00
0.00.046.930 I print_info: f_logit_scale    = 0.0e+00
0.00.046.931 I print_info: n_ff             = 8192
0.00.046.931 I print_info: n_expert         = 0
0.00.046.931 I print_info: n_expert_used    = 0
0.00.046.931 I print_info: causal attn      = 1
0.00.046.932 I print_info: pooling type     = 0
0.00.046.932 I print_info: rope type        = 2
0.00.046.932 I print_info: rope scaling     = linear
0.00.046.932 I print_info: freq_base_train  = 10000.0
0.00.046.933 I print_info: freq_scale_train = 1
0.00.046.933 I print_info: n_ctx_orig_yarn  = 2048
0.00.046.933 I print_info: rope_finetuned   = unknown
0.00.046.933 I print_info: ssm_d_conv       = 0
0.00.046.933 I print_info: ssm_d_inner      = 0
0.00.046.934 I print_info: ssm_d_state      = 0
0.00.046.934 I print_info: ssm_dt_rank      = 0
0.00.046.934 I print_info: ssm_dt_b_c_rms   = 0
0.00.046.934 I print_info: model type       = 1.4B
0.00.046.935 I print_info: model params     = 1.41 B
0.00.046.935 I print_info: general.name     = 1.4B
0.00.046.936 I print_info: vocab type       = BPE
0.00.046.936 I print_info: n_vocab          = 50304
0.00.046.936 I print_info: n_merges         = 50009
0.00.046.936 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.046.936 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.046.937 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.046.937 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.046.937 I print_info: LF token         = 128 'Ä'
0.00.046.937 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.046.937 I print_info: max token length = 1024
0.00.643.726 I load_tensors: offloading 24 repeating layers to GPU
0.00.643.738 I load_tensors: offloading output layer to GPU
0.00.643.738 I load_tensors: offloaded 25/25 layers to GPU
0.00.643.770 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.643.772 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.645.321 I llama_init_from_model: n_seq_max     = 1
0.00.645.327 I llama_init_from_model: n_ctx         = 2048
0.00.645.327 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.645.328 I llama_init_from_model: n_batch       = 2048
0.00.645.328 I llama_init_from_model: n_ubatch      = 512
0.00.645.328 I llama_init_from_model: flash_attn    = 0
0.00.645.331 I llama_init_from_model: freq_base     = 10000.0
0.00.645.331 I llama_init_from_model: freq_scale    = 1
0.00.645.334 I ggml_metal_init: allocating
0.00.645.410 I ggml_metal_init: found device: Apple M4
0.00.645.424 I ggml_metal_init: picking default device: Apple M4
0.00.647.135 I ggml_metal_init: using embedded metal library
0.00.652.963 I ggml_metal_init: GPU name:   Apple M4
0.00.652.968 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.652.969 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.652.970 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.652.971 I ggml_metal_init: simdgroup reduction   = true
0.00.652.971 I ggml_metal_init: simdgroup matrix mul. = true
0.00.652.971 I ggml_metal_init: has residency sets    = true
0.00.652.972 I ggml_metal_init: has bfloat            = true
0.00.652.972 I ggml_metal_init: use bfloat            = true
0.00.652.973 I ggml_metal_init: hasUnifiedMemory      = true
0.00.652.974 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.671.710 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.731.768 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.731.775 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.731.797 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.735.902 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.735.904 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.735.904 I llama_init_from_model: graph nodes  = 967
0.00.735.904 I llama_init_from_model: graph splits = 2
0.00.735.910 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.736.032 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.736.032 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.794.506 I main: llama threadpool init, n_threads = 4
0.00.794.550 I 
0.00.794.576 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.794.576 I 
0.00.794.727 I sampler seed: 1234
0.00.794.732 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.794.753 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.794.754 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.794.754 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.487.425 I llama_perf_sampler_print:    sampling time =       1.40 ms /    71 runs   (    0.02 ms per token, 50678.09 tokens per second)
0.01.487.425 I llama_perf_context_print:        load time =     779.45 ms
0.01.487.426 I llama_perf_context_print: prompt eval time =      49.16 ms /     7 tokens (    7.02 ms per token,   142.38 tokens per second)
0.01.487.427 I llama_perf_context_print:        eval time =     640.57 ms /    63 runs   (   10.17 ms per token,    98.35 tokens per second)
0.01.487.427 I llama_perf_context_print:       total time =     693.83 ms /    70 tokens
0.01.487.720 I ggml_metal_free: deallocating

real	0m1.506s
user	0m0.111s
sys	0m0.200s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.276 I build: 4586 (2711d021) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.447 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.115 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.018.121 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.124 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.125 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.125 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.125 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.126 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.127 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.127 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.127 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.128 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.130 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.130 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.131 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.132 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.133 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.133 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.002 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.009 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.820 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.821 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.821 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.822 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.822 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.822 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.026.823 I llama_model_loader: - type  f32:  194 tensors
0.00.026.823 I llama_model_loader: - type q4_0:   97 tensors
0.00.026.824 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.824 I print_info: file format = GGUF V3 (latest)
0.00.026.825 I print_info: file type   = Q4_0
0.00.026.825 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.034.932 I load: special tokens cache size = 25
0.00.040.906 I load: token to piece cache size = 0.2984 MB
0.00.040.908 I print_info: arch             = gptneox
0.00.040.908 I print_info: vocab_only       = 0
0.00.040.909 I print_info: n_ctx_train      = 2048
0.00.040.909 I print_info: n_embd           = 2048
0.00.040.909 I print_info: n_layer          = 24
0.00.040.912 I print_info: n_head           = 16
0.00.040.913 I print_info: n_head_kv        = 16
0.00.040.915 I print_info: n_rot            = 32
0.00.040.915 I print_info: n_swa            = 0
0.00.040.915 I print_info: n_embd_head_k    = 128
0.00.040.915 I print_info: n_embd_head_v    = 128
0.00.040.916 I print_info: n_gqa            = 1
0.00.040.917 I print_info: n_embd_k_gqa     = 2048
0.00.040.918 I print_info: n_embd_v_gqa     = 2048
0.00.040.918 I print_info: f_norm_eps       = 1.0e-05
0.00.040.918 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.919 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.919 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.919 I print_info: f_logit_scale    = 0.0e+00
0.00.040.921 I print_info: n_ff             = 8192
0.00.040.922 I print_info: n_expert         = 0
0.00.040.922 I print_info: n_expert_used    = 0
0.00.040.922 I print_info: causal attn      = 1
0.00.040.922 I print_info: pooling type     = 0
0.00.040.922 I print_info: rope type        = 2
0.00.040.923 I print_info: rope scaling     = linear
0.00.040.923 I print_info: freq_base_train  = 10000.0
0.00.040.923 I print_info: freq_scale_train = 1
0.00.040.923 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.924 I print_info: rope_finetuned   = unknown
0.00.040.924 I print_info: ssm_d_conv       = 0
0.00.040.924 I print_info: ssm_d_inner      = 0
0.00.040.924 I print_info: ssm_d_state      = 0
0.00.040.924 I print_info: ssm_dt_rank      = 0
0.00.040.924 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.925 I print_info: model type       = 1.4B
0.00.040.929 I print_info: model params     = 1.41 B
0.00.040.929 I print_info: general.name     = 1.4B
0.00.040.929 I print_info: vocab type       = BPE
0.00.040.930 I print_info: n_vocab          = 50304
0.00.040.930 I print_info: n_merges         = 50009
0.00.040.930 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.930 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.931 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.931 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.931 I print_info: LF token         = 128 'Ä'
0.00.040.931 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.932 I print_info: max token length = 1024
0.00.588.172 I load_tensors: offloading 24 repeating layers to GPU
0.00.588.184 I load_tensors: offloading output layer to GPU
0.00.588.185 I load_tensors: offloaded 25/25 layers to GPU
0.00.588.223 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.588.225 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.589.520 I llama_init_from_model: n_seq_max     = 1
0.00.589.532 I llama_init_from_model: n_ctx         = 128
0.00.589.533 I llama_init_from_model: n_ctx_per_seq = 128
0.00.589.537 I llama_init_from_model: n_batch       = 128
0.00.589.537 I llama_init_from_model: n_ubatch      = 128
0.00.589.538 I llama_init_from_model: flash_attn    = 0
0.00.589.540 I llama_init_from_model: freq_base     = 10000.0
0.00.589.541 I llama_init_from_model: freq_scale    = 1
0.00.589.541 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.589.544 I ggml_metal_init: allocating
0.00.589.640 I ggml_metal_init: found device: Apple M4
0.00.589.656 I ggml_metal_init: picking default device: Apple M4
0.00.591.490 I ggml_metal_init: using embedded metal library
0.00.597.351 I ggml_metal_init: GPU name:   Apple M4
0.00.597.367 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.597.368 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.597.369 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.597.369 I ggml_metal_init: simdgroup reduction   = true
0.00.597.370 I ggml_metal_init: simdgroup matrix mul. = true
0.00.597.370 I ggml_metal_init: has residency sets    = true
0.00.597.371 I ggml_metal_init: has bfloat            = true
0.00.597.371 I ggml_metal_init: use bfloat            = true
0.00.597.373 I ggml_metal_init: hasUnifiedMemory      = true
0.00.597.379 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.617.935 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.621.735 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.621.739 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.621.767 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.625.003 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.625.006 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.625.007 I llama_init_from_model: graph nodes  = 967
0.00.625.007 I llama_init_from_model: graph splits = 2
0.00.625.010 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.625.010 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.649.928 I 
0.00.649.996 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.650.012 I perplexity: tokenizing the input ..
0.00.656.492 I perplexity: tokenization took 6.476 ms
0.00.656.507 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.792.900 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.794.236 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.794.250 I llama_perf_context_print:        load time =     639.47 ms
0.00.794.251 I llama_perf_context_print: prompt eval time =     135.88 ms /   128 tokens (    1.06 ms per token,   941.98 tokens per second)
0.00.794.251 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.794.252 I llama_perf_context_print:       total time =     144.32 ms /   129 tokens
0.00.794.701 I ggml_metal_free: deallocating

real	0m0.812s
user	0m0.081s
sys	0m0.127s
```
- q4_1:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4586 (2711d021) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.077 I main: llama backend init
0.00.000.079 I main: load the model and apply lora adapter, if any
0.00.008.846 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.739 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.018.743 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.745 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.745 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.746 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.746 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.746 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.747 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.747 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.748 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.748 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.751 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.751 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.751 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.754 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.754 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.755 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.526 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.531 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.308 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.309 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.309 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.310 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.310 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.310 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.027.311 I llama_model_loader: - type  f32:  194 tensors
0.00.027.311 I llama_model_loader: - type q4_1:   97 tensors
0.00.027.311 I llama_model_loader: - type q6_K:    1 tensors
0.00.027.312 I print_info: file format = GGUF V3 (latest)
0.00.027.313 I print_info: file type   = Q4_1
0.00.027.313 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.035.478 I load: special tokens cache size = 25
0.00.041.427 I load: token to piece cache size = 0.2984 MB
0.00.041.430 I print_info: arch             = gptneox
0.00.041.430 I print_info: vocab_only       = 0
0.00.041.431 I print_info: n_ctx_train      = 2048
0.00.041.431 I print_info: n_embd           = 2048
0.00.041.431 I print_info: n_layer          = 24
0.00.041.434 I print_info: n_head           = 16
0.00.041.435 I print_info: n_head_kv        = 16
0.00.041.435 I print_info: n_rot            = 32
0.00.041.435 I print_info: n_swa            = 0
0.00.041.436 I print_info: n_embd_head_k    = 128
0.00.041.436 I print_info: n_embd_head_v    = 128
0.00.041.436 I print_info: n_gqa            = 1
0.00.041.437 I print_info: n_embd_k_gqa     = 2048
0.00.041.438 I print_info: n_embd_v_gqa     = 2048
0.00.041.438 I print_info: f_norm_eps       = 1.0e-05
0.00.041.439 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.439 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.439 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.439 I print_info: f_logit_scale    = 0.0e+00
0.00.041.440 I print_info: n_ff             = 8192
0.00.041.440 I print_info: n_expert         = 0
0.00.041.440 I print_info: n_expert_used    = 0
0.00.041.440 I print_info: causal attn      = 1
0.00.041.440 I print_info: pooling type     = 0
0.00.041.442 I print_info: rope type        = 2
0.00.041.444 I print_info: rope scaling     = linear
0.00.041.445 I print_info: freq_base_train  = 10000.0
0.00.041.445 I print_info: freq_scale_train = 1
0.00.041.445 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.445 I print_info: rope_finetuned   = unknown
0.00.041.445 I print_info: ssm_d_conv       = 0
0.00.041.445 I print_info: ssm_d_inner      = 0
0.00.041.446 I print_info: ssm_d_state      = 0
0.00.041.446 I print_info: ssm_dt_rank      = 0
0.00.041.446 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.446 I print_info: model type       = 1.4B
0.00.041.446 I print_info: model params     = 1.41 B
0.00.041.447 I print_info: general.name     = 1.4B
0.00.041.447 I print_info: vocab type       = BPE
0.00.041.447 I print_info: n_vocab          = 50304
0.00.041.448 I print_info: n_merges         = 50009
0.00.041.448 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.448 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.448 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.452 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.453 I print_info: LF token         = 128 'Ä'
0.00.041.453 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.453 I print_info: max token length = 1024
0.00.635.370 I load_tensors: offloading 24 repeating layers to GPU
0.00.635.381 I load_tensors: offloading output layer to GPU
0.00.635.381 I load_tensors: offloaded 25/25 layers to GPU
0.00.635.414 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.635.415 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.636.855 I llama_init_from_model: n_seq_max     = 1
0.00.636.859 I llama_init_from_model: n_ctx         = 2048
0.00.636.860 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.636.860 I llama_init_from_model: n_batch       = 2048
0.00.636.861 I llama_init_from_model: n_ubatch      = 512
0.00.636.861 I llama_init_from_model: flash_attn    = 0
0.00.636.863 I llama_init_from_model: freq_base     = 10000.0
0.00.636.863 I llama_init_from_model: freq_scale    = 1
0.00.636.869 I ggml_metal_init: allocating
0.00.636.929 I ggml_metal_init: found device: Apple M4
0.00.636.942 I ggml_metal_init: picking default device: Apple M4
0.00.638.647 I ggml_metal_init: using embedded metal library
0.00.644.075 I ggml_metal_init: GPU name:   Apple M4
0.00.644.087 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.644.088 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.644.089 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.644.089 I ggml_metal_init: simdgroup reduction   = true
0.00.644.090 I ggml_metal_init: simdgroup matrix mul. = true
0.00.644.090 I ggml_metal_init: has residency sets    = true
0.00.644.090 I ggml_metal_init: has bfloat            = true
0.00.644.090 I ggml_metal_init: use bfloat            = true
0.00.644.094 I ggml_metal_init: hasUnifiedMemory      = true
0.00.644.099 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.663.608 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.721.266 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.721.274 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.721.298 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.725.775 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.725.777 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.725.777 I llama_init_from_model: graph nodes  = 967
0.00.725.778 I llama_init_from_model: graph splits = 2
0.00.725.782 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.725.912 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.725.912 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.779.690 I main: llama threadpool init, n_threads = 4
0.00.779.734 I 
0.00.779.757 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.779.758 I 
0.00.779.904 I sampler seed: 1234
0.00.779.909 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.779.927 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.779.927 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.779.927 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.511.058 I llama_perf_sampler_print:    sampling time =       1.28 ms /    71 runs   (    0.02 ms per token, 55555.56 tokens per second)
0.01.511.059 I llama_perf_context_print:        load time =     769.98 ms
0.01.511.061 I llama_perf_context_print: prompt eval time =      48.87 ms /     7 tokens (    6.98 ms per token,   143.25 tokens per second)
0.01.511.061 I llama_perf_context_print:        eval time =     679.43 ms /    63 runs   (   10.78 ms per token,    92.72 tokens per second)
0.01.511.062 I llama_perf_context_print:       total time =     732.23 ms /    70 tokens
0.01.511.317 I ggml_metal_free: deallocating

real	0m1.526s
user	0m0.111s
sys	0m0.193s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.096 I build: 4586 (2711d021) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.205 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.873 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.878 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.885 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.885 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.886 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.886 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.887 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.889 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.890 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.890 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.891 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.891 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.891 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.892 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.894 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.894 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.894 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.719 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.737 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.587 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.588 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.589 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.589 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.589 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.590 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.025.590 I llama_model_loader: - type  f32:  194 tensors
0.00.025.590 I llama_model_loader: - type q4_1:   97 tensors
0.00.025.591 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.591 I print_info: file format = GGUF V3 (latest)
0.00.025.592 I print_info: file type   = Q4_1
0.00.025.593 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.033.725 I load: special tokens cache size = 25
0.00.039.511 I load: token to piece cache size = 0.2984 MB
0.00.039.513 I print_info: arch             = gptneox
0.00.039.514 I print_info: vocab_only       = 0
0.00.039.514 I print_info: n_ctx_train      = 2048
0.00.039.514 I print_info: n_embd           = 2048
0.00.039.514 I print_info: n_layer          = 24
0.00.039.517 I print_info: n_head           = 16
0.00.039.518 I print_info: n_head_kv        = 16
0.00.039.518 I print_info: n_rot            = 32
0.00.039.519 I print_info: n_swa            = 0
0.00.039.519 I print_info: n_embd_head_k    = 128
0.00.039.519 I print_info: n_embd_head_v    = 128
0.00.039.521 I print_info: n_gqa            = 1
0.00.039.522 I print_info: n_embd_k_gqa     = 2048
0.00.039.523 I print_info: n_embd_v_gqa     = 2048
0.00.039.523 I print_info: f_norm_eps       = 1.0e-05
0.00.039.524 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.524 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.524 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.524 I print_info: f_logit_scale    = 0.0e+00
0.00.039.525 I print_info: n_ff             = 8192
0.00.039.525 I print_info: n_expert         = 0
0.00.039.525 I print_info: n_expert_used    = 0
0.00.039.525 I print_info: causal attn      = 1
0.00.039.525 I print_info: pooling type     = 0
0.00.039.526 I print_info: rope type        = 2
0.00.039.526 I print_info: rope scaling     = linear
0.00.039.530 I print_info: freq_base_train  = 10000.0
0.00.039.531 I print_info: freq_scale_train = 1
0.00.039.531 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.531 I print_info: rope_finetuned   = unknown
0.00.039.531 I print_info: ssm_d_conv       = 0
0.00.039.531 I print_info: ssm_d_inner      = 0
0.00.039.531 I print_info: ssm_d_state      = 0
0.00.039.532 I print_info: ssm_dt_rank      = 0
0.00.039.532 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.532 I print_info: model type       = 1.4B
0.00.039.532 I print_info: model params     = 1.41 B
0.00.039.533 I print_info: general.name     = 1.4B
0.00.039.533 I print_info: vocab type       = BPE
0.00.039.533 I print_info: n_vocab          = 50304
0.00.039.534 I print_info: n_merges         = 50009
0.00.039.534 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.534 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.534 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.534 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.535 I print_info: LF token         = 128 'Ä'
0.00.039.536 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.536 I print_info: max token length = 1024
0.00.643.591 I load_tensors: offloading 24 repeating layers to GPU
0.00.643.607 I load_tensors: offloading output layer to GPU
0.00.643.608 I load_tensors: offloaded 25/25 layers to GPU
0.00.643.641 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.643.642 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.644.921 I llama_init_from_model: n_seq_max     = 1
0.00.644.927 I llama_init_from_model: n_ctx         = 128
0.00.644.927 I llama_init_from_model: n_ctx_per_seq = 128
0.00.644.928 I llama_init_from_model: n_batch       = 128
0.00.644.928 I llama_init_from_model: n_ubatch      = 128
0.00.644.929 I llama_init_from_model: flash_attn    = 0
0.00.644.931 I llama_init_from_model: freq_base     = 10000.0
0.00.644.931 I llama_init_from_model: freq_scale    = 1
0.00.644.932 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.644.939 I ggml_metal_init: allocating
0.00.645.022 I ggml_metal_init: found device: Apple M4
0.00.645.037 I ggml_metal_init: picking default device: Apple M4
0.00.646.804 I ggml_metal_init: using embedded metal library
0.00.653.036 I ggml_metal_init: GPU name:   Apple M4
0.00.653.042 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.653.043 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.653.044 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.653.045 I ggml_metal_init: simdgroup reduction   = true
0.00.653.045 I ggml_metal_init: simdgroup matrix mul. = true
0.00.653.046 I ggml_metal_init: has residency sets    = true
0.00.653.046 I ggml_metal_init: has bfloat            = true
0.00.653.046 I ggml_metal_init: use bfloat            = true
0.00.653.047 I ggml_metal_init: hasUnifiedMemory      = true
0.00.653.049 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.672.068 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.675.483 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.675.486 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.675.513 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.678.744 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.678.746 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.678.747 I llama_init_from_model: graph nodes  = 967
0.00.678.747 I llama_init_from_model: graph splits = 2
0.00.678.750 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.678.751 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.705.356 I 
0.00.705.415 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.705.430 I perplexity: tokenizing the input ..
0.00.711.284 I perplexity: tokenization took 5.853 ms
0.00.711.295 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.847.407 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.848.822 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.848.835 I llama_perf_context_print:        load time =     696.14 ms
0.00.848.836 I llama_perf_context_print: prompt eval time =     135.86 ms /   128 tokens (    1.06 ms per token,   942.17 tokens per second)
0.00.848.837 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.848.837 I llama_perf_context_print:       total time =     143.48 ms /   129 tokens
0.00.849.218 I ggml_metal_free: deallocating

real	0m0.863s
user	0m0.078s
sys	0m0.134s
```
- q5_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4586 (2711d021) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.078 I main: llama backend init
0.00.000.080 I main: load the model and apply lora adapter, if any
0.00.014.668 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.022.141 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.022.145 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.022.151 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.022.152 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.022.152 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.022.153 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.022.153 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.022.154 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.022.154 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.022.155 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.022.155 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.022.155 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.022.155 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.022.156 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.022.157 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.022.158 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.022.158 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.025.924 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.026.998 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.030.725 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.030.726 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.030.726 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.030.727 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.030.727 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.030.727 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.030.728 I llama_model_loader: - type  f32:  194 tensors
0.00.030.728 I llama_model_loader: - type q5_0:   97 tensors
0.00.030.728 I llama_model_loader: - type q6_K:    1 tensors
0.00.030.729 I print_info: file format = GGUF V3 (latest)
0.00.030.729 I print_info: file type   = Q5_0
0.00.030.733 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.038.946 I load: special tokens cache size = 25
0.00.044.880 I load: token to piece cache size = 0.2984 MB
0.00.044.883 I print_info: arch             = gptneox
0.00.044.883 I print_info: vocab_only       = 0
0.00.044.883 I print_info: n_ctx_train      = 2048
0.00.044.883 I print_info: n_embd           = 2048
0.00.044.883 I print_info: n_layer          = 24
0.00.044.886 I print_info: n_head           = 16
0.00.044.887 I print_info: n_head_kv        = 16
0.00.044.887 I print_info: n_rot            = 32
0.00.044.888 I print_info: n_swa            = 0
0.00.044.888 I print_info: n_embd_head_k    = 128
0.00.044.890 I print_info: n_embd_head_v    = 128
0.00.044.890 I print_info: n_gqa            = 1
0.00.044.891 I print_info: n_embd_k_gqa     = 2048
0.00.044.893 I print_info: n_embd_v_gqa     = 2048
0.00.044.894 I print_info: f_norm_eps       = 1.0e-05
0.00.044.894 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.044.894 I print_info: f_clamp_kqv      = 0.0e+00
0.00.044.894 I print_info: f_max_alibi_bias = 0.0e+00
0.00.044.894 I print_info: f_logit_scale    = 0.0e+00
0.00.044.895 I print_info: n_ff             = 8192
0.00.044.895 I print_info: n_expert         = 0
0.00.044.895 I print_info: n_expert_used    = 0
0.00.044.896 I print_info: causal attn      = 1
0.00.044.896 I print_info: pooling type     = 0
0.00.044.896 I print_info: rope type        = 2
0.00.044.896 I print_info: rope scaling     = linear
0.00.044.897 I print_info: freq_base_train  = 10000.0
0.00.044.898 I print_info: freq_scale_train = 1
0.00.044.898 I print_info: n_ctx_orig_yarn  = 2048
0.00.044.898 I print_info: rope_finetuned   = unknown
0.00.044.898 I print_info: ssm_d_conv       = 0
0.00.044.898 I print_info: ssm_d_inner      = 0
0.00.044.898 I print_info: ssm_d_state      = 0
0.00.044.899 I print_info: ssm_dt_rank      = 0
0.00.044.900 I print_info: ssm_dt_b_c_rms   = 0
0.00.044.904 I print_info: model type       = 1.4B
0.00.044.904 I print_info: model params     = 1.41 B
0.00.044.904 I print_info: general.name     = 1.4B
0.00.044.905 I print_info: vocab type       = BPE
0.00.044.905 I print_info: n_vocab          = 50304
0.00.044.905 I print_info: n_merges         = 50009
0.00.044.905 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.044.906 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.044.906 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.044.906 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.044.906 I print_info: LF token         = 128 'Ä'
0.00.044.907 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.044.907 I print_info: max token length = 1024
0.00.692.701 I load_tensors: offloading 24 repeating layers to GPU
0.00.692.707 I load_tensors: offloading output layer to GPU
0.00.692.708 I load_tensors: offloaded 25/25 layers to GPU
0.00.692.727 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.692.728 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.693.609 I llama_init_from_model: n_seq_max     = 1
0.00.693.619 I llama_init_from_model: n_ctx         = 2048
0.00.693.619 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.693.620 I llama_init_from_model: n_batch       = 2048
0.00.693.620 I llama_init_from_model: n_ubatch      = 512
0.00.693.620 I llama_init_from_model: flash_attn    = 0
0.00.693.622 I llama_init_from_model: freq_base     = 10000.0
0.00.693.622 I llama_init_from_model: freq_scale    = 1
0.00.693.625 I ggml_metal_init: allocating
0.00.693.702 I ggml_metal_init: found device: Apple M4
0.00.693.717 I ggml_metal_init: picking default device: Apple M4
0.00.695.141 I ggml_metal_init: using embedded metal library
0.00.700.092 I ggml_metal_init: GPU name:   Apple M4
0.00.700.102 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.700.103 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.700.103 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.700.104 I ggml_metal_init: simdgroup reduction   = true
0.00.700.104 I ggml_metal_init: simdgroup matrix mul. = true
0.00.700.105 I ggml_metal_init: has residency sets    = true
0.00.700.105 I ggml_metal_init: has bfloat            = true
0.00.700.105 I ggml_metal_init: use bfloat            = true
0.00.700.107 I ggml_metal_init: hasUnifiedMemory      = true
0.00.700.110 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.712.920 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.745.310 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.745.320 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.745.351 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.749.944 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.749.946 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.749.946 I llama_init_from_model: graph nodes  = 967
0.00.749.947 I llama_init_from_model: graph splits = 2
0.00.749.952 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.750.079 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.750.080 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.810.932 I main: llama threadpool init, n_threads = 4
0.00.810.969 I 
0.00.810.991 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.810.991 I 
0.00.811.153 I sampler seed: 1234
0.00.811.157 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.811.168 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.811.168 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.811.168 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.616.489 I llama_perf_sampler_print:    sampling time =       1.56 ms /    71 runs   (    0.02 ms per token, 45454.55 tokens per second)
0.01.616.491 I llama_perf_context_print:        load time =     795.37 ms
0.01.616.491 I llama_perf_context_print: prompt eval time =      53.08 ms /     7 tokens (    7.58 ms per token,   131.87 tokens per second)
0.01.616.492 I llama_perf_context_print:        eval time =     749.67 ms /    63 runs   (   11.90 ms per token,    84.04 tokens per second)
0.01.616.493 I llama_perf_context_print:       total time =     806.45 ms /    70 tokens
0.01.616.717 I ggml_metal_free: deallocating

real	0m1.635s
user	0m0.103s
sys	0m0.180s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.102 I build: 4586 (2711d021) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.903 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.224 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.017.229 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.233 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.233 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.234 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.234 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.234 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.236 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.236 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.236 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.237 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.237 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.237 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.238 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.240 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.240 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.241 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.057 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.066 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.847 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.848 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.849 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.849 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.849 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.850 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.025.850 I llama_model_loader: - type  f32:  194 tensors
0.00.025.850 I llama_model_loader: - type q5_0:   97 tensors
0.00.025.851 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.851 I print_info: file format = GGUF V3 (latest)
0.00.025.852 I print_info: file type   = Q5_0
0.00.025.853 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.033.997 I load: special tokens cache size = 25
0.00.039.980 I load: token to piece cache size = 0.2984 MB
0.00.039.983 I print_info: arch             = gptneox
0.00.039.984 I print_info: vocab_only       = 0
0.00.039.984 I print_info: n_ctx_train      = 2048
0.00.039.984 I print_info: n_embd           = 2048
0.00.039.984 I print_info: n_layer          = 24
0.00.039.987 I print_info: n_head           = 16
0.00.039.988 I print_info: n_head_kv        = 16
0.00.039.988 I print_info: n_rot            = 32
0.00.039.988 I print_info: n_swa            = 0
0.00.039.989 I print_info: n_embd_head_k    = 128
0.00.039.989 I print_info: n_embd_head_v    = 128
0.00.039.990 I print_info: n_gqa            = 1
0.00.039.991 I print_info: n_embd_k_gqa     = 2048
0.00.039.991 I print_info: n_embd_v_gqa     = 2048
0.00.039.992 I print_info: f_norm_eps       = 1.0e-05
0.00.039.992 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.992 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.993 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.993 I print_info: f_logit_scale    = 0.0e+00
0.00.039.993 I print_info: n_ff             = 8192
0.00.039.994 I print_info: n_expert         = 0
0.00.039.994 I print_info: n_expert_used    = 0
0.00.039.994 I print_info: causal attn      = 1
0.00.039.994 I print_info: pooling type     = 0
0.00.039.996 I print_info: rope type        = 2
0.00.039.997 I print_info: rope scaling     = linear
0.00.039.997 I print_info: freq_base_train  = 10000.0
0.00.039.997 I print_info: freq_scale_train = 1
0.00.039.997 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.998 I print_info: rope_finetuned   = unknown
0.00.039.998 I print_info: ssm_d_conv       = 0
0.00.039.998 I print_info: ssm_d_inner      = 0
0.00.039.998 I print_info: ssm_d_state      = 0
0.00.039.998 I print_info: ssm_dt_rank      = 0
0.00.039.998 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.999 I print_info: model type       = 1.4B
0.00.039.999 I print_info: model params     = 1.41 B
0.00.039.999 I print_info: general.name     = 1.4B
0.00.040.000 I print_info: vocab type       = BPE
0.00.040.001 I print_info: n_vocab          = 50304
0.00.040.005 I print_info: n_merges         = 50009
0.00.040.005 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.005 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.005 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.005 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.006 I print_info: LF token         = 128 'Ä'
0.00.040.006 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.006 I print_info: max token length = 1024
0.00.709.918 I load_tensors: offloading 24 repeating layers to GPU
0.00.709.930 I load_tensors: offloading output layer to GPU
0.00.709.931 I load_tensors: offloaded 25/25 layers to GPU
0.00.709.966 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.709.967 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.711.491 I llama_init_from_model: n_seq_max     = 1
0.00.711.496 I llama_init_from_model: n_ctx         = 128
0.00.711.496 I llama_init_from_model: n_ctx_per_seq = 128
0.00.711.497 I llama_init_from_model: n_batch       = 128
0.00.711.497 I llama_init_from_model: n_ubatch      = 128
0.00.711.498 I llama_init_from_model: flash_attn    = 0
0.00.711.500 I llama_init_from_model: freq_base     = 10000.0
0.00.711.500 I llama_init_from_model: freq_scale    = 1
0.00.711.501 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.711.503 I ggml_metal_init: allocating
0.00.711.582 I ggml_metal_init: found device: Apple M4
0.00.711.596 I ggml_metal_init: picking default device: Apple M4
0.00.713.305 I ggml_metal_init: using embedded metal library
0.00.719.894 I ggml_metal_init: GPU name:   Apple M4
0.00.719.898 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.719.899 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.719.899 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.719.900 I ggml_metal_init: simdgroup reduction   = true
0.00.719.900 I ggml_metal_init: simdgroup matrix mul. = true
0.00.719.900 I ggml_metal_init: has residency sets    = true
0.00.719.900 I ggml_metal_init: has bfloat            = true
0.00.719.901 I ggml_metal_init: use bfloat            = true
0.00.719.901 I ggml_metal_init: hasUnifiedMemory      = true
0.00.719.903 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.737.476 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.741.078 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.741.082 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.741.109 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.744.292 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.744.294 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.744.294 I llama_init_from_model: graph nodes  = 967
0.00.744.295 I llama_init_from_model: graph splits = 2
0.00.744.298 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.744.298 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.776.636 I 
0.00.776.721 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.776.742 I perplexity: tokenizing the input ..
0.00.783.727 I perplexity: tokenization took 6.983 ms
0.00.783.739 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.930.322 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.931.661 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.931.683 I llama_perf_context_print:        load time =     766.73 ms
0.00.931.684 I llama_perf_context_print: prompt eval time =     146.35 ms /   128 tokens (    1.14 ms per token,   874.61 tokens per second)
0.00.931.685 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.931.686 I llama_perf_context_print:       total time =     155.05 ms /   129 tokens
0.00.932.074 I ggml_metal_free: deallocating

real	0m0.948s
user	0m0.079s
sys	0m0.148s
```
- q5_1:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.044 I build: 4586 (2711d021) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.078 I main: llama backend init
0.00.000.080 I main: load the model and apply lora adapter, if any
0.00.009.362 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.412 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.017.418 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.419 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.425 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.426 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.426 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.427 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.429 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.429 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.429 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.429 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.430 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.430 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.430 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.432 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.432 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.433 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.336 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.423 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.466 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.468 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.468 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.468 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.469 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.469 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.026.470 I llama_model_loader: - type  f32:  194 tensors
0.00.026.470 I llama_model_loader: - type q5_1:   97 tensors
0.00.026.470 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.471 I print_info: file format = GGUF V3 (latest)
0.00.026.472 I print_info: file type   = Q5_1
0.00.026.473 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.034.796 I load: special tokens cache size = 25
0.00.040.691 I load: token to piece cache size = 0.2984 MB
0.00.040.696 I print_info: arch             = gptneox
0.00.040.696 I print_info: vocab_only       = 0
0.00.040.696 I print_info: n_ctx_train      = 2048
0.00.040.697 I print_info: n_embd           = 2048
0.00.040.697 I print_info: n_layer          = 24
0.00.040.701 I print_info: n_head           = 16
0.00.040.702 I print_info: n_head_kv        = 16
0.00.040.702 I print_info: n_rot            = 32
0.00.040.702 I print_info: n_swa            = 0
0.00.040.703 I print_info: n_embd_head_k    = 128
0.00.040.703 I print_info: n_embd_head_v    = 128
0.00.040.703 I print_info: n_gqa            = 1
0.00.040.704 I print_info: n_embd_k_gqa     = 2048
0.00.040.707 I print_info: n_embd_v_gqa     = 2048
0.00.040.707 I print_info: f_norm_eps       = 1.0e-05
0.00.040.708 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.709 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.709 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.709 I print_info: f_logit_scale    = 0.0e+00
0.00.040.710 I print_info: n_ff             = 8192
0.00.040.710 I print_info: n_expert         = 0
0.00.040.710 I print_info: n_expert_used    = 0
0.00.040.710 I print_info: causal attn      = 1
0.00.040.710 I print_info: pooling type     = 0
0.00.040.710 I print_info: rope type        = 2
0.00.040.711 I print_info: rope scaling     = linear
0.00.040.711 I print_info: freq_base_train  = 10000.0
0.00.040.711 I print_info: freq_scale_train = 1
0.00.040.711 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.712 I print_info: rope_finetuned   = unknown
0.00.040.712 I print_info: ssm_d_conv       = 0
0.00.040.712 I print_info: ssm_d_inner      = 0
0.00.040.712 I print_info: ssm_d_state      = 0
0.00.040.712 I print_info: ssm_dt_rank      = 0
0.00.040.712 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.713 I print_info: model type       = 1.4B
0.00.040.713 I print_info: model params     = 1.41 B
0.00.040.713 I print_info: general.name     = 1.4B
0.00.040.715 I print_info: vocab type       = BPE
0.00.040.715 I print_info: n_vocab          = 50304
0.00.040.715 I print_info: n_merges         = 50009
0.00.040.715 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.715 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.716 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.716 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.716 I print_info: LF token         = 128 'Ä'
0.00.040.716 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.716 I print_info: max token length = 1024
0.00.630.842 I load_tensors: offloading 24 repeating layers to GPU
0.00.630.848 I load_tensors: offloading output layer to GPU
0.00.630.848 I load_tensors: offloaded 25/25 layers to GPU
0.00.630.870 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.630.871 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.631.724 I llama_init_from_model: n_seq_max     = 1
0.00.631.729 I llama_init_from_model: n_ctx         = 2048
0.00.631.729 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.631.729 I llama_init_from_model: n_batch       = 2048
0.00.631.730 I llama_init_from_model: n_ubatch      = 512
0.00.631.730 I llama_init_from_model: flash_attn    = 0
0.00.631.732 I llama_init_from_model: freq_base     = 10000.0
0.00.631.732 I llama_init_from_model: freq_scale    = 1
0.00.631.734 I ggml_metal_init: allocating
0.00.631.795 I ggml_metal_init: found device: Apple M4
0.00.631.808 I ggml_metal_init: picking default device: Apple M4
0.00.632.997 I ggml_metal_init: using embedded metal library
0.00.637.325 I ggml_metal_init: GPU name:   Apple M4
0.00.637.330 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.637.331 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.637.332 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.637.332 I ggml_metal_init: simdgroup reduction   = true
0.00.637.332 I ggml_metal_init: simdgroup matrix mul. = true
0.00.637.333 I ggml_metal_init: has residency sets    = true
0.00.637.333 I ggml_metal_init: has bfloat            = true
0.00.637.333 I ggml_metal_init: use bfloat            = true
0.00.637.335 I ggml_metal_init: hasUnifiedMemory      = true
0.00.637.337 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.651.930 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.682.958 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.682.965 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.682.987 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.687.465 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.687.467 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.687.467 I llama_init_from_model: graph nodes  = 967
0.00.687.467 I llama_init_from_model: graph splits = 2
0.00.687.473 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.687.601 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.687.602 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.745.532 I main: llama threadpool init, n_threads = 4
0.00.745.568 I 
0.00.745.588 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.745.588 I 
0.00.745.702 I sampler seed: 1234
0.00.745.707 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.745.716 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.745.717 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.745.717 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.595.040 I llama_perf_sampler_print:    sampling time =       1.31 ms /    71 runs   (    0.02 ms per token, 54239.88 tokens per second)
0.01.595.040 I llama_perf_context_print:        load time =     735.30 ms
0.01.595.041 I llama_perf_context_print: prompt eval time =      52.19 ms /     7 tokens (    7.46 ms per token,   134.13 tokens per second)
0.01.595.042 I llama_perf_context_print:        eval time =     794.27 ms /    63 runs   (   12.61 ms per token,    79.32 tokens per second)
0.01.595.043 I llama_perf_context_print:       total time =     850.38 ms /    70 tokens
0.01.595.265 I ggml_metal_free: deallocating

real	0m1.612s
user	0m0.105s
sys	0m0.154s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.084 I build: 4586 (2711d021) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.844 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.038 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.042 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.044 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.045 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.045 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.045 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.046 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.049 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.049 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.049 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.053 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.054 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.054 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.055 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.059 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.059 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.059 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.827 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.861 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.609 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.611 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.611 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.611 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.611 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.612 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.612 I llama_model_loader: - type  f32:  194 tensors
0.00.024.612 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.613 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.613 I print_info: file format = GGUF V3 (latest)
0.00.024.614 I print_info: file type   = Q5_1
0.00.024.614 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.032.522 I load: special tokens cache size = 25
0.00.038.488 I load: token to piece cache size = 0.2984 MB
0.00.038.491 I print_info: arch             = gptneox
0.00.038.491 I print_info: vocab_only       = 0
0.00.038.491 I print_info: n_ctx_train      = 2048
0.00.038.491 I print_info: n_embd           = 2048
0.00.038.492 I print_info: n_layer          = 24
0.00.038.494 I print_info: n_head           = 16
0.00.038.495 I print_info: n_head_kv        = 16
0.00.038.495 I print_info: n_rot            = 32
0.00.038.495 I print_info: n_swa            = 0
0.00.038.496 I print_info: n_embd_head_k    = 128
0.00.038.496 I print_info: n_embd_head_v    = 128
0.00.038.497 I print_info: n_gqa            = 1
0.00.038.498 I print_info: n_embd_k_gqa     = 2048
0.00.038.499 I print_info: n_embd_v_gqa     = 2048
0.00.038.499 I print_info: f_norm_eps       = 1.0e-05
0.00.038.500 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.500 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.500 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.500 I print_info: f_logit_scale    = 0.0e+00
0.00.038.501 I print_info: n_ff             = 8192
0.00.038.501 I print_info: n_expert         = 0
0.00.038.501 I print_info: n_expert_used    = 0
0.00.038.501 I print_info: causal attn      = 1
0.00.038.502 I print_info: pooling type     = 0
0.00.038.502 I print_info: rope type        = 2
0.00.038.502 I print_info: rope scaling     = linear
0.00.038.504 I print_info: freq_base_train  = 10000.0
0.00.038.505 I print_info: freq_scale_train = 1
0.00.038.505 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.506 I print_info: rope_finetuned   = unknown
0.00.038.506 I print_info: ssm_d_conv       = 0
0.00.038.506 I print_info: ssm_d_inner      = 0
0.00.038.506 I print_info: ssm_d_state      = 0
0.00.038.506 I print_info: ssm_dt_rank      = 0
0.00.038.506 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.506 I print_info: model type       = 1.4B
0.00.038.507 I print_info: model params     = 1.41 B
0.00.038.507 I print_info: general.name     = 1.4B
0.00.038.508 I print_info: vocab type       = BPE
0.00.038.508 I print_info: n_vocab          = 50304
0.00.038.508 I print_info: n_merges         = 50009
0.00.038.509 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.513 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.513 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.513 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.513 I print_info: LF token         = 128 'Ä'
0.00.038.514 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.515 I print_info: max token length = 1024
0.00.605.596 I load_tensors: offloading 24 repeating layers to GPU
0.00.605.602 I load_tensors: offloading output layer to GPU
0.00.605.603 I load_tensors: offloaded 25/25 layers to GPU
0.00.605.629 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.605.631 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.606.817 I llama_init_from_model: n_seq_max     = 1
0.00.606.819 I llama_init_from_model: n_ctx         = 128
0.00.606.819 I llama_init_from_model: n_ctx_per_seq = 128
0.00.606.820 I llama_init_from_model: n_batch       = 128
0.00.606.820 I llama_init_from_model: n_ubatch      = 128
0.00.606.821 I llama_init_from_model: flash_attn    = 0
0.00.606.821 I llama_init_from_model: freq_base     = 10000.0
0.00.606.822 I llama_init_from_model: freq_scale    = 1
0.00.606.823 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.606.824 I ggml_metal_init: allocating
0.00.606.849 I ggml_metal_init: found device: Apple M4
0.00.606.863 I ggml_metal_init: picking default device: Apple M4
0.00.608.274 I ggml_metal_init: using embedded metal library
0.00.614.391 I ggml_metal_init: GPU name:   Apple M4
0.00.614.394 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.614.395 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.614.396 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.614.396 I ggml_metal_init: simdgroup reduction   = true
0.00.614.397 I ggml_metal_init: simdgroup matrix mul. = true
0.00.614.397 I ggml_metal_init: has residency sets    = true
0.00.614.397 I ggml_metal_init: has bfloat            = true
0.00.614.397 I ggml_metal_init: use bfloat            = true
0.00.614.398 I ggml_metal_init: hasUnifiedMemory      = true
0.00.614.400 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.630.860 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.634.232 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.634.239 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.634.280 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.637.453 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.637.455 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.637.456 I llama_init_from_model: graph nodes  = 967
0.00.637.456 I llama_init_from_model: graph splits = 2
0.00.637.458 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.637.458 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.666.179 I 
0.00.666.253 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.666.277 I perplexity: tokenizing the input ..
0.00.674.128 I perplexity: tokenization took 7.848 ms
0.00.674.149 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.810.191 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.811.524 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.811.539 I llama_perf_context_print:        load time =     657.33 ms
0.00.811.539 I llama_perf_context_print: prompt eval time =     135.12 ms /   128 tokens (    1.06 ms per token,   947.31 tokens per second)
0.00.811.540 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.811.541 I llama_perf_context_print:       total time =     145.36 ms /   129 tokens
0.00.811.928 I ggml_metal_free: deallocating

real	0m0.826s
user	0m0.078s
sys	0m0.133s
```
- q2_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4586 (2711d021) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.083 I main: llama backend init
0.00.000.085 I main: load the model and apply lora adapter, if any
0.00.010.649 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.354 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.017.359 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.361 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.361 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.362 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.362 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.362 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.363 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.364 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.364 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.364 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.365 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.365 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.366 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.367 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.367 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.367 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.160 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.211 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.966 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.967 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.967 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.968 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.968 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.968 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.969 I llama_model_loader: - type  f32:  194 tensors
0.00.025.969 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.969 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.970 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.970 I print_info: file format = GGUF V3 (latest)
0.00.025.971 I print_info: file type   = Q2_K - Medium
0.00.025.972 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.034.143 I load: special tokens cache size = 25
0.00.040.138 I load: token to piece cache size = 0.2984 MB
0.00.040.142 I print_info: arch             = gptneox
0.00.040.142 I print_info: vocab_only       = 0
0.00.040.142 I print_info: n_ctx_train      = 2048
0.00.040.142 I print_info: n_embd           = 2048
0.00.040.142 I print_info: n_layer          = 24
0.00.040.145 I print_info: n_head           = 16
0.00.040.146 I print_info: n_head_kv        = 16
0.00.040.146 I print_info: n_rot            = 32
0.00.040.149 I print_info: n_swa            = 0
0.00.040.149 I print_info: n_embd_head_k    = 128
0.00.040.149 I print_info: n_embd_head_v    = 128
0.00.040.150 I print_info: n_gqa            = 1
0.00.040.151 I print_info: n_embd_k_gqa     = 2048
0.00.040.156 I print_info: n_embd_v_gqa     = 2048
0.00.040.157 I print_info: f_norm_eps       = 1.0e-05
0.00.040.157 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.157 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.158 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.158 I print_info: f_logit_scale    = 0.0e+00
0.00.040.162 I print_info: n_ff             = 8192
0.00.040.162 I print_info: n_expert         = 0
0.00.040.162 I print_info: n_expert_used    = 0
0.00.040.162 I print_info: causal attn      = 1
0.00.040.162 I print_info: pooling type     = 0
0.00.040.164 I print_info: rope type        = 2
0.00.040.164 I print_info: rope scaling     = linear
0.00.040.164 I print_info: freq_base_train  = 10000.0
0.00.040.165 I print_info: freq_scale_train = 1
0.00.040.165 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.165 I print_info: rope_finetuned   = unknown
0.00.040.165 I print_info: ssm_d_conv       = 0
0.00.040.165 I print_info: ssm_d_inner      = 0
0.00.040.165 I print_info: ssm_d_state      = 0
0.00.040.166 I print_info: ssm_dt_rank      = 0
0.00.040.166 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.166 I print_info: model type       = 1.4B
0.00.040.166 I print_info: model params     = 1.41 B
0.00.040.166 I print_info: general.name     = 1.4B
0.00.040.167 I print_info: vocab type       = BPE
0.00.040.167 I print_info: n_vocab          = 50304
0.00.040.167 I print_info: n_merges         = 50009
0.00.040.167 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.167 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.168 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.168 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.168 I print_info: LF token         = 128 'Ä'
0.00.040.169 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.169 I print_info: max token length = 1024
0.00.337.280 I load_tensors: offloading 24 repeating layers to GPU
0.00.337.294 I load_tensors: offloading output layer to GPU
0.00.337.295 I load_tensors: offloaded 25/25 layers to GPU
0.00.337.332 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.337.333 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.338.886 I llama_init_from_model: n_seq_max     = 1
0.00.338.895 I llama_init_from_model: n_ctx         = 2048
0.00.338.895 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.338.896 I llama_init_from_model: n_batch       = 2048
0.00.338.896 I llama_init_from_model: n_ubatch      = 512
0.00.338.896 I llama_init_from_model: flash_attn    = 0
0.00.338.898 I llama_init_from_model: freq_base     = 10000.0
0.00.338.903 I llama_init_from_model: freq_scale    = 1
0.00.338.905 I ggml_metal_init: allocating
0.00.338.996 I ggml_metal_init: found device: Apple M4
0.00.339.010 I ggml_metal_init: picking default device: Apple M4
0.00.340.816 I ggml_metal_init: using embedded metal library
0.00.346.507 I ggml_metal_init: GPU name:   Apple M4
0.00.346.517 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.346.517 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.346.518 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.346.518 I ggml_metal_init: simdgroup reduction   = true
0.00.346.519 I ggml_metal_init: simdgroup matrix mul. = true
0.00.346.519 I ggml_metal_init: has residency sets    = true
0.00.346.519 I ggml_metal_init: has bfloat            = true
0.00.346.519 I ggml_metal_init: use bfloat            = true
0.00.346.522 I ggml_metal_init: hasUnifiedMemory      = true
0.00.346.527 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.368.246 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.420.640 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.420.646 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.420.671 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.425.836 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.425.838 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.425.838 I llama_init_from_model: graph nodes  = 967
0.00.425.838 I llama_init_from_model: graph splits = 2
0.00.425.845 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.425.969 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.425.969 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.484.808 I main: llama threadpool init, n_threads = 4
0.00.484.851 I 
0.00.484.879 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.484.879 I 
0.00.485.056 I sampler seed: 1234
0.00.485.061 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.485.082 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.485.082 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.485.082 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.174.358 I llama_perf_sampler_print:    sampling time =       1.31 ms /    71 runs   (    0.02 ms per token, 54239.88 tokens per second)
0.01.174.358 I llama_perf_context_print:        load time =     473.27 ms
0.01.174.360 I llama_perf_context_print: prompt eval time =      44.85 ms /     7 tokens (    6.41 ms per token,   156.07 tokens per second)
0.01.174.361 I llama_perf_context_print:        eval time =     641.60 ms /    63 runs   (   10.18 ms per token,    98.19 tokens per second)
0.01.174.362 I llama_perf_context_print:       total time =     690.44 ms /    70 tokens
0.01.174.572 I ggml_metal_free: deallocating

real	0m1.194s
user	0m0.114s
sys	0m0.159s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.094 I build: 4586 (2711d021) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.197 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.028 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.017.033 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.039 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.040 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.040 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.041 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.041 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.042 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.042 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.043 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.043 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.043 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.044 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.044 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.046 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.046 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.047 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.697 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.766 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.536 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.538 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.538 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.538 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.539 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.539 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.539 I llama_model_loader: - type  f32:  194 tensors
0.00.025.540 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.540 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.540 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.541 I print_info: file format = GGUF V3 (latest)
0.00.025.541 I print_info: file type   = Q2_K - Medium
0.00.025.542 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.033.373 I load: special tokens cache size = 25
0.00.039.964 I load: token to piece cache size = 0.2984 MB
0.00.039.967 I print_info: arch             = gptneox
0.00.039.968 I print_info: vocab_only       = 0
0.00.039.968 I print_info: n_ctx_train      = 2048
0.00.039.968 I print_info: n_embd           = 2048
0.00.039.968 I print_info: n_layer          = 24
0.00.039.971 I print_info: n_head           = 16
0.00.039.972 I print_info: n_head_kv        = 16
0.00.039.972 I print_info: n_rot            = 32
0.00.039.972 I print_info: n_swa            = 0
0.00.039.973 I print_info: n_embd_head_k    = 128
0.00.039.975 I print_info: n_embd_head_v    = 128
0.00.039.976 I print_info: n_gqa            = 1
0.00.039.976 I print_info: n_embd_k_gqa     = 2048
0.00.039.978 I print_info: n_embd_v_gqa     = 2048
0.00.039.979 I print_info: f_norm_eps       = 1.0e-05
0.00.039.980 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.980 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.981 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.982 I print_info: f_logit_scale    = 0.0e+00
0.00.039.984 I print_info: n_ff             = 8192
0.00.039.984 I print_info: n_expert         = 0
0.00.039.985 I print_info: n_expert_used    = 0
0.00.039.985 I print_info: causal attn      = 1
0.00.039.985 I print_info: pooling type     = 0
0.00.039.985 I print_info: rope type        = 2
0.00.039.985 I print_info: rope scaling     = linear
0.00.039.986 I print_info: freq_base_train  = 10000.0
0.00.039.986 I print_info: freq_scale_train = 1
0.00.039.987 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.988 I print_info: rope_finetuned   = unknown
0.00.039.988 I print_info: ssm_d_conv       = 0
0.00.039.988 I print_info: ssm_d_inner      = 0
0.00.039.988 I print_info: ssm_d_state      = 0
0.00.039.988 I print_info: ssm_dt_rank      = 0
0.00.039.988 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.989 I print_info: model type       = 1.4B
0.00.039.989 I print_info: model params     = 1.41 B
0.00.039.989 I print_info: general.name     = 1.4B
0.00.039.989 I print_info: vocab type       = BPE
0.00.039.990 I print_info: n_vocab          = 50304
0.00.039.990 I print_info: n_merges         = 50009
0.00.039.990 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.990 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.990 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.990 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.991 I print_info: LF token         = 128 'Ä'
0.00.039.991 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.991 I print_info: max token length = 1024
0.00.338.893 I load_tensors: offloading 24 repeating layers to GPU
0.00.338.905 I load_tensors: offloading output layer to GPU
0.00.338.905 I load_tensors: offloaded 25/25 layers to GPU
0.00.338.939 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.338.940 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.340.261 I llama_init_from_model: n_seq_max     = 1
0.00.340.266 I llama_init_from_model: n_ctx         = 128
0.00.340.270 I llama_init_from_model: n_ctx_per_seq = 128
0.00.340.270 I llama_init_from_model: n_batch       = 128
0.00.340.271 I llama_init_from_model: n_ubatch      = 128
0.00.340.271 I llama_init_from_model: flash_attn    = 0
0.00.340.280 I llama_init_from_model: freq_base     = 10000.0
0.00.340.280 I llama_init_from_model: freq_scale    = 1
0.00.340.280 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.340.283 I ggml_metal_init: allocating
0.00.340.355 I ggml_metal_init: found device: Apple M4
0.00.340.381 I ggml_metal_init: picking default device: Apple M4
0.00.342.101 I ggml_metal_init: using embedded metal library
0.00.347.653 I ggml_metal_init: GPU name:   Apple M4
0.00.347.668 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.347.669 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.347.669 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.347.670 I ggml_metal_init: simdgroup reduction   = true
0.00.347.670 I ggml_metal_init: simdgroup matrix mul. = true
0.00.347.671 I ggml_metal_init: has residency sets    = true
0.00.347.671 I ggml_metal_init: has bfloat            = true
0.00.347.671 I ggml_metal_init: use bfloat            = true
0.00.347.674 I ggml_metal_init: hasUnifiedMemory      = true
0.00.347.678 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.369.219 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.372.924 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.372.928 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.372.958 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.376.255 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.376.257 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.376.257 I llama_init_from_model: graph nodes  = 967
0.00.376.257 I llama_init_from_model: graph splits = 2
0.00.376.260 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.376.261 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.405.138 I 
0.00.405.213 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.405.230 I perplexity: tokenizing the input ..
0.00.410.963 I perplexity: tokenization took 5.731 ms
0.00.410.973 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.542.619 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.544.022 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.544.035 I llama_perf_context_print:        load time =     394.93 ms
0.00.544.036 I llama_perf_context_print: prompt eval time =     131.42 ms /   128 tokens (    1.03 ms per token,   974.01 tokens per second)
0.00.544.037 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.544.037 I llama_perf_context_print:       total time =     138.90 ms /   129 tokens
0.00.544.404 I ggml_metal_free: deallocating

real	0m0.562s
user	0m0.078s
sys	0m0.091s
```
- q3_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4586 (2711d021) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.077 I main: llama backend init
0.00.000.079 I main: load the model and apply lora adapter, if any
0.00.008.842 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.351 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.362 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.363 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.364 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.364 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.364 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.365 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.366 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.366 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.366 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.367 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.367 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.367 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.368 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.369 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.369 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.370 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.227 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.277 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.094 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.095 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.095 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.096 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.096 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.096 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.097 I llama_model_loader: - type  f32:  194 tensors
0.00.025.097 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.097 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.098 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.098 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.098 I print_info: file format = GGUF V3 (latest)
0.00.025.099 I print_info: file type   = Q3_K - Medium
0.00.025.100 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.032.947 I load: special tokens cache size = 25
0.00.038.800 I load: token to piece cache size = 0.2984 MB
0.00.038.802 I print_info: arch             = gptneox
0.00.038.803 I print_info: vocab_only       = 0
0.00.038.803 I print_info: n_ctx_train      = 2048
0.00.038.803 I print_info: n_embd           = 2048
0.00.038.803 I print_info: n_layer          = 24
0.00.038.806 I print_info: n_head           = 16
0.00.038.807 I print_info: n_head_kv        = 16
0.00.038.807 I print_info: n_rot            = 32
0.00.038.807 I print_info: n_swa            = 0
0.00.038.807 I print_info: n_embd_head_k    = 128
0.00.038.808 I print_info: n_embd_head_v    = 128
0.00.038.808 I print_info: n_gqa            = 1
0.00.038.811 I print_info: n_embd_k_gqa     = 2048
0.00.038.812 I print_info: n_embd_v_gqa     = 2048
0.00.038.814 I print_info: f_norm_eps       = 1.0e-05
0.00.038.814 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.815 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.815 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.815 I print_info: f_logit_scale    = 0.0e+00
0.00.038.816 I print_info: n_ff             = 8192
0.00.038.816 I print_info: n_expert         = 0
0.00.038.817 I print_info: n_expert_used    = 0
0.00.038.817 I print_info: causal attn      = 1
0.00.038.818 I print_info: pooling type     = 0
0.00.038.818 I print_info: rope type        = 2
0.00.038.818 I print_info: rope scaling     = linear
0.00.038.818 I print_info: freq_base_train  = 10000.0
0.00.038.819 I print_info: freq_scale_train = 1
0.00.038.819 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.819 I print_info: rope_finetuned   = unknown
0.00.038.819 I print_info: ssm_d_conv       = 0
0.00.038.820 I print_info: ssm_d_inner      = 0
0.00.038.820 I print_info: ssm_d_state      = 0
0.00.038.820 I print_info: ssm_dt_rank      = 0
0.00.038.824 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.824 I print_info: model type       = 1.4B
0.00.038.824 I print_info: model params     = 1.41 B
0.00.038.825 I print_info: general.name     = 1.4B
0.00.038.825 I print_info: vocab type       = BPE
0.00.038.825 I print_info: n_vocab          = 50304
0.00.038.825 I print_info: n_merges         = 50009
0.00.038.826 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.826 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.826 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.826 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.826 I print_info: LF token         = 128 'Ä'
0.00.038.827 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.829 I print_info: max token length = 1024
0.00.435.385 I load_tensors: offloading 24 repeating layers to GPU
0.00.435.402 I load_tensors: offloading output layer to GPU
0.00.435.403 I load_tensors: offloaded 25/25 layers to GPU
0.00.435.436 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.435.443 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.436.915 I llama_init_from_model: n_seq_max     = 1
0.00.436.922 I llama_init_from_model: n_ctx         = 2048
0.00.436.923 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.436.923 I llama_init_from_model: n_batch       = 2048
0.00.436.923 I llama_init_from_model: n_ubatch      = 512
0.00.436.924 I llama_init_from_model: flash_attn    = 0
0.00.436.926 I llama_init_from_model: freq_base     = 10000.0
0.00.436.926 I llama_init_from_model: freq_scale    = 1
0.00.436.940 I ggml_metal_init: allocating
0.00.437.028 I ggml_metal_init: found device: Apple M4
0.00.437.058 I ggml_metal_init: picking default device: Apple M4
0.00.438.855 I ggml_metal_init: using embedded metal library
0.00.444.398 I ggml_metal_init: GPU name:   Apple M4
0.00.444.402 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.444.404 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.444.404 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.444.405 I ggml_metal_init: simdgroup reduction   = true
0.00.444.405 I ggml_metal_init: simdgroup matrix mul. = true
0.00.444.405 I ggml_metal_init: has residency sets    = true
0.00.444.406 I ggml_metal_init: has bfloat            = true
0.00.444.406 I ggml_metal_init: use bfloat            = true
0.00.444.407 I ggml_metal_init: hasUnifiedMemory      = true
0.00.444.409 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.463.835 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.517.552 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.517.558 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.517.583 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.522.015 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.522.017 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.522.018 I llama_init_from_model: graph nodes  = 967
0.00.522.018 I llama_init_from_model: graph splits = 2
0.00.522.024 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.522.158 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.522.158 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.576.894 I main: llama threadpool init, n_threads = 4
0.00.576.936 I 
0.00.576.961 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.576.961 I 
0.00.577.115 I sampler seed: 1234
0.00.577.120 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.577.140 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.577.141 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.577.141 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.318.784 I llama_perf_sampler_print:    sampling time =       1.30 ms /    71 runs   (    0.02 ms per token, 54489.64 tokens per second)
0.01.318.785 I llama_perf_context_print:        load time =     567.18 ms
0.01.318.786 I llama_perf_context_print: prompt eval time =      40.13 ms /     7 tokens (    5.73 ms per token,   174.45 tokens per second)
0.01.318.787 I llama_perf_context_print:        eval time =     698.71 ms /    63 runs   (   11.09 ms per token,    90.17 tokens per second)
0.01.318.787 I llama_perf_context_print:       total time =     742.76 ms /    70 tokens
0.01.318.994 I ggml_metal_free: deallocating

real	0m1.336s
user	0m0.108s
sys	0m0.178s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.102 I build: 4586 (2711d021) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.824 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.061 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.067 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.074 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.074 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.075 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.075 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.076 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.077 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.077 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.077 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.078 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.078 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.078 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.079 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.080 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.081 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.081 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.940 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.946 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.754 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.755 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.755 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.756 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.756 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.756 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.757 I llama_model_loader: - type  f32:  194 tensors
0.00.024.757 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.757 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.758 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.758 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.759 I print_info: file format = GGUF V3 (latest)
0.00.024.759 I print_info: file type   = Q3_K - Medium
0.00.024.760 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.032.952 I load: special tokens cache size = 25
0.00.038.976 I load: token to piece cache size = 0.2984 MB
0.00.038.979 I print_info: arch             = gptneox
0.00.038.979 I print_info: vocab_only       = 0
0.00.038.980 I print_info: n_ctx_train      = 2048
0.00.038.980 I print_info: n_embd           = 2048
0.00.038.980 I print_info: n_layer          = 24
0.00.038.983 I print_info: n_head           = 16
0.00.038.984 I print_info: n_head_kv        = 16
0.00.038.984 I print_info: n_rot            = 32
0.00.038.984 I print_info: n_swa            = 0
0.00.038.984 I print_info: n_embd_head_k    = 128
0.00.038.986 I print_info: n_embd_head_v    = 128
0.00.038.986 I print_info: n_gqa            = 1
0.00.038.987 I print_info: n_embd_k_gqa     = 2048
0.00.038.988 I print_info: n_embd_v_gqa     = 2048
0.00.038.993 I print_info: f_norm_eps       = 1.0e-05
0.00.038.993 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.993 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.994 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.994 I print_info: f_logit_scale    = 0.0e+00
0.00.038.995 I print_info: n_ff             = 8192
0.00.038.995 I print_info: n_expert         = 0
0.00.038.995 I print_info: n_expert_used    = 0
0.00.038.995 I print_info: causal attn      = 1
0.00.038.995 I print_info: pooling type     = 0
0.00.038.995 I print_info: rope type        = 2
0.00.038.996 I print_info: rope scaling     = linear
0.00.038.996 I print_info: freq_base_train  = 10000.0
0.00.038.996 I print_info: freq_scale_train = 1
0.00.038.998 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.998 I print_info: rope_finetuned   = unknown
0.00.038.999 I print_info: ssm_d_conv       = 0
0.00.038.999 I print_info: ssm_d_inner      = 0
0.00.038.999 I print_info: ssm_d_state      = 0
0.00.038.999 I print_info: ssm_dt_rank      = 0
0.00.038.999 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.999 I print_info: model type       = 1.4B
0.00.039.000 I print_info: model params     = 1.41 B
0.00.039.000 I print_info: general.name     = 1.4B
0.00.039.001 I print_info: vocab type       = BPE
0.00.039.001 I print_info: n_vocab          = 50304
0.00.039.001 I print_info: n_merges         = 50009
0.00.039.001 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.001 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.001 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.001 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.002 I print_info: LF token         = 128 'Ä'
0.00.039.003 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.003 I print_info: max token length = 1024
0.00.436.556 I load_tensors: offloading 24 repeating layers to GPU
0.00.436.565 I load_tensors: offloading output layer to GPU
0.00.436.566 I load_tensors: offloaded 25/25 layers to GPU
0.00.436.583 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.436.584 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.437.432 I llama_init_from_model: n_seq_max     = 1
0.00.437.434 I llama_init_from_model: n_ctx         = 128
0.00.437.434 I llama_init_from_model: n_ctx_per_seq = 128
0.00.437.435 I llama_init_from_model: n_batch       = 128
0.00.437.435 I llama_init_from_model: n_ubatch      = 128
0.00.437.435 I llama_init_from_model: flash_attn    = 0
0.00.437.436 I llama_init_from_model: freq_base     = 10000.0
0.00.437.437 I llama_init_from_model: freq_scale    = 1
0.00.437.437 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.437.439 I ggml_metal_init: allocating
0.00.437.473 I ggml_metal_init: found device: Apple M4
0.00.437.484 I ggml_metal_init: picking default device: Apple M4
0.00.438.463 I ggml_metal_init: using embedded metal library
0.00.442.681 I ggml_metal_init: GPU name:   Apple M4
0.00.442.686 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.442.687 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.442.688 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.442.688 I ggml_metal_init: simdgroup reduction   = true
0.00.442.688 I ggml_metal_init: simdgroup matrix mul. = true
0.00.442.689 I ggml_metal_init: has residency sets    = true
0.00.442.689 I ggml_metal_init: has bfloat            = true
0.00.442.689 I ggml_metal_init: use bfloat            = true
0.00.442.691 I ggml_metal_init: hasUnifiedMemory      = true
0.00.442.693 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.457.965 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.459.606 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.459.609 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.459.631 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.461.281 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.461.282 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.461.282 I llama_init_from_model: graph nodes  = 967
0.00.461.282 I llama_init_from_model: graph splits = 2
0.00.461.284 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.461.284 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.487.425 I 
0.00.487.462 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.487.471 I perplexity: tokenizing the input ..
0.00.491.261 I perplexity: tokenization took 3.788 ms
0.00.491.271 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.635.927 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.637.338 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.637.352 I llama_perf_context_print:        load time =     478.60 ms
0.00.637.353 I llama_perf_context_print: prompt eval time =     144.43 ms /   128 tokens (    1.13 ms per token,   886.24 tokens per second)
0.00.637.354 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.637.354 I llama_perf_context_print:       total time =     149.93 ms /   129 tokens
0.00.637.698 I ggml_metal_free: deallocating

real	0m0.651s
user	0m0.069s
sys	0m0.091s
```
- q4_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.043 I build: 4586 (2711d021) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.080 I main: llama backend init
0.00.000.082 I main: load the model and apply lora adapter, if any
0.00.008.881 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.575 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.581 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.582 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.583 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.583 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.584 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.584 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.585 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.585 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.586 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.586 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.586 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.587 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.587 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.590 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.590 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.590 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.516 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.523 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.323 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.324 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.324 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.324 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.325 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.325 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.326 I llama_model_loader: - type  f32:  194 tensors
0.00.025.326 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.326 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.326 I llama_model_loader: - type q6_K:   13 tensors
0.00.025.327 I print_info: file format = GGUF V3 (latest)
0.00.025.327 I print_info: file type   = Q4_K - Medium
0.00.025.328 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.033.538 I load: special tokens cache size = 25
0.00.039.571 I load: token to piece cache size = 0.2984 MB
0.00.039.574 I print_info: arch             = gptneox
0.00.039.574 I print_info: vocab_only       = 0
0.00.039.575 I print_info: n_ctx_train      = 2048
0.00.039.575 I print_info: n_embd           = 2048
0.00.039.575 I print_info: n_layer          = 24
0.00.039.578 I print_info: n_head           = 16
0.00.039.578 I print_info: n_head_kv        = 16
0.00.039.579 I print_info: n_rot            = 32
0.00.039.579 I print_info: n_swa            = 0
0.00.039.579 I print_info: n_embd_head_k    = 128
0.00.039.580 I print_info: n_embd_head_v    = 128
0.00.039.581 I print_info: n_gqa            = 1
0.00.039.582 I print_info: n_embd_k_gqa     = 2048
0.00.039.582 I print_info: n_embd_v_gqa     = 2048
0.00.039.583 I print_info: f_norm_eps       = 1.0e-05
0.00.039.583 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.583 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.584 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.584 I print_info: f_logit_scale    = 0.0e+00
0.00.039.584 I print_info: n_ff             = 8192
0.00.039.585 I print_info: n_expert         = 0
0.00.039.585 I print_info: n_expert_used    = 0
0.00.039.585 I print_info: causal attn      = 1
0.00.039.586 I print_info: pooling type     = 0
0.00.039.588 I print_info: rope type        = 2
0.00.039.588 I print_info: rope scaling     = linear
0.00.039.588 I print_info: freq_base_train  = 10000.0
0.00.039.589 I print_info: freq_scale_train = 1
0.00.039.589 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.589 I print_info: rope_finetuned   = unknown
0.00.039.589 I print_info: ssm_d_conv       = 0
0.00.039.589 I print_info: ssm_d_inner      = 0
0.00.039.590 I print_info: ssm_d_state      = 0
0.00.039.590 I print_info: ssm_dt_rank      = 0
0.00.039.590 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.590 I print_info: model type       = 1.4B
0.00.039.591 I print_info: model params     = 1.41 B
0.00.039.591 I print_info: general.name     = 1.4B
0.00.039.591 I print_info: vocab type       = BPE
0.00.039.591 I print_info: n_vocab          = 50304
0.00.039.592 I print_info: n_merges         = 50009
0.00.039.593 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.593 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.594 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.594 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.594 I print_info: LF token         = 128 'Ä'
0.00.039.594 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.595 I print_info: max token length = 1024
0.00.517.672 I load_tensors: offloading 24 repeating layers to GPU
0.00.517.690 I load_tensors: offloading output layer to GPU
0.00.517.691 I load_tensors: offloaded 25/25 layers to GPU
0.00.517.723 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.517.724 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.519.235 I llama_init_from_model: n_seq_max     = 1
0.00.519.240 I llama_init_from_model: n_ctx         = 2048
0.00.519.240 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.519.241 I llama_init_from_model: n_batch       = 2048
0.00.519.241 I llama_init_from_model: n_ubatch      = 512
0.00.519.241 I llama_init_from_model: flash_attn    = 0
0.00.519.244 I llama_init_from_model: freq_base     = 10000.0
0.00.519.249 I llama_init_from_model: freq_scale    = 1
0.00.519.255 I ggml_metal_init: allocating
0.00.519.330 I ggml_metal_init: found device: Apple M4
0.00.519.344 I ggml_metal_init: picking default device: Apple M4
0.00.521.162 I ggml_metal_init: using embedded metal library
0.00.527.718 I ggml_metal_init: GPU name:   Apple M4
0.00.527.722 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.527.723 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.527.724 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.527.724 I ggml_metal_init: simdgroup reduction   = true
0.00.527.725 I ggml_metal_init: simdgroup matrix mul. = true
0.00.527.725 I ggml_metal_init: has residency sets    = true
0.00.527.725 I ggml_metal_init: has bfloat            = true
0.00.527.726 I ggml_metal_init: use bfloat            = true
0.00.527.727 I ggml_metal_init: hasUnifiedMemory      = true
0.00.527.728 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.546.456 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.604.006 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.604.014 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.604.036 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.607.945 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.607.947 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.607.948 I llama_init_from_model: graph nodes  = 967
0.00.607.948 I llama_init_from_model: graph splits = 2
0.00.607.954 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.608.082 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.608.083 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.667.856 I main: llama threadpool init, n_threads = 4
0.00.667.901 I 
0.00.667.925 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.667.926 I 
0.00.668.096 I sampler seed: 1234
0.00.668.101 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.668.111 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.668.112 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.668.112 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.439.859 I llama_perf_sampler_print:    sampling time =       1.43 ms /    71 runs   (    0.02 ms per token, 49511.85 tokens per second)
0.01.439.859 I llama_perf_context_print:        load time =     658.08 ms
0.01.439.860 I llama_perf_context_print: prompt eval time =      57.50 ms /     7 tokens (    8.21 ms per token,   121.74 tokens per second)
0.01.439.861 I llama_perf_context_print:        eval time =     711.21 ms /    63 runs   (   11.29 ms per token,    88.58 tokens per second)
0.01.439.861 I llama_perf_context_print:       total time =     772.90 ms /    70 tokens
0.01.440.149 I ggml_metal_free: deallocating

real	0m1.457s
user	0m0.110s
sys	0m0.198s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.095 I build: 4586 (2711d021) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.996 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.057 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.017.063 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.065 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.065 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.066 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.066 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.066 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.068 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.069 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.069 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.069 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.069 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.070 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.070 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.072 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.073 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.073 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.100 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.179 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.214 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.215 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.215 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.216 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.217 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.218 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.026.219 I llama_model_loader: - type  f32:  194 tensors
0.00.026.219 I llama_model_loader: - type q4_K:   61 tensors
0.00.026.219 I llama_model_loader: - type q5_K:   24 tensors
0.00.026.219 I llama_model_loader: - type q6_K:   13 tensors
0.00.026.220 I print_info: file format = GGUF V3 (latest)
0.00.026.221 I print_info: file type   = Q4_K - Medium
0.00.026.222 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.034.682 I load: special tokens cache size = 25
0.00.040.568 I load: token to piece cache size = 0.2984 MB
0.00.040.573 I print_info: arch             = gptneox
0.00.040.573 I print_info: vocab_only       = 0
0.00.040.573 I print_info: n_ctx_train      = 2048
0.00.040.573 I print_info: n_embd           = 2048
0.00.040.573 I print_info: n_layer          = 24
0.00.040.577 I print_info: n_head           = 16
0.00.040.581 I print_info: n_head_kv        = 16
0.00.040.581 I print_info: n_rot            = 32
0.00.040.581 I print_info: n_swa            = 0
0.00.040.581 I print_info: n_embd_head_k    = 128
0.00.040.582 I print_info: n_embd_head_v    = 128
0.00.040.582 I print_info: n_gqa            = 1
0.00.040.583 I print_info: n_embd_k_gqa     = 2048
0.00.040.584 I print_info: n_embd_v_gqa     = 2048
0.00.040.585 I print_info: f_norm_eps       = 1.0e-05
0.00.040.585 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.585 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.585 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.585 I print_info: f_logit_scale    = 0.0e+00
0.00.040.586 I print_info: n_ff             = 8192
0.00.040.586 I print_info: n_expert         = 0
0.00.040.586 I print_info: n_expert_used    = 0
0.00.040.586 I print_info: causal attn      = 1
0.00.040.587 I print_info: pooling type     = 0
0.00.040.587 I print_info: rope type        = 2
0.00.040.588 I print_info: rope scaling     = linear
0.00.040.589 I print_info: freq_base_train  = 10000.0
0.00.040.589 I print_info: freq_scale_train = 1
0.00.040.590 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.590 I print_info: rope_finetuned   = unknown
0.00.040.590 I print_info: ssm_d_conv       = 0
0.00.040.590 I print_info: ssm_d_inner      = 0
0.00.040.590 I print_info: ssm_d_state      = 0
0.00.040.590 I print_info: ssm_dt_rank      = 0
0.00.040.590 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.591 I print_info: model type       = 1.4B
0.00.040.591 I print_info: model params     = 1.41 B
0.00.040.591 I print_info: general.name     = 1.4B
0.00.040.592 I print_info: vocab type       = BPE
0.00.040.592 I print_info: n_vocab          = 50304
0.00.040.592 I print_info: n_merges         = 50009
0.00.040.592 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.593 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.593 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.593 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.593 I print_info: LF token         = 128 'Ä'
0.00.040.594 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.594 I print_info: max token length = 1024
0.00.498.264 I load_tensors: offloading 24 repeating layers to GPU
0.00.498.281 I load_tensors: offloading output layer to GPU
0.00.498.282 I load_tensors: offloaded 25/25 layers to GPU
0.00.498.316 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.498.317 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.499.293 I llama_init_from_model: n_seq_max     = 1
0.00.499.298 I llama_init_from_model: n_ctx         = 128
0.00.499.298 I llama_init_from_model: n_ctx_per_seq = 128
0.00.499.299 I llama_init_from_model: n_batch       = 128
0.00.499.299 I llama_init_from_model: n_ubatch      = 128
0.00.499.299 I llama_init_from_model: flash_attn    = 0
0.00.499.301 I llama_init_from_model: freq_base     = 10000.0
0.00.499.302 I llama_init_from_model: freq_scale    = 1
0.00.499.303 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.499.308 I ggml_metal_init: allocating
0.00.499.392 I ggml_metal_init: found device: Apple M4
0.00.499.406 I ggml_metal_init: picking default device: Apple M4
0.00.501.119 I ggml_metal_init: using embedded metal library
0.00.507.788 I ggml_metal_init: GPU name:   Apple M4
0.00.507.796 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.507.796 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.507.797 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.507.798 I ggml_metal_init: simdgroup reduction   = true
0.00.507.798 I ggml_metal_init: simdgroup matrix mul. = true
0.00.507.799 I ggml_metal_init: has residency sets    = true
0.00.507.799 I ggml_metal_init: has bfloat            = true
0.00.507.799 I ggml_metal_init: use bfloat            = true
0.00.507.800 I ggml_metal_init: hasUnifiedMemory      = true
0.00.507.803 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.525.647 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.529.091 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.529.095 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.529.144 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.532.482 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.532.484 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.532.484 I llama_init_from_model: graph nodes  = 967
0.00.532.485 I llama_init_from_model: graph splits = 2
0.00.532.488 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.532.488 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.559.099 I 
0.00.559.178 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.559.196 I perplexity: tokenizing the input ..
0.00.565.615 I perplexity: tokenization took 6.418 ms
0.00.565.633 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.699.520 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.700.842 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.700.856 I llama_perf_context_print:        load time =     550.09 ms
0.00.700.858 I llama_perf_context_print: prompt eval time =     133.50 ms /   128 tokens (    1.04 ms per token,   958.77 tokens per second)
0.00.700.859 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.700.859 I llama_perf_context_print:       total time =     141.76 ms /   129 tokens
0.00.701.191 I ggml_metal_free: deallocating

real	0m0.714s
user	0m0.079s
sys	0m0.110s
```
- q5_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4586 (2711d021) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.075 I main: llama backend init
0.00.000.077 I main: load the model and apply lora adapter, if any
0.00.010.382 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.982 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.017.987 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.990 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.991 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.991 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.991 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.992 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.993 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.993 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.993 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.994 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.994 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.995 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.995 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.997 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.997 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.998 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.866 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.912 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.655 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.656 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.657 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.657 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.657 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.658 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.026.658 I llama_model_loader: - type  f32:  194 tensors
0.00.026.659 I llama_model_loader: - type q5_K:   61 tensors
0.00.026.659 I llama_model_loader: - type q6_K:   37 tensors
0.00.026.659 I print_info: file format = GGUF V3 (latest)
0.00.026.660 I print_info: file type   = Q5_K - Medium
0.00.026.661 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.034.543 I load: special tokens cache size = 25
0.00.040.254 I load: token to piece cache size = 0.2984 MB
0.00.040.257 I print_info: arch             = gptneox
0.00.040.257 I print_info: vocab_only       = 0
0.00.040.257 I print_info: n_ctx_train      = 2048
0.00.040.257 I print_info: n_embd           = 2048
0.00.040.257 I print_info: n_layer          = 24
0.00.040.260 I print_info: n_head           = 16
0.00.040.261 I print_info: n_head_kv        = 16
0.00.040.261 I print_info: n_rot            = 32
0.00.040.261 I print_info: n_swa            = 0
0.00.040.263 I print_info: n_embd_head_k    = 128
0.00.040.263 I print_info: n_embd_head_v    = 128
0.00.040.264 I print_info: n_gqa            = 1
0.00.040.265 I print_info: n_embd_k_gqa     = 2048
0.00.040.265 I print_info: n_embd_v_gqa     = 2048
0.00.040.266 I print_info: f_norm_eps       = 1.0e-05
0.00.040.266 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.266 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.266 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.267 I print_info: f_logit_scale    = 0.0e+00
0.00.040.267 I print_info: n_ff             = 8192
0.00.040.267 I print_info: n_expert         = 0
0.00.040.268 I print_info: n_expert_used    = 0
0.00.040.268 I print_info: causal attn      = 1
0.00.040.268 I print_info: pooling type     = 0
0.00.040.268 I print_info: rope type        = 2
0.00.040.268 I print_info: rope scaling     = linear
0.00.040.269 I print_info: freq_base_train  = 10000.0
0.00.040.269 I print_info: freq_scale_train = 1
0.00.040.269 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.269 I print_info: rope_finetuned   = unknown
0.00.040.270 I print_info: ssm_d_conv       = 0
0.00.040.271 I print_info: ssm_d_inner      = 0
0.00.040.271 I print_info: ssm_d_state      = 0
0.00.040.273 I print_info: ssm_dt_rank      = 0
0.00.040.273 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.273 I print_info: model type       = 1.4B
0.00.040.273 I print_info: model params     = 1.41 B
0.00.040.274 I print_info: general.name     = 1.4B
0.00.040.274 I print_info: vocab type       = BPE
0.00.040.274 I print_info: n_vocab          = 50304
0.00.040.275 I print_info: n_merges         = 50009
0.00.040.275 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.275 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.276 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.276 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.276 I print_info: LF token         = 128 'Ä'
0.00.040.276 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.276 I print_info: max token length = 1024
0.00.583.092 I load_tensors: offloading 24 repeating layers to GPU
0.00.583.106 I load_tensors: offloading output layer to GPU
0.00.583.106 I load_tensors: offloaded 25/25 layers to GPU
0.00.583.139 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.583.140 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.584.350 I llama_init_from_model: n_seq_max     = 1
0.00.584.353 I llama_init_from_model: n_ctx         = 2048
0.00.584.354 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.584.354 I llama_init_from_model: n_batch       = 2048
0.00.584.355 I llama_init_from_model: n_ubatch      = 512
0.00.584.355 I llama_init_from_model: flash_attn    = 0
0.00.584.356 I llama_init_from_model: freq_base     = 10000.0
0.00.584.357 I llama_init_from_model: freq_scale    = 1
0.00.584.358 I ggml_metal_init: allocating
0.00.584.374 I ggml_metal_init: found device: Apple M4
0.00.584.383 I ggml_metal_init: picking default device: Apple M4
0.00.585.762 I ggml_metal_init: using embedded metal library
0.00.592.035 I ggml_metal_init: GPU name:   Apple M4
0.00.592.038 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.592.039 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.592.040 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.592.041 I ggml_metal_init: simdgroup reduction   = true
0.00.592.041 I ggml_metal_init: simdgroup matrix mul. = true
0.00.592.041 I ggml_metal_init: has residency sets    = true
0.00.592.041 I ggml_metal_init: has bfloat            = true
0.00.592.042 I ggml_metal_init: use bfloat            = true
0.00.592.043 I ggml_metal_init: hasUnifiedMemory      = true
0.00.592.044 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.609.372 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.667.970 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.667.976 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.668.000 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.672.369 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.672.371 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.672.372 I llama_init_from_model: graph nodes  = 967
0.00.672.372 I llama_init_from_model: graph splits = 2
0.00.672.378 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.672.509 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.672.509 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.734.230 I main: llama threadpool init, n_threads = 4
0.00.734.272 I 
0.00.734.308 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.734.309 I 
0.00.734.459 I sampler seed: 1234
0.00.734.464 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.734.483 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.734.483 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.734.484 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.593.318 I llama_perf_sampler_print:    sampling time =       1.35 ms /    71 runs   (    0.02 ms per token, 52670.62 tokens per second)
0.01.593.318 I llama_perf_context_print:        load time =     722.96 ms
0.01.593.319 I llama_perf_context_print: prompt eval time =      51.19 ms /     7 tokens (    7.31 ms per token,   136.74 tokens per second)
0.01.593.320 I llama_perf_context_print:        eval time =     804.58 ms /    63 runs   (   12.77 ms per token,    78.30 tokens per second)
0.01.593.320 I llama_perf_context_print:       total time =     859.97 ms /    70 tokens
0.01.593.553 I ggml_metal_free: deallocating

real	0m1.612s
user	0m0.108s
sys	0m0.206s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.099 I build: 4586 (2711d021) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.185 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.115 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.017.120 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.122 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.122 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.122 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.123 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.123 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.124 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.124 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.124 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.125 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.125 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.125 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.126 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.128 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.128 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.129 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.991 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.034 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.848 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.849 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.849 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.849 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.850 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.850 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.851 I llama_model_loader: - type  f32:  194 tensors
0.00.025.851 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.851 I llama_model_loader: - type q6_K:   37 tensors
0.00.025.852 I print_info: file format = GGUF V3 (latest)
0.00.025.852 I print_info: file type   = Q5_K - Medium
0.00.025.855 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.033.663 I load: special tokens cache size = 25
0.00.039.468 I load: token to piece cache size = 0.2984 MB
0.00.039.471 I print_info: arch             = gptneox
0.00.039.471 I print_info: vocab_only       = 0
0.00.039.471 I print_info: n_ctx_train      = 2048
0.00.039.472 I print_info: n_embd           = 2048
0.00.039.472 I print_info: n_layer          = 24
0.00.039.474 I print_info: n_head           = 16
0.00.039.475 I print_info: n_head_kv        = 16
0.00.039.475 I print_info: n_rot            = 32
0.00.039.475 I print_info: n_swa            = 0
0.00.039.475 I print_info: n_embd_head_k    = 128
0.00.039.476 I print_info: n_embd_head_v    = 128
0.00.039.476 I print_info: n_gqa            = 1
0.00.039.477 I print_info: n_embd_k_gqa     = 2048
0.00.039.478 I print_info: n_embd_v_gqa     = 2048
0.00.039.478 I print_info: f_norm_eps       = 1.0e-05
0.00.039.479 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.479 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.479 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.479 I print_info: f_logit_scale    = 0.0e+00
0.00.039.480 I print_info: n_ff             = 8192
0.00.039.480 I print_info: n_expert         = 0
0.00.039.480 I print_info: n_expert_used    = 0
0.00.039.480 I print_info: causal attn      = 1
0.00.039.481 I print_info: pooling type     = 0
0.00.039.481 I print_info: rope type        = 2
0.00.039.481 I print_info: rope scaling     = linear
0.00.039.481 I print_info: freq_base_train  = 10000.0
0.00.039.482 I print_info: freq_scale_train = 1
0.00.039.482 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.482 I print_info: rope_finetuned   = unknown
0.00.039.482 I print_info: ssm_d_conv       = 0
0.00.039.482 I print_info: ssm_d_inner      = 0
0.00.039.483 I print_info: ssm_d_state      = 0
0.00.039.483 I print_info: ssm_dt_rank      = 0
0.00.039.483 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.483 I print_info: model type       = 1.4B
0.00.039.484 I print_info: model params     = 1.41 B
0.00.039.484 I print_info: general.name     = 1.4B
0.00.039.484 I print_info: vocab type       = BPE
0.00.039.484 I print_info: n_vocab          = 50304
0.00.039.484 I print_info: n_merges         = 50009
0.00.039.485 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.485 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.485 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.485 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.486 I print_info: LF token         = 128 'Ä'
0.00.039.486 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.486 I print_info: max token length = 1024
0.00.597.825 I load_tensors: offloading 24 repeating layers to GPU
0.00.597.840 I load_tensors: offloading output layer to GPU
0.00.597.841 I load_tensors: offloaded 25/25 layers to GPU
0.00.597.872 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.597.873 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.599.311 I llama_init_from_model: n_seq_max     = 1
0.00.599.315 I llama_init_from_model: n_ctx         = 128
0.00.599.315 I llama_init_from_model: n_ctx_per_seq = 128
0.00.599.319 I llama_init_from_model: n_batch       = 128
0.00.599.320 I llama_init_from_model: n_ubatch      = 128
0.00.599.320 I llama_init_from_model: flash_attn    = 0
0.00.599.322 I llama_init_from_model: freq_base     = 10000.0
0.00.599.322 I llama_init_from_model: freq_scale    = 1
0.00.599.327 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.599.329 I ggml_metal_init: allocating
0.00.599.367 I ggml_metal_init: found device: Apple M4
0.00.599.377 I ggml_metal_init: picking default device: Apple M4
0.00.600.828 I ggml_metal_init: using embedded metal library
0.00.607.046 I ggml_metal_init: GPU name:   Apple M4
0.00.607.050 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.607.051 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.607.052 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.607.052 I ggml_metal_init: simdgroup reduction   = true
0.00.607.053 I ggml_metal_init: simdgroup matrix mul. = true
0.00.607.053 I ggml_metal_init: has residency sets    = true
0.00.607.054 I ggml_metal_init: has bfloat            = true
0.00.607.054 I ggml_metal_init: use bfloat            = true
0.00.607.055 I ggml_metal_init: hasUnifiedMemory      = true
0.00.607.056 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.624.238 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.627.808 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.627.811 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.627.839 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.630.928 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.630.930 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.630.930 I llama_init_from_model: graph nodes  = 967
0.00.630.931 I llama_init_from_model: graph splits = 2
0.00.630.933 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.630.934 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.663.193 I 
0.00.663.289 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.663.311 I perplexity: tokenizing the input ..
0.00.670.811 I perplexity: tokenization took 7.495 ms
0.00.670.832 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.812.778 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.814.118 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.814.139 I llama_perf_context_print:        load time =     653.00 ms
0.00.814.140 I llama_perf_context_print: prompt eval time =     141.00 ms /   128 tokens (    1.10 ms per token,   907.77 tokens per second)
0.00.814.141 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.814.141 I llama_perf_context_print:       total time =     150.95 ms /   129 tokens
0.00.814.533 I ggml_metal_free: deallocating

real	0m0.830s
user	0m0.079s
sys	0m0.143s
```
- q6_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.043 I build: 4586 (2711d021) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.087 I main: llama backend init
0.00.000.089 I main: load the model and apply lora adapter, if any
0.00.009.189 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.333 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.017.337 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.343 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.344 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.344 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.344 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.345 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.346 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.346 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.347 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.347 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.347 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.348 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.348 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.350 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.350 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.350 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.183 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.231 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.999 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.000 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.000 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.001 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.001 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.001 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.026.002 I llama_model_loader: - type  f32:  194 tensors
0.00.026.002 I llama_model_loader: - type q6_K:   98 tensors
0.00.026.003 I print_info: file format = GGUF V3 (latest)
0.00.026.003 I print_info: file type   = Q6_K
0.00.026.004 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.033.844 I load: special tokens cache size = 25
0.00.039.654 I load: token to piece cache size = 0.2984 MB
0.00.039.657 I print_info: arch             = gptneox
0.00.039.657 I print_info: vocab_only       = 0
0.00.039.657 I print_info: n_ctx_train      = 2048
0.00.039.657 I print_info: n_embd           = 2048
0.00.039.658 I print_info: n_layer          = 24
0.00.039.661 I print_info: n_head           = 16
0.00.039.662 I print_info: n_head_kv        = 16
0.00.039.662 I print_info: n_rot            = 32
0.00.039.662 I print_info: n_swa            = 0
0.00.039.662 I print_info: n_embd_head_k    = 128
0.00.039.663 I print_info: n_embd_head_v    = 128
0.00.039.663 I print_info: n_gqa            = 1
0.00.039.664 I print_info: n_embd_k_gqa     = 2048
0.00.039.665 I print_info: n_embd_v_gqa     = 2048
0.00.039.666 I print_info: f_norm_eps       = 1.0e-05
0.00.039.666 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.666 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.666 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.666 I print_info: f_logit_scale    = 0.0e+00
0.00.039.667 I print_info: n_ff             = 8192
0.00.039.667 I print_info: n_expert         = 0
0.00.039.667 I print_info: n_expert_used    = 0
0.00.039.668 I print_info: causal attn      = 1
0.00.039.668 I print_info: pooling type     = 0
0.00.039.668 I print_info: rope type        = 2
0.00.039.669 I print_info: rope scaling     = linear
0.00.039.670 I print_info: freq_base_train  = 10000.0
0.00.039.670 I print_info: freq_scale_train = 1
0.00.039.670 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.670 I print_info: rope_finetuned   = unknown
0.00.039.670 I print_info: ssm_d_conv       = 0
0.00.039.672 I print_info: ssm_d_inner      = 0
0.00.039.673 I print_info: ssm_d_state      = 0
0.00.039.673 I print_info: ssm_dt_rank      = 0
0.00.039.673 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.673 I print_info: model type       = 1.4B
0.00.039.673 I print_info: model params     = 1.41 B
0.00.039.673 I print_info: general.name     = 1.4B
0.00.039.674 I print_info: vocab type       = BPE
0.00.039.674 I print_info: n_vocab          = 50304
0.00.039.674 I print_info: n_merges         = 50009
0.00.039.675 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.675 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.675 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.675 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.675 I print_info: LF token         = 128 'Ä'
0.00.039.679 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.679 I print_info: max token length = 1024
0.00.669.044 I load_tensors: offloading 24 repeating layers to GPU
0.00.669.060 I load_tensors: offloading output layer to GPU
0.00.669.061 I load_tensors: offloaded 25/25 layers to GPU
0.00.669.092 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.669.094 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.670.188 I llama_init_from_model: n_seq_max     = 1
0.00.670.195 I llama_init_from_model: n_ctx         = 2048
0.00.670.195 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.670.196 I llama_init_from_model: n_batch       = 2048
0.00.670.196 I llama_init_from_model: n_ubatch      = 512
0.00.670.197 I llama_init_from_model: flash_attn    = 0
0.00.670.199 I llama_init_from_model: freq_base     = 10000.0
0.00.670.199 I llama_init_from_model: freq_scale    = 1
0.00.670.202 I ggml_metal_init: allocating
0.00.670.284 I ggml_metal_init: found device: Apple M4
0.00.670.298 I ggml_metal_init: picking default device: Apple M4
0.00.672.157 I ggml_metal_init: using embedded metal library
0.00.678.953 I ggml_metal_init: GPU name:   Apple M4
0.00.678.958 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.678.959 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.678.959 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.678.960 I ggml_metal_init: simdgroup reduction   = true
0.00.678.960 I ggml_metal_init: simdgroup matrix mul. = true
0.00.678.960 I ggml_metal_init: has residency sets    = true
0.00.678.961 I ggml_metal_init: has bfloat            = true
0.00.678.961 I ggml_metal_init: use bfloat            = true
0.00.678.962 I ggml_metal_init: hasUnifiedMemory      = true
0.00.678.964 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.696.876 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.755.003 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.755.009 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.755.034 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.759.193 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.759.194 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.759.195 I llama_init_from_model: graph nodes  = 967
0.00.759.195 I llama_init_from_model: graph splits = 2
0.00.759.200 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.759.328 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.759.329 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.818.592 I main: llama threadpool init, n_threads = 4
0.00.818.638 I 
0.00.818.662 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.818.664 I 
0.00.818.783 I sampler seed: 1234
0.00.818.787 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.818.808 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.818.808 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.818.808 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.740.138 I llama_perf_sampler_print:    sampling time =       1.33 ms /    71 runs   (    0.02 ms per token, 53343.35 tokens per second)
0.01.740.138 I llama_perf_context_print:        load time =     808.53 ms
0.01.740.139 I llama_perf_context_print: prompt eval time =      54.23 ms /     7 tokens (    7.75 ms per token,   129.07 tokens per second)
0.01.740.140 I llama_perf_context_print:        eval time =     864.12 ms /    63 runs   (   13.72 ms per token,    72.91 tokens per second)
0.01.740.140 I llama_perf_context_print:       total time =     922.42 ms /    70 tokens
0.01.740.408 I ggml_metal_free: deallocating

real	0m1.756s
user	0m0.111s
sys	0m0.234s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.090 I build: 4586 (2711d021) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.874 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.827 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.831 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.837 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.837 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.838 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.838 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.838 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.839 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.840 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.840 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.840 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.841 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.841 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.842 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.843 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.843 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.844 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.627 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.638 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.359 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.360 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.361 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.361 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.361 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.362 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.362 I llama_model_loader: - type  f32:  194 tensors
0.00.024.362 I llama_model_loader: - type q6_K:   98 tensors
0.00.024.363 I print_info: file format = GGUF V3 (latest)
0.00.024.364 I print_info: file type   = Q6_K
0.00.024.365 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.032.224 I load: special tokens cache size = 25
0.00.038.265 I load: token to piece cache size = 0.2984 MB
0.00.038.268 I print_info: arch             = gptneox
0.00.038.268 I print_info: vocab_only       = 0
0.00.038.268 I print_info: n_ctx_train      = 2048
0.00.038.268 I print_info: n_embd           = 2048
0.00.038.269 I print_info: n_layer          = 24
0.00.038.272 I print_info: n_head           = 16
0.00.038.273 I print_info: n_head_kv        = 16
0.00.038.273 I print_info: n_rot            = 32
0.00.038.273 I print_info: n_swa            = 0
0.00.038.273 I print_info: n_embd_head_k    = 128
0.00.038.273 I print_info: n_embd_head_v    = 128
0.00.038.274 I print_info: n_gqa            = 1
0.00.038.275 I print_info: n_embd_k_gqa     = 2048
0.00.038.276 I print_info: n_embd_v_gqa     = 2048
0.00.038.276 I print_info: f_norm_eps       = 1.0e-05
0.00.038.276 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.277 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.277 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.277 I print_info: f_logit_scale    = 0.0e+00
0.00.038.278 I print_info: n_ff             = 8192
0.00.038.278 I print_info: n_expert         = 0
0.00.038.278 I print_info: n_expert_used    = 0
0.00.038.278 I print_info: causal attn      = 1
0.00.038.280 I print_info: pooling type     = 0
0.00.038.280 I print_info: rope type        = 2
0.00.038.280 I print_info: rope scaling     = linear
0.00.038.280 I print_info: freq_base_train  = 10000.0
0.00.038.281 I print_info: freq_scale_train = 1
0.00.038.281 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.281 I print_info: rope_finetuned   = unknown
0.00.038.281 I print_info: ssm_d_conv       = 0
0.00.038.281 I print_info: ssm_d_inner      = 0
0.00.038.282 I print_info: ssm_d_state      = 0
0.00.038.282 I print_info: ssm_dt_rank      = 0
0.00.038.282 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.282 I print_info: model type       = 1.4B
0.00.038.282 I print_info: model params     = 1.41 B
0.00.038.283 I print_info: general.name     = 1.4B
0.00.038.283 I print_info: vocab type       = BPE
0.00.038.283 I print_info: n_vocab          = 50304
0.00.038.284 I print_info: n_merges         = 50009
0.00.038.286 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.286 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.286 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.287 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.287 I print_info: LF token         = 128 'Ä'
0.00.038.287 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.287 I print_info: max token length = 1024
0.00.608.459 I load_tensors: offloading 24 repeating layers to GPU
0.00.608.464 I load_tensors: offloading output layer to GPU
0.00.608.465 I load_tensors: offloaded 25/25 layers to GPU
0.00.608.491 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.608.494 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.609.886 I llama_init_from_model: n_seq_max     = 1
0.00.609.888 I llama_init_from_model: n_ctx         = 128
0.00.609.889 I llama_init_from_model: n_ctx_per_seq = 128
0.00.609.889 I llama_init_from_model: n_batch       = 128
0.00.609.893 I llama_init_from_model: n_ubatch      = 128
0.00.609.894 I llama_init_from_model: flash_attn    = 0
0.00.609.895 I llama_init_from_model: freq_base     = 10000.0
0.00.609.896 I llama_init_from_model: freq_scale    = 1
0.00.609.897 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.609.898 I ggml_metal_init: allocating
0.00.609.954 I ggml_metal_init: found device: Apple M4
0.00.609.966 I ggml_metal_init: picking default device: Apple M4
0.00.611.447 I ggml_metal_init: using embedded metal library
0.00.617.389 I ggml_metal_init: GPU name:   Apple M4
0.00.617.393 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.617.394 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.617.394 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.617.395 I ggml_metal_init: simdgroup reduction   = true
0.00.617.395 I ggml_metal_init: simdgroup matrix mul. = true
0.00.617.396 I ggml_metal_init: has residency sets    = true
0.00.617.396 I ggml_metal_init: has bfloat            = true
0.00.617.396 I ggml_metal_init: use bfloat            = true
0.00.617.397 I ggml_metal_init: hasUnifiedMemory      = true
0.00.617.402 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.634.239 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.637.727 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.637.730 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.637.755 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.641.162 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.641.163 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.641.164 I llama_init_from_model: graph nodes  = 967
0.00.641.164 I llama_init_from_model: graph splits = 2
0.00.641.167 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.641.167 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.678.769 I 
0.00.678.842 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.678.860 I perplexity: tokenizing the input ..
0.00.685.798 I perplexity: tokenization took 6.934 ms
0.00.685.821 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.825.415 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.826.742 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.826.757 I llama_perf_context_print:        load time =     669.89 ms
0.00.826.758 I llama_perf_context_print: prompt eval time =     138.93 ms /   128 tokens (    1.09 ms per token,   921.35 tokens per second)
0.00.826.758 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.826.759 I llama_perf_context_print:       total time =     147.99 ms /   129 tokens
0.00.827.134 I ggml_metal_free: deallocating

real	0m0.841s
user	0m0.077s
sys	0m0.147s
```
- save-load-state: 
```
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4586 (2711d021)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 'Ä'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x119604280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x119604a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x119604e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x1196052e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x119605750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x119605bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x119606030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x1196064a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x119606910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x119606d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x1196071f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x119607890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x1196083b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x119608b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x119609370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x119609a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x11960a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x11960a8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x11960aff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x11960b7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x11960bee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x11960c600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x11960cd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x11960d5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x11960dce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x11960dfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x11960e260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x11960e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x11960ed80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x11960f1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x11960f660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x11960fbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x119610060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x119610320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x119610790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x119611040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x119611300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x119611770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x119611be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x119612050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x1196124c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x119612930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x119612da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x119613210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x119613680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x119613af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x119613f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x119614990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x119614c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x1196150c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x119615530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x1196159a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x119615e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x119616280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x1196166f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x119616da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x119617240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x119617500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x119617970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x119618040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x119618440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x119618700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x119618c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x119619100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x119619600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x119619b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x11961a000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x11961a500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x11961aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x11961af00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x11961b400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x11961b900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x11961be00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x11961c300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x11961c8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x11961ce60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x11961d410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x11961d9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x11961df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x11961e520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x11961ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x11961f080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x11961f630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x11961fbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x119620190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x119620740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x119620cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x1196212a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x119621850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x119621e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x1196223b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x119622960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x119622f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x1196234c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x119623a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x119624020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x1196245d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x119614580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x119624d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x1196251a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x119625610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x119625bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x119626170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x119626720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x119626cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x119627280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x119627830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x119627de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x119628390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x119628940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x119628ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x1196294a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x119629a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x11962a000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x11962a500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x11962aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x11962af00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x11962b400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x11962b900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x11962be00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x11962c300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x11962c800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x11962cd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x11962d200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x11962d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x11962dc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x11962e100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x11962e600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x11962eb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x11962f000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x11962f500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x11962fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x11962ff00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x119630400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x119630900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x119630e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x119631300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x119631800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x119631d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x119632200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x119632700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x119632c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x119633100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x119633600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x119633b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x119634000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x119634500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x119634a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x119634f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x119635400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x119635900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x119635e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x119636300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x119636800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x119636d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x119637200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x119637700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x119637c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x119638100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x119638600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x119638b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x119639000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x119639500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x119639a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x119639f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x11963a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x11963a900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x11963ae00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x11963b300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x11963b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x11963bd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x11963c200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x11963c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x11963cc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x11963d100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x11963d600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x11963db00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x11963e000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x11963e500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x11963ea00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x11963ef00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x11963f400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x11963f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x11963fe00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x119640300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x119640800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x119640d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x119641200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x119641700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x119641c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x119642100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x119642600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x119642b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x119643000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x1196435b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x119643b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x119644110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x1196446c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x119644cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x1196452e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x1196458f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x1196460e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x119646580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x119646840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x119646e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x119647460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x119647c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x1196480f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x119648590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x119648a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x1196491e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x119649730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x119649c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x11964a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x11964a720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x11964ac70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x11964b1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x11964b710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x11964bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x11964c1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x11964c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x11964cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x11964d1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x11964d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x11964dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x11964e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x11964e6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x11964ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x11964f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x11964f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x11964fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x119650170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x1196506c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x119650c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x119651160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x1196516b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x119651c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x119652150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x1196526a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x119652bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x119653140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x119653690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x119653be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x119654130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x119654680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x119654bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x119655120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x119655670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x119655bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x119656110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x119656660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x119656bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x119657100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x119657650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x119657ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x1196580f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x119658640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x119658b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x1196590e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x119659630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x119659b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x11965a0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x11965a620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x11965ab70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x11965b0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x11965b610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x11965bb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x11965c000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x11965c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x11965c940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x11965cde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x11965d280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x11965d720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x11965dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x11965e060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x11965e500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x11965e9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x11965ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x11965f2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x11965f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x11965fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x1196600c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x119660610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x119660d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x119661450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x119661b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x119662290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x119662550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x119662d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x119663000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x119663610 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.724.108 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.724.110 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13970bca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13970c110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13970c580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13970c9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13970ce60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13970d2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13970d740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13970dbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13970e020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13970e490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13970e900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13970efc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13970fae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x139710290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x139710aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x1397111c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x1397118e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x139712000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x139712720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x139712ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x139713610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x139713d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x139714450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x139714b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x139715290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x139715550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x139715810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x139715c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x1397160f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x139716560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x139716a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x139716f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x1397173e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x1397176a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x139717b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x139717f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x1397184e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x1397189e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x139718ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x1397193e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x1397198e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x139719de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13971a2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13971a7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13971ace0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13971b150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13971b5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13971ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13971bea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13971c310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13971c780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13971cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13971d060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13971d4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13971d940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13971e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13971e5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13971e870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13971ee80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13971f670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13971fb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13971ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x139720450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x1397208f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x139720d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x139721230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x1397216d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x139721b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x139722010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x1397224b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x139722950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x139722df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x139723290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x1397237e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x139723d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x139724280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x1397247d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x139724d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x139725270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x1397257c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x139725d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x139726260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x1397267b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x139726d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x139727250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x1397277a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x139727cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x139728240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x139728790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x139728ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x139729230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x139729780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x139729cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13972a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13972a770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13972acc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13972b210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13972b760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13972bcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13972c200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13972c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13972cca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13972d1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13972d740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13972dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13972e1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13972e730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13972ec80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13972f1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13972f720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13972fc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x1397301c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x139730710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x139730bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x139731050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x1397314f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x139731990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x139731e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x1397322d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x139732770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x139732c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x1397330b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x139733550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x1397339f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x139733e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x139734330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x1397347d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x139734c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x139735110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x1397355b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x139735a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x139735ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x139736390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x139736830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x139736cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x139737170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x139737610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x139737ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x139737f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x1397383f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x139738890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x139738d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x1397391d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x139739670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x139739b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x139739fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13973a450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13973a8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13973ad90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13973b230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13973b6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13973bb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13973c010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13973c4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13973c950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13973cdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13973d290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13973d730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13973dbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13973e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13973e510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13973e9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13973ee50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13973f2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13973f790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13973fc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x1397400d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x139740570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x139740a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x139740eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x139741350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x1397417f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x139741c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x139742130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x1397425d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x139742a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x139742f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x1397433b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x139743850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x139743cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x139744190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x139744630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x139744ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x139744f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x139745410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x1397458b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x139745d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x1397461f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x139746690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x139746b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x139746fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x139747470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x139747910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x139747e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x1397483b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x139748900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x139748e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x139749110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x139749720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x139749d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13974a340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13974ab30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13974afd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13974b290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13974b8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13974beb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13974c6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13974cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13974cfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13974d480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13974dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13974e180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13974e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13974ec20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13974f170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13974f6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13974fc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x139750160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x1397506b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x139750c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x139751150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x1397516a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x139751bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x139752140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x139752690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x139752be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x139753130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x139753680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x139753bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x139754120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x139754670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x139754bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x139755110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x139755660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x139755bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x139756100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x139756650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x139756ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x1397570f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x139757640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x139757b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x1397580e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x139758630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x139758b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x1397590d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x139759620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x139759b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13975a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13975a610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13975ab60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13975b0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13975b600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13975bb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13975c0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13975c5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13975cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13975d090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13975d5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13975db30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13975e080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13975e5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13975eb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13975f070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13975f5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13975fb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x139760060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x1397605b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x139760a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x139760ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x139761390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x139761830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x139761cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x139762170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x139762610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x139762ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x139762f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x1397633f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x139763890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x139763d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x1397641d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x139764670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x139764b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x139765060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x139765780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x139765ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x1397665c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x139766ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x139766fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x139767790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x139767a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x139768060 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x1396044e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x139604950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x139604dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x139605230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x1396056a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x139605b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x139605f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x1396063f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x139606860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x139606cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x139607140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x1396077b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x1396082d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x139608a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x139609290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x1396099b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13960a0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13960a7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13960af10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13960b6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13960be00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13960c520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13960cc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13960d360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13960da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13960dd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13960e000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13960e470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13960e8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13960ed50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13960f1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13960f6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13960fb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13960fe20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x139610290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x139610700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x139610b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x139610fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x139611450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x1396118c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x139611d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x1396121a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x139612610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x139612a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x139612ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x139613360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x1396137d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x139613c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x1396140b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x139614520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x139614990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x139614e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x139615270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x1396156e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x139615b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x139615fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x139616530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x139616a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x139616ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x139617310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x139617780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x139617bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x139618060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x1396184d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x139618940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x139618db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x139619220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x139619690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x139619b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x139619f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13961a3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13961a850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13961acc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13961b130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13961b5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13961ba10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13961be80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13961c2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13961c760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13961cbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13961d040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13961d4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13961d920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13961dd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13961e200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13961e670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13961eae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13961ef50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13961f3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13961f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13961fca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x139620110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x139620580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x1396209f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x139620e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x1396212d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x139621740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x139621bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x139622020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x139622490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x139622900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x139622d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x1396231e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x139623a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x139623d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x1396241a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x139624610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x139624a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x139624ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x139625360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x1396257d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x139625c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x1396260b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x139626520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x139626990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x139626e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x139627270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x1396276e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x139627b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x139627fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x139628430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x1396288a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x139628d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x139629180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x1396295f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x139629a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x139629ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13962a340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13962a7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13962ac20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13962b090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13962b500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13962b970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13962bde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13962c250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13962c6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13962cb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13962cfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13962d410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13962d880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13962dcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13962e160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13962e5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13962ea40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13962eeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13962f320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13962f790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13962fc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x139630070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x1396304e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x139630950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x139630dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x139631230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x1396316a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x139631b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x139631f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x1396323f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x139632860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x139632cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x139633140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x1396335b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x139633a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x139633e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x139634300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x139634770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x139634be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x139635050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x1396354c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x139635930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x139635da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x139636210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x139636680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x139636af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x139636f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x1396373d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x139637840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x139637cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x139638120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x139638590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x139638a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x139638e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x1396392e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x139639750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x139639bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13963a030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13963a4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13963a910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13963ad80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13963b1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13963b660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13963bad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13963bf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13963c3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13963c820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13963cc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13963d100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13963d570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13963d9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13963de50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13963e2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13963e730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13963eba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13963f010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13963f480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13963f8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13963fd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x1396401d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x139640640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x139640ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x139640f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x139641aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x139641d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x139642020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x139642490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x139642900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x139642d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x1396431e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x139643650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x139643ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x139643f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x1396443a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x139644810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x139644c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x1396450f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x139645560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x1396459d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x139645e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x1396462b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x139646720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x139646b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x139647000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x139647470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x1396478e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x139647d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x1396481c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x139648630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x139648aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x139648f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x139649380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x1396497f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x139649c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13964a0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13964a540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13964a9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13964ae20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13964b290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13964b700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13964bb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13964bfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13964c450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13964c8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13964cd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13964d1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13964d610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13964da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13964def0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13964e360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13964e7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13964ec40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13964f0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13964f520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13964f990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13964fe00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x139650270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x1396506e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x139650b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x139650fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x139651430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x1396518a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x139651d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x139652180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x1396525f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x139652a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x139652ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x139653340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1396537b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x139653c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x139654090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x139654500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x139654970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x139654de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x139655250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x1396556c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x139656130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x139656850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x139656f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x139657690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x139657950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x139657dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1396583c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x1396589d0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.809s
user	0m0.281s
sys	0m0.325s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4586 (2711d021)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 'Ä'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14560c8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14560cfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14560d580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14560db30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14560e0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14560e690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14560ec40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14560f1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14560f7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14560fca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x1456101a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x1456106a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x1456111c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x145611970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x145612180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x1456128a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x145612fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x1456136e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x145613e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x1456145d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x145614cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x145615410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x145615b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x1456163d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x145616af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x145616db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x1456173c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x145618030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x145618570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x145618830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x145618cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x145618f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x145619820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x145619d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14561a020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14561a4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14561a960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14561ae00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14561b2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14561b740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14561bbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14561c080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14561c520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14561c9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14561cc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14561d290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14561d8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14561e1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14561e7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14561ede0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14561f3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14561fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x145620010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x145620620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x145620e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x1456212b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x145621750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x145621a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x145622020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x145622810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x145622ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x145622f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x145623410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x1456238b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x145623d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x1456241f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x145624690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x145624b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x145624fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x145625470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x145625910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x145625db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x145626250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x1456267a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x145626cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x145627240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x145627790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x145627ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x145628230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x145628780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x145628cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x145629220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x145629770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x145629cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x14562a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x14562a760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x14562acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x14562b200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x14562b750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x14562bca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x14562c1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x14562c740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x14562cc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x14562d1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x14562d730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x14562dc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x14562e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x14561deb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x14562e640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x14562edf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x14562f340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x14562f890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x14562fde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x145630330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x145630880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x145630dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x145631320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x145631870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x145631dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x145632310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x145632860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x145632db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x145633300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x1456337a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x145633c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x1456340e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x145634580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x145634a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x145634ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x145635360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x145635800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x145635ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x145636140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x1456365e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x145636a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x145636f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x1456373c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x145637860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x145637d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x1456381a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x145638640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x145638ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x145638f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x145639420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x1456398c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x145639d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14563a200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14563a6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14563ab40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14563afe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14563b480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14563b920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14563bdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14563c260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14563c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14563cba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14563d040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14563d4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14563d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14563de20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14563e2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14563e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14563ec00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14563f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14563f540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14563f9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14563fe80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x145640320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x1456407c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x145640c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x145641100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x1456415a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x145641a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x145641ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x145642380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x145642820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x145642cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x145643160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x145643600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x145643aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x145643f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x1456443e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x145644880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x145644d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x1456451c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x145645660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x145645b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x145645fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x145646440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x1456468e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x145646d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x145647220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x1456476c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x145647b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x145648000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x1456484a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x145648940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x145648de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x145649280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x145649720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x145649bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14564a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14564a500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14564aa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14564afa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14564b4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14564ba40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14564bd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14564c310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14564c920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14564cf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14564d720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14564dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14564de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14564e490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x14564eaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14564f290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14564f730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14564fbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x145650070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x145650820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x145650d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x1456512c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x145651810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x145651d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x1456522b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x145652800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x145652d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x1456532a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x1456537f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x145653d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x145654290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x1456547e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x145654d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x145655280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x1456557d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x145655d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x145656270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x1456567c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x145656d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x145657260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x1456577b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x145657d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x145658250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x1456587a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x145658cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x145659240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x145659790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x145659ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14565a230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14565a780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14565acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14565b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14565b770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14565bcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14565c210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14565c760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14565ccb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14565d200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14565d750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14565dca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14565e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14565e740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14565ec90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14565f1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14565f730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14565fc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x1456601d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x145660720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x145660c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x1456611c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x145661710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x145661c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x1456621b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x145662700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x145662c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x1456631a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x145663640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x145663ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x145663f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x145664420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x1456648c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x145664d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x145665200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x1456656a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x145665b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x145665fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x145666480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x145666920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x145666dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x145667260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x145667700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x145667c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x145668370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x145668a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x1456691b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1456698d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x145669b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x14566a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14566a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14566ac50 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.097.396 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.097.400 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x133504bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x133505040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x1335054b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x133505920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x133505d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x133506200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x133506670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x133506ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x133506f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x1335073c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x133507830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x133507f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x133508a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x1335091f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x133509a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13350a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13350a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13350af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13350b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13350bdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13350c4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13350cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13350d310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13350da30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13350e150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13350e410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13350e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13350eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13350efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13350f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13350f890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13350fdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x133510230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x1335104f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x133510960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x133510dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x133511240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x1335116b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x133511b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x133511f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x133512400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x133512870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x133512ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x133513150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x1335135c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x133513a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x133513ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x133514310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x133514780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x133514bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x133515060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x1335154d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x133515940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x133515db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x133516220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x133516690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x133516c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x133517100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x133517570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x1335179e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x133517e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x1335182c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x133518730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x133518ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x133519010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x133519480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x1335198f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x133519d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13351a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13351a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13351aab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13351af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13351b390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13351b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13351bc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13351c0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13351c550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13351c9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13351ce30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13351d2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13351d710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13351db80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13351dff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13351e460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13351e8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13351ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13351f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13351f620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13351fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13351ff00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x133520370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x1335207e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x133520c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x1335210c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x133521530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x1335219a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x133521e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x133522280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x1335226f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x133522b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x133522fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x133523440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x1335238b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x133523d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x133524190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x133524600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x133524a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x133524ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x133525350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x1335257c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x133525c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x1335260a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x133526510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x133526980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x133526df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x133527260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x1335276d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x133527b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x133527fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x133528420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x133528890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x133528d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x133529170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x1335295e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x133529a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x133529ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13352a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13352a7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13352ac10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13352b080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13352b4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13352b960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13352bdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13352c240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13352c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13352cb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13352cf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13352d400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13352d870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13352dce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13352e150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13352e5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13352ea30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13352eea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13352f310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13352f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13352fbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x133530060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x1335304d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x133530940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x133530db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x133531220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x133531690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x133531b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x133531f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x1335323e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x133532850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x133532cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x133533130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x1335335a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x133533a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x133533e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x1335342f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x133534760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x133534bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x133535040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x133535c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x133535f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x1335361f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x133536660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x133536ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x133536f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x1335373b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x133537820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x133537c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x133538100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x133538570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x1335389e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x133538e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x1335392c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x133539730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x133539ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13353a010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13353a480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13353a8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13353ad60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13353b1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13353b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13353bab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13353bf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13353c390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13353c800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13353cc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13353d0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13353d550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13353d9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13353de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13353e2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13353e710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13353eb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13353eff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13353f460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13353f9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13353fed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x133540340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x1335407b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x133540c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x133541090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x1335415b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x133541ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x133542630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x1335428f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x133542eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x133543470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x133543a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x133543ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x1335445b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x133544b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x133545130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x1335456f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x133545cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x133546270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x133546830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x133546df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x1335473b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x133547970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x133547f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x1335484f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x133548ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x133549070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x133549630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x133549bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13354a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13354a770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13354ad30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13354b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13354b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13354be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13354c430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13354c9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13354cfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13354d570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13354db30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13354e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13354e6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13354ec70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13354f230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13354f7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13354fdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x133550370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x133550930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x133550ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x1335514b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x133551a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x133552030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x1335525f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x133552bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x133553170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x133553730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x133553cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x1335542b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x133554870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x133554e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x1335553f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x1335559b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x133555f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x133556530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x133556af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x133556ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x1335574f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x1335579f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x133557ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x1335583f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x1335588f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x133558df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1335592f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x1335597f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x133559cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13355a1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13355a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13355abf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13355b0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13355b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13355c000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13355c720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13355ce40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13355d560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13355d820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13355e010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13355e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13355e8e0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14566a900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14564c5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14564bfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14564cbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14561fcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14561f6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x145621cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14564e750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x145617070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14561db60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14561e480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14561ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14561cf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14561f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x145616070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14560bef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x1456208e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x1456222e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14562e900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x145669e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x145619250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x145619510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14564ed60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14564d1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x145617680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x145617940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x145617c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14566b0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14566b370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14566b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14566b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14566bbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14566be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14566c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14566c3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14566c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14566c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14566cc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14566cef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14566d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14566d470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14566d730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14566d9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14566dcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14566df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14566e230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14566e4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14566e7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14566ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14566ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14566eff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14566f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14566f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14566f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14566faf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14566fdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x145670070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x145670330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x1456705f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x1456708b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x145670b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x145670e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x1456710f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x1456713b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x145671670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x145671930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x145671bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x145671eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x145672170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x145672430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x1456726f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x1456729b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x145672c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x145672f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x1456731f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x1456734b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x145673770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x145673a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x145673cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x145673fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x145674270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x145674530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x1456747f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x145674ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x145674d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x145675030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x1456752f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x1456755b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x145675870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x145675b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x145675df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x1456760b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x145676370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x145676630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x1456768f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x145676bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x145676e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x145677130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x1456773f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x1456776b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x145677970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x145677c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x145677ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x1456781b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x145678470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x145678730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x1456789f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x145678cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x145678f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x145679230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x1456794f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x1456797b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x145679a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x145679d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x145679ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14567a2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14567a570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14567a830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14567aaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14567adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14567b070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14567b330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14567b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14567b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14567bb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14567be30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14567c0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14567c3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14567c670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14567c930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14567cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14567ceb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14567d170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14567d430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14567d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14567d9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14567dc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14567df30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14567e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14567e4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14567e770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14567ea30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14567ecf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14567efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14567f270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14567f530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14567f7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14567fab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14567fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x145680030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x1456802f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x1456805b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x145680870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x145680b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x145680df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x1456810b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x145681370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x145681630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x1456818f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x145681bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x145681e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x145682130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x1456823f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x1456826b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x145682970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x145682c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x145682ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x1456831b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x145683470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x145683730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x1456839f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x145683cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x145683f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x145684230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x1456844f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x1456847b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x145684a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x145684d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x145684ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x1456852b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x145685570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x145685830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x145685af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x145685db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x145686070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x145686330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x1456865f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x1456868b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x145686b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x145686e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x1456870f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x1456873b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x145687670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x145687930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x145687bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x145687eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x145688170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x145688430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x1456886f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x1456889b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x145688c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x145688f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x1456891f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x1456894b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x145689770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x145689a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x145689cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x145689fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14568a270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14568a530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14568ab00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14568adc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14568b080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14568b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14568b600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14568b8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14568bb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14568be40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14568c100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14568c3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14568c680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14568c940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14568cc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14568cec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14568d180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14568d440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14568d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14568d9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14568dc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14568df40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14568e200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14568e4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14568e780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14568ea40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14568ed00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14568efc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14568f280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14568f540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14568f800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14568fac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14568fd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x145690040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x145690300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x1456905c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x145690880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x145690b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x145691090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x1456915e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x145691b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x145692080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x1456925d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x145692b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x145693070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x1456935c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x145693b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x145694060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x1456945b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x145694b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x145695050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x1456955a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x145695af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x145696040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x145696590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x145696ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x145697030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x145697580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x145697ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x145697d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x145698050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x145698550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x145698a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x145698f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x145699450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x145699950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x145699e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14569a350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14569a850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14569ad50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14569b250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14569b750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14569bc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14569c150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14569c650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14569d060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14569d780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14569dea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14569e5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14569e880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x14569f070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14569f330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14569f940 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.948s
user	0m0.235s
sys	0m0.176s
```
### ctest_with_model_debug

Runs ctest with model files in debug mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-debug
    Start 25: test-model-load-cancel
1/2 Test #25: test-model-load-cancel ...........   Passed    0.43 sec
    Start 26: test-autorelease
2/2 Test #26: test-autorelease .................   Passed    1.58 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   2.02 sec*proc (2 tests)

Total Test time (real) =   2.03 sec
        2.05 real         0.51 user         0.25 sys
```
### ctest_with_model_release

Runs ctest with model files in release mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-release
    Start 25: test-model-load-cancel
1/2 Test #25: test-model-load-cancel ...........   Passed    0.32 sec
    Start 26: test-autorelease
2/2 Test #26: test-autorelease .................   Passed    0.31 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   0.63 sec*proc (2 tests)

Total Test time (real) =   0.65 sec
        0.65 real         0.14 user         0.09 sys
```
